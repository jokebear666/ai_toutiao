<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_LG/20251215-20251221" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251215-20251221 (cs.LG) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/cs_LG/20251215-20251221"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251215-20251221 (cs.LG) | AI头条"><meta data-rh="true" name="description" content="2025-12-18"><meta data-rh="true" property="og:description" content="2025-12-18"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/cs_LG/20251215-20251221"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cs_LG/20251215-20251221" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cs_LG/20251215-20251221" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.LG","item":"https://jokebear666.github.io/ai_toutiao/daily/cslg"},{"@type":"ListItem","position":3,"name":"20251215-20251221 (cs.LG)","item":"https://jokebear666.github.io/ai_toutiao/daily/cs_LG/20251215-20251221"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.bc6dbd00.css">
<script src="/ai_toutiao/assets/js/runtime~main.a48d6d4f.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.becdf5a1.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/daily">Daily</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/paper">Paper</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Collapse sidebar category &#x27;cs.LG&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/cs_LG/20251215-20251221"><span title="20251215-20251221 (cs.LG)" class="linkLabel_WmDU">20251215-20251221 (cs.LG)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cslg/20251222-20251228"><span title="20251222-20251228 (cs.LG)" class="linkLabel_WmDU">20251222-20251228 (cs.LG)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/daily/cslg"><span>cs.LG</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251215-20251221 (cs.LG)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251215-20251221 (cs.LG)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-18">2025-12-18<a href="#2025-12-18" class="hash-link" aria-label="Direct link to 2025-12-18" title="Direct link to 2025-12-18" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251218] Autonomous Source Knowledge Selection in Multi-Domain Adaptation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [domain adaptation], [multi-domain adaptation, density-driven selection, pseudo-label enhancement, feature alignment, self-supervision]</li>
<li class=""><strong>authors:</strong> Keqiuyin Li, Jie Lu, Hua Zuo, Guangquan Zhang</li>
<li class=""><strong>institution:</strong> University of Technology Sydney</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14710" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14710</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes AutoS, a method for unsupervised multi-domain adaptation that autonomously selects relevant source samples and models via a density-driven strategy and uses a pre-trained multimodal model to enhance pseudo-labels for self-supervision. Experiments show the method effectively improves transfer performance by focusing on the most transferable knowledge from multiple source domains.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [Latent Class Reinforcement Learning (LCRL), Variational Bayes, Bayesian estimation, discrete choice models]</li>
<li class=""><strong>authors:</strong> Georges Sfeir, Stephane Hess, Thomas O. Hancock, Filipe Rodrigues, Jamal Amani Rad, Michiel Bliemer, Matthew Beck, Fayyaz Khan</li>
<li class=""><strong>institution:</strong> University of Leeds, Technical University of Denmark, University of Sydney, Al Yamamah University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14713" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14713</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Latent Class Reinforcement Learning (LCRL) model, estimated using Variational Bayes, to capture how travelers adapt their preferences through experience. The application to a driving simulator dataset reveals three distinct behavioral classes, showing heterogeneity in how individuals balance exploration and exploitation in their travel decisions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SGEMAS: A Self-Growing Ephemeral Multi-Agent System for Unsupervised Online Anomaly Detection via Entropic Homeostasis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-agent system, structural plasticity, variational free energy, metabolic lagrangian, stochastic thermodynamics, unsupervised anomaly detection]</li>
<li class=""><strong>authors:</strong> Mustapha Hamdi</li>
<li class=""><strong>institution:</strong> InnoDeep</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14708" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14708</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SGEMAS, a bio-inspired multi-agent system that uses agent birth/death and a variational free energy objective to achieve energy-efficient, unsupervised anomaly detection in physiological signals. It validates the approach on the MIT-BIH Arrhythmia Database, showing that this physics-based, energy-constrained model can detect anomalies in a zero-shot setting, outperforming a standard autoencoder baseline.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] The Graph-Embedded Hazard Model (GEHM): Stochastic Network Survival Dynamics on Economic Graphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [network economics], [graph p-Laplacian, accretive operator theory, nonlinear semigroup methods, stochastic analysis, PDE-SDE systems]</li>
<li class=""><strong>authors:</strong> Diego Vallarino</li>
<li class=""><strong>institution:</strong> Inter-American Development Bank (IDB)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14705" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14705</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Graph-Embedded Hazard Model (GEHM), a nonlinear PDE-SDE framework that couples graph-based p-Laplacian diffusion with stochastic drift to model survival dynamics on economic networks. The main conclusion is that hub dominance in networks like Barabási–Albert magnifies nonlinear gradients, compresses stability margins, and leads to heavy-tailed survival distributions with occasional explosive behavior.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [learnable Gabor filters, ResNeXt, squeeze-and-excitation attention, spectrograms, deep learning]</li>
<li class=""><strong>authors:</strong> Lucas Cesar Ferreira Domingos, Russell Brinkworth, Paulo Eduardo Santos, Karl Sammut</li>
<li class=""><strong>institution:</strong> Flinders University, PrioriAnalytica</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14714" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14714</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes GSE ResNeXt, a deep learning model that integrates learnable Gabor filter convolutions with a ResNeXt backbone and squeeze-and-excitation attention mechanisms for underwater acoustic target classification. The model demonstrates improved classification accuracy and a 28% reduction in training time compared to baseline models, highlighting the effectiveness of combining adaptive signal processing with attention for better generalization in data-limited scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [neural architecture search, large language models, image captioning, prompt engineering, CNN encoder, LSTM, GRU, Transformer, BLEU-4]</li>
<li class=""><strong>authors:</strong> Krunal Jesani, Dmitry Ignatov, Radu Timofte</li>
<li class=""><strong>institution:</strong> University of Würzburg</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14706" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14706</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents NN-Caption, a pipeline that uses a large language model (LLM) to automatically generate runnable image-captioning model architectures by composing CNN encoders and sequence decoders under a strict API. The method successfully produced dozens of models, with over half training successfully, demonstrating the promise of LLM-guided neural architecture search while highlighting challenges like code hallucinations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [neural-symbolic AI], [Vector Symbolic Architecture, self-attention, binding, unbinding, residual streams, hyperdimensional computing, chain-of-thought]</li>
<li class=""><strong>authors:</strong> Sahil Rajesh Dhayalkar</li>
<li class=""><strong>institution:</strong> Arizona State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14709" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14709</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper interprets transformer self-attention and residual streams as implementing an approximate Vector Symbolic Architecture (VSA), where attention performs soft binding/unbinding of roles and fillers. It uses this perspective to explain the models&#x27; reasoning capabilities and brittleness, and proposes VSA-inspired architectural modifications to improve logical reliability. The core conclusion is that viewing attention as soft vector-symbolic computation offers a principled route toward more interpretable and robust reasoning systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [Mixture-of-Experts (MoE), Hierarchical Gated Attention Network, CatBoost meta-learner, multimodal fusion, deep fusion, expert stacking, Quad-Modal Ensemble]</li>
<li class=""><strong>authors:</strong> Ryan Cartularo</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14712" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14712</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares two multimodal AI architectures for sepsis prediction and antibiotic selection: a complex end-to-end deep fusion model (SepsisFusionFormer) and a leaner, context-aware Mixture-of-Experts stacking model (SepsisLateFusion). The main conclusion is that for this high-stakes, data-sparse clinical domain, the interpretable expert stacking approach, which treats modalities as orthogonal experts and uses a CatBoost meta-learner, significantly outperformed the deep fusion model, achieving state-of-the-art predictive performance and enabling a prescriptive window for intervention.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Is GPT-OSS All You Need? Benchmarking Large Language Models for Financial Intelligence and the Surprising Efficiency Paradox</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [benchmark evaluation, token efficiency score, model efficiency, performance metrics, GPT-OSS]</li>
<li class=""><strong>authors:</strong> Ziqian Bi, Danyang Zhang, Junhao Song, Chiung-Yi Tseng</li>
<li class=""><strong>institution:</strong> Purdue University, Imperial College London, Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14717" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14717</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper benchmarks large language models, specifically the GPT-OSS family, across ten financial NLP tasks. It finds that the smaller GPT-OSS-20B model achieves comparable accuracy to its larger counterpart while demonstrating superior computational efficiency, challenging the assumption that model scale directly correlates with performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [bit-level fault injection, gradient-based sensitivity estimation, differentiable fault analysis, semantic steering, SBERT embeddings, perplexity scoring]</li>
<li class=""><strong>authors:</strong> Zafaryab Haider, Md Hafizur Rahman, Shane Moeykens, Vijay Devabhaktuni, Prabuddha Chakraborty</li>
<li class=""><strong>institution:</strong> University of Maine, Illinois State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14715" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14715</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces BLADE, a differentiable fault analysis framework that uses gradient-based sensitivity estimation to identify and flip specific bits in a quantized vision-language model&#x27;s weights, steering its generated captions to change semantic meaning while preserving grammatical fluency. It concludes that semantic drift from low-level bit flips is predictable and controllable, revealing vulnerabilities in generative AI systems and opening pathways for robustness testing and adversarial defense.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Automatic Extraction of Rules for Generating Synthetic Patient Data From Real-World Population Data Using Glioblastoma as an Example</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical data synthesis], [Synthea, rule-based generation, statistical rules, glioblastoma, privacy-preserving, synthetic patient data]</li>
<li class=""><strong>authors:</strong> Arno Appenzeller, Nick Terzer, André Hohmeyer, Jan-Philipp Redlich, Sabine Luttmann, Friedrich Feuerhake, Nadine S. Schaadt, Timm Intemann, Sarah Teuber-Hanselmann, Stefan Nikolin, Joachim Weis, Klaus Kraywinkel, Pascal Birnstill</li>
<li class=""><strong>institution:</strong> Fraunhofer Institute of Optronics, System Technologies and Image Exploitation IOSB</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14721" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14721</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces an automated method to generate Synthea rules for creating synthetic patient data from real-world tabular statistics, using glioblastoma as an example. The synthetic data reproduced known disease courses and largely retained the statistical properties of the original dataset. This demonstrates the potential of synthetic data for privacy-preserving medical research, though with noted limitations for clinical interpretation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Hybrid Attribution Priors for Explainable and Robust Model Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [explainable ai], [class-aware attribution prior, explanation-guided learning, attribution priors, small language models, model interpretability]</li>
<li class=""><strong>authors:</strong> Zhuoran Zhang, Feng Zhang, Shangyuan Li, Yang Shi, Yuanxing Zhang, Wei Chen, Tengjiao Wang, Kam-Fai Wong</li>
<li class=""><strong>institution:</strong> Peking University, Kling Team, CUHK</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14719" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14719</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Class-Aware Attribution Prior (CAP) framework and its hybrid variant, CAP Hybrid, to generate more discriminative attribution priors for training small language models. By aligning a model&#x27;s self-attribution with these enriched priors, the method encourages the learning of diverse, decision-relevant features. Experiments show the approach consistently enhances both model interpretability and robustness across various data settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SEED: Spectral Entropy-Guided Evaluation of SpatialTemporal Dependencies for Multivariate Time Series Forecasting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [time series forecasting], [spectral entropy, dependency evaluator, signed graph constructor, context spatial extractor, channel independence, channel dependence]</li>
<li class=""><strong>authors:</strong> Feng Xiong, Zongxia Xie, Yanru Sun, Haoyu Wang, Jianhong Lin</li>
<li class=""><strong>institution:</strong> Tianjin University, Fudan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14718" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14718</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes SEED, a framework for multivariate time series forecasting that uses spectral entropy to evaluate and model spatial-temporal dependencies. It introduces components like a Dependency Evaluator and a Signed Graph Constructor to adaptively balance modeling strategies and preserve negative correlations. Experiments on 12 datasets show SEED achieves state-of-the-art performance, demonstrating its effectiveness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Generative Urban Flow Modeling: From Geometry to Airflow with Graph Diffusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [graph neural network, diffusion model, generative modeling, unstructured meshes, urban flow simulation, score-based diffusion]</li>
<li class=""><strong>authors:</strong> Francisco Giral, Álvaro Manzano, Ignacio Gómez, Petros Koumoutsakos, Soledad Le Clainche</li>
<li class=""><strong>institution:</strong> Universidad Politécnica de Madrid, Harvard University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14725" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14725</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a generative diffusion framework that combines a hierarchical graph neural network with score-based diffusion modeling to synthesize steady-state urban wind fields from geometry data on unstructured meshes. The model generalizes to unseen geometries, accurately capturing key flow structures like wakes and recirculation zones, and provides uncertainty-aware predictions. This work represents a step towards foundation models for the built environment, enabling rapid evaluation of urban design decisions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] HATSolver: Learning Groebner Bases with Hierarchical Attention Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hierarchical attention transformers, curriculum learning, groebner bases, polynomial systems, computational cost analysis]</li>
<li class=""><strong>authors:</strong> Mohamed Malhou, Ludovic Perret, Kristin Lauter</li>
<li class=""><strong>institution:</strong> Meta Superintelligence Labs (FAIR), Sorbonne Université, CNRS, LIP6, EPITA, EPITA Research Lab (LRE)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14722" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14722</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces HATSolver, a method that improves upon prior transformer-based approaches by using Hierarchical Attention Transformers (HATs) to compute Gröbner bases for solving systems of multivariate polynomial equations. The HAT architecture incorporates a tree-structured inductive bias to model hierarchical data relationships, achieving significant computational savings over flat attention models. Combined with curriculum learning, the method can solve much larger problem instances than previous work.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Quantum Decision Transformers (QDT): Synergistic Entanglement and Interference for Offline Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [Quantum Decision Transformer, Quantum-Inspired Attention, Quantum Feedforward Networks, entanglement, interference, offline reinforcement learning, Decision Transformer]</li>
<li class=""><strong>authors:</strong> Abraham Itzhak Weinberg</li>
<li class=""><strong>institution:</strong> AI-WEINBERG, AI Experts</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14726" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14726</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Quantum Decision Transformer (QDT), a novel architecture that integrates quantum-inspired attention with entanglement and quantum feedforward networks with interference to improve offline reinforcement learning. It demonstrates a dramatic performance improvement over standard Decision Transformers and shows that the synergy between its quantum-inspired components is crucial for its success.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Critical Perspective on Finite Sample Conformal Prediction Theory in Medical Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical machine learning], [conformal prediction, uncertainty quantification, finite sample theory, calibration]</li>
<li class=""><strong>authors:</strong> Klaus-Rudolf Kladny, Bernhard Schölkopf, Lisa Koch, Christian F. Baumgartner, Michael Muehlebach</li>
<li class=""><strong>institution:</strong> Max Planck Institute for Intelligent Systems, University of Bern, University of Tübingen, University of Lucerne</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14727" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14727</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper critically examines conformal prediction, a method for providing statistically guaranteed uncertainty estimates from machine learning models using a calibration dataset. It argues that while the theoretical guarantees hold for any calibration set size, the practical utility of these guarantees is highly dependent on having a sufficiently large calibration sample. This critique is particularly relevant for medical applications where data is often scarce, and the authors support their argument with an empirical demonstration on a medical image classification task.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A data-driven approach to inferring travel trajectory during peak hours in urban rail transit systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [KLEM, KL divergence, EM algorithm, Automatic Fare Collection, Automatic Vehicle Location, data-driven parameter estimation]</li>
<li class=""><strong>authors:</strong> Jie He, Yong Qin, Jianyuan Guo, Xuan Sun, Xuanchuan Zheng</li>
<li class=""><strong>institution:</strong> Beijing Jiaotong University, China Railway Signaling and Communication Research and Design Institute, Beijing Urban Construction Design &amp; Development Group Co.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14728" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14728</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a fully data-driven method for inferring individual travel trajectories in urban rail transit systems by combining AFC and AVL data, using a novel parameter estimation method (KLEM) based on KL divergence and the EM algorithm. The approach achieves over 90% accuracy in trajectory inference during peak hours, validated with real-world data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Inference Time Feature Injection: A Lightweight Approach for Real-Time Recommendation Freshness</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [inference time feature injection, intra-day personalization, model-agnostic, batch-trained models, real-time recommendation]</li>
<li class=""><strong>authors:</strong> Qiang Chen, Venkatesh Ganapati Hegde, Hongfei Li</li>
<li class=""><strong>institution:</strong> Tubi</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14734" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14734</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a lightweight, model-agnostic method that injects a user&#x27;s recent watch history into a batch-trained recommender system at inference time, overriding stale features without model retraining. The approach achieved a statistically significant 0.47% increase in user engagement metrics, demonstrating that intra-day personalization can meaningfully improve recommendations for long-form video streaming.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Where to Explore: A Reach and Cost-Aware Approach for Unbiased Data Collection in Recommender Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [A/B testing, cost-aware optimization, scroll-depth analysis, unbiased data collection, dedicated exploration container]</li>
<li class=""><strong>authors:</strong> Qiang Chen, Venkatesh Ganapati Hegde</li>
<li class=""><strong>institution:</strong> Tubi</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14733" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14733</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a method for safe content exploration in recommender systems by strategically placing a &quot;Something Completely Different&quot; row of randomized content only in low-engagement, high-reach scroll-depth regions of the UI. This approach preserves key business metrics while collecting unbiased interaction data. The collected data, when used for downstream candidate generation, significantly improves user engagement.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [planner-executor framework, large language models (LLMs), vision-language models (VLMs), segmentation models, image processing]</li>
<li class=""><strong>authors:</strong> Idan Tankel, Nir Mazor, Rafi Brada, Christina LeBedis, Guy ben-Yosef</li>
<li class=""><strong>institution:</strong> GE Healthcare Technology and Innovation Center, Boston Medical Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14732" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14732</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a framework that uses a planner-executor approach, where an LLM planner generates Python scripts to automate the detection and reporting of incidental findings in abdominal CT scans, and an executor runs these scripts using VLMs and segmentation models. The method is fully automatic and end-to-end. The results show that this framework outperforms pure VLM-based approaches in accuracy and efficiency for managing incidental findings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Semantic Geometry for policy-constrained interpretation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [semantic interpretation], [geometric framework, spherical convex regions, constrained optimization, policy constraints, information theory, Bayesian inference, sheaf-theoretic semantics]</li>
<li class=""><strong>authors:</strong> Nikit Phadke</li>
<li class=""><strong>institution:</strong> Independent researcher (based on gmail address)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14731" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14731</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a geometric framework for semantic interpretation where meaning is represented as directions on a unit sphere and policy constraints are applied as explicit priors. This approach separates evidence processing from policy rules, enabling provable prevention of hallucinated commitments. Empirical validation on financial data shows zero hallucinated approvals, demonstrating the method&#x27;s effectiveness in high-stakes, policy-constrained domains.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] NoveltyRank: Estimating Conceptual Novelty of AI Papers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [natural language processing], [binary classification, pairwise comparison, semantic similarity, SPECTER2, fine-tuning, Qwen3-4B-Instruct-2507, SciBERT]</li>
<li class=""><strong>authors:</strong> Zhengxu Yan, Han Li, Yuming Feng</li>
<li class=""><strong>institution:</strong> Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14738" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14738</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes NoveltyRank, a model that estimates the conceptual novelty of AI papers using title, abstract, and semantic similarity to prior literature, evaluated through binary classification and pairwise comparison tasks. It fine-tunes Qwen3-4B-Instruct-2507 and SciBERT, benchmarking against GPT-5.1, and finds that task formulation and modeling choices significantly impact performance, with the implementation made publicly available.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Quantum-Augmented AI/ML for O-RAN: Hierarchical Threat Detection with Synergistic Intelligence and Interpretability (Technical Report)</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [quantum machine learning, hybrid quantum computing, amplitude encoding, entanglement encoding, hierarchical threat detection, anomaly detection, intrusion confirmation, multiattack classification, deep neural networks, ensemble classifiers]</li>
<li class=""><strong>authors:</strong> Tan Le, Van Le, Sachin Shetty</li>
<li class=""><strong>institution:</strong> Hampton University, Virginia Polytechnic Institute and State University, Old Dominion University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14742" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14742</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hierarchical cybersecurity framework for Open Radio Access Networks (O-RAN) that integrates hybrid quantum computing and machine learning. The method uses quantum-inspired feature encodings (amplitude- and entanglement-based) with deep and ensemble classifiers for anomaly detection, intrusion confirmation, and multiattack classification. The framework demonstrates near-perfect accuracy, high recall, and strong interpretability, indicating readiness for scalable deployment in O-RAN environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Compute the edge p-Laplacian centrality for air traffic network</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [graph theory, network analysis], [p-Laplacian centrality, line graph, un-normalized graph Laplacian, node centrality, edge centrality]</li>
<li class=""><strong>authors:</strong> Loc Hoang Tran, Bao Nguyen Tran, Luong Anh Tuan Nguyen</li>
<li class=""><strong>institution:</strong> Vietnam Aviation Academy</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14749" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14749</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method to compute edge p-Laplacian centrality for an air traffic network by first converting the network into a line graph and then computing the node p-Laplacian centrality on that line graph. The method is based on a novel un-normalized graph p-Laplacian operator. The experimental results show that this ranking method can be successfully implemented.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] CAPE: Capability Achievement via Policy Execution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [capability engineering, policy execution, specification language, verification, DPO, contextual objectivity, verification-fidelity scaling]</li>
<li class=""><strong>authors:</strong> David Ball</li>
<li class=""><strong>institution:</strong> Superficial Labs</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14761" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14761</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces CAPE, a protocol for Capability Engineering that implements a Specify-&gt;Verify-&gt;Correct-&gt;Train loop to convert requirements into executable specifications and train models to satisfy them by default. It demonstrates that CAPE reduces policy violation rates by 81% compared to DPO and significantly lowers costs and development timelines by using reusable specifications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Guided Discrete Diffusion for Constraint Satisfaction Problems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [generative models], [discrete diffusion, unsupervised learning, constraint satisfaction, sudoku]</li>
<li class=""><strong>authors:</strong> Justin Jung</li>
<li class=""><strong>institution:</strong> unknown</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14765" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14765</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an unsupervised discrete diffusion model to learn the distribution of Sudoku puzzles, aiming to capture their structural patterns. The method avoids the need for supervised datasets and directly handles the discrete nature of the constraint satisfaction problem. The authors demonstrate its capability to solve Sudoku puzzles without supervision.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [knowledge graph reasoning], [GR-Agent, adaptive graph reasoning, incomplete knowledge graphs, agent environment interaction, reasoning paths]</li>
<li class=""><strong>authors:</strong> Dongzhuoran Zhou, Yuqicheng Zhu, Xiaxia Wang, Hongkuan Zhou, Jiaoyan Chen, Steffen Staab, Yuan He, Evgeny Kharlamov</li>
<li class=""><strong>institution:</strong> University of Oslo, Bosch Center for AI, University of Stuttgart, University of Oxford, Amazon, The University of Manchester, University of Southampton</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14766" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14766</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces GR-Agent, an adaptive graph reasoning agent that formalizes knowledge graph question answering as agent-environment interaction to handle incomplete knowledge graphs by using graph reasoning tools and memory of evidence. It demonstrates that existing methods degrade under incompleteness, while GR-Agent outperforms non-training baselines and matches training-based methods in both complete and incomplete settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Evaluating Weather Forecasts from a Decision Maker&#x27;s Perspective</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [weather forecasting], [decision calibration, probabilistic forecasting, ensemble forecasts, machine learning weather prediction, numerical weather prediction]</li>
<li class=""><strong>authors:</strong> Kornelius Raeth, Nicole Ludwig</li>
<li class=""><strong>institution:</strong> University of Tübingen, Tübingen AI Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14779" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14779</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a framework called decision calibration to evaluate weather forecasts based on their ability to improve decision-making, rather than just statistical accuracy. It compares Machine Learning and classical numerical weather prediction models on various decision tasks. The main conclusion is that forecast-level performance does not reliably predict decision-level performance, and the optimal model can change depending on the specific decision task.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Incentives or Ontology? A Structural Rebuttal to OpenAI&#x27;s Hallucination Thesis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [large language models], [transformer architecture, structural hallucination, Licensing Oracle, hybrid systems, statistical ontology]</li>
<li class=""><strong>authors:</strong> Richard Ackermann, Simeon Emanuilov</li>
<li class=""><strong>institution:</strong> RA Software, Sofia University “St. Kliment Ohridski”</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14801" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14801</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper argues that hallucination in LLMs is an architectural inevitability of transformers, which model token co-occurrence rather than the world, and demonstrates through a Licensing Oracle that external truth-validation modules are required for reliable abstention. It concludes that hallucination is a structural property, not a correctable incentive problem, necessitating hybrid systems to separate linguistic fluency from epistemic responsibility.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Unreliable Uncertainty Estimates with Monte Carlo Dropout</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [uncertainty estimation], [Monte Carlo dropout, Bayesian inference, Gaussian processes, Bayesian neural networks, epistemic uncertainty, aleatoric uncertainty]</li>
<li class=""><strong>authors:</strong> Aslak Djupskås, Alexander Johannes Stasik, Signe Riemer-Sørensen</li>
<li class=""><strong>institution:</strong> Norwegian University of Life Sciences, SINTEF AS</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14851" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14851</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper empirically evaluates Monte Carlo Dropout (MCD) as a method for uncertainty estimation in deep learning. It finds that MCD&#x27;s uncertainty estimates are unreliable, particularly in extrapolation and interpolation regions, compared to traditional Bayesian approaches like Gaussian Processes and Bayesian Neural Networks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Audio MultiChallenge: A Multi-Turn Evaluation of Spoken Dialogue Systems on Natural Human Interaction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [end-to-end spoken dialogue systems, audio language models, multi-turn evaluation, speech-to-speech, audio-native benchmark, inference memory, instruction retention, self coherence, voice editing]</li>
<li class=""><strong>authors:</strong> Advait Gosai, Tyler Vuong, Utkarsh Tyagi, Steven Li, Wenjia You, Miheer Bavare, Arda Uçar, Zhongwang Fang, Brian Jang, Bing Liu, Yunzhong He</li>
<li class=""><strong>institution:</strong> Scale AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14865" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14865</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Audio MultiChallenge, a benchmark for evaluating end-to-end spoken dialogue systems on natural, multi-turn conversations. It extends a text-based framework with a new &quot;Voice Editing&quot; axis and audio-specific augmentations, using a hybrid pipeline to curate conversations with natural disfluencies. The evaluation shows even top models like Gemini 3 Pro Preview struggle, highlighting difficulties in tracking audio edits, cues, and long context in spoken dialogue.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] How Does Fourier Analysis Network Work? A Mechanism Analysis and a New Dual-Activation Layer Proposal</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [neural network activation functions], [Fourier Analysis Network, Dual-Activation Layer, sine activation, gradient analysis, dying-ReLU problem]</li>
<li class=""><strong>authors:</strong> Sam Jeong, Hae Yong Kim</li>
<li class=""><strong>institution:</strong> University of São Paulo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14873" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14873</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes the mechanism of the Fourier Analysis Network (FAN) and finds its benefit stems from the sine function&#x27;s non-zero derivative near zero, which mitigates vanishing gradients and the dying-ReLU problem. Based on this insight, the authors propose a new Dual-Activation Layer (DAL) that accelerates convergence and achieves equal or higher accuracy than conventional activations in evaluated tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [machine learning theory], [Bregman projection, entropy reservoir, information geometry, model collapse, self-referential learning]</li>
<li class=""><strong>authors:</strong> Jingwei Chen</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14879" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14879</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Entropy-Reservoir Bregman Projection (ERBP) framework, which models self-referential learning as a stochastic Bregman projection sequence in distribution space. It shows that injecting an entropy reservoir stabilizes the dynamics and prevents model collapse, unifying various empirical fixes into a single quantitative design rule.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Task Matrices: Linear Maps for Cross-Model Finetuning Transfer</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [model adaptation], [task matrix, linear transformation, finetuning, linear probe, embedding space]</li>
<li class=""><strong>authors:</strong> Darrin O&#x27; Brien, Dhikshith Gajulapalli, Eric Xia</li>
<li class=""><strong>institution:</strong> Algoverse AI Research, Brown University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14880" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14880</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the concept of a &quot;task matrix,&quot; a linear transformation that maps a base model&#x27;s embedding state to a finetuned model&#x27;s state. It demonstrates that applying this matrix to a base model can outperform linear probes and sometimes approach full finetuning performance across vision and text models on various datasets. The results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] OLR-WA: Online Weighted Average Linear Regression in Multivariate Data Streams</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [online learning], [weighted average, exponential weighted moving average, online linear regression, pseudo-inverse, coefficient of determination]</li>
<li class=""><strong>authors:</strong> Mohammad Abu-Shaira, Alejandro Rodriguez, Greg Speegle, Victor Sheng, Ishfaq Ahmad</li>
<li class=""><strong>institution:</strong> Baylor University, Texas Tech University, University of Texas at Arlington</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14892" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14892</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces OLR-WA, a novel multivariate online linear regression model based on a weighted average approach. It demonstrates rapid convergence and performance comparable to batch regression, while uniquely handling both temporal drift and confidence-based data scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] ATLAS: Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [graph learning], [graph neural networks, community detection, multilayer perceptrons, heterophilic graphs, homophilic graphs, node classification]</li>
<li class=""><strong>authors:</strong> Turja Kundu, Sanjukta Bhowmick</li>
<li class=""><strong>institution:</strong> University of North Texas</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14908" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14908</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ATLAS introduces a graph learning method that extracts multi-resolution community information and concatenates it with node features, then applies MLPs for classification, avoiding iterative aggregation. This approach improves accuracy on both homophilic and heterophilic graphs while enhancing scalability compared to traditional GNNs. The results show significant performance gains, particularly on heterophilic graphs, and offer a path toward explainable graph learning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [imitation learning], [on-policy expert corrections, DAgger, covariate shift, rejection sampling, supervised fine-tuning, multi-turn agents]</li>
<li class=""><strong>authors:</strong> Niklas Lauffer, Xiang Deng, Srivatsa Kundurthy, Brad Kenstler, Jeff Da</li>
<li class=""><strong>institution:</strong> UC Berkeley, Cornell University, Scale AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14895" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14895</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a novel data generation method called on-policy expert corrections (OECs) to address covariate shift in imitation learning for multi-turn language model agents. The method generates partially on-policy data by starting rollouts with a student model and switching to an expert model mid-trajectory. Experiments on software engineering tasks show OECs yield a 13-14% improvement over traditional imitation learning, demonstrating the need to combine expert demonstrations with on-policy data for effective agent training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Parameter Efficient Multimodal Instruction Tuning for Romanian Vision Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [LoRA, dataset translation, visual question answering, instruction tuning, parameter-efficient fine-tuning]</li>
<li class=""><strong>authors:</strong> George-Andrei Dima, Dumitru-Clementin Cercel</li>
<li class=""><strong>institution:</strong> National University of Science and Technology POLITEHNICA Bucharest</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14926" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14926</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a parameter-efficient method for adapting vision-language models to Romanian by translating the Flickr30k dataset and generating QA pairs, then fine-tuning models like LLaMA, LLaVA, and Qwen2 using LoRA. The results show significant improvements in Romanian visual QA and image captioning, with the Qwen2-VL-RoVQA model achieving the best performance and reduced grammatical errors.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Low-rank MMSE filters, Kronecker-product representation, and regularization: a new perspective</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [signal processing], [low-rank MMSE filter, Kronecker product representation, regularization parameter selection]</li>
<li class=""><strong>authors:</strong> Daniel Gomes de Pinho Zanco, Leszek Szczecinski, Jacob Benesty, Eduardo Vinicius Kuhn</li>
<li class=""><strong>institution:</strong> INRS–Institut National de la Recherche Scientifique, Federal University of Technology - Paraná</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14932" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14932</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an efficient method to determine the regularization parameter for low-rank Minimum Mean Squared Error (MMSE) filters using a Kronecker-product representation. It demonstrates that this parameter is intrinsically linked to rank selection and is critical for low-rank system performance. Simulation results validate the method, showing it outperforms commonly used approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Cloud Security Leveraging AI: A Fusion-Based AISOC for Malware and Log Behaviour Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [AI-Augmented Security Operations Center (AISOC), malware detection, log-anomaly detection, score fusion, Elasticsearch, Kibana, Metasploit, EC2]</li>
<li class=""><strong>authors:</strong> Nnamdi Philip Okonkwo, Lubna Luxmi Dhirani</li>
<li class=""><strong>institution:</strong> University of Limerick</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14935" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14935</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper implements an AI-Augmented Security Operations Center (AISOC) on AWS that fuses outputs from a malware detector and a log-anomaly detector to triage threats. The method uses calibrated score fusion to classify activity as NORMAL, SUSPICIOUS, or HIGH_CONFIDENCE_ATTACK. It concludes that this simple, fused approach can effectively enhance cloud SOC capabilities in cost-sensitive environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Boundary condition enforcement with PINNs: a comparative study and verification on 3D geometries</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [physics-informed neural networks (PINNs), boundary condition enforcement, Dirichlet, Neumann, Robin, strong form PDE, finite element method]</li>
<li class=""><strong>authors:</strong> Conor Rowan, Kai Hampleman, Kurt Maute, Alireza Doostan</li>
<li class=""><strong>institution:</strong> University of Colorado Boulder</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14941" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14941</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically compares techniques for enforcing boundary conditions in Physics-Informed Neural Networks (PINNs) on complex 3D geometries and proposes a general solution framework. It verifies the methodology on 3D linear and nonlinear test problems, aiming to establish PINNs as a competitive numerical method against traditional approaches like the finite element method.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Cross-Tokenizer Likelihood Scoring Algorithms for Language Model Distillation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [knowledge distillation, byte-pair encoding, cross-tokenizer likelihood scoring, vocabulary misalignment, next-token probability]</li>
<li class=""><strong>authors:</strong> Buu Phan, Ashish Khisti, Karen Ullrich</li>
<li class=""><strong>institution:</strong> University of Toronto, Meta AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14954" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14954</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a method for cross-tokenizer likelihood scoring to enable knowledge distillation between language models with different vocabularies. It leverages the recursive structure of Byte-Pair Encoding to compute exact or approximate next-token probabilities. The approach reduces memory footprint and improves model performance on tasks like mathematical reasoning compared to baseline distillation methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [KV-cache management, lossy compression, adaptive eviction, utility function, multi-tier storage]</li>
<li class=""><strong>authors:</strong> Shaoting Feng, Yuhan Liu, Hanchen Li, Xiaokun Chen, Samuel Shen, Kuntai Du, Zhuohan Gu, Rui Zhang, Yuyang Huang, Yihua Cheng, Jiayi Yao, Qizheng Zhang, Ganesh Ananthanarayanan, Junchen Jiang</li>
<li class=""><strong>institution:</strong> University of Chicago, UC Berkeley, Tensormesh, Inc., MIT, UC Santa Cruz, Stanford, Microsoft</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14946" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14946</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EVICPRESS is a KV-cache management system that jointly optimizes lossy compression and adaptive eviction across multiple storage tiers using a unified utility function. It improves LLM inference efficiency by maximizing fast-tier cache hit rates while preserving generation quality through context-aware compression. Evaluations show it achieves up to 2.19x faster time-to-first-token at equivalent quality compared to baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Intrusion Detection in Internet of Vehicles Using Machine Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [intrusion detection system, machine learning, multi-class classification, CAN bus, CICIoV2024 dataset]</li>
<li class=""><strong>authors:</strong> Hop Le, Izzat Alsmadi</li>
<li class=""><strong>institution:</strong> Texas A&amp;M University-San Antonio</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14958" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14958</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops a machine learning-based intrusion detection system to classify malicious CAN bus traffic in the Internet of Vehicles. It uses the CICIoV2024 benchmark dataset to analyze attack patterns like DoS and spoofing. The initial findings confirm a clear structural difference between attack types and benign data, providing a strong foundation for machine learning models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Softly Constrained Denoisers for Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [diffusion models, denoiser, soft inductive bias, constraint compliance, guidance, misspecification]</li>
<li class=""><strong>authors:</strong> Victor M. Yeom Song, Severi Rissanen, Arno Solin, Samuel Kaski, Mingfei Sun</li>
<li class=""><strong>institution:</strong> Aalto University, University of Manchester, ELLIS Institute Finland</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14980" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14980</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes softly constrained denoisers for diffusion models, integrating a guidance-inspired adjustment directly into the denoiser to bias samples towards satisfying given constraints. The method improves constraint compliance compared to standard denoisers while remaining flexible enough to deviate from misspecified constraints to better match observed data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Prompt Repetition Improves Non-Reasoning LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [prompt repetition, causal language model, attention mechanism, non-reasoning tasks]</li>
<li class=""><strong>authors:</strong> Yaniv Leviathan, Matan Kalman, Yossi Matias</li>
<li class=""><strong>institution:</strong> Google Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14982" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14982</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a simple method of repeating the input prompt to improve the performance of LLMs on non-reasoning tasks. This technique allows all prompt tokens to attend to each other within the causal attention mechanism, addressing order sensitivity. The authors demonstrate that this method boosts accuracy for models like Gemini, GPT, Claude, and Deepseek without increasing output length or latency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [stochastic differential equations], [deep learning, elicitability, Picard iterations, recurrent neural network, feedforward network, McKean-Vlasov FBSDEs]</li>
<li class=""><strong>authors:</strong> Felipe J. P. Antunes, Yuri F. Saporito, Sebastian Jaimungal</li>
<li class=""><strong>institution:</strong> Getulio Vargas Foundation, University of Toronto, University of Oxford</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14967" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14967</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a new numerical method for solving McKean-Vlasov forward-backward stochastic differential equations with common noise by combining deep learning and elicitability to create a path-wise loss function. The approach uses neural networks to approximate the backward process and conditional expectations, avoiding costly nested Monte Carlo simulations. It is validated on models with known solutions and applied to complex economic models, demonstrating its accuracy and flexibility.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Adaptive Partitioning and Learning for Stochastic Control of Diffusion Processes</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [adaptive partitioning, model-based algorithm, diffusion processes, regret bounds, zooming dimension]</li>
<li class=""><strong>authors:</strong> Hanqing Jin, Renyuan Xu, Yanzhao Yang</li>
<li class=""><strong>institution:</strong> University of Oxford, Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14991" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14991</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a model-based reinforcement learning algorithm that adaptively partitions the state-action space for controlled diffusion processes with unbounded continuous states. It refines partitions based on estimation bias versus statistical confidence, achieving regret bounds that extend to unbounded domains. The approach is validated through numerical experiments, including high-dimensional portfolio selection.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [bug reproduction, LLM, iterative generate-validate-refine, agentic AI]</li>
<li class=""><strong>authors:</strong> Mehil B Shah, Mohammad Masudur Rahman, Foutse Khomh</li>
<li class=""><strong>institution:</strong> Dalhousie University, Polytechnique Montreal</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14990" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14990</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents RepGen, an automated approach that uses an LLM-based intelligent agent to reproduce deep learning bugs by constructing a learning-enhanced context and employing an iterative generate-validate-refine mechanism. It achieves an 80.19% reproduction rate on real-world bugs, significantly outperforming the state-of-the-art, and a developer study confirms it improves success rates and reduces time and cognitive load for bug reproduction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [Process Reward Model (PRM), Chain-of-Function, meta-learning, label correction, bi-level optimization, test-time scaling, LiveCodeBench]</li>
<li class=""><strong>authors:</strong> Ruiyi Zhang, Peijia Qin, Qi Cao, Pengtao Xie</li>
<li class=""><strong>institution:</strong> University of California, San Diego</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15000" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15000</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes DreamPRM-Code, a process reward model for coding that treats functions as reasoning steps using a Chain-of-Function strategy and employs a meta-learning-based label correction mechanism to refine noisy intermediate training labels. It achieves state-of-the-art performance on LiveCodeBench, surpassing OpenAI o4-mini.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SeBERTis: A Framework for Producing Classifiers of Security-Related Issue Reports</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [masked language model, fine-tuning, semantic surrogates, deep neural network, BERT]</li>
<li class=""><strong>authors:</strong> Sogol Masoumzadeh, Yufei Li, Shane McIntosh, Dániel Varró, Lili Wei</li>
<li class=""><strong>institution:</strong> McGill University, University of Waterloo, Linköping University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15003" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15003</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SEBERTIS, a framework that fine-tunes bidirectional transformer models (like BERT) as Masked Language Models using semantically equivalent vocabulary (Semantic Surrogates) to create classifiers for security-related issue reports. This method reduces reliance on lexical shortcuts, enabling better detection of complex issues. The resulting classifier significantly outperforms existing ML and LLM baselines in precision, recall, and F1-score, demonstrating high effectiveness for real-time issue triage.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Stock Pattern Assistant (SPA): A Deterministic and Explainable Framework for Structural Price Run Extraction and Event Correlation in Equity Markets</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [financial time series analysis], [deterministic segmentation, event correlation, monotonic price runs, OHLCV data, explainable AI, structural decomposition]</li>
<li class=""><strong>authors:</strong> Sandeep Neela</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15008" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15008</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Stock Pattern Assistant (SPA), a deterministic framework that extracts structural monotonic price runs from daily stock data and correlates them with public events using a symmetric window to generate factual explanations. It demonstrates the method on several equities, showing it provides transparent, reproducible historical analysis without forecasting. The main conclusion is that SPA offers an interpretable tool for decomposing price structure, complementing analyst workflows and explainable-AI pipelines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Epistemic diversity across language models mitigates knowledge collapse</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [model collapse, epistemic diversity, AI ecosystem, self-training, distributed training]</li>
<li class=""><strong>authors:</strong> Damian Hodel, Jevin D. West</li>
<li class=""><strong>institution:</strong> University of Washington</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15011" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15011</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper investigates whether diversity across language models (an &quot;AI ecosystem&quot;) can mitigate performance decay from training on model-generated data. It segments training data across multiple models and evaluates performance over self-training iterations. The main conclusion is that increased epistemic diversity mitigates knowledge collapse, but only up to an optimal level, with too few or too many models leading to poor performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Spectral Representation-based Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [spectral representations, spectral decomposition, transition operator, partially observable MDPs, model-free, model-based]</li>
<li class=""><strong>authors:</strong> Chenxiao Gao, Haotian Sun, Na Li, Dale Schuurmans, Bo Dai</li>
<li class=""><strong>institution:</strong> Georgia Tech, Harvard University, Google DeepMind, University of Alberta</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15036" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15036</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces spectral representations, derived from the spectral decomposition of the transition operator, as a framework for reinforcement learning to address issues like theoretical ambiguity and optimization instability. It shows how to construct these representations for different system structures and extends the approach to partially observable environments. The proposed algorithms achieve performance comparable to or better than state-of-the-art methods on over 20 challenging control tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [Meta-Prompting Protocol, Adversarial Trinity, DSPy, TextGrad, textual gradients, semantic computation graph]</li>
<li class=""><strong>authors:</strong> Fanzhe Fu</li>
<li class=""><strong>institution:</strong> Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15053" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15053</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Meta-Prompting Protocol, a framework that formalizes LLM orchestration as a programmable system using an adversarial topology (Generator, Auditor, Optimizer) to treat prompts as differentiable variables. It leverages textual critiques as gradients within a semantic computation graph to mitigate hallucination and improve reliability. The authors demonstrate its theoretical viability with tools like DSPy and TextGrad, proposing a foundation for deterministic &quot;Observable Software Engineering&quot; for probabilistic models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] EMFusion: Conditional Diffusion Framework for Trustworthy Frequency Selective EMF Forecasting in Wireless Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [conditional diffusion model, probabilistic forecasting, residual U-Net, cross-attention, uncertainty quantification, imputation-based sampling]</li>
<li class=""><strong>authors:</strong> Zijiang Yan, Yixiang Huang, Jianhua Pei, Hina Tabassum, Luca Chiaraviglio</li>
<li class=""><strong>institution:</strong> York University, Huazhong University of Science and Technology, University of Rome Tor Vergata</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15067" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15067</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces EMFusion, a conditional diffusion-based probabilistic forecasting framework that uses a residual U-Net with cross-attention to integrate contextual factors for frequency-selective EMF level prediction. It treats forecasting as an inpainting task to handle irregular data and provides explicit uncertainty estimates. The results show that EMFusion significantly outperforms baseline models in key forecasting metrics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [retrieval-augmented generation], [conformal prediction, semantic similarity, natural language inference, hallucination detection, text embeddings]</li>
<li class=""><strong>authors:</strong> Debu Sinha</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15068" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15068</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper applies conformal prediction to provide statistical guarantees for hallucination detection in RAG systems, rigorously evaluating embedding-based methods. It finds that while these methods work on synthetic data, they fail on real benchmarks due to the &quot;semantic illusion,&quot; where plausible hallucinations remain semantically similar to source documents. The study concludes that embedding-based detection is insufficient for production, as reasoning-based methods like GPT-4 as a judge perform significantly better.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multi-label classification], [feature engineering, large language models, code generation, label co-occurrence matrices, Pearson correlation, feedback mechanism]</li>
<li class=""><strong>authors:</strong> Wanfu Gao, Zebin He, Jun Gao</li>
<li class=""><strong>institution:</strong> Jilin University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15082" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15082</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FEAML, an automated feature engineering method for multi-label classification that uses LLMs to generate code for creating new features based on metadata and label dependencies. It employs a feedback loop where feature effectiveness is evaluated using model accuracy and redundancy is checked with Pearson correlation to iteratively improve the LLM&#x27;s code generation. Empirical results show that FEAML outperforms other feature engineering methods on various multi-label datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Neural Modular Physics for Elastic Simulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [neural modular physics, modular neural simulator, elastic simulation, physical interpretability, intermediate physical quantities, graph neural networks, physics-informed neural networks]</li>
<li class=""><strong>authors:</strong> Yifei Li, Haixu Wu, Zeyi Xu, Tuur Stuyck, Wojciech Matusik</li>
<li class=""><strong>institution:</strong> MIT CSAIL, Shanghai University, Meta Reality Labs</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15083" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15083</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Neural Modular Physics (NMP), a method that decomposes elastic dynamics into physically meaningful neural modules connected by intermediate physical quantities, combining neural network approximation with traditional simulator reliability. It demonstrates superior generalization, long-horizon stability, and better preservation of physical properties compared to monolithic neural simulators.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SigMA: Path Signatures and Multi-head Attention for Learning Parameters in fBm-driven SDEs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [stochastic modeling], [path signatures, multi-head self-attention, convolutional neural network, fractional Brownian motion, parameter estimation, Hurst parameter]</li>
<li class=""><strong>authors:</strong> Xianglin Wu, Chiheb Ben Hammouda, Cornelis W. Oosterlee</li>
<li class=""><strong>institution:</strong> Southwestern University of Finance and Economics, Utrecht University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15088" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15088</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SigMA, a neural architecture that combines path signatures with multi-head self-attention for parameter estimation in fractional Brownian motion-driven stochastic differential equations. The method outperforms baseline models like CNN, LSTM, and vanilla Transformer in accuracy and robustness on synthetic and real-world datasets. The results demonstrate that integrating signature transforms with attention provides an effective framework for inference in systems with rough temporal structure.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] PIP<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> Net: Physics-informed Partition Penalty Deep Operator Network</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [scientific machine learning], [operator learning, DeepONet, partition-of-unity, physics-informed neural networks, regularization, Burgers equation, Allen-Cahn equation]</li>
<li class=""><strong>authors:</strong> Hongjin Mi, Huiqiang Lun, Changhong Mou, Yeyu Zhang</li>
<li class=""><strong>institution:</strong> Shanghai University of Finance and Economics, York University, Utah State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15086" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15086</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes PIP² Net, a physics-informed operator learning network that introduces a simplified partition penalty based on partition-of-unity methods to regularize and stabilize trunk-network features in DeepONet. The method is evaluated on nonlinear PDEs like Burgers and Allen-Cahn equations, where it consistently outperforms baseline models in prediction accuracy and robustness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Feature-Centric Unsupervised Node Representation Learning Without Homophily Assumption</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [graph representation learning], [graph convolution, unsupervised learning, node embeddings, homophily, non-homophilic graphs, intra-class similarity, inter-class separability]</li>
<li class=""><strong>authors:</strong> Sunwoo Kim, Soo Yong Lee, Kyungho Kim, Hyunjin Hwang, Jaemin Yoo, Kijung Shin</li>
<li class=""><strong>institution:</strong> KAIST (Korea Advanced Institute of Science and Technology)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15112" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15112</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FUEL, an unsupervised node representation learning method that adaptively adjusts the degree of graph convolution usage to enhance intra-class similarity and inter-class separability in the embedding space, using node feature clusters as proxy classes. It demonstrates state-of-the-art performance across diverse homophily levels in graphs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [sequence modeling], [attention, state space models, unified framework, interaction rank gap, head-count theorem, gradient highway]</li>
<li class=""><strong>authors:</strong> Ali Ghodsi</li>
<li class=""><strong>institution:</strong> University of Waterloo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15115" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15115</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a unified theoretical framework that connects attention mechanisms and state space models through an input-dependent interaction operator. It proves that representing a linear SSM requires a number of attention heads equal to the subspace dimension of its lag operators, and reveals a trade-off between the algebraic expressivity of the model and its ability to propagate gradients over long sequences.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series Imputation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [time series imputation], [diffusion model, Fourier transform, attention mechanism, gated convolution, frequency-domain modeling]</li>
<li class=""><strong>authors:</strong> Runze Li, Hanchen Wang, Wenjie Zhang, Binghao Li, Yu Zhang, Xuemin Lin, Ying Zhang</li>
<li class=""><strong>institution:</strong> University of New South Wales, University of Technology Sydney, Shanghai Jiao Tong University, Zhejiang Gongshang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15116" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15116</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FADTI is a diffusion-based framework for multivariate time series imputation that integrates a learnable Fourier Bias Projection module with self-attention and gated convolution to inject frequency-domain inductive bias. It outperforms state-of-the-art methods across multiple benchmarks, especially under high missing rates, by adaptively encoding both stationary and non-stationary patterns.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Automatic Reward Shaping from Multi-Objective Human Heuristics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [reward shaping, multi-objective optimization, bi-level optimization, stochastic exploration]</li>
<li class=""><strong>authors:</strong> Yuqing Xie, Jiayu Chen, Wenhao Tang, Ya Zhang, Chao Yu, Yu Wang</li>
<li class=""><strong>institution:</strong> Tsinghua University, Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15120" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15120</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MORSE, a framework that automatically combines multiple human-designed heuristic rewards into a unified reward function using bi-level optimization with stochastic exploration. It effectively balances conflicting objectives in robotic tasks, achieving performance comparable to manually tuned rewards in MuJoCo and Isaac Sim environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] TrajSyn: Privacy-Preserving Dataset Distillation from Federated Model Trajectories for Server-Side Adversarial Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, adversarial training, dataset distillation, model trajectories, privacy-preserving]</li>
<li class=""><strong>authors:</strong> Mukur Gupta, Niharika Gupta, Saifur Rahman, Shantanu Pal, Chandan Karmakar</li>
<li class=""><strong>institution:</strong> Columbia University, Vellore Institute of Technology, Deakin University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15123" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15123</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces TrajSyn, a framework that synthesizes a proxy dataset from the trajectories of client model updates in Federated Learning to enable server-side adversarial training without accessing raw client data. It demonstrates that this method improves adversarial robustness on image classification benchmarks without imposing extra computational burden on client devices.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Generalization and Feature Attribution in Machine Learning Models for Crop Yield and Anomaly Prediction in Germany</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [agricultural machine learning], [XGBoost, Random Forest, LSTM, TCN, SHAP, temporal validation, generalization]</li>
<li class=""><strong>authors:</strong> Roland Baatz</li>
<li class=""><strong>institution:</strong> Leibniz Centre for Agricultural Landscape Research (ZALF)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15140" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15140</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This study compares ensemble tree-based models (XGBoost, Random Forest) and deep learning approaches (LSTM, TCN) for predicting crop yield and anomalies in Germany, using SHAP for interpretability. It finds that models perform well on conventional test sets but generalize poorly to unseen temporal data, and that feature importance explanations can appear credible even when generalization fails, highlighting the need for validation-aware interpretation in agricultural ML.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] From Isolation to Entanglement: When Do Interpretability Methods Identify and Disentangle Known Concepts?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [interpretability], [sparse autoencoders, sparse probes, concept disentanglement, steering experiments, multi-concept evaluation]</li>
<li class=""><strong>authors:</strong> Aaron Mueller, Andrew Lee, Shruti Joshi, Ekdeep Singh Lubana, Dhanya Sridhar, Patrik Reizinger</li>
<li class=""><strong>institution:</strong> Boston University, Harvard University, Mila – Quebec AI Institute, Goodfire, University of Tübingen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15134" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15134</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a multi-concept evaluation framework to test whether interpretability methods like sparse autoencoders and sparse probes recover disentangled and independently manipulable concept representations. It finds that features often correspond to single concepts, but concepts are distributed across many features, and steering one feature typically affects multiple concepts, indicating a lack of true independence. The results highlight that correlational metrics are insufficient for proving disentanglement and underscore the need for compositional evaluations in interpretability research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] An Efficient Gradient-Based Inference Attack for Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [membership inference attack, federated learning, gradient-based methods, shadow training, attribute inference]</li>
<li class=""><strong>authors:</strong> Pablo Montaña-Fernández, Ines Ortega-Fernandez</li>
<li class=""><strong>institution:</strong> Gradiant</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15143" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15143</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a new gradient-based inference attack for federated learning that analyzes the temporal evolution of last-layer gradients across multiple training rounds. The attack uses a shadow model technique to learn gradient patterns and can be extended to attribute inference. The findings show that multi-round federated learning increases vulnerability to such attacks, with aggregators posing a greater threat than data owners.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Understanding NTK Variance in Implicit Neural Representations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [neural representations], [Neural Tangent Kernel, positional encoding, spherical normalization, Hadamard modulation, spectral bias]</li>
<li class=""><strong>authors:</strong> Chengguang Ou, Yixin Zhuang</li>
<li class=""><strong>institution:</strong> Fuzhou University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15169" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15169</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes how architectural choices in Implicit Neural Representations (INRs), such as positional encoding and normalization, affect the conditioning of the Neural Tangent Kernel (NTK) to mitigate spectral bias. It shows these components reduce NTK eigenvalue variance, leading to improved convergence and better high-frequency detail recovery. Experiments confirm faster convergence and enhanced reconstruction quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] BEAT2AASIST model with layer fusion for ESDD 2026 Challenge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [audio deepfake detection], [BEATs, AASIST, transformer layer fusion, vocoder-based data augmentation, dual-branch architecture]</li>
<li class=""><strong>authors:</strong> Sanghyeok Chung, Eujin Kim, Donggun Kim, Gaeun Heo, Jeongbin You, Nahyun Lee, Sunmook Choi, Soyul Han, Seungsang Oh, Il-Youp Kwak</li>
<li class=""><strong>institution:</strong> Korea University, Chung-Ang University, Cornell University, Hannam University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15180" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15180</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes BEAT2AASIST, a model for environmental sound deepfake detection that extends BEATs-AASIST by splitting features into dual AASIST branches and incorporating transformer layer fusion strategies. It also uses vocoder-based data augmentation for robustness. The approach demonstrates competitive performance on the ESDD 2026 Challenge test sets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] DEER: Draft with Diffusion, Verify with Autoregressive Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [speculative decoding, diffusion large language model, parallel decoding, draft-verify scheme, two-stage training, single-step decoding]</li>
<li class=""><strong>authors:</strong> Zicong Cheng, Guo-Wei Yang, Jia Li, Zhijie Deng, Meng-Hao Guo, Shi-Min Hu</li>
<li class=""><strong>institution:</strong> Tsinghua University, Proxseer Inc, Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15176" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15176</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces DEER, a speculative decoding framework that uses a diffusion large language model (dLLM) as a parallel drafter to generate candidate tokens, which are then verified by a target autoregressive model. The method overcomes the sequential bottleneck and trust collapse of traditional AR drafters through a two-stage training pipeline and single-step decoding. Experiments show DEER achieves significantly longer draft acceptance and higher speedups (e.g., 5.54x) compared to prior methods like EAGLE-3.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Chorus: Harmonizing Context and Sensing Signals for Data-Free Model Customization in IoT</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [cross-modal reconstruction, context-aware gating, context-caching, domain generalization, data-free customization]</li>
<li class=""><strong>authors:</strong> Liyu Zhang, Yejia Liu, Kwun Ho Liu, Runxi Huang, Xiaomin Ouyang</li>
<li class=""><strong>institution:</strong> Hong Kong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15206" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15206</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Chorus, a method that learns context representations via unsupervised cross-modal reconstruction and uses a lightweight gated head to dynamically integrate sensor and context information for model adaptation without target data. It demonstrates improved performance in unseen IoT contexts and maintains low latency on edge devices through a context-caching mechanism.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Label-consistent clustering for evolving data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [clustering algorithms], [k-center, approximation algorithms, label-consistent clustering]</li>
<li class=""><strong>authors:</strong> Ameet Gadekar, Aristides Gionis, Thibault Marette</li>
<li class=""><strong>institution:</strong> CISPA Helmholtz Center for Information Security, KTH Royal Institute of Technology, Digital Futures</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15210" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15210</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes the label-consistent k-center problem, which aims to update an existing clustering solution with new data while limiting changes to a given budget. It introduces two constant-factor approximation algorithms for this problem. The experimental evaluation on real-world datasets demonstrates the effectiveness of the proposed methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Accelerating High-Throughput Catalyst Screening by Direct Generation of Equilibrium Adsorption Structures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational catalysis], [deep generative model, periodic Brownian-bridge, equivariant graph neural network, DFT relaxation, adsorption energy, outlier detection]</li>
<li class=""><strong>authors:</strong> Songze Huo, Xiao-Ming Cao</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15228" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15228</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces DBCata, a deep generative model that combines a periodic Brownian-bridge framework with an equivariant graph neural network to directly generate equilibrium adsorption structures from unrelaxed ones, bypassing explicit energy or force calculations. It achieves high-fidelity geometry generation and improves DFT accuracy, enabling accelerated high-throughput screening for catalysts like those in the oxygen reduction reaction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [EEND-EDA, RNN-based stitching, centroid refinement decoder, permutation-invariant training (PIT), online speaker diarization]</li>
<li class=""><strong>authors:</strong> Elio Gruttadauria, Mathieu Fontaine, Jonathan Le Roux, Slim Essid</li>
<li class=""><strong>institution:</strong> Télécom Paris, Institut polytechnique de Paris, Mitsubishi Electric Research Laboratories (MERL)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15229" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15229</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces O-EENC-SD, an efficient online end-to-end neural clustering system for speaker diarization. It is based on EEND-EDA and features a novel RNN-based stitching mechanism and a centroid refinement decoder for online prediction. The system is shown to be competitive with state-of-the-art methods on the CallHome dataset while offering a better trade-off between diarization error rate and computational complexity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [visual enumeration, vision-language models, zero-shot counting, intermediate representations, object counting]</li>
<li class=""><strong>authors:</strong> Kuinan Hou, Jing Mi, Marco Zorzi, Lamberto Ballan, Alberto Testolin</li>
<li class=""><strong>institution:</strong> University of Padova</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15254" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15254</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically compares specialized counting architectures and vision-language models (VLMs) for visual enumeration. It finds that VLMs can match or surpass specialized models, especially when prompted to generate intermediate object representations, but still struggle with complex scenes, indicating a need for further research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Leveraging Foundational Models and Simple Fusion for Multi-modal Physiological Signal Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [self-supervised pretraining, dual-masking strategy, embedding concatenation, foundational models, CBraMod encoder]</li>
<li class=""><strong>authors:</strong> Youssef Ghallab, Omar Iraqy, Mohamed Kandil, Mohamed Ashraf, Saadeldine Eletter, Morougue Ghazal, Ayman Khalafallah, Nagwa El-Makky</li>
<li class=""><strong>institution:</strong> Alexandria University, Mohamed bin Zayed University of Artificial Intelligence</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15250" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15250</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method for multi-modal physiological signal analysis by adapting a self-supervised foundational model (CBraMod) for ECG and EEG, using a dual-masking strategy for ECG, and fusing the modalities via simple embedding concatenation. The approach achieves near state-of-the-art performance in emotion recognition, demonstrating that well-designed foundational encoders with straightforward fusion can effectively leverage limited multi-modal data. The results highlight the potential of foundation-model approaches for scalable and label-efficient solutions in healthcare and affective computing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Distillation-Guided Structural Transfer for Continual Learning Beyond Sparse Distributed Memory</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [continual learning], [selective subnetwork distillation, sparse distributed memory, top-k activation, knowledge distillation, catastrophic forgetting]</li>
<li class=""><strong>authors:</strong> Huiyan Xue, Xuming Ran, Yaxin Li, Qi Xu, Enhui Li, Yi Xu, Qiang Zhang</li>
<li class=""><strong>institution:</strong> Dalian University of Technology, National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15267" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15267</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Selective Subnetwork Distillation (SSD), a continual learning method that uses distillation to transfer knowledge between task-specific sparse subnetworks in a Sparse Distributed Memory MLP, without needing replay or task labels. The method improves accuracy and retention by enabling structural realignment and cross-task knowledge reuse while preserving sparsity. Experiments on benchmark datasets show SSD offers a structurally grounded solution to mitigate forgetting in sparse continual learning systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Model inference for ranking from pairwise comparisons</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [ranking algorithms], [Bayesian inference, pairwise comparisons, latent strength, model inference, Chebyshev approximation, neural networks]</li>
<li class=""><strong>authors:</strong> Daniel Sánchez Catalina, George T. Cantwell</li>
<li class=""><strong>institution:</strong> University of Cambridge</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15269" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15269</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an efficient Bayesian algorithm to jointly infer both the latent strengths of objects and the unknown function mapping strengths to win probabilities from pairwise comparison data. The method uses Chebyshev-based and neural network approaches to model this function without prior assumptions. Experimental results show robustness across model specifications and improved predictive accuracy, such as overcoming bookmaker profit margins in tennis data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Topological Metric for Unsupervised Embedding Quality Evaluation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [representation learning evaluation], [persistent homology, unsupervised metric, topological data analysis, embedding quality]</li>
<li class=""><strong>authors:</strong> Aleksei Shestov, Anton Klenitskiy, Daria Denisova, Amurkhan Dzagkoev, Daniil Petrovich, Andrey Savchenko, Maksim Makarenko</li>
<li class=""><strong>institution:</strong> Sber AI Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15285" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15285</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes &quot;Persistence&quot;, a fully unsupervised metric for evaluating embedding quality based on persistent homology from topological data analysis. It captures the multi-scale geometric and topological structure of embedding spaces. Empirical results show it achieves strong correlation with downstream task performance, outperforming existing unsupervised metrics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Quantum Machine Learning for Cybersecurity: A Taxonomy and Future Directions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Quantum Neural Networks (QNNs), Quantum Support Vector Machines (QSVMs), Variational Quantum Circuits (VQCs), Quantum Generative Adversarial Networks (QGANs)]</li>
<li class=""><strong>authors:</strong> Siva Sai, Ishika Goyal, Shubham Sharma, Sri Harshita Manuri, Vinay Chamola, Rajkumar Buyya</li>
<li class=""><strong>institution:</strong> The University of Melbourne, Birla Institute of Technology and Science, Pilani, APPCAIR</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15286" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15286</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This survey paper explores Quantum Machine Learning (QML) techniques, including QNNs, QSVMs, VQCs, and QGANs, for cybersecurity applications like intrusion detection and malware classification. It concludes that QML offers potential advantages for processing high-dimensional data and enhancing security in areas like cloud computing, but also discusses current limitations and future research directions needed to address them.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [8-bit training, activation checkpointing, offloading, copy-engine collectives, dynamic tensor-level scaling, ZeRO-1]</li>
<li class=""><strong>authors:</strong> Erik Schultheis, Dan Alistarh</li>
<li class=""><strong>institution:</strong> IST Austria</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15306" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15306</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LLMQ is an end-to-end CUDA/C++ framework that enables efficient 8-bit pretraining and fine-tuning of medium-sized language models on consumer GPUs by employing optimizations like activation checkpointing, offloading, and copy-engine based collectives to overcome memory and communication bottlenecks. It demonstrates that models up to 32B parameters can be trained on affordable hardware like a 4xRTX 4090 workstation while maintaining high FLOP utilization, rivaling the efficiency of production systems on more expensive cloud-grade GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Time-Varying Audio Effect Modeling by End-to-End Adversarial Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Generative Adversarial Network (GAN), convolutional-recurrent architecture, adversarial training, State Prediction Network (SPN), chirp-train signals]</li>
<li class=""><strong>authors:</strong> Yann Bourdin, Pierrick Legrand, Fanny Roche</li>
<li class=""><strong>institution:</strong> Arturia, Inria, IMS, University of Bordeaux, Bordeaux INP</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15313" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15313</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a two-stage GAN framework for black-box modeling of time-varying audio effects, using only input-output recordings without needing control signals. The method involves an adversarial training phase followed by supervised fine-tuning with a State Prediction Network for synchronization. Experiments on a vintage phaser demonstrate the approach&#x27;s effectiveness in capturing time-varying dynamics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Bits for Privacy: Evaluating Post-Training Quantization via Membership Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [post-training quantization, membership inference, AdaRound, BRECQ, OBC, quantization, privacy-utility trade-off]</li>
<li class=""><strong>authors:</strong> Chenxiang Zhang, Tongxi Qu, Zhong Li, Tian Zhang, Jun Pang, Sjouke Mauw</li>
<li class=""><strong>institution:</strong> University of Luxembourg, Nanjing University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15335" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15335</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically evaluates how post-training quantization (PTQ) affects privacy by using membership inference attacks on models quantized with algorithms like AdaRound, BRECQ, and OBC. The study finds that lower-precision PTQ (e.g., 4-bit, 2-bit, 1.58-bit) significantly reduces privacy leakage, offering up to an order of magnitude less vulnerability compared to full-precision models, though at the cost of decreased utility. The results provide practical insights for balancing efficiency, accuracy, and privacy in model deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [phase adjustment, three-axis independent alignment, single-axis reference alignment, deep learning, vibration signals, predictive maintenance]</li>
<li class=""><strong>authors:</strong> Hiroyoshi Nagahama, Katsufumi Inoue, Masayoshi Todorokihara, Michifumi Yoshioka</li>
<li class=""><strong>institution:</strong> Osaka Metropolitan University, Seiko Epson Corp.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15344" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15344</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces two phase-aware preprocessing strategies—three-axis independent and single-axis reference phase adjustment—to handle random phase variations in multi-axis vibration data for fault diagnosis. The methods are evaluated using a new rotor dataset and six deep learning models, showing consistent performance improvements, with the single-axis reference approach achieving up to 96.2% accuracy by preserving spatial phase relationships. The findings demonstrate that these phase alignment strategies are practical and scalable enhancements for predictive maintenance systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Expand and Prune: Maximizing Trajectory Diversity for Effective GRPO in Generative Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [Group Relative Policy Optimization (GRPO), Optimal Variance Filtering (OVF), Pro-GRPO, Expand-and-Prune, reward clustering, latent feature-based pruning]</li>
<li class=""><strong>authors:</strong> Shiran Ge, Chenyi Huang, Yuang Ai, Qihang Fan, Huaibo Huang, Ran He</li>
<li class=""><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences (CAS); University of Chinese Academy of Sciences (UCAS)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15347" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15347</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Pro-GRPO, a dynamic framework that improves Group Relative Policy Optimization (GRPO) for generative model alignment by integrating an &quot;Expand-and-Prune&quot; strategy. It first expands the sampling group for diversity and then prunes reward-clustered trajectories early using latent features to reduce computational cost. Experiments on diffusion and flow models show that Pro-GRPO is more efficient and effective than standard GRPO.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [adaptive computation, early exit, dual-path training, image complexity classification, ConvNeXt-IC]</li>
<li class=""><strong>authors:</strong> Mikel Williams-Lekuona, Georgina Cosma</li>
<li class=""><strong>institution:</strong> Loughborough University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15372" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15372</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ICAR, an adaptive retrieval method that reduces computation for simple images in vision-language models by using early exits, while maintaining cross-modal alignment through dual-path training. It also introduces ConvNeXt-IC, a classifier for image complexity to decide the compute depth. The method achieves a 20% practical speedup with minimal performance loss, enabling more efficient scaling of vision-language systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Regime-Aware Fusion Framework for Time Series Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [time series classification], [ROCKET, SAX, SFA, SHAP, representation fusion, meta-features]</li>
<li class=""><strong>authors:</strong> Honey Singh Chauhan, Zahraa S. Abdallah</li>
<li class=""><strong>institution:</strong> University of Bristol</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15378" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15378</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Fusion-3 (F3), a lightweight framework that adaptively fuses ROCKET, SAX, and SFA representations for time series classification. It shows that this selective fusion yields consistent improvements over ROCKET on specific types of datasets, which are identified by clustering datasets into interpretable regimes based on meta-features.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Remotely Detectable Robot Policy Watermarking</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [policy watermarking, colored noise coherency, glimpse sequence, remote detection, spectral signal, stochasticity]</li>
<li class=""><strong>authors:</strong> Michael Amir, Manon Flageat, Amanda Prorok</li>
<li class=""><strong>institution:</strong> University of Cambridge</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15379" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15379</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Colored Noise Coherency (CoNoCo), a watermarking method that embeds a spectral signal into a robot&#x27;s motions using the policy&#x27;s inherent stochasticity for remote detection. It is designed to be detectable from noisy external observations like video footage without degrading policy performance. The work demonstrates robust detection across various remote modalities, providing a non-invasive way to verify the provenance of physical robot policies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Robustness Evaluation of Machine Learning Models for Fault Classification and Localization In Power System Protection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [fault classification, fault localization, robustness evaluation, EMT simulation, sensor degradation]</li>
<li class=""><strong>authors:</strong> Julian Oelhaf, Mehran Pashaei, Georg Kordowich, Christian Bergler, Andreas Maier, Johann Jäger, Siming Bayer</li>
<li class=""><strong>institution:</strong> Friedrich-Alexander-Universität Erlangen-Nürnberg, Ostbayerische Technische Hochschule Amberg-Weiden</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15385" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15385</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a unified framework using high-fidelity EMT simulations to systematically evaluate the robustness of machine learning models for fault classification and localization in power systems under degraded sensor data conditions. The main conclusion is that fault classification is generally stable but fault localization is highly sensitive, with voltage data loss increasing error by over 150%, providing guidance for designing resilient ML-assisted protection systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multi-view clustering], [contrastive learning, distribution alignment, semantic graph, partially view-aligned clustering]</li>
<li class=""><strong>authors:</strong> Liang Peng, Yixuan Ye, Cheng Liu, Hangjun Che, Fei Wang, Zhiwen Yu, Si Wu, Hau-San Wong</li>
<li class=""><strong>institution:</strong> Shantou University, Huaqiao University, Southwest University, South China University of Technology, City University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15396" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15396</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SMART, a model for partially view-aligned clustering that uses view distribution alignment and semantic matching contrastive learning to handle misaligned multi-view data. It aligns cross-view covariance matrices to reduce distribution shifts and leverages a semantic graph to guide contrastive learning, improving clustering performance. Experiments on eight datasets show that SMART outperforms existing methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [bayesian reinforcement learning, epistemic uncertainty, minimax-optimal regret, sample complexity, infinite-horizon MDPs]</li>
<li class=""><strong>authors:</strong> Jianfei Ma, Wee Sun Lee</li>
<li class=""><strong>institution:</strong> National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15405" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15405</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes EUBRL, a Bayesian reinforcement learning algorithm that uses epistemic uncertainty to guide exploration and reduce per-step regret. It provides theoretical guarantees for regret and sample complexity and demonstrates superior sample efficiency and scalability in sparse-reward, long-horizon tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] FlowBind: Efficient Any-to-Any Generation with Bidirectional Flows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [flow-based generative models, flow-matching, invertible flows, shared latent space, any-to-any generation]</li>
<li class=""><strong>authors:</strong> Yeonwoo Cha, Semin Kim, Jinhyeon Kwon, Seunghoon Hong</li>
<li class=""><strong>institution:</strong> KAIST</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15420" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15420</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlowBind is an efficient framework for any-to-any cross-modal generation that learns a shared latent space and uses modality-specific invertible flows, all optimized jointly with a single flow-matching objective. It reduces data requirements and computational cost by factorizing interactions through the latent space. Experiments show it achieves comparable quality to prior methods while using up to 6x fewer parameters and training 10x faster.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Statistics of Min-max Normalized Eigenvalues in Random Matrices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [random matrix theory], [min-max normalization, eigenvalue distribution, matrix factorization, cumulative distribution, scaling law]</li>
<li class=""><strong>authors:</strong> Hyakka Nakada, Shu Tanaka</li>
<li class=""><strong>institution:</strong> Keio University, Waseda University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15427" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15427</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper studies the statistical properties of min-max normalized eigenvalues in random matrices. It applies a proposed effective distribution to evaluate a scaling law for the cumulative distribution and derives the residual error from matrix factorization. Numerical experiments are conducted to verify these theoretical predictions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [feature model-based reinforcement learning, actor-critic, Dyna-Q, model-based RL, model-free RL, multi-task control]</li>
<li class=""><strong>authors:</strong> Quanxi Zhou, Wencan Mao, Manabu Tsukada, John C.S. Lui, Yusheng Ji</li>
<li class=""><strong>institution:</strong> The University of Tokyo, National Institute of Informatics, The Chinese University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15430" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15430</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FM-EAC, a feature model-based enhanced actor-critic algorithm that integrates planning, acting, and learning for multi-task control. It combines model-based and model-free reinforcement learning to improve generalizability across tasks. Simulations show it outperforms state-of-the-art methods in urban and agricultural applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Double Horizon Model-Based Policy Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [model-based reinforcement learning, policy optimization, distribution rollout, training rollout, double horizon]</li>
<li class=""><strong>authors:</strong> Akihiro Kubo, Paavo Parmas, Shin Ishii</li>
<li class=""><strong>institution:</strong> Advanced Telecommunications Research Institute, Kyoto University, The University of Tokyo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15439" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15439</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Double Horizon Model-Based Policy Optimization (DHMBPO), a method that separates the model rollout process into a long &quot;distribution rollout&quot; to mitigate distribution shift and a short &quot;training rollout&quot; for stable gradient estimation. This approach balances model bias and gradient variance. The method demonstrates superior sample efficiency and runtime compared to existing MBRL methods on continuous-control benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Copyright Infringement Risk Reduction via Chain-of-Thought and Task Instruction Prompting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [chain-of-thought prompting, task instruction prompting, negative prompting, prompt re-writing, copyright mitigation]</li>
<li class=""><strong>authors:</strong> Neeraj Sarna, Yuanyuan Li, Michael von Gablenz</li>
<li class=""><strong>institution:</strong> Munich RE</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15442" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15442</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes combining chain-of-thought and task instruction prompting with negative prompting and prompt re-writing to reduce the generation of copyrighted content by text-to-image models. It evaluates the generated images based on their similarity to copyrighted material and relevance to the user input. The experiments across various models provide insights into the effectiveness of these techniques for different model complexities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] From Risk to Resilience: Towards Assessing and Mitigating the Risk of Data Reconstruction Attacks in Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Invertibility Loss, Jacobian matrix, spectral properties, adaptive noise perturbation, data reconstruction attacks, federated learning]</li>
<li class=""><strong>authors:</strong> Xiangrui Xu, Zhize Li, Yufei Han, Bin Wang, Jiqiang Liu, Wei Wang</li>
<li class=""><strong>institution:</strong> Beijing Jiaotong University, Singapore Management University, INRIA Rennes-Bretagne-Atlantique, Zhejiang Key Laboratory of AIoT Network and Data Security, Xi&#x27;an Jiaotong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15460" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15460</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a theoretical framework to assess and mitigate data reconstruction attacks in federated learning by proposing Invertibility Loss (InvLoss) to quantify attack risk and deriving a computable upper bound. The authors develop an attack-agnostic risk estimator and two adaptive noise perturbation defenses based on this framework. Experiments show the framework effectively evaluates and reduces privacy risks without harming model accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Metanetworks as Regulatory Operators: Learning to Edit for Requirement Compliance</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [metanetworks, graph neural networks, model editing, bias mitigation, weight pruning, data minimisation]</li>
<li class=""><strong>authors:</strong> Ioannis Kalogeropoulos, Giorgos Bouritsas, Yannis Panagakis</li>
<li class=""><strong>institution:</strong> National and Kapodistrian University of Athens, Archimedes/Athena RC, Visible Machines AI Research &amp; Social Awareness Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15469" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15469</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a framework using graph metanetworks to edit neural networks in a single inference step, aiming to enforce requirements like fairness or efficiency while preserving model utility. It demonstrates improved trade-offs in performance, compliance, and time efficiency compared to traditional post-processing or retraining methods across tasks such as bias mitigation and pruning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Multi-stage Bayesian optimisation for dynamic decision-making in self-driving labs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Bayesian optimisation, multi-stage workflows, proxy measurements, self-driving labs, autonomous experimentation]</li>
<li class=""><strong>authors:</strong> Luca Torresi, Pascal Friederich</li>
<li class=""><strong>institution:</strong> Karlsruhe Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15483" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15483</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a multi-stage Bayesian optimisation method that uses intermediate proxy measurements to make dynamic decisions in self-driving labs. The method allows for flexible sampling of complex experimental workflows. The authors conclude that using proxy measurements significantly improves both the speed and optimality of finding solutions compared to conventional Bayesian optimisation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Robustness and uncertainty: two complementary aspects of the reliability of the predictions of a classifier</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [machine learning reliability], [robustness quantification, uncertainty quantification, classifier reliability, epistemic uncertainty]</li>
<li class=""><strong>authors:</strong> Adrián Detavernier, Jasper De Bock</li>
<li class=""><strong>institution:</strong> Ghent University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15492" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15492</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares two methods for assessing the reliability of individual classifier predictions: Robustness Quantification (RQ) and Uncertainty Quantification (UQ). It finds that neither method is universally superior; instead, they are complementary. A hybrid approach combining RQ and UQ is shown to outperform each method individually.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [transformer, multi-head self-attention, positional encoding, precision-focused loss, edge deployment, TensorFlow Lite, ONNX, TensorRT]</li>
<li class=""><strong>authors:</strong> Konstantinos Kalogiannis, Ahmed Mohamed Hussain, Hexu Li, Panos Papadimitratos</li>
<li class=""><strong>institution:</strong> KTH Royal Institute of Technology, Lenovo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15503" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15503</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes AIMformer, a transformer-based framework for real-time misbehavior detection in vehicular platoons. It uses multi-head self-attention to capture spatiotemporal dependencies and a precision-focused loss to minimize false positives. The method demonstrates high performance and achieves sub-millisecond inference latency, making it suitable for deployment on resource-constrained edge platforms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Soft Geometric Inductive Bias for Object Centric Dynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [geometric deep learning], [geometric algebra neural networks, soft geometric inductive bias, equivariance, object-centric world models, autoregressive training, long-horizon rollouts]</li>
<li class=""><strong>authors:</strong> Hampus Linander, Conor Heins, Alexander Tschantz, Marco Perin, Christopher Buckley</li>
<li class=""><strong>institution:</strong> VERSES AI, University of Sussex</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15493" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15493</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using object-centric world models built with geometric algebra neural networks to provide a soft geometric inductive bias for learning physical dynamics. The method is evaluated on 2D rigid body simulations and shows that this soft bias leads to better long-horizon prediction fidelity compared to non-equivariant baselines, effectively balancing between strict symmetry constraints and unstructured learning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Conditioned UNet for Music Source Separation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [conditioned UNet, music source separation, QSCNet, Sparse Compressed Network, Bandsplit RNN, Banquet, MoisesDb]</li>
<li class=""><strong>authors:</strong> Ken O&#x27;Hanlon, Basil Woods, Lin Wang, Mark Sandler</li>
<li class=""><strong>institution:</strong> Queen Mary University of London, AudioStrip Ltd.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15532" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15532</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes QSCNet, a novel conditioned UNet architecture for music source separation that uses an audio query to specify the target stem, eliminating the need for a predefined instrument vocabulary. The method integrates conditioning elements into a Sparse Compressed Network and is shown to outperform the prior Banquet model by over 1dB SNR on certain tasks while using fewer than half the parameters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Tracking Temporal Dynamics of Vector Sets with Gaussian Process</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [temporal analysis], [Gaussian Process, Random Fourier Features, vector sets, temporal dynamics, low-dimensional visualization]</li>
<li class=""><strong>authors:</strong> Taichi Aida, Mamoru Komachi, Toshinobu Ogiso, Hiroya Takamura, Daichi Mochihashi</li>
<li class=""><strong>institution:</strong> Tokyo Metropolitan University, Hitotsubashi University, National Institute for Japanese Language and Linguistics, National Institute of Advanced Industrial Science and Technology, The Institute of Statistical Mathematics</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15538" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15538</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a method to model time-varying vector sets using infinite-dimensional Gaussian Processes, approximating the latent function with Random Fourier Features to obtain compact, comparable representations over time. It demonstrates effectiveness in capturing temporal dynamics in crime distributions and word embeddings, providing interpretable, low-dimensional visualizations of structural changes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Joint Learning of Unsupervised Multi-view Feature and Instance Co-selection with Cross-view Imputation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [unsupervised learning], [multi-view learning, feature selection, instance selection, data imputation, cross-view neighborhood, data reconstruction]</li>
<li class=""><strong>authors:</strong> Yuxin Cai, Yanyong Huang, Jinyuan Chang, Dongjie Wang, Tianrui Li, Xiaoyi Jiang</li>
<li class=""><strong>institution:</strong> Southwestern University of Finance and Economics, University of Kansas, Southwest Jiaotong University, University of Münster</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15574" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15574</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes JUICE, a method that jointly performs unsupervised feature and instance co-selection and cross-view data imputation for incomplete multi-view data in a unified framework. It uses cross-view neighborhood information to refine missing data imputation during reconstruction, which improves the selection of representative features and instances. Experiments show that JUICE outperforms existing state-of-the-art methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [autonomous driving perception], [knowledge distillation, camera-radar fusion, intensity-aware distillation, multi-level distillation, BEV representation]</li>
<li class=""><strong>authors:</strong> Shashank Mishra, Karan Patil, Didier Stricker, Jason Rambach</li>
<li class=""><strong>institution:</strong> German Research Center for Artificial Intelligence (DFKI), RPTU</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15581" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15581</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes IMKD, an intensity-aware multi-level knowledge distillation framework for camera-radar 3D object detection. It uses a three-stage distillation strategy from LiDAR to enhance radar and fused representations while preserving each sensor&#x27;s unique characteristics. Experiments on nuScenes show IMKD outperforms prior distillation-based fusion methods, achieving 67.0% NDS and 61.0% mAP.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Corrective Diffusion Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [masked diffusion language model, corrective behavior, code revision benchmark, confidence-guided refinement, correction-oriented post-training]</li>
<li class=""><strong>authors:</strong> Shuibai Zhang, Fred Zhangzhi Peng, Yiheng Zhang, Jin Pan, Grigorios G. Chrysos</li>
<li class=""><strong>institution:</strong> University of Wisconsin–Madison, Duke University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15596" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15596</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a correction-oriented post-training method for diffusion language models to enable error-aware confidence and targeted refinement, which standard masked diffusion training fails to achieve. The method explicitly supervises visible incorrect tokens to improve corrective behavior. Experiments show the approach substantially outperforms standard models in correction tasks while also enhancing completion performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Evaluating Large Language Models in Scientific Discovery</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [scientific discovery evaluation], [scenario-grounded benchmark, two-phase evaluation, hypothesis generation, experiment design, result interpretation]</li>
<li class=""><strong>authors:</strong> Zhangde Song, Jieyu Lu, Yuanqi Du, Botao Yu, Thomas M. Pruyn, Yue Huang, Kehan Guo, Xiuzhe Luo, Yuanhao Qu, Yi Qu, Yinkai Wang, Haorui Wang, Jeff Guo, Jingru Gan, Parshin Shojaee, Di Luo, Andres M Bran, Gen Li, Qiyuan Zhao, Shao-Xiong Lennon Luo, Yuxuan Zhang, Xiang Zou, Wanru Zhao, Yifan F. Zhang, Wucheng Zhang, Shunan Zheng, Saiyang Zhang, Sartaaj Takrim Khan, Mahyar Rajabi-Kochi, Samantha Paradi-Maropakis, Tony Baltoiu, Fengyu Xie, Tianyang Chen, Kexin Huang, Weiliang Luo, Meijing Fang, Xin Yang, Lixue Cheng, Jiajun He, Soha Hassoun, Xiangliang Zhang, Wei Wang, Chandan K. Reddy, Chao Zhang, Zhiling Zheng, Mengdi Wang, Le Cong, Carla P. Gomes, Chang-Yu Hsieh, Aditya Nandy, Philippe Schwaller, Heather J. Kulik, Haojun Jia, Huan Sun, Seyed Mohamad Moosavi, Chenru Duan</li>
<li class=""><strong>institution:</strong> Deep Principle, Cornell University, The Ohio State University, University of Toronto, University of Notre Dame, QuEra Computing Inc., Stanford University, Harvard Law School, Tufts University, Georgia Institute of Technology, Ecole Polytechnique Federale de Lausanne, University of California, Los Angeles, Virginia Tech, Tsinghua University, Princeton University, Harvard University, University of Cambridge, The University of Texas at Austin, McGill University, University of Science and Technology of China, Massachusetts Institute of Technology, Zhejiang University, The Hong Kong University of Science and Technology, Washington University in St. Louis</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15567" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15567</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a two-phase Scientific Discovery Evaluation (SDE) framework, which uses scenario-grounded benchmarks across multiple scientific domains to assess LLMs on question-level accuracy and project-level tasks like hypothesis generation and experiment design. It concludes that current LLMs show a significant performance gap in scientific discovery compared to general benchmarks, exhibit diminishing returns from scaling, and are far from being general scientific &quot;superintelligences,&quot; though they still demonstrate promise in various discovery projects.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] How Smoothing is N-simplicial Attention?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [attention mechanisms], [N-simplicial attention, higher-order interactions, Rotary Position Embeddings (RoPE), simplex selection, over-smoothing, Lipschitz bound]</li>
<li class=""><strong>authors:</strong> Alexandre Dussolle, Pietro Liò</li>
<li class=""><strong>institution:</strong> University of Cambridge, École des Ponts, IP Paris</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15600" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15600</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces N-simplicial attention, a method that generalizes standard attention to higher-order token interactions and adapts it for Rotary Position Embeddings (RoPE). It also proposes a cost-effective simplex selection mechanism to manage computational complexity. The authors demonstrate that, despite enabling higher-order interactions, N-simplicial attention itself suffers from over-smoothing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [language models], [autoregressive models, energy-based models, soft Bellman equation, maximum entropy reinforcement learning, distillation]</li>
<li class=""><strong>authors:</strong> Mathieu Blondel, Michael E. Sander, Germain Vivier-Ardisson, Tianlin Liu, Vincent Roulet</li>
<li class=""><strong>institution:</strong> Google DeepMind</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15605" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15605</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper establishes a formal bijection between autoregressive language models (ARMs) and energy-based models (EBMs) in function space, showing this equivalence corresponds to a special case of the soft Bellman equation in maximum entropy RL. It demonstrates the equivalence of their supervised learning and provides theoretical error bounds for distilling an EBM into an ARM. The results explain how ARMs, despite being trained on next-token prediction, can implicitly perform lookahead planning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [explainable recommendation], [behavior tokens, vector-quantized autoencoding, semantic alignment regularization, zero-shot recommendation]</li>
<li class=""><strong>authors:</strong> Xinshun Feng, Mingzhe Liu, Yi Qiao, Tongyu Zhu, Leilei Sun, Shuai Wang</li>
<li class=""><strong>institution:</strong> Beihang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15614" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15614</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes BEAT, a framework that tokenizes user and item behaviors into discrete, interpretable sequences using a behavior vocabulary constructed via vector-quantized autoencoding. It introduces multi-level semantic supervision and semantic alignment regularization to bridge behavioral signals with language models. Experiments show BEAT improves zero-shot recommendation performance and generates coherent explanations, demonstrating that behavior tokens effectively capture fine-grained semantics for integration with large language models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] How Much is Too Much? Exploring LoRA Rank Trade-offs for Retaining Knowledge and Domain Robustness</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [Low-Rank Adaptation (LoRA), supervised fine-tuning (SFT), parameter-efficient fine-tuning (PEFT), rank sweep, representational drift, attention patterns]</li>
<li class=""><strong>authors:</strong> Darshita Rathore, Vineet Kumar, Chetna Bansal, Anindya Moitra</li>
<li class=""><strong>institution:</strong> PayPal</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15634" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15634</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper comprehensively evaluates the trade-offs between full supervised fine-tuning (SFT) and Low-Rank Adaptation (LoRA) for fine-tuning large language models. It finds that LoRA, especially at specific rank values, can achieve competitive or even superior performance to SFT on reasoning tasks, while also analyzing the structural changes in model representations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] PPSEBM: An Energy-Based Model with Progressive Parameter Selection for Continual Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [continual learning], [energy-based model, progressive parameter selection, pseudo-sample generation, catastrophic forgetting mitigation]</li>
<li class=""><strong>authors:</strong> Xiaodi Li, Dingcheng Li, Rujun Gao, Mahmoud Zamani, Feng Mi, Latifur Khan</li>
<li class=""><strong>institution:</strong> Mayo Clinic, Google, Texas A&amp;M University, The University of Texas at Dallas</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15658" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15658</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces PPSEBM, a framework combining an Energy-Based Model with Progressive Parameter Selection to address catastrophic forgetting in continual learning for NLP tasks. It uses task-specific parameters and generates pseudo-samples from prior tasks to retain past knowledge. Experimental results show PPSEBM outperforms state-of-the-art methods in mitigating forgetting.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SoFlow: Solution Flow Models for One-Step Generative Modeling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [Flow Matching, Classifier-Free Guidance, solution consistency loss, velocity ODE, one-step generation, Diffusion Transformer]</li>
<li class=""><strong>authors:</strong> Tianze Luo, Haotian Yuan, Zhuang Liu</li>
<li class=""><strong>institution:</strong> Princeton University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15657" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15657</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SoFlow, a framework for one-step generative modeling that uses a Flow Matching loss and a solution consistency loss to train models without requiring Jacobian-vector product calculations. It improves training efficiency by enabling Classifier-Free Guidance and achieves better FID-50K scores than MeanFlow models on ImageNet 256x256 when using the same DiT architecture and training epochs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [llm interpretability], [LatentQA, Activation Oracles, activation analysis, fine-tuning detection, natural language queries]</li>
<li class=""><strong>authors:</strong> Adam Karvonen, James Chua, Clément Dumas, Kit Fraser-Taliente, Subhash Kantamneni, Julian Minder, Euan Ong, Arnab Sen Sharma, Daniel Wen, Owain Evans, Samuel Marks</li>
<li class=""><strong>institution:</strong> MATS, Truthful AI, EPFL, ENS Paris-Saclay, Northeastern University, Anthropic</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15674" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15674</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Activation Oracles, models trained using the LatentQA approach to answer natural language questions about the internal activations of other LLMs. The core finding is that these oracles, especially when trained on diverse datasets, can generalize to out-of-distribution tasks and effectively verbalize hidden information, such as knowledge from fine-tuning, often matching or exceeding prior white-box interpretability methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Stylized Synthetic Augmentation further improves Corruption Robustness</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [data augmentation, neural style transfer, synthetic data, corruption robustness, TrivialAugment]</li>
<li class=""><strong>authors:</strong> Georg Siedel, Rojan Regmi, Abhirami Anand, Weijia Shao, Silvia Vock, Andrey Morozov</li>
<li class=""><strong>institution:</strong> University of Stuttgart, Federal Institute for Occupational Safety and Health (BAuA)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15675</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a training data augmentation pipeline that combines synthetic image data with neural style transfer to improve the corruption robustness of deep vision models. It finds that stylizing synthetic images, despite lowering their FID score, is beneficial for training, and that this method can be effectively combined with rule-based augmentations like TrivialAugment. The approach achieves state-of-the-art robust accuracy on several image classification benchmarks under common corruptions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [gradient-guided reinforcement learning, G2RL, PPO, KL control, final-layer sensitivity]</li>
<li class=""><strong>authors:</strong> Zhenwen Liang, Sidi Lu, Wenhao Yu, Kishan Panaganti, Yujun Zhou, Haitao Mi, Dong Yu</li>
<li class=""><strong>institution:</strong> Tencent AI Lab, University of Notre Dame</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15687" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15687</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces G2RL, a gradient-guided reinforcement learning framework that uses the model&#x27;s own gradient directions to guide exploration, rather than external heuristics like entropy bonuses. It shows that G2RL improves reasoning performance across multiple benchmarks by encouraging diverse and orthogonal update directions. The results indicate that a policy&#x27;s internal update geometry provides a more effective basis for exploration in LLM reinforcement learning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Multivariate Statistical Framework for Detection, Classification and Pre-localization of Anomalies in Water Distribution Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [anomaly detection], [multivariate statistical analysis, Hotelling&#x27;s T^2 statistic, whitening transformation, Mahalanobis space, Laplacian interpolation, hypothesis testing]</li>
<li class=""><strong>authors:</strong> Oleg Melnikov, Yurii Dorofieiev, Yurii Shakhnovskiy, Huy Truong, Victoria Degeler</li>
<li class=""><strong>institution:</strong> Not explicitly provided; inferred from author names and context (likely academic/research institution in Ukraine and international collaboration, but cannot be confirmed without explicit affiliations)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15685" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15685</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes SICAMS, a multivariate statistical framework that uses a whitening transformation and Hotelling&#x27;s T^2 statistic to detect, classify, and pre-localize anomalies like leaks in water distribution networks. The method demonstrates high sensitivity and reliability on benchmark data, enabling leak detection and size estimation without requiring a calibrated hydraulic model.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Multi-Modal Semantic Communication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [semantic communication, cross-modal attention, transformer, adaptive resolution encoding, bandwidth-constrained transmission]</li>
<li class=""><strong>authors:</strong> Matin Mortaheb, Erciyes Karakaya, Sennur Ulukus</li>
<li class=""><strong>institution:</strong> University of Maryland, College Park</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15691" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15691</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a multi-modal semantic communication framework that uses text-based user queries and a cross-modal attention mechanism to guide the extraction and adaptive-resolution transmission of task-relevant image patches. This approach overcomes limitations of self-attention-only methods in complex scenes and enables flexible, efficient communication under bandwidth constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [video-action model, flow matching, inverse dynamics model, imitation learning, vision-language-action model]</li>
<li class=""><strong>authors:</strong> Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, Elvis Nava</li>
<li class=""><strong>institution:</strong> mimic robotics, Microsoft Zurich, ETH Zurich, ETH AI Center, UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15692" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15692</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces mimic-video, a Video-Action Model that pairs a pretrained internet-scale video model with a flow matching-based action decoder to generate robot actions from video latent representations. This approach leverages video pretraining to capture both semantics and visual dynamics, isolating the control problem and achieving state-of-the-art performance with 10x greater sample efficiency compared to traditional Vision-Language-Action models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] FrontierCS: Evolving Challenges for Evolving Intelligence</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [benchmarking], [benchmark, open-ended problems, competitive programming, NP-hard, automatic evaluation, expert reference solution]</li>
<li class=""><strong>authors:</strong> Qiuyang Mang, Wenhao Chai, Zhifei Li, Huanzhi Mao, Shang Zhou, Alexander Du, Hanchen Li, Shu Liu, Edwin Chen, Yichuan Wang, Xieting Chu, Zerui Cheng, Yuan Xu, Tian Xia, Zirui Wang, Tianneng Shi, Jianzhu Yao, Yilong Zhao, Qizheng Zhang, Charlie Ruan, Zeyu Shen, Kaiyuan Liu, Runyuan He, Dong Xing, Zerui Li, Zirong Zeng, Yige Jiang, Lufeng Cheng, Ziyi Zhao, Youran Sun, Wesley Zheng, Meiyuwang Zhang, Ruyi Ji, Xuechang Tu, Zihan Zheng, Zexing Chen, Kangyang Zhou, Zhaozi Wang, Jingbang Chen, Aleksandra Korolova, Peter Henderson, Pramod Viswanath, Vijay Ganesh, Saining Xie, Zhuang Liu, Dawn Song, Sewon Min, Ion Stoica, Joseph E. Gonzalez, Jingbo Shang, Alvin Cheung</li>
<li class=""><strong>institution:</strong> UC Berkeley, Princeton University, UCSD, X-camp Academy, Georgia Tech, Stanford University, University of Washington, Nanyang Technological University, University of Toronto, UIUC, University of Michigan, New York University, MIT</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15699" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15699</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces FrontierCS, a benchmark of 156 open-ended computer science problems where the optimal solution is unknown but can be objectively evaluated, requiring models to generate executable programs. It finds that current frontier reasoning models significantly lag behind human experts, and that merely increasing reasoning budgets or generating workable code does not close this performance gap.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Dynamic Rebatching for Efficient Early-Exit Inference with DREX</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [early-exit, dynamic rebatching, copy-free buffer, SLA-aware scheduler, KV cache, state-copying]</li>
<li class=""><strong>authors:</strong> Xuting Liu, Daniel Alexander, Siva Kesava Reddy Kakarla, Behnaz Arzani, Vincent Liu</li>
<li class=""><strong>institution:</strong> University of Pennsylvania, Microsoft Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15705" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15705</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Dynamic Rebatching and the DREX system to efficiently batch requests in Early-Exit LLMs, where tokens can exit at different layers. DREX dynamically reorganizes batches at exit points using a copy-free buffer and a predictive scheduler, improving throughput by 2-12% while eliminating involuntary exits and preserving output quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Learning Model Parameter Dynamics in a Combination Therapy for Bladder Cancer from Sparse Biological Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational biology/oncology], [physics-informed neural network (PINN), time-varying parameters, sparse data, combination therapy, mathematical modeling]</li>
<li class=""><strong>authors:</strong> Kayode Olumoyin, Lamees El Naqa, Katarzyna Rejniak</li>
<li class=""><strong>institution:</strong> H. Lee Moffitt Cancer Center and Research Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15706" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15706</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using a physics-informed neural network (PINN) to learn time-varying interactions between cell populations, such as bladder cancer tumors and immune cells, from sparse biological data. The method predicts subpopulation trajectories in response to combination anticancer therapies where direct observations are unavailable. The authors demonstrate that their approach yields results consistent with biological explanations, providing a framework for modeling evolving dynamics under external interventions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [predictive concept decoder, communication bottleneck, sparse concept list, encoder-decoder, auto-interp score, fine-tuning]</li>
<li class=""><strong>authors:</strong> Vincent Huang, Dami Choi, Daniel D. Johnson, Sarah Schwettmann, Jacob Steinhardt</li>
<li class=""><strong>institution:</strong> Transluce</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15712" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15712</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Predictive Concept Decoders (PCDs), an end-to-end trained architecture where an encoder compresses a model&#x27;s internal activations into a sparse list of concepts, and a decoder uses this list to answer questions about the model&#x27;s behavior. The method is pretrained on large datasets and then finetuned, showing that the interpretability and downstream performance of the bottleneck concepts improve with more data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Magnification-Aware Distillation (MAD): A Self-Supervised Framework for Unified Representation Learning in Gigapixel Whole-Slide Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational pathology], [self-supervised learning, vision transformer, cross-scale distillation, magnification-invariant representation, whole-slide image analysis]</li>
<li class=""><strong>authors:</strong> Mahmut S. Gokmen, Mitchell A. Klusty, Peter T. Nelson, Allison M. Neltner, Sen-Ching Samson Cheung, Thomas M. Pearce, David A Gutman, Brittany N. Dugger, Devavrat S. Bisht, Margaret E. Flanagan, V. K. Cody Bumgardner</li>
<li class=""><strong>institution:</strong> University of Kentucky, University of Pittsburgh, Emory University, University of California Davis, University of Texas Health</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14796" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14796</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Magnification-Aware Distillation (MAD), a self-supervised framework that learns unified image representations by linking low-magnification context with spatially aligned high-magnification detail in whole-slide images. The resulting foundation model, MAD-NP, demonstrates strong resolution-invariant learning, as shown by a classifier trained on 10x embeddings maintaining 96.7% performance on unseen 40x tiles. The work concludes that this approach enables scalable, magnification-robust analysis using a unified embedding space.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Deep learning water-unsuppressed MRSI at ultra-high field for simultaneous quantitative metabolic, susceptibility and myelin water imaging</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [deep learning, water-unsuppressed MRSI, WALINET+, ECCENTRIC sampling, quantitative susceptibility mapping, myelin water fraction imaging]</li>
<li class=""><strong>authors:</strong> Paul J. Weiser, Jiye Kim, Jongho Lee, Amirmohammad Shamaei, Gulnur Ungan, Malte Hoffmann, Antoine Klauser, Berkin Bilgic, Ovidiu C. Andronesi</li>
<li class=""><strong>institution:</strong> Massachusetts General Hospital, Seoul National University, University of Calgary, Medical University of Vienna, Siemens Healthineers International AG</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14929" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14929</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a deep learning pipeline for water-unsuppressed MRSI at 7T, using a network called WALINET+ to remove water and lipid signals and sidebands, enabling simultaneous reconstruction of metabolic, susceptibility, and myelin water maps. The method achieves high-resolution quantitative imaging in a short scan time and shows good agreement with conventional techniques.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Efficient Nudged Elastic Band Method using Neural Network Bayesian Algorithm Execution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Neural Network Bayesian Algorithm Execution (NN-BAX), Nudged Elastic Band (NEB), active learning, foundation models, Bayesian Experimental Design, minimum energy pathway (MEP)]</li>
<li class=""><strong>authors:</strong> Pranav Kakhandiki, Sathya Chitturi, Daniel Ratner, Sean Gasiorowski</li>
<li class=""><strong>institution:</strong> Stanford University, SLAC National Accelerator Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14993" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14993</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Neural Network Bayesian Algorithm Execution (NN-BAX), a framework that actively learns both the energy landscape and the minimum energy pathway (MEP) by fine-tuning a foundation model with targeted sampling. It achieves a one to two order of magnitude reduction in computationally expensive energy and force evaluations compared to the standard NEB method, with minimal accuracy loss, enabling faster MEP discovery for complex systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Adaptive Weighted Genetic Algorithm-Optimized SVR for Robust Long-Term Forecasting of Global Stock Indices for investment decisions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [financial forecasting], [genetic algorithm, support vector regression, long short-term memory, mean absolute percentage error, hyperparameter optimization]</li>
<li class=""><strong>authors:</strong> Mohit Beniwal</li>
<li class=""><strong>institution:</strong> Delhi Technological University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15113" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15113</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an improved genetic algorithm-optimized support vector regression (IGA-SVR) model for long-term forecasting of global stock indices. It demonstrates superior accuracy and computational efficiency compared to LSTM and another GA-optimized SVR baseline, achieving significant reductions in MAPE.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Online Partitioned Local Depth for semi-supervised applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [online algorithms], [partitioned local depth, cohesion network, semi-supervised learning, anomaly detection, online algorithm]</li>
<li class=""><strong>authors:</strong> John D. Foley, Justin T. Lee</li>
<li class=""><strong>institution:</strong> Metron, Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15436" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15436</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces online PaLD, an extension of the partitioned local depth algorithm designed for online and semi-supervised applications. It allows for pre-computing a cohesion network from a reference dataset with an O(n^3) upfront cost, enabling new data points to be integrated in O(n^2) time. The method is demonstrated for online anomaly detection and semi-supervised classification in healthcare datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Autonomous Pressure Control in MuVacAS via Deep Reinforcement Learning and Deep Learning Surrogate Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [deep reinforcement learning, deep learning surrogate model, digital twin, autonomous control, pressure control]</li>
<li class=""><strong>authors:</strong> Guillermo Rodriguez-Llorente, Galo Gallardo, Rodrigo Morant Navascués, Nikita Khvatkin Petrovsky, Anderson Sabogal, Roberto Gómez-Espinosa Martín</li>
<li class=""><strong>institution:</strong> HI Iberia, Universidad Carlos III de Madrid, IFMIF-DONES Spain</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15521" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15521</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a fully data-driven approach for autonomous pressure control in a particle accelerator prototype. It uses a deep learning surrogate model as a digital twin to emulate system dynamics and trains a deep reinforcement learning agent within this simulation. The agent successfully learns a control policy to maintain pressure within strict operational limits, advancing intelligent autonomous control for next-generation accelerator facilities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] ColliderML: The First Release of an OpenDataDetector High-Luminosity Physics Benchmark Dataset</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [OpenDataDetector, next-to-leading order matrix element, full simulation, digitisation, detector-level data, proton-proton collisions, High-Luminosity LHC, Hugging Face]</li>
<li class=""><strong>authors:</strong> Doğa Elitez, Paul Gessinger, Daniel Murnane, Marcus Selchou Raaholt, Andreas Salzburger, Stine Kofoed Skov, Andreas Stefl, Anna Zaborowska</li>
<li class=""><strong>institution:</strong> CERN, Johannes Gutenberg-Universität Mainz, Niels Bohr Institute, Lawrence Berkeley National Laboratory, University of Copenhagen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15230" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15230</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces ColliderML, a large-scale dataset of fully simulated and digitized proton-proton collision events for the High-Luminosity LHC. It is generated using modern next-to-leading order matrix element calculations, realistic pile-up overlay, and a validated detector geometry. The dataset aims to fill a gap for machine learning research on detector-level data, providing a benchmark for collider physics applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Photonics-Enhanced Graph Convolutional Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [photonic positional embeddings, graph convolutional networks, synthetic frequency lattices, intensity correlation matrices, hybrid photonic-electronic workflow]</li>
<li class=""><strong>authors:</strong> Yuan Wang, Oleksandr Kyriienko</li>
<li class=""><strong>institution:</strong> University of Sheffield</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15549" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15549</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid workflow that enhances Graph Convolutional Networks (GCNs) by using photonic positional embeddings derived from simulating light propagation on synthetic frequency lattices that match the input graph structure. The method generates internode intensity correlation matrices to provide global structural information to the GCN. The results show that these photonic embeddings outperform baseline Laplacian-based embeddings on molecular graph benchmarks, improving regression and classification performance, and support the potential for optical acceleration in graph machine learning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Teacher-Student Perspective on the Dynamics of Learning Near the Optimal Point</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [neural network theory], [teacher-student model, gradient descent, Hessian eigenspectrum, Marchenko-Pastur distribution, polynomial activation]</li>
<li class=""><strong>authors:</strong> Carlos Couto, José Mourão, Mário A. T. Figueiredo, Pedro Ribeiro</li>
<li class=""><strong>institution:</strong> Instituto Superior Técnico, University of Lisbon</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15606" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15606</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes the learning dynamics of neural networks near an optimal point by characterizing the Hessian matrix&#x27;s eigenspectrum in teacher-student problems. It shows that for linear networks, the spectrum follows a convolution of scaled chi-square and Marchenko-Pastur distributions, and that smaller eigenvalues dictate long-term performance. Furthermore, it demonstrates that the Hessian rank acts as an effective parameter count for polynomial networks, while it remains full rank for networks with generic nonlinear activations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Learning continuous SOC-dependent thermal decomposition kinetics for Li-ion cathodes using KA-CRNNs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Kolmogorov-Arnold Chemical Reaction Neural Network (KA-CRNN), physics-encoded neural network, differential scanning calorimetry (DSC), kinetic parameter inference, thermal runaway prediction]</li>
<li class=""><strong>authors:</strong> Benjamin C. Koenig, Sili Deng</li>
<li class=""><strong>institution:</strong> Massachusetts Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15628" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15628</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper applies a physics-encoded Kolmogorov-Arnold Chemical Reaction Neural Network (KA-CRNN) to learn continuous state-of-charge (SOC)-dependent kinetic parameters for lithium-ion cathode decomposition directly from differential scanning calorimetry data. The method embeds a reaction pathway into the network, allowing parameters like activation energies to be represented as interpretable functions of SOC. The framework successfully models heat-release for several cathode materials, providing interpretable insights into decomposition mechanisms and establishing a foundation for more accurate thermal runaway prediction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Prospects for quantum advantage in machine learning from the representability of functions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [quantum machine learning], [parametrized quantum circuits, classical simulation, dequantization, function representability, tensor networks, stabilizer methods]</li>
<li class=""><strong>authors:</strong> Sergi Masot-Llima, Elies Gil-Fuster, Carlos Bravo-Prieto, Jens Eisert, and Tommaso Guaita</li>
<li class=""><strong>institution:</strong> Universitat de Barcelona, Barcelona Supercomputing Center, Freie Universität Berlin, Fraunhofer Heinrich Hertz Institute, Helmholtz-Zentrum Berlin für Materialien und Energie</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15661" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15661</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a framework connecting the structure of parametrized quantum circuits to the mathematical functions they can learn, analyzing how properties like circuit depth and non-Clifford gate count determine classical simulability. It concludes that this analysis reveals pathways to dequantization and distinguishes between fully simulatable, classically tractable, and robustly quantum models, providing a map to identify where genuine quantum advantage in machine learning may exist.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] High-Dimensional Partial Least Squares: Spectral Analysis and Fundamental Limitations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [high-dimensional statistics], [Partial Least Squares, Singular Value Decomposition, random matrix theory, spiked random matrix models, low-rank approximations]</li>
<li class=""><strong>authors:</strong> Victor Léger, Florent Chatelain</li>
<li class=""><strong>institution:</strong> Université Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15684" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15684</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper provides a theoretical analysis of the Partial Least Squares method based on Singular Value Decomposition (PLS-SVD) for high-dimensional data integration. Using tools from random matrix theory, it characterizes the alignment between estimated and true latent directions, explaining the method&#x27;s performance and limitations. The analysis shows that PLS-SVD is asymptotically superior to separate principal component analysis for detecting a common latent subspace.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-19">2025-12-19<a href="#2025-12-19" class="hash-link" aria-label="Direct link to 2025-12-19" title="Direct link to 2025-12-19" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251219] DiscoverDCP: A Data-Driven Approach for Construction of Disciplined Convex Programs via Symbolic Regression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [optimization], [symbolic regression, disciplined convex programming, system identification, convex optimization]</li>
<li class=""><strong>authors:</strong> Sveinung Myhre</li>
<li class=""><strong>institution:</strong> University of California, Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15721" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15721</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes DiscoverDCP, a method that integrates symbolic regression with Disciplined Convex Programming (DCP) rules to discover convex models directly from data. This ensures the discovered expressions are globally convex by construction, avoiding the need for difficult convexity verification. The approach yields interpretable and flexible convex models that can be more accurate than traditional fixed-form convex functions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] A Unified Generative-Predictive Framework for Deterministic Inverse Design</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [encoder-decoder, generative model, latent manifold, disentanglement, inverse design, KHRONOS head, UMAP, cycle consistency loss]</li>
<li class=""><strong>authors:</strong> Reza T. Batley, Sourav Saha</li>
<li class=""><strong>institution:</strong> Virginia Polytechnic Institute and State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15746" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15746</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Janus, a unified generative-predictive framework that couples a deep encoder-decoder with a predictive KHRONOS head to enable deterministic inverse design of material microstructures. It learns a disentangled latent manifold for both accurate property prediction and stable generative inversion. The method achieves high-fidelity reconstruction and accurate inverse solutions with low computational cost compared to classical optimization approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] How Do Graph Signals Affect Recommendation: Unveiling the Mystery of Low and High-Frequency Graph Signals</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [recommender systems], [spectral graph neural networks, low-frequency signals, high-frequency signals, frequency signal scaler, space flip method, collaborative filtering]</li>
<li class=""><strong>authors:</strong> Feng Liu, Hao Cang, Huanhuan Yuan, Jiaqing Fan, Yongjing Hao, Fuzhen Zhuang, Guanfeng Liu, Pengpeng Zhao</li>
<li class=""><strong>institution:</strong> Soochow University, Suzhou University of Science and Technology, Beihang University, Macquarie University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15744" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15744</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a frequency signal scaler and a space flip method to adjust graph signal filtering in spectral GNNs for recommendation. It theoretically shows that low-frequency and high-frequency graph signals are equally effective in smoothing user-item similarities, and either alone suffices for good recommendations. Experiments on public datasets validate the approach.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [vector quantization, spline networks, memory optimization, cache optimization, hardware-aware compilation]</li>
<li class=""><strong>authors:</strong> Jeff Smith</li>
<li class=""><strong>institution:</strong> 2nd Set AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15742" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15742</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SHARe-KAN, a framework that uses Gain-Shape-Bias Vector Quantization to compress Kolmogorov-Arnold Networks (KANs) by exploiting functional redundancy while preserving their dense, holographic topology. Coupled with a hardware-aware compiler called LUTHAM, it achieves an 88x reduction in runtime memory while matching baseline accuracy, effectively decoupling the workload from DRAM bandwidth constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Hybrid Quantum-Classical Ensemble Learning for S&amp;P 500 Directional Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [financial machine learning], [ensemble learning, quantum sentiment analysis, decision transformer, variational quantum circuit, smart filtering, LSTM, XGBoost, Random Forest, Logistic Regression]</li>
<li class=""><strong>authors:</strong> Abraham Itzhak Weinberg</li>
<li class=""><strong>institution:</strong> AI Experts</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15738" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15738</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a hybrid quantum-classical ensemble learning framework for S&amp;P 500 directional prediction, combining diverse classical models (e.g., Decision Transformer, LSTM) with a 4-qubit quantum circuit for sentiment analysis and strategic model selection. It concludes that architecture diversity is more critical than dataset diversity for ensemble performance, achieving 60.14% directional accuracy and a statistically significant improvement over individual models. Preliminary backtesting suggests the framework has practical trading potential.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-agent AI, semantic retrieval, physics simulation, QuTiP, FreeSim, intent routing, knowledge-augmented generation, dual-mode validation]</li>
<li class=""><strong>authors:</strong> S. K. Rithvik</li>
<li class=""><strong>institution:</strong> Quantum Science and Technology Laboratory, Physical Research Laboratory; Indian Institute of Technology Gandhinagar</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15736" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15736</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Anubuddhi is a multi-agent AI system that designs and simulates quantum optics experiments from natural language prompts using semantic retrieval and physics simulation with dual validation modes. It achieves high design-simulation alignment, demonstrating that flexible simulation frameworks are crucial for diverse quantum optics experiments. The system democratizes computational experiment design, enabling iterative refinement through conversation for research and education.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] LLaDA2.0: Scaling Up Diffusion Language Models to 100B</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [discrete diffusion language model, block-level WSD training, mixture-of-experts, knowledge inheritance, parallel decoding, SFT, DPO]</li>
<li class=""><strong>authors:</strong> Tiwei Bie, Maosong Cao, Kun Chen, Lun Du, Mingliang Gong, Zhuochen Gong, Yanmei Gu, Jiaqi Hu, Zenan Huang, Zhenzhong Lan, Chengxi Li, Chongxuan Li, Jianguo Li, Zehuan Li, Huabin Liu, Ling Liu, Guoshan Lu, Xiaocheng Lu, Yuxin Ma, Jianfeng Tan, Lanning Wei, Ji-Rong Wen, Yipeng Xing, Xiaolu Zhang, Junbo Zhao, Da Zheng, Jun Zhou, Junlin Zhou, Zhanchao Zhou, Liwang Zhu, Yihong Zhuang</li>
<li class=""><strong>institution:</strong> Ant Group, Renmin University of China, Zhejiang University, Westlake University, HongKong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15745" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15745</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces LLaDA2.0, a method for converting pre-trained auto-regressive language models into large-scale discrete diffusion models (dLLMs) using a novel three-phase block-level training scheme. The resulting instruction-tuned models, including a 100B-parameter variant, achieve superior performance and efficiency through parallel decoding. The work establishes a new paradigm for frontier-scale model deployment by enabling efficient scaling and knowledge inheritance from existing models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [CLIP, Stable Diffusion XL, zero-shot classification, demographic bias mitigation, data generation]</li>
<li class=""><strong>authors:</strong> Javon Hickmon</li>
<li class=""><strong>institution:</strong> University of Washington</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15747" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15747</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes D3G, a training-free method that uses Stable Diffusion XL to generate diverse demographic data at inference time to improve zero-shot image classification with CLIP. The method is shown to boost classification accuracy while reducing harmful demographic bias in pre-trained multimodal models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [Direct Preference Optimization (DPO), parameter-efficient fine-tuning, sequence modeling, Monte Carlo simulation]</li>
<li class=""><strong>authors:</strong> Yoonpyo Lee</li>
<li class=""><strong>institution:</strong> Hanyang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15756" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15756</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces ReactorFold, a generative framework that uses language models fine-tuned with Direct Preference Optimization on Monte Carlo simulation data to design nuclear reactor fuel assemblies as a sequence modeling task. The model demonstrates emergent physical reasoning by autonomously adjusting design parameters like gadolinium rod inventory and discovering high-performing asymmetric configurations, transcending human-imposed design constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Surely Large Multimodal Models (Don&#x27;t) Excel in Visual Species Recognition?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [few-shot learning, post-hoc correction, large multimodal models, visual species recognition, prompting, re-ranking]</li>
<li class=""><strong>authors:</strong> Tian Liu, Anwesha Basu, James Caverlee, Shu Kong</li>
<li class=""><strong>institution:</strong> Texas A&amp;M University, University of Macau, Institute of Collaborative Innovation</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15748" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15748</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Post-hoc Correction (POC), a method that uses a Large Multimodal Model (LMM) to re-rank the top predictions from a few-shot learning expert model for Visual Species Recognition. It concludes that while LMMs alone underperform on this specialized task, they can effectively correct expert model errors, and POC significantly boosts few-shot learning accuracy without additional training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] KAN-Matrix: Visualizing Nonlinear Pairwise and Multivariate Contributions for Physical Insight</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [interpretability], [Kolmogorov-Arnold Networks, Pairwise KAN Matrix, Multivariate KAN Contribution Matrix, nonlinear association, feature ranking]</li>
<li class=""><strong>authors:</strong> Luis A. De la Fuente, Hernan A. Moreno, Laura V. Alvarez, Hoshin V. Gupta</li>
<li class=""><strong>institution:</strong> University of Texas, El Paso, The University of Arizona</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15755" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15755</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces KAN-Matrix, a visualization tool using Kolmogorov-Arnold Networks (KANs) to analyze nonlinear pairwise and multivariate contributions in datasets. It presents two interpretable matrices (PKAN and MKAN) that outperform traditional correlation methods in robustness and insight. The approach aids in feature selection and uncovering hidden physical patterns for domain-informed model development.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] A Special Case of Quadratic Extrapolation Under the Neural Tangent Kernel</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [neural networks theory], [neural tangent kernel, ReLU MLP, quadratic extrapolation, reproducing kernel hilbert space, over-parameterized networks]</li>
<li class=""><strong>authors:</strong> Abiel Kim</li>
<li class=""><strong>institution:</strong> Unknown</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15749" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15749</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes the extrapolation behavior of over-parameterized ReLU MLPs under the Neural Tangent Kernel (NTK) regime, focusing on the special case of evaluating a point near the origin. It finds that, in contrast to the known linear extrapolation for points far from the origin, the model extrapolates quadratically when evaluated close to the origin, a distinction arising from the non-translation invariance of the NTK&#x27;s feature map.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] TAO-Net: Two-stage Adaptive OOD Classification Network for Fine-grained Encrypted Traffic Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [transformer, large language model, OOD detection, semantic-enhanced prompt, two-stage classification, PCA residuals, inter-layer smoothness]</li>
<li class=""><strong>authors:</strong> Zihao Wang, Wei Peng, Junming Zhang, Jian Li, Wenxin Fang</li>
<li class=""><strong>institution:</strong> Not specified in provided text</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15753" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15753</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes TAO-Net, a two-stage network for fine-grained encrypted traffic classification. It first uses a hybrid OOD detector to separate known and unknown traffic, then applies an LLM with semantic prompts to classify the unknown traffic without predefined labels. Experiments show it significantly outperforms previous methods in precision and F1 score, especially for identifying new applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] GLOW: Graph-Language Co-Reasoning for Agentic Workflow Performance Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [graph neural networks, large language models, contrastive alignment, instruction tuning, graph-language co-reasoning]</li>
<li class=""><strong>authors:</strong> Wei Guan, Jian Cao, Jinyu Cai, Qiqi Cai, Jianqi Gao, See-Kiong Ng</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, National University of Singapore, Shanghai University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15751" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15751</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes GLOW, a framework that combines Graph Neural Networks (GNNs) and instruction-tuned Large Language Models (LLMs) to predict the performance of Agentic Workflows by jointly modeling their graph structure and semantic logic. It uses a contrastive alignment strategy to refine the feature space. Experiments show GLOW outperforms existing methods in prediction accuracy and ranking utility on the FLORA-Bench benchmark.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Twin Restricted Kernel Machines for Multiview Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multi-view learning], [twin restricted kernel machine, regularized least squares, kernel trick, early fusion, late fusion]</li>
<li class=""><strong>authors:</strong> A. Quadir, M. Sajid, Mushir Akhtar, M. Tanveer</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology Indore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15757" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15757</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a Twin Multiview Restricted Kernel Machine (TMvRKM), which uses a regularized least squares approach instead of quadratic programming to efficiently find an optimal separating hyperplane for multi-view classification. It incorporates a coupling term and fusion strategies to balance errors and leverage information across views. Experimental results show it outperforms baseline models in generalization performance on benchmark datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Yantra AI -- An intelligence platform which interacts with manufacturing operations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [random forest classifier, isolation forest, gpt-4, streamlit, predictive maintenance, real-time data visualization]</li>
<li class=""><strong>authors:</strong> Varshini Krishnamurthy</li>
<li class=""><strong>institution:</strong> Department of Computer Science (implied from first page)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15758" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15758</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops an AI-powered intelligence platform for manufacturing, integrating machine learning models like Random Forest and Isolation Forest for predictive maintenance and decision support, alongside a GPT-4 virtual assistant. The system, tested with synthetic data and featuring real-time dashboards via Streamlit, demonstrates improvements in operational efficiency and energy management. Future work will focus on real-time data integration and further system enhancements.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Semantic-Constrained Federated Aggregation: Convergence Theory and Privacy-Utility Bounds for Knowledge-Enhanced Distributed Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, semantic constraints, knowledge graphs, differential privacy, convergence theory, non-IID data]</li>
<li class=""><strong>authors:</strong> Jahidul Arafat</li>
<li class=""><strong>institution:</strong> Auburn University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15759" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15759</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Semantic-Constrained Federated Aggregation (SCFA), a framework that incorporates domain knowledge constraints into federated learning to address slow convergence under non-IID data. It theoretically proves SCFA&#x27;s convergence and demonstrates that these constraints improve privacy-utility trade-offs and reduce model divergence. Empirical validation on manufacturing data shows faster convergence and reduced performance degradation compared to standard federated learning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] A Tutorial on Dimensionless Learning: Geometric Interpretation and the Effect of Noise</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [physics-informed machine learning], [dimensionless learning, dimensional analysis, neural networks, regularization, scaling laws]</li>
<li class=""><strong>authors:</strong> Zhengtao Jake Gan, Xiaoyu Xie</li>
<li class=""><strong>institution:</strong> Arizona State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15760" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15760</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a data-driven framework called dimensionless learning, which combines classical dimensional analysis with neural networks to discover dimensionless numbers and scaling laws from experimental data. A key innovation is a regularization technique that encourages interpretable, simple coefficients (like integers) in the learned laws. The method is shown to be robust to measurement noise and can reveal compact physical relationships from data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Machine Learning Framework for Thrombosis Risk Prediction in Rotary Blood Pumps</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [machine learning, logistic regression, computational fluid dynamics, feature selection, thrombosis risk prediction]</li>
<li class=""><strong>authors:</strong> Christopher Blum, Michael Neidlin</li>
<li class=""><strong>institution:</strong> RWTH Aachen University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15761" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15761</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an interpretable machine learning framework that uses logistic regression and a feature-selection pipeline on computational fluid dynamics data to predict thrombosis risk in rotary blood pumps. The model successfully identifies flow features linked to thrombus formation and generalizes to predict risk in a different pump design. The approach provides a computationally efficient and transparent method for thrombogenicity screening in medical device design.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Data Valuation for LLM Fine-Tuning: Efficient Shapley Value Approximation via Language Model Arithmetic</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [Shapley value, Direct Preference Optimization (DPO), data valuation, cooperative game theory, language model arithmetic]</li>
<li class=""><strong>authors:</strong> Mélissa Tamine, Otmane Sakhi, Benjamin Heymann</li>
<li class=""><strong>institution:</strong> Criteo AI Lab, Fairplay joint team</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15765" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15765</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an efficient method for computing Shapley values to value data contributions for LLM fine-tuning. It leverages the specific mathematical structure of Direct Preference Optimization (DPO) to enable scalable Shapley value approximation without requiring numerous model retrainings. The main conclusion is that this approach dramatically simplifies the computational challenge of data valuation for LLMs, unlocking applications in data markets and collaborative training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] AdaGradSelect: An adaptive gradient-guided layer selection method for efficient fine-tuning of SLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [AdaGradSelect, gradient-guided selection, Dirichlet-based sampling, epsilon-greedy exploration, selective block update, Parameter-Efficient Fine-Tuning (PEFT)]</li>
<li class=""><strong>authors:</strong> Anshul Kumar, Gagan Raj Gupta, Manisha Chawla</li>
<li class=""><strong>institution:</strong> IIT Bhilai</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15764" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15764</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces AdaGradSelect, an adaptive method for efficiently fine-tuning Small Language Models (SLMs) by selecting which transformer blocks to update based on gradient norms, using a combination of Dirichlet-based sampling and epsilon-greedy exploration. It achieves performance close to full fine-tuning while training about 12% faster and using 35% less GPU memory, outperforming methods like LoRA on benchmarks such as GSM8K.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Cross-Sample Augmented Test-Time Adaptation for Personalized Intraoperative Hypotension Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [healthcare, medical time series], [test-time adaptation, cross-sample augmentation, K-Shape clustering, masked reconstruction, sequence forecasting]</li>
<li class=""><strong>authors:</strong> Kanxue Li, Yibing Zhan, Hua Jin, Chongchong Qi, Xu Lin, Baosheng Yu</li>
<li class=""><strong>institution:</strong> Wuhan University, First People’s Hospital of Yunnan Province, Yunnan United Vision Technology Company Limited, Nanyang Technological University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15762" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15762</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes CSA-TTA, a Cross-Sample Augmented Test-Time Adaptation framework for personalized intraoperative hypotension prediction. It enhances test-time training by retrieving similar hypotension events from other patients using a coarse-to-fine strategy and integrates self-supervised signals for adaptation. The method improves prediction recall and F1 scores on real-world datasets, demonstrating strong robustness and generalization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] PHANTOM: Progressive High-fidelity Adversarial Network for Threat Object Modeling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [adversarial variational framework, progressive training, dual-path VAE-GAN, domain-specific feature matching, synthetic data generation]</li>
<li class=""><strong>authors:</strong> Jamal Al-Karaki, Muhammad Al-Zafar Khan, Rand Derar Mohammad Al Athamneh</li>
<li class=""><strong>institution:</strong> Zayed University, The Hashemite University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15768" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15768</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces PHANTOM, a progressive adversarial variational framework that uses a dual-path VAE-GAN architecture with domain-specific feature matching to generate high-fidelity synthetic cyberattack data. The method achieves 98% weighted accuracy in intrusion detection when models are trained on its synthetic data, demonstrating its effectiveness for augmenting training datasets, though it faces challenges with generating rare attack types.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Bridging Data and Physics: A Graph Neural Network-Based Hybrid Twin Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Graph Neural Networks, Hybrid Twin, Finite Element Method, ignorance model, nonlinear heat transfer]</li>
<li class=""><strong>authors:</strong> M. Gorpinich, B. Moya, S. Rodriguez, F. Meraghni, Y. Jaafra, A. Briot, M. Henner, R. Leon, F. Chinesta</li>
<li class=""><strong>institution:</strong> Valeo, PIMM Lab. ENSAM Institute of Technology, CNRS@CREATE LTD.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15767" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15767</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid twin framework that uses Graph Neural Networks (GNNs) to model the discrepancy (&quot;ignorance&quot;) between physics-based simulations (e.g., FEM) and reality, correcting the model with sparse data. The method is evaluated on nonlinear heat transfer problems across different geometries and meshes. Results show the GNN successfully captures and generalizes the corrections, improving simulation accuracy while minimizing data requirements.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] TENG++: Time-Evolving Natural Gradient for Solving PDEs With Deep Neural Nets under General Boundary Conditions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Physics-Informed Neural Networks (PINNs), Time-Evolving Natural Gradient (TENG), Dirichlet boundary conditions, natural gradient optimization, Euler method, Heun method, heat equation]</li>
<li class=""><strong>authors:</strong> Xinjie He, Chenggong Zhang</li>
<li class=""><strong>institution:</strong> University of California, Los Angeles</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15771" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15771</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper extends the Time-Evolving Natural Gradient (TENG) framework to solve PDEs with deep neural networks, specifically by incorporating penalty terms to enforce Dirichlet boundary conditions and integrating numerical time-stepping schemes like Euler and Heun methods. Experiments on the heat equation show that the Heun method provides superior accuracy due to its second-order corrections, while the Euler method is computationally efficient for simpler scenarios. The work establishes a foundation for handling more complex boundary conditions and advancing neural network-based PDE solvers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Data-Chain Backdoor: Do You Trust Diffusion Models as Generative Data Supplier?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [Data-Chain Backdoor (DCB), clean-label attack, Early-Stage Trigger Manifestation (ESTM), backdoor triggers, synthetic data generation]</li>
<li class=""><strong>authors:</strong> Junchi Lu, Xinke Li, Yuheng Liu, Qi Alfred Chen</li>
<li class=""><strong>institution:</strong> University of California, Irvine, City University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15769" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15769</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates the Data-Chain Backdoor (DCB) threat, where backdoor triggers are injected into and propagated through open-source diffusion models used for synthetic data generation. The method reveals that these triggers are memorized and reproduced in the generated data, subsequently poisoning downstream models, even in clean-label attack scenarios. The main conclusion is that this poses a severe, previously underexplored security risk in generative data pipelines, highlighting the need for mitigation strategies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] TS-DP: Reinforcement Speculative Decoding For Temporal Adaptive Diffusion Policy Acceleration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [speculative decoding, reinforcement learning, knowledge distillation, transformer, temporal adaptivity]</li>
<li class=""><strong>authors:</strong> Ye Li, Jiahe Feng, Yuan Meng, Kangye Ji, Chen Tang, Xinwan Wen, Shutao Xia, Zhi Wang, Wenwu Zhu</li>
<li class=""><strong>institution:</strong> Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15773" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15773</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes TS-DP, a framework that accelerates Diffusion Policy inference using speculative decoding. It employs a distilled Transformer-based drafter to generate denoising steps and an RL-based scheduler to dynamically adapt to time-varying task difficulty. The method achieves up to 4.17x faster inference with high accuracy, enabling real-time diffusion-based control.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Two-Step Data Augmentation for Masked Face Detection and Recognition: Turning Fake Masks to Real</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [generative adversarial networks, data augmentation, image-to-image translation, rule-based mask warping, non-mask preservation loss]</li>
<li class=""><strong>authors:</strong> Yan Yang, George Bebis, Mircea Nicolescu</li>
<li class=""><strong>institution:</strong> University of Nevada, Reno</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15774" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15774</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a two-step data augmentation method for masked face detection and recognition, combining rule-based mask warping with GAN-based unpaired image-to-image translation to generate realistic masked faces. It introduces a non-mask preservation loss and stochastic noise injection to improve training and diversity. The approach shows qualitative improvements over rule-based methods alone and complements other GAN-based generation techniques.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [adversarial robustness], [adversarial training, FGSM, PGD, SHAP, Expected Calibration Error (ECE), Value-at-Risk (VaR), Expected Shortfall (ES), bootstrap inference]</li>
<li class=""><strong>authors:</strong> Samruddhi Baviskar</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15780" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15780</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a dataset-agnostic pipeline to evaluate adversarial robustness in tabular financial ML models using gradient-based attacks like FGSM and PGD. It finds that small perturbations significantly degrade model performance and increase financial risk, but adversarial training can partially recover utility. The study also suggests SHAP stability as an early-warning indicator for adversarial influence.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] AI Epidemiology: achieving explainable AI through expert oversight patterns</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [AI governance and interpretability], [AI Epidemiology, population-level surveillance, expert oversight patterns, exposure variables, risk level, alignment score, accuracy score]</li>
<li class=""><strong>authors:</strong> Kit Tempest-Walters</li>
<li class=""><strong>institution:</strong> Logia Compliance</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15783" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15783</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces AI Epidemiology, a framework that applies epidemiological surveillance methods to AI outputs by standardizing expert-AI interactions into structured assessment fields to predict failures. This approach bypasses model complexity issues, provides automatic audit trails, and enables domain experts to govern AI systems without requiring machine learning expertise.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Hyperparameter Tuning-Based Optimized Performance Analysis of Machine Learning Algorithms for Network Intrusion Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hyperparameter tuning, grid search, random search, recursive feature elimination, support vector machine, KDD CUP 1999 dataset, tenfold cross-validation]</li>
<li class=""><strong>authors:</strong> Sudhanshu Sekhar Tripathy, Bichitrananda Behera</li>
<li class=""><strong>institution:</strong> C.V. Raman Global University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15779" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15779</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates multiple machine learning algorithms for network intrusion detection, applying hyperparameter tuning via grid and random search along with Recursive Feature Elimination. The optimized Support Vector Machine classifier achieved the highest accuracy of 99.12% on the KDD CUP 1999 dataset, demonstrating that systematic tuning significantly enhances detection performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Auto-Tuning Safety Guardrails for Black-Box Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [hyperparameter optimization, system prompts, content filters, jailbreak detection, malware generation, Optuna, ModernBERT, grid search]</li>
<li class=""><strong>authors:</strong> Perry Abdulkadir</li>
<li class=""><strong>institution:</strong> University of St. Thomas</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15782" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15782</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes treating the design of safety guardrails (like system prompts and content filters) for a frozen black-box LLM as a hyperparameter optimization problem. Using a proof-of-concept with Mistral-7B-Instruct and ModernBERT, it shows that a black-box optimizer (Optuna) can efficiently find safe configurations, matching the best grid search results with far fewer evaluations and less time. The conclusion is that this auto-tuning approach is a feasible method to harden LLM deployments under practical constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] RAMBO: Reliability Analysis for Mamba through Bit-flip attack Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [bit-flip attack, state-space models, Mamba, hardware faults, adversarial robustness]</li>
<li class=""><strong>authors:</strong> Sanjay Das, Swastik Bhattacharya, Shamik Kundu, Arnab Raha, Souvik Kundu, Kanad Basu</li>
<li class=""><strong>institution:</strong> University of Texas at Dallas, Intel Corporation, Rensselaer Polytechnic Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15778" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15778</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces RAMBO, a framework for conducting bit-flip attacks on Mamba-based state-space models to analyze their reliability. It demonstrates that flipping a single critical bit can catastrophically degrade model performance, reducing accuracy to 0% and drastically increasing perplexity. The results highlight the pronounced vulnerability of these efficient sequence models to hardware-level adversarial perturbations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Enhanced Web User Interface Design Via Cross-Device Responsiveness Assessment Using An Improved HCI-INTEGRATED DL Schemes</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Finite Exponential Continuous State Machine (FECSM), Quokka Nonlinear Difference Swarm Optimization Algorithm (QNDSOA), Bidirectional Gated Luong and Mish Recurrent Unit (BiGLMRU), HDBSCAN, min-max normalization, User Interface Change Prediction Index (UICPI)]</li>
<li class=""><strong>authors:</strong> Shrinivass Arunachalam Balasubramanian</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15775" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15775</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a dynamic web UI optimization method that uses a Finite Exponential Continuous State Machine for cross-device responsiveness assessment and a novel Quokka Nonlinear Difference Swarm Optimization Algorithm for design optimization. The core technique involves classifying user experience changes with a Bidirectional Gated Luong and Mish Recurrent Unit model. The main conclusion is that this integrated approach achieves an average fitness of 98.5632% for optimal UI design by incorporating cross-responsiveness assessment and user behavior patterns.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [memory-centric agent, profile memory, experience memory, action memory, distance-graph (DisGraph), agent record-and-replay (AgentRR)]</li>
<li class=""><strong>authors:</strong> Zibin Liu, Cheng Zhang, Xi Zhao, Yunfei Feng, Bingyu Bai, Dahu Feng, Erhu Feng, Yubin Xia, Haibo Chen</li>
<li class=""><strong>institution:</strong> Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15784" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15784</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes MOBIMEM, a memory-centric agent system that enables self-evolution of LLM agents without model retraining by using three specialized memory primitives (Profile, Experience, and Action Memory) and OS-inspired orchestration services. It demonstrates significant improvements in profile alignment, task success rates, and latency reduction on mobile devices compared to baseline approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] An empirical analysis of zero-day vulnerabilities disclosed by the zero day initiative</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [CVSS, vulnerability classification, machine learning, deep learning, predictive modeling]</li>
<li class=""><strong>authors:</strong> Apurva Shet, Izzat Alsmadi</li>
<li class=""><strong>institution:</strong> Texas A&amp;M University - San Antonio</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15803" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15803</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes zero-day vulnerabilities disclosed by the Zero Day Initiative, employing machine learning and deep learning models on structured metadata and textual descriptions to classify severity. It aims to identify trends and characteristics indicative of high-severity vulnerabilities. The findings are intended to improve patch prioritization and vulnerability management strategies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Secure AI-Driven Super-Resolution for Real-Time Mixed Reality Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [point cloud super-resolution, attribute-based encryption, downsampling, upscaling]</li>
<li class=""><strong>authors:</strong> Mohammad Waquas Usmani, Sankalpa Timilsina, Michael Zink, Susmit Shannigrahi</li>
<li class=""><strong>institution:</strong> University of Massachusetts Amherst, Tennessee Technological University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15823" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15823</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a system to reduce latency in AR/VR streaming by downsampling and partially encrypting point cloud content at the server, then using a machine learning-based super-resolution model to reconstruct it at the client. The evaluation shows this approach effectively reduces bandwidth and encryption overhead while accurately reconstructing the original point clouds with minimal error.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Boosting t-SNE Efficiency for Sequencing Data: Insights from Kernel Selection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [bioinformatics], [t-SNE, kernel selection, cosine similarity kernel, One-Hot Encoding, Spike2Vec, minimizers]</li>
<li class=""><strong>authors:</strong> Avais Jan, Prakash Chourasia, Sarwan Ali, Murray Patterson</li>
<li class=""><strong>institution:</strong> Georgia State University, Columbia University Irving Medical Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15900" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15900</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates nine different kernel functions for the t-SNE dimensionality reduction technique when applied to biological sequence data. It finds that the cosine similarity kernel generally outperforms others, including the traditional Gaussian kernel, by providing better runtime efficiency and more accurate preservation of pairwise distances in visualizations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Introduction to Symbolic Regression in the Physical Sciences</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [symbolic regression], [symbolic regression, automated equation discovery, effective theories, surrogate models, complexity control, feature selection]</li>
<li class=""><strong>authors:</strong> Deaglan J. Bartlett, Harry Desmond, Pedro G. Ferreira, Gabriel Kronberger</li>
<li class=""><strong>institution:</strong> University of Oxford, University of Portsmouth, University of Applied Sciences Upper Austria</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15920" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15920</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces symbolic regression (SR) as a method for discovering interpretable mathematical equations from data in the physical sciences. It reviews SR&#x27;s foundations, applications, methodological considerations, and challenges. The authors conclude that SR is a rapidly advancing and increasingly relevant tool for scientific discovery and empirical modeling.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] DSO: Direct Steering Optimization for Bias Mitigation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [fairness and bias mitigation], [activation steering, reinforcement learning, linear transformations, inference-time control]</li>
<li class=""><strong>authors:</strong> Lucas Monteiro Paes, Nivedha Sivakumar, Yinong Oliver Wang, Masha Fedzechkina Donaldson, Luca Zappella, Nicholas Apostoloff</li>
<li class=""><strong>institution:</strong> Apple, Carnegie Mellon University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15926" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15926</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Direct Steering Optimization (DSO), a method using reinforcement learning to find linear transformations for steering activations in generative models to mitigate bias while maintaining performance. It demonstrates state-of-the-art trade-offs between fairness and capabilities in VLMs and LLMs, offering inference-time control over bias reduction. The work highlights the advantage of directly optimized steering strategies over heuristic-based approaches for effective bias intervention.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] A Unification of Discrete, Gaussian, and Simplicial Diffusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [generative models], [diffusion models, Wright-Fisher model, simplicial diffusion, Gaussian diffusion, discrete diffusion]</li>
<li class=""><strong>authors:</strong> Nuria Alina Chandra, Yucen Lily Li, Alan N. Amin, Alex Ali, Joshua Rollins, Sebastian W. Ober, Aniruddh Raghu, Andrew Gordon Wilson</li>
<li class=""><strong>institution:</strong> New York University, CUNY, BigHat Biosciences</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15923" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15923</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper unifies three major discrete sequence diffusion methods—discrete, Gaussian, and simplicial diffusion—by framing them as different parameterizations of the Wright-Fisher population genetics model. It shows that simplicial and Gaussian diffusion emerge as large-population limits, enabling stable simplicial diffusion and allowing a single model to perform diffusion in any of the three domains at test time. The proposed Wright-Fisher simplicial diffusion is shown to be more stable and outperform previous methods on conditional DNA generation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] BarcodeMamba+: Advancing State-Space Models for Fungal Biodiversity Research</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [bioinformatics], [state-space model, pretrain and fine-tune, hierarchical label smoothing, weighted loss function, multi-head output layer]</li>
<li class=""><strong>authors:</strong> Tiancheng Gao, Scott C. Lowe, Brendan Furneaux, Angel X Chang, Graham W. Taylor</li>
<li class=""><strong>institution:</strong> University of Guelph, Vector Institute, University of Jyväskylä, Simon Fraser University, Amii</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15931" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15931</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces BarcodeMamba+, a foundation model for fungal DNA barcode classification built on a state-space model architecture. It employs a pretrain and fine-tune paradigm with enhancements like hierarchical label smoothing to address sparse labeling and long-tailed distributions. The model outperforms existing methods across taxonomic levels, providing an effective tool for genomics-based biodiversity research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] In-Context Semi-Supervised Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [in-context learning], [in-context semi-supervised learning, Transformers, representation learning, functional gradient descent, RKHS]</li>
<li class=""><strong>authors:</strong> Jiashuo Fan, Paul Rosu, Aaron T. Wang, Michael Li, Lawrence Carin, Xiang Cheng</li>
<li class=""><strong>institution:</strong> Duke University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15934" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15934</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces in-context semi-supervised learning (IC-SSL), a method where Transformers leverage both a few labeled examples and many unlabeled points within their context to learn a robust, context-dependent representation. The authors demonstrate that this approach enables accurate predictions and significantly improves performance in low-label regimes, providing insights into how Transformers exploit unlabeled data for representation learning in-context.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Social Story Frames: Contextual Reasoning about Narrative Intent and Reception</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [natural language processing], [SocialStoryFrames, SSF-Generator, SSF-Classifier, narrative theory, linguistic pragmatics, taxonomy, reader response]</li>
<li class=""><strong>authors:</strong> Joel Mire, Maria Antoniak, Steven R. Wilson, Zexin Ma, Achyutarama R. Ganti, Andrew Piper, Maarten Sap</li>
<li class=""><strong>institution:</strong> Carnegie Mellon University, University of Colorado Boulder, University of Michigan-Flint, University of Connecticut, McGill University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15925" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15925</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SocialStoryFrames, a formalism and computational framework for modeling nuanced reader responses to social media stories, including inferences about author intent and affective reactions. It develops two models (SSF-Generator and SSF-Classifier) and applies them to a corpus of online narratives to analyze storytelling practices across communities. The main conclusion is that this approach enables scalable, context-sensitive research into the social dynamics of online storytelling.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] SALVE: Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [mechanistic interpretability], [sparse autoencoder, ℓ1-regularization, Grad-FAM, weight-space editing, critical suppression threshold]</li>
<li class=""><strong>authors:</strong> Vegard Flovik</li>
<li class=""><strong>institution:</strong> DNV</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15938" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15938</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SALVE, a framework that uses an ℓ1-regularized sparse autoencoder to discover and validate latent features in neural networks, then performs precise weight-space edits to control model behavior. It demonstrates consistent control across ResNet-18 and ViT models, providing a methodology to turn feature discovery into actionable model edits for more transparent AI systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] AIE4ML: An End-to-End Framework for Compiling Neural Networks for the Next Generation of AMD AI Engines</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [AI Engine (AIE), GEMV, quantization, on-chip data movement, graph placement and search, low-latency inference, fused bias addition and ReLU]</li>
<li class=""><strong>authors:</strong> Dimitrios Danopoulos, Enrico Lupi, Chang Sun, Sebastian Dittmeier, Michael Kagan, Vladimir Loncar, Maurizio Pierini</li>
<li class=""><strong>institution:</strong> European Organization for Nuclear Research (CERN), Institute of Physics Belgrade, Heidelberg University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15946" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15946</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents AIE4ML, an end-to-end framework that automatically compiles neural networks into optimized firmware for AMD&#x27;s AI Engine-ML accelerators. It achieves high efficiency through a structured parallelization method that scales across the 2D fabric and uses a novel graph placement algorithm for on-chip execution. The framework delivers GPU-class throughput with microsecond latency, making it suitable for ultra-low-latency applications like particle physics trigger systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Governance by Evidence: Regulated Predictors in Decision-Tree Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [privacy and governance], [decision-tree models, regulated data categories, privacy laws, data extraction, legal compliance]</li>
<li class=""><strong>authors:</strong> Alexios Veskoukis, Dimitris Kalles</li>
<li class=""><strong>institution:</strong> Hellenic Open University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15955" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15955</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes published decision-tree studies to map the use of predictors that fall under regulated data categories, such as health or financial data, by linking them to specific EU and US privacy laws. The study finds that a significant portion of reported predictors are legally regulated, with the highest prevalence in healthcare, highlighting potential legal risks. The evidence supports the need for privacy-preserving methods and governance checks in machine learning practice.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Tracking Wildfire Assets with Commodity RFID and Gaussian Process Modeling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Gaussian Process, weighted log-likelihood, RF signal response signatures, model dictionary, passive commodity RFID]</li>
<li class=""><strong>authors:</strong> John Hateley, Sriram Narasimhan, Omid Abari</li>
<li class=""><strong>institution:</strong> University of California, Los Angeles</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15956" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15956</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method for tracking wildfire assets using commodity RFID and Gaussian Process modeling to localize tags in forested environments without needing known reference locations. It matches unknown RF signals to a dictionary of pre-modeled environments using a weighted log-likelihood method. The approach achieves GPS-level localization accuracy at a lower cost, enabling scalable, real-time tracking of numerous assets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [reinforcement learning, low-rank approximation, dynamic rank selection, matrix perturbation theory, singular value decomposition]</li>
<li class=""><strong>authors:</strong> Caner Erden</li>
<li class=""><strong>institution:</strong> Sakarya University of Applied Sciences</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15973" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15973</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Dynamic Rank Reinforcement Learning (DR-RL), a framework that uses a reinforcement learning agent to dynamically select low-rank approximations for Multi-Head Self-Attention in LLMs during inference, balancing accuracy and computational cost. It employs online matrix perturbation theory for efficient updates. Experiments show the method maintains accuracy equivalent to full-rank attention while significantly reducing FLOPs, especially for long sequences.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Provably Extracting the Features from a General Superposition</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [feature learning], [superposition, query algorithm, Fourier space search, overcomplete regime]</li>
<li class=""><strong>authors:</strong> Allen Liu</li>
<li class=""><strong>institution:</strong> UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15987" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15987</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents an efficient query algorithm that searches in Fourier space to recover hidden feature directions from a general superposition function. It successfully identifies all non-degenerate features and reconstructs the function, working under more general conditions than prior methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Time-Frequency Analysis for Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [approximation theory], [time-frequency analysis, modulation spaces, Sobolev norms, shallow neural networks, Barron spaces]</li>
<li class=""><strong>authors:</strong> Ahmed Abdeljawad, Elena Cordero</li>
<li class=""><strong>institution:</strong> Johann Radon Institute of Computational and Applied Mathematics (RICAM), Austrian Academy of Sciences; Università degli Studi di Torino</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15992" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15992</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops a quantitative approximation theory for shallow neural networks using time-frequency analysis and weighted modulation spaces. It proves dimension-independent approximation rates in Sobolev norms for networks combining standard activations with localized time-frequency windows, achieving better performance than standard ReLU networks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Hierarchical Neural Surfaces for 3D Mesh Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [3D geometry processing], [implicit neural representations, spherical parameterization, displacement vector field, hierarchical structure, spherical harmonics]</li>
<li class=""><strong>authors:</strong> Sai Karthikey Pentapati, Gregoire Phillips, Alan Bovik</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin, Ericsson Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15985" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15985</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a method for 3D mesh compression using a hierarchical implicit neural representation (INR) built upon a spherical parameterization of the mesh. The INR encodes a displacement vector field to reconstruct the original shape, first recovering coarse structure and then adding high-frequency details. The approach achieves a state-of-the-art trade-off between reconstruction quality and compressed representation size, enabling real-time decoding of meshes at arbitrary resolutions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Surrogate Neural Architecture Codesign Package (SNAC-Pack)</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [neural architecture search, FPGA deployment, hardware-aware optimization, surrogate modeling, multi-objective optimization]</li>
<li class=""><strong>authors:</strong> Jason Weitz, Dmitri Demler, Benjamin Hawks, Nhan Tran, Javier Duarte</li>
<li class=""><strong>institution:</strong> University of California San Diego, Fermi National Accelerator Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15998" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15998</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SNAC-Pack, a framework that automates neural network design for FPGA deployment by combining architecture search with a resource and latency estimator, avoiding time-intensive synthesis for each candidate. It demonstrates the framework on a physics classification task, achieving competitive accuracy and resource efficiency on an FPGA, showing the potential of hardware-aware NAS for constrained environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Higher-Order LaSDI: Reduced Order Modeling with Multiple Time Derivatives</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [reduced-order models, latent space dynamics identification, higher-order finite-difference, rollout loss, parameterized PDEs]</li>
<li class=""><strong>authors:</strong> Robert Stephany, William Michael Anderson, Youngsoo Choi</li>
<li class=""><strong>institution:</strong> Lawrence Livermore National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15997" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15997</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Higher-Order LaSDI (HLaSDI), a framework that extends the Latent Space Dynamics Identification method to handle partial differential equations with arbitrary time derivative orders. It employs a flexible finite-difference scheme and a Rollout loss to improve long-term prediction accuracy. The method demonstrates improved accuracy and efficiency on benchmark problems, expanding the applicability of reduced-order modeling to systems like hyperbolic PDEs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Information theory and discriminative sampling for model discovery</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [dynamical systems], [Fisher Information Matrix, SINDy, sparse identification, bagging, entropy]</li>
<li class=""><strong>authors:</strong> Yuxuan Bao, J. Nathan Kutz</li>
<li class=""><strong>institution:</strong> University of Washington</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16000" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16000</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper uses the Fisher Information Matrix within the SINDy framework to quantify the information content in different segments of dynamical system trajectories. It demonstrates that prioritizing more informative data through information-theoretic metrics improves sampling efficiency and model performance in data-driven model discovery.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Towards Fine-Tuning-Based Site Calibration for Knowledge-Guided Machine Learning: A Summary of Results</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [knowledge-guided machine learning, fine-tuning, transfer learning, site calibration, spatial heterogeneity, pretraining, agroecosystem carbon cycle]</li>
<li class=""><strong>authors:</strong> Ruolei Zeng, Arun Sharma, Shuai An, Mingzhou Yang, Shengya Zhang, Licheng Liu, David Mulla, Shashi Shekhar</li>
<li class=""><strong>institution:</strong> University of Minnesota, Twin Cities</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16013" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16013</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FTBSC-KGML, a framework that enhances knowledge-guided machine learning by incorporating a pretraining and fine-tuning process with site-specific parameters to better capture spatial variability. It uses a globally pretrained model that is fine-tuned per site or state to improve local accuracy for land emissions estimation. The method achieves lower validation error and more consistent explanatory power than a purely global model.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Explainable AI in Big Data Fraud Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [LIME, SHAP, counterfactual explanations, attention mechanisms, anomaly detection, graph-based models, ensemble classifiers]</li>
<li class=""><strong>authors:</strong> Ayush Jain, Rahul Kulkarni, Siyi Lin</li>
<li class=""><strong>institution:</strong> Northeastern University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16037" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16037</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper reviews how explainable AI (XAI) methods like LIME and SHAP can be integrated into Big Data analytics pipelines for fraud detection. It concludes by proposing a conceptual framework to address scalability and real-time processing challenges, and identifies open research directions in scalable and privacy-aware XAI.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Techno-economic optimization of a heat-pipe microreactor, part I: theory and cost optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [reinforcement learning, Gaussian processes, multi-layer perceptrons, surrogate models, levelized cost of electricity, techno-economic analysis]</li>
<li class=""><strong>authors:</strong> Paul Seurin, Dean Price, Luis Nunez</li>
<li class=""><strong>institution:</strong> Idaho National Laboratory, Massachusetts Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16032" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16032</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a reinforcement learning-based optimization framework using surrogate models (Gaussian processes and multi-layer perceptrons) to minimize the levelized cost of electricity for a heat-pipe microreactor design. The method successfully reduces costs by over 57% by adjusting design parameters while satisfying key physical and safety constraints. The main cost drivers identified are operation and maintenance costs, capital costs, and specifically the costs of axial reflectors and control drum materials.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [causal representation learning], [causal spatio-temporal learning, causal discovery, directed acyclic graph, streamflow forecasting, hydrological modeling]</li>
<li class=""><strong>authors:</strong> Shu Wan, Reepal Shah, John Sabo, Huan Liu, K. Selçuk Candan</li>
<li class=""><strong>institution:</strong> Arizona State University, Tulane University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16046" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16046</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes CauSTream, a framework that jointly learns a causal graph for meteorological forcings and a spatio-temporal routing graph for streamflow forecasting. It establishes identifiability conditions for these structures and demonstrates that the model outperforms state-of-the-art methods, especially for longer forecast horizons, while providing interpretable causal graphs aligned with domain knowledge.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] In-Context Multi-Operator Learning with DeepOSets</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [scientific machine learning], [DeepOSets, in-context learning, operator learning, DeepONets, DeepSets, PDE, universal approximation]</li>
<li class=""><strong>authors:</strong> Shao-Ting Chiu, Aditya Nambiar, Ali Syed, Jonathan W. Siegel, Ulisses Braga-Neto</li>
<li class=""><strong>institution:</strong> Texas A&amp;M University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16074" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16074</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces DeepOSets, a neural architecture combining DeepSets and DeepONets, as a multi-operator in-context learner for parametric PDEs. It demonstrates that a single DeepOSets model can approximate any continuous operator in a class from in-context examples without weight updates, and validates this on forward and inverse boundary-value problems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [schema filtering, functional dependency graph, graph transformer, Steiner-tree heuristic, query-aware LLM encoder]</li>
<li class=""><strong>authors:</strong> Thanh Dat Hoang, Thanh Tam Nguyen, Thanh Trung Huynh, Hongzhi Yin, Quoc Viet Hung Nguyen</li>
<li class=""><strong>institution:</strong> Griffith University, VinUniversity, The University of Queensland</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16083" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16083</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces GRAST-SQL, a framework for scaling Text2SQL systems by efficiently filtering and compacting database schemas before prompting an LLM. It uses a query-aware LLM encoder, a graph transformer over functional dependencies, and a Steiner-tree heuristic to select a relevant, connectivity-preserving sub-schema. The method achieves high recall and precision while maintaining low latency and scaling to schemas with over 23,000 columns.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] FOD-Diff: 3D Multi-Channel Patch Diffusion Model for Fiber Orientation Distribution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [3D multi-channel patch diffusion model, FOD-patch adapter, voxel-level conditional coordinating module, SH attention module, spherical harmonics]</li>
<li class=""><strong>authors:</strong> Hao Tang, Hanyu Liu, Alessandro Perelli, Xi Chen, Chao Li</li>
<li class=""><strong>institution:</strong> University of Dundee, University of Bath, University of Cambridge</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16075" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16075</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes FOD-Diff, a 3D multi-channel patch diffusion model that predicts high angular resolution fiber orientation distributions (HAR-FOD) from low angular resolution inputs. The method incorporates a patch adapter using brain anatomy priors, a conditional coordinating module, and a spherical harmonics attention module to handle complex coefficient correlations. The experimental results demonstrate that this approach achieves state-of-the-art performance in HAR-FOD prediction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Privacy Blur: Quantifying Privacy and Utility for Image Data Release</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision, privacy], [Gaussian blur, pixelization, DP-Pix, cropping, privacy-utility tradeoff, reversal attacks, discrimination attacks]</li>
<li class=""><strong>authors:</strong> Saeed Mahloujifar, Narine Kokhlikyan, Chuan Guo, Kamalika Chaudhuri</li>
<li class=""><strong>institution:</strong> FAIR at Meta</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16086" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16086</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates the privacy and utility trade-offs of four image obfuscation methods (Gaussian blur, pixelization, DP-Pix, and cropping) for responsible data release. It finds that the standard Gaussian blur is the least private due to being susceptible to reversal attacks, while pixelization and DP-Pix, at appropriate granularity, can offer both privacy and utility for computer vision tasks. The authors release their methods in a software package called Privacy Blur.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [SageAttention, Sparse-Linear Attention (SLA), rCM, W8A8 quantization, step distillation]</li>
<li class=""><strong>authors:</strong> Jintao Zhang, Kaiwen Zheng, Kai Jiang, Haoxu Wang, Ion Stoica, Joseph E. Gonzalez, Jianfei Chen, Jun Zhu</li>
<li class=""><strong>institution:</strong> Tsinghua University, Shengshu Technology, UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16093" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16093</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TurboDiffusion is a framework that accelerates video diffusion models by 100-200 times using attention acceleration (low-bit SageAttention and Sparse-Linear Attention), step distillation (rCM), and W8A8 quantization. Experiments on several models show it achieves this speedup on a single GPU while maintaining comparable video quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [AI-driven framework, multimodal fusion, synthetic social features, forward-walk evaluation, Streamlit dashboard, parquet-native pipeline, OHLCV data, Reddit activity, coordination metrics, bot indicators]</li>
<li class=""><strong>authors:</strong> Sandeep Neela</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16103" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16103</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces AIMM, an AI-driven multimodal framework that fuses Reddit activity, bot/coordination indicators, and OHLCV market data to generate a daily manipulation risk score for stocks. The system uses a parquet-native pipeline with a Streamlit dashboard for analysis and employs synthetic social features due to API restrictions. The preliminary results show the framework can provide early warnings, such as flagging GME 22 days before its peak in January 2021.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] BUILD with Precision: Bottom-Up Inference of Linear DAGs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [causal discovery], [precision matrix, structural equation model, bottom-up inference, DAG learning, leaf node identification]</li>
<li class=""><strong>authors:</strong> Hamed Ajorlou, Samuel Rey, Gonzalo Mateos, Geert Leus, Antonio G. Marques</li>
<li class=""><strong>institution:</strong> University of Rochester, Universidad Rey Juan Carlos, Delft University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16111" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16111</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes BUILD, a deterministic bottom-up algorithm that learns directed acyclic graph (DAG) structures by iteratively identifying leaf nodes and their parents using properties of the precision matrix under a linear Gaussian model with equal noise variances. It shows that BUILD can exactly reconstruct the DAG from the true precision matrix and, with periodic re-estimation for robustness, performs competitively against state-of-the-art methods on synthetic benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Dual-View Inference Attack: Machine Unlearning Amplifies Privacy Exposure</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [privacy and security], [machine unlearning, dual-view inference attack, privacy knowledge gain, membership inference, black-box attack]</li>
<li class=""><strong>authors:</strong> Lulu Xue, Shengshan Hu, Linqiang Qian, Peijin Guo, Yechao Zhang, Minghui Li, Yanjun Zhang, Dayong Ye, Leo Yu Zhang</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology, Tsinghua University, Nanyang Technological University, University of Technology Sydney, City University of Macau, Griffith University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16126" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16126</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes DVIA, a Dual-View Inference Attack, which extracts membership information on retained data by querying both the original and unlearned models. It demonstrates that this dual-view setting amplifies privacy leakage compared to querying a single model. The main conclusion is that machine unlearning introduces new vulnerabilities, increasing the privacy risk for data that was not removed.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [staggered batch scheduling, load-aware global allocation, DP+EP, time-to-first-token, throughput, data parallelism, expert parallelism]</li>
<li class=""><strong>authors:</strong> Jian Tian, Shuailong Li, Yang Cao, Wenbo Cui, Minghan Zhu, Wenkang Wu, Jianming Zhang, Yanpeng Wang, Zhiwen Xiao, Zhenyu Hou, Dou Shen</li>
<li class=""><strong>institution:</strong> Baidu Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16134" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16134</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Staggered Batch Scheduling (SBS), a method that buffers requests to form optimal batches before dispatching them to a DP+EP inference cluster, eliminating internal queuing. It also introduces a Load-Aware Global Allocation strategy to balance computational load. The system reduces Time-to-First-Token by 30-40% and improves throughput by 15-20% compared to immediate scheduling baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] INTELLECT-3: Technical Report</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [mixture-of-experts, reinforcement learning, large-scale training, prime-rl, verifiers library, environments hub, SFT, RL training]</li>
<li class=""><strong>authors:</strong> Prime Intellect Team, Mika Senghaas, Fares Obeid, Sami Jaghouar, William Brown, Jack Min Ong, Daniel Auras, Matej Sirovatka, Jannik Straube, Andrew Baker, Sebastian Müller, Justus Mattern, Manveer Basra, Aiman Ismail, Dominik Scherm, Cooper Miller, Ameen Patel, Simon Kirsten, Mario Sieg, Christian Reetz, Kemal Erdem, Vincent Weisser, Johannes Hagemann</li>
<li class=""><strong>institution:</strong> Prime Intellect, Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16144" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16144</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces INTELLECT-3, a 106B-parameter Mixture-of-Experts model trained using large-scale reinforcement learning on a custom end-to-end RL infrastructure stack. It achieves state-of-the-art performance for its size across multiple benchmarks by leveraging the open-source prime-rl framework and scaling training to 512 H200 GPUs. The authors open-source the model and the full training infrastructure, including the RL frameworks and environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Science Consultant Agent</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Retrieval-Augmented Generation (RAG), fine-tuning, knowledge distillation, prompting, AutoML]</li>
<li class=""><strong>authors:</strong> Karthikeyan K, Philip Wu, Xin Tang, Alexandre Alves</li>
<li class=""><strong>institution:</strong> Duke University, Amazon</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16171" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16171</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Science Consultant Agent, a web-based AI tool that uses structured questionnaires, literature-backed recommendations, and prototype generation to guide practitioners in selecting optimal AI modeling strategies. It aims to prevent resource misallocation by providing evidence-based guidance, moving beyond brute-force exploration or example-induced bias to accelerate development.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] A Multimodal Approach to Alzheimer&#x27;s Diagnosis: Geometric Insights from Cube Copying and Cognitive Assessments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical AI], [graph neural networks, multimodal fusion, SHAP interpretability, graphlet motifs, cube copying test]</li>
<li class=""><strong>authors:</strong> Jaeho Yang, Kijung Yoon</li>
<li class=""><strong>institution:</strong> Hanyang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16184" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16184</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a multimodal framework for Alzheimer&#x27;s disease diagnosis by converting hand-drawn cube sketches into graph-structured representations and integrating them with demographic and cognitive test data using graph neural networks and late-fusion. The method significantly outperforms pixel-based models, and interpretability analysis reveals that specific geometric distortions in the drawings are key predictors, aligning with clinical observations. The results establish graph-based analysis of cube copying as an interpretable and scalable screening tool for Alzheimer&#x27;s.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] A Multi-scale Fused Graph Neural Network with Inter-view Contrastive Learning for Spatial Transcriptomics Data Clustering</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [spatial transcriptomics clustering], [graph neural network, multi-scale fusion, contrastive learning, cross-view attention]</li>
<li class=""><strong>authors:</strong> Jianping Mei, Siqi Ai, Ye Yuan</li>
<li class=""><strong>institution:</strong> Southwest University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16188" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16188</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes stMFG, a multi-scale fused graph neural network that uses layer-wise cross-view attention to dynamically integrate spatial and gene features, combined with contrastive learning for clustering. It demonstrates improved performance over existing methods on spatial transcriptomics datasets, achieving up to 14% higher ARI on certain slices.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithms on Smooth Functions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [optimization theory], [rank-based zeroth-order optimization, query complexity, CMA-ES, non-asymptotic analysis, smooth functions]</li>
<li class=""><strong>authors:</strong> Haishan Ye</li>
<li class=""><strong>institution:</strong> Xi&#x27;an Jiaotong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16200" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16200</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper provides a theoretical analysis of a simple rank-based zeroth-order optimization algorithm, which selects search directions based on the ordering of function evaluations. It establishes the first explicit, non-asymptotic query complexities for finding solutions on smooth strongly convex and nonconvex functions. The main conclusion is that the algorithm achieves efficient convergence rates, offering new insight into why rank-based heuristics are effective for zeroth-order optimization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Neural emulation of gravity-driven geohazard runout</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [neural emulation, machine learning, digital elevation model, numerical simulations, runout prediction]</li>
<li class=""><strong>authors:</strong> Lorenzo Nava, Ye Chen, Maximillian Van Wyk de Vries</li>
<li class=""><strong>institution:</strong> University of Cambridge, Tongji University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16221" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16221</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper trains a machine learning model to emulate numerical simulations for predicting geohazard runout, such as landslides and avalanches, across real-world terrains. The model, trained on over 100,000 simulations, predicts flow extent and deposit thickness with high accuracy and is 100 to 10,000 times faster than traditional solvers. It demonstrates that neural emulation enables rapid and physically realistic runout prediction, offering new opportunities for disaster risk reduction and early warning systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [graph neural networks], [open-set classification, out-of-distribution detection, coarse-to-fine classification, large language models, graph neural networks, semantic OOD]</li>
<li class=""><strong>authors:</strong> Xueqi Ma, Xingjun Ma, Sarah Monazam Erfani, Danilo Mandic, James Bailey</li>
<li class=""><strong>institution:</strong> The University of Melbourne, Fudan University, Imperial College London</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16244" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16244</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a Coarse-to-Fine open-set Classification (CFC) framework that uses large language models for OOD detection and label generation, combined with a GNN-based fine classifier, to perform open-set node classification on graphs. It shows that CFC improves OOD detection by 10% over state-of-the-art methods and achieves up to 70% accuracy in OOD classification on graph datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Sharpness-aware Federated Graph Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated graph learning, sharpness-aware minimization, dimensional collapse, graph neural networks, empirical risk minimization, correlation matrix regularization]</li>
<li class=""><strong>authors:</strong> Ruiyu Li, Peige Zhao, Guangxia Li, Pengcheng Wu, Xingyu Gao, Zhiqiang Xu</li>
<li class=""><strong>institution:</strong> Xidian University, iFLYTEK, Nanyang Technological University, University of Chinese Academy of Sciences, MBZUAI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16247" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16247</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SEAL, a sharpness-aware federated graph learning algorithm that improves model generalization on heterogeneous graph data by simultaneously minimizing the loss function and its sharpness, and alleviating dimensional collapse via a correlation matrix regularizer. Experimental results show that SEAL consistently outperforms state-of-the-art federated graph learning baselines in classification accuracy and generalization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Pixel Super-Resolved Fluorescence Lifetime Imaging Using Deep Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [conditional generative adversarial network (cGAN), pixel super-resolution (PSR), fluorescence lifetime imaging microscopy (FLIM)]</li>
<li class=""><strong>authors:</strong> Paloma Casteleiro Costa, Parnian Ghapandar Kashani, Xuhui Liu, Alexander Chen, Ary Portes, Julien Bec, Laura Marcu, Aydogan Ozcan</li>
<li class=""><strong>institution:</strong> University of California, Los Angeles; University of California, Davis</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16266" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16266</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces FLIM_PSR_k, a deep learning framework using a conditional generative adversarial network (cGAN) to perform pixel super-resolution on fluorescence lifetime images. It reconstructs high-resolution images from low-resolution inputs, achieving up to a 5x super-resolution factor and enabling faster acquisition. The method improves image quality and spatial resolution, advancing FLIM for faster, higher-resolution clinical applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Sharpness-aware Second-order Latent Factor Model for High-dimensional and Incomplete Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [recommender systems], [second-order latent factor model, sharpness-aware minimization, hessian-vector product, high-dimensional incomplete data, matrix factorization]</li>
<li class=""><strong>authors:</strong> Jialiang Wang, Xueyan Bao, Hao Wu</li>
<li class=""><strong>institution:</strong> Southwest University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16277" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16277</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a Sharpness-aware Second-order Latent Factor (SSLF) model that integrates Sharpness-aware Minimization (SAM) with second-order optimization via Hessian-vector products to find flatter minima for better generalization on high-dimensional and incomplete data. Experiments on industrial datasets show that SSLF consistently outperforms state-of-the-art baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] In-Context Probing for Membership Inference in Fine-Tuned Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [membership inference attacks, in-context probing, optimization gap, black-box attacks, fine-tuning, PEFT]</li>
<li class=""><strong>authors:</strong> Zhexi Lu, Hongliang Chi, Nathalie Baracaldo, Swanand Ravindra Kadhe, Yuseok Jeon, Lei Yu</li>
<li class=""><strong>institution:</strong> Rensselaer Polytechnic Institute, IBM Research, Korea University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16292" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16292</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ICP-MIA, a membership inference attack framework that estimates the optimization gap via in-context probing to simulate fine-tuning behavior without retraining. It shows that this method outperforms prior black-box attacks, especially at low false positive rates, providing a practical tool for auditing privacy risks in deployed LLMs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [post-training quantization, CKA, GPTQ, AWQ, SmoothQuant, mixed-precision, algorithmic heterogeneity]</li>
<li class=""><strong>authors:</strong> Jinhao Zhang, Yunquan Zhang, Daning Chen</li>
<li class=""><strong>institution:</strong> Beijing University of Posts and Telecommunications, Institute of Computing Technology, Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16282" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16282</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes CKA-Guided Modular Quantization, a framework that automatically selects the best post-training quantization algorithm for each layer of a large language model using Linear CKA similarity as a selection metric. It combines different algorithms like GPTQ, AWQ, and SmoothQuant across layers to create a hybrid quantized model. The method outperforms uniform quantization and mixed-precision baselines on models like LLaMA and Qwen without requiring fine-tuning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Feature-Selective Representation Misdirection for Machine Unlearning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [machine unlearning, activation editing, feature-aware perturbation, representation misdirection, SRMU]</li>
<li class=""><strong>authors:</strong> Taozhao Chen, Linghan Huang, Kim-Kwang Raymond Choo, Huaming Chen</li>
<li class=""><strong>institution:</strong> University of Sydney, University of Texas at San Antonio</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16297" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16297</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Selective Representation Misdirection for Unlearning (SRMU), a novel activation-editing framework that applies feature-aware, directional perturbations to selectively suppress harmful knowledge in LLMs. It demonstrates state-of-the-art unlearning performance with minimal utility loss, remaining robust even when the data to forget and retain are highly entangled.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Pretrained Battery Transformer (PBT): A battery life prediction foundation model</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [foundation model, transformer, mixture-of-expert layers, transfer learning, battery life prediction]</li>
<li class=""><strong>authors:</strong> Ruifeng Tan, Weixiang Hong, Jia Li, Jiaqiang Huang, Tong-Yi Zhang</li>
<li class=""><strong>institution:</strong> The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology, Shanghai University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16334" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16334</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the Pretrained Battery Transformer (PBT), a foundation model for battery life prediction that uses domain-knowledge-encoded mixture-of-expert layers. It is trained on diverse lithium-ion battery datasets and achieves state-of-the-art performance through transfer learning, establishing a pathway for universal battery lifetime prediction systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [sparse attention, dynamic programming, top-k selection, anchor layers, reuse layers, FlashAttention-3]</li>
<li class=""><strong>authors:</strong> Dhruv Deshmukh, Saurabh Goyal, Nipun Kwatra, Ramachandran Ramjee</li>
<li class=""><strong>institution:</strong> Microsoft Research India</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16391" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16391</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Kascade, a training-free sparse attention method that accelerates long-context LLM inference by computing exact Top-k indices in selected anchor layers and reusing them in intermediate layers, based on the stability of high-weight keys across layers. It uses a dynamic programming algorithm to select anchor layers and achieves significant speedups in both prefill and decode phases while maintaining accuracy close to dense attention on benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Multivariate Uncertainty Quantification with Tomographic Quantile Forests</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multivariate uncertainty quantification], [Tomographic Quantile Forests, conditional quantiles, directional projections, sliced Wasserstein distance, nonparametric estimation, tree-based regression]</li>
<li class=""><strong>authors:</strong> Takuya Kanazawa</li>
<li class=""><strong>institution:</strong> Kobe Gakuin University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16383" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16383</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Tomographic Quantile Forests (TQF), a nonparametric tree-based method for multivariate uncertainty quantification that learns conditional quantiles of directional projections. It reconstructs the full multivariate conditional distribution by aggregating quantiles across directions and minimizing the sliced Wasserstein distance. The method avoids convexity restrictions and trains a single model for all directions, showing promise on synthetic and real-world datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Quantitative Verification of Fairness in Tree Ensembles</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [quantitative verification, fairness, tree ensembles, Counterexample-Guided Abstraction Refinement, SMT, formal method]</li>
<li class=""><strong>authors:</strong> Zhenjiang Zhao, Takahisa Toda, Takashi Kitamura</li>
<li class=""><strong>institution:</strong> University of Electro-Communications, National Institute of Advanced Industrial Science and Technology (AIST)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16386" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16386</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a new quantitative verification technique for tree ensembles that efficiently computes both upper and lower bounds on the ratio of fairness violations, overcoming the limitations of prior model-agnostic methods. By exploiting the discrete structure of tree ensembles, the method provides more informative diagnostics for bias. Experiments show it significantly outperforms state-of-the-art fairness testing techniques in effectiveness and efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [nested dual-agent reinforcement learning, water stress factor, nitrogen stress factor, dssat simulation, hierarchical decision making]</li>
<li class=""><strong>authors:</strong> Ruifeng Xu, Liang He</li>
<li class=""><strong>institution:</strong> Xinjiang University, Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16408" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16408</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Nested Dual-Agent Reinforcement Learning (NDRL) method to optimize cotton irrigation and nitrogen fertilization, using a parent agent for macro-action selection and a child agent incorporating quantified stress factors for daily dynamic control. The method, validated with DSSAT simulations using 2023-2024 field data, achieved increased simulated yield, irrigation water productivity, and nitrogen partial factor productivity compared to baselines. It provides a new approach for improving precision and sustainability in agricultural resource management.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Geometric Laplace Neural Operator</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [neural operators], [pole-residue decomposition, Laplace spectral representation, Laplace-Beltrami operator, grid-invariant architecture]</li>
<li class=""><strong>authors:</strong> Hao Tang, Jiongyu Zhu, Zimeng Feng, Hao Li, Chao Li</li>
<li class=""><strong>institution:</strong> University of Dundee, Fudan University, University of Cambridge</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16409" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16409</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes the Geometric Laplace Neural Operator (GLNO), a framework that uses a pole-residue decomposition with exponential basis functions and embeds a Laplace spectral representation into the eigen-basis of the Laplace-Beltrami operator. This enables learning mappings on arbitrary Riemannian manifolds, effectively handling aperiodic, transient signals and irregular geometries. Experiments show it outperforms other state-of-the-art models on PDE/ODE and real-world datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Emergent Bias and Fairness in Multi-Agent Decision Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [fairness and bias in multi-agent systems], [multi-agent systems, emergent bias, fairness evaluation, large-scale simulation, credit scoring, income estimation]</li>
<li class=""><strong>authors:</strong> Maeve Madigan, Parameswaran Kamalaruban, Glenn Moynihan, Tom Kempton, David Sutton, Stuart Burrell</li>
<li class=""><strong>institution:</strong> Visa Inc., University of Manchester</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16433" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16433</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops fairness evaluation methodologies for multi-agent predictive systems in the financial domain using large-scale simulations across diverse agent configurations. It finds that these systems exhibit emergent bias in tasks like credit scoring, which cannot be traced to individual agents, indicating genuinely collective behaviors. The authors conclude that multi-agent decision systems must be evaluated as holistic entities rather than through reductionist analysis of their components.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Multi-Fidelity Delayed Acceptance: hierarchical MCMC sampling for Bayesian inverse problems combining multiple solvers through deep neural networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Multi-Fidelity Delayed Acceptance, Markov Chain Monte Carlo, Bayesian inverse problems, deep neural networks, multi-fidelity neural networks, hierarchical MCMC]</li>
<li class=""><strong>authors:</strong> Filippo Zacchei, Paolo Conti, Attilio Alberto Frangi, Andrea Manzoni</li>
<li class=""><strong>institution:</strong> Politecnico di Milano, The Alan Turing Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16430" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16430</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a Multi-Fidelity Delayed Acceptance MCMC scheme that uses deep neural networks to combine predictions from solvers of varying fidelity, avoiding expensive high-fidelity simulations during online inference. This method improves the accuracy of low-fidelity approximations, leading to better mixing and faster posterior sampling. The approach is validated on two benchmark inverse problems, demonstrating significant computational savings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] A Novel Proposal in Wind Turbine Blade Failure Detection: An Integrated Approach to Energy Efficiency and Sustainability</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [fault detection], [logistic regression, clustering, neural networks, decision trees, naive Bayes, Orange Data Mining]</li>
<li class=""><strong>authors:</strong> Jordan Abarca-Albores, Danna Cristina Gutiérrez Cabrera, Luis Antonio Salazar-Licea, Dante Ruiz-Robles, Jesus Alejandro Franco, Alberto-Jesus Perea-Moreno, David Muñoz-Rodríguez, Quetzalcoatl Hernandez-Escobedo</li>
<li class=""><strong>institution:</strong> Universidad Nacional Autónoma de México, Universidad de Córdoba</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16437" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16437</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a methodology for wind turbine blade fault detection using computational learning techniques, evaluating logistic regression and clustering models. The results indicate that logistic regression outperformed other supervised methods, while clustering showed superior precision in capturing data characteristics. The study highlights the potential of integrating these techniques for early fault detection to enhance system reliability in the wind energy sector.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Topic Modelling Black Box Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [hyperparameter optimization], [latent dirichlet allocation, black-box optimization, genetic algorithm, evolution strategy, preferential amortized black-box optimization, sharpness-aware black-box optimization]</li>
<li class=""><strong>authors:</strong> Roman Akramov, Artem Khamatullin, Svetlana Glazyrina, Maksim Kryzhanovskiy, Roman Ischenko</li>
<li class=""><strong>institution:</strong> Lomonosov Moscow State University, Institute for Artificial Intelligence, Lomonosov Moscow State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16445" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16445</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper formulates selecting the number of topics in Latent Dirichlet Allocation (LDA) as a discrete black-box optimization problem. It compares evolutionary methods (GA, ES) against learned amortized optimizers (PABBO, SABBO), finding that the amortized approaches are substantially more sample- and time-efficient, with SABBO often finding a near-optimal topic number after essentially a single evaluation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] IoMT-based Automated Leukemia Classification using CNN and Higher Order Singular Value</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Convolutional Neural Network (CNN), Higher Order Singular Value Decomposition (HOSVD), Internet of Medical Things (IoMT), Acute Lymphoblastic Leukemia Image Database (ALL-IDB2)]</li>
<li class=""><strong>authors:</strong> Shabnam Bagheri Marzijarani, Mohammad Zolfaghari, Hedieh Sajedi</li>
<li class=""><strong>institution:</strong> Islamic Azad University, University of Tehran</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16448" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16448</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method for automated leukemia classification by combining a Convolutional Neural Network (CNN) with a Higher Order Singular Value Decomposition (HOSVD) classifier, deployed within an Internet of Medical Things (IoMT) framework. The model was tested on the ALL-IDB2 database and achieved an average test accuracy of 98.88%, demonstrating its effectiveness for rapid and accurate identification of Acute Lymphocytic Leukemia cells.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Batch Normalization-Free Fully Integer Quantized Neural Networks via Progressive Tandem Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [quantization, batch normalization, integer-only inference, knowledge distillation, progressive tandem learning, layer-wise distillation]</li>
<li class=""><strong>authors:</strong> Pengfei Sun, Wenyu Jiang, Piew Yoong Chee, Paul Devos, Dick Botteldooren</li>
<li class=""><strong>institution:</strong> Ghent University, Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (A*STAR)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16476" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16476</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a progressive tandem-learning framework to train fully integer quantized neural networks without batch normalization layers. The method uses a pretrained, BN-enabled teacher model to distill a BN-free student via layer-wise knowledge distillation and a per-layer scale factor. The resulting model achieves competitive accuracy on ImageNet with pure integer arithmetic, enabling efficient deployment on resource-constrained hardware.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Pseudo-Cepstrum: Pitch Modification for Mel-Based Neural Vocoders</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [speech synthesis], [cepstrum, pitch shifting, mel-spectrogram, DCT, pseudo-inverse mel transform, neural vocoder]</li>
<li class=""><strong>authors:</strong> Nikolaos Ellinas, Alexandra Vioni, Panos Kakoulidis, Georgios Vamvoukakis, Myrsini Christidou, Konstantinos Markopoulos, Junkwang Oh, Gunu Jho, Inchul Hwang, Aimilios Chalamandaris, Pirros Tsiakoulis</li>
<li class=""><strong>institution:</strong> Innoetics, Samsung Electronics, Samsung Electronics Mobile eXperience Business</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16519" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16519</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a cepstrum-based pitch modification method that directly manipulates the cepstral feature space to shift the harmonic structure in a mel-spectrogram, making it compatible with any mel-based neural vocoder without retraining. The method is validated through objective and subjective evaluations, showing its effectiveness compared to traditional pitch modification techniques.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [code obfuscation, layout obfuscation, data flow obfuscation, control flow obfuscation, LLM-driven implementation, vulnerability detection, adversarial robustness]</li>
<li class=""><strong>authors:</strong> Xiao Li, Yue Li, Hao Wu, Yue Zhang, Yechao Zhang, Fengyuan Xu, Sheng Zhong</li>
<li class=""><strong>institution:</strong> Nanjing University, Shandong University, Nanyang Technological University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16538" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16538</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically studies the impact of code obfuscation on LLM-based vulnerability detection by categorizing obfuscation techniques into layout, data flow, and control flow classes and implementing them across multiple programming languages. It evaluates these techniques against 15 LLMs and two coding agents, finding that obfuscation can both improve and degrade detection performance depending on the vulnerability type, code properties, and model attributes. The study highlights the need to enhance LLM robustness for reliable real-world security auditing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Persistent Multiscale Density-based Clustering</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [clustering algorithms], [density-based clustering, HDBSCAN*, DBSCAN, persistent homology, scale-space clustering]</li>
<li class=""><strong>authors:</strong> Daniël Bot, Leland McInnes, Jan Aerts</li>
<li class=""><strong>institution:</strong> UHasselt, Tutte Institute for Mathematics and Computing, KU Leuven</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16558" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16558</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces PLSCAN, a density-based clustering algorithm that identifies all minimum cluster sizes for stable leaf clusters in HDBSCAN* using scale-space principles and persistent homology. It demonstrates higher average ARI and less sensitivity to parameter changes compared to HDBSCAN*, with competitive run-times on low-dimensional datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Abacus: Self-Supervised Event Counting-Aligned Distributional Pretraining for Sequential User Modeling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [display advertising], [self-supervised learning, sequential modeling, distributional pretraining, hybrid objective]</li>
<li class=""><strong>authors:</strong> Sullivan Castro, Artem Betlei, Thomas Di Martino, Nadir El Manouzi</li>
<li class=""><strong>institution:</strong> Criteo AI Lab, École Nationale des Ponts et Chaussées</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16581" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16581</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Abacus, a self-supervised pretraining method for sequential user modeling that predicts the empirical frequency distribution of user events to align with useful counting statistics. It combines this with a hybrid objective that unites distributional prediction with sequential learning. Experiments show that this approach accelerates downstream task convergence and improves AUC by up to 6.1% compared to baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [preference optimization], [Stackelberg game, sequential-move game, inference-time refinement, preference models, Nash equilibrium, Bradley-Terry model]</li>
<li class=""><strong>authors:</strong> Barna Pásztor, Thomas Kleine Buening, Andreas Krause</li>
<li class=""><strong>institution:</strong> ETH Zürich</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16626" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16626</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Stackelberg Learning from Human Feedback (SLHF), a new framework that frames preference optimization as a sequential-move game between a Leader and a Follower policy. It demonstrates that this approach offers advantages in consistency, data sensitivity, and robustness to intransitive preferences compared to RLHF and NLHF. Experiments on large language models show that SLHF achieves strong alignment and enables inference-time refinements that transfer across model families.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] SARMAE: Masked Autoencoder for SAR Representation Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [remote sensing], [masked autoencoder, self-supervised learning, speckle noise, representation learning, SAR-1M dataset, Speckle-Aware Representation Enhancement (SARE), Semantic Anchor Representation Constraint (SARC)]</li>
<li class=""><strong>authors:</strong> Danxu Liu, Di Wang, Hebaixu Wang, Haoyang Chen, Wentao Jiang, Yilin Cheng, Haonan Guo, Wei Cui, Jing Zhang</li>
<li class=""><strong>institution:</strong> Beijing Institute of Technology, Wuhan University, Fudan University, Zhongguancun Academy</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16635" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16635</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes SARMAE, a noise-aware masked autoencoder for self-supervised representation learning of Synthetic Aperture Radar (SAR) imagery. It introduces a large-scale dataset (SAR-1M) and two key components: SARE for speckle noise injection and SARC for semantic alignment using optical image priors. The method achieves state-of-the-art performance on various SAR image interpretation tasks like classification, detection, and segmentation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Exploiting Radio Frequency Fingerprints for Device Identification: Tackling Cross-receiver Challenges in the Source-data-free Scenario</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Momentum Soft pseudo-label Source Hypothesis Transfer (MS-SHOT), constrained pseudo-labeling, cross-receiver adaptation, source-data-free, radio frequency fingerprint identification]</li>
<li class=""><strong>authors:</strong> Liu Yang, Qiang Li, Luxiong Wen, Jian Yang</li>
<li class=""><strong>institution:</strong> University of Electronic Science and Technology of China, Beijing Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16648" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16648</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes MS-SHOT, a method for source-data-free cross-receiver adaptation in Radio Frequency Fingerprint Identification (RFFI). It uses momentum-center-guided soft pseudo-labeling and global structural constraints to adapt a pre-trained model to a new receiver without access to the original source data. Experiments show that MS-SHOT outperforms existing methods in accuracy and robustness, especially under label shift and non-uniform class distributions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [data preparation, workflow automation, pipeline construction, operator synthesis, natural-language specification, model-in-the-loop data generation]</li>
<li class=""><strong>authors:</strong> Hao Liang, Xiaochen Ma, Zhou Liu, Zhen Hao Wong, Zhengyang Zhao, Zimo Meng, Runming He, Chengyu Shen, Qifeng Cai, Zhaoyang Han, Meiyi Qiang, Yalin Feng, Tianyi Bai, Zewei Pan, Ziyi Guo, Yizhen Jiang, Jingwen Deng, Qijie You, Peichao Lai, Tianyu Guo, Chi Hsu Tsai, Hengyi Feng, Rui Hu, Wenkai Yu, Junbo Niu, Bohan Zeng, Ruichuan An, Lu Ma, Jihao Huang, Yaowei Zheng, Conghui He, Linpeng Tang, Bin Cui, Weinan E, Wentao Zhang</li>
<li class=""><strong>institution:</strong> Peking University, Institute for Advanced Algorithms Research, OriginHub Technology, OpenDataLab, Shanghai Artificial Intelligence Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16676" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16676</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces DataFlow, an LLM-driven framework that provides system-level abstractions and a PyTorch-style API for building modular, reusable, and automated data preparation pipelines. It includes a DataFlow-Agent to translate natural language into executable workflows and demonstrates that its generated data consistently improves downstream LLM performance across multiple domains, establishing a foundation for scalable, data-centric AI development.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Blog Data Showdown: Machine Learning vs Neuro-Symbolic Models for Gender Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [natural language processing], [support vector machines, naive bayes, logistic regression, adaboost, xgboost, neuro-symbolic ai, tf-idf, universal sentence encoder, roberta, chi-square, mutual information, principal component analysis]</li>
<li class=""><strong>authors:</strong> Natnael Tilahun Sinshaw, Mengmei He, Tadesse K. Bahiru, Sudhir Kumar Mohapatra</li>
<li class=""><strong>institution:</strong> University of Houston, Sri Sri University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16687" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16687</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares machine learning algorithms (SVM, Naive Bayes, Logistic Regression, AdaBoost, XGBoost) and deep learning text representations (TF-IDF, USE, RoBERTa) with a neuro-symbolic AI approach for gender classification from blog posts. The results show that the neuro-symbolic model achieves performance comparable to strong MLP models, even with a limited dataset. Future work will expand the knowledge base and explore more embedding types and hyperparameters to further evaluate the neuro-symbolic approach.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] CLARiTy: A Vision Transformer for Multi-Label Classification and Weakly-Supervised Localization of Chest X-ray Pathologies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical imaging], [vision transformer, weakly-supervised localization, multi-label classification, SegmentCAM, attention maps, class-specific tokens, DINO pretraining, distillation]</li>
<li class=""><strong>authors:</strong> John M. Statheros, Hairong Wang, Richard Klein</li>
<li class=""><strong>institution:</strong> University of the Witwatersrand</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16700" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16700</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces CLARiTy, a vision transformer model that uses class-specific tokens and a SegmentCAM module for multi-label classification and weakly-supervised localization of chest X-ray pathologies. It achieves state-of-the-art localization performance, especially for small pathologies, while being trained only on image-level labels. The model&#x27;s efficiency also makes it suitable for low-resource settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Olaf: Bringing an Animated Character to Life in the Physical World</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [reinforcement learning, imitation rewards, thermal actuator modeling, spherical linkages, planar linkages, asymmetric leg design]</li>
<li class=""><strong>authors:</strong> David Müller, Espen Knoop, Dario Mylonopoulos, Agon Serifi, Michael A. Hopkins, Ruben Grandia, Moritz Bächer</li>
<li class=""><strong>institution:</strong> Disney Research Imagineering</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16705" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16705</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper brings the animated character Olaf to life as a physical robot using reinforcement learning guided by animation references for control. Key innovations include a compact mechanical design with hidden asymmetric legs and linkages, and a policy that incorporates actuator temperature inputs to prevent overheating. The approach successfully achieves a high level of believability for the costumed robotic character.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Towards Reproducibility in Predictive Process Mining: SPICE - A Deep Learning Library</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [predictive process mining, deep learning, PyTorch, LSTM, Transformer, reproducibility, benchmarking]</li>
<li class=""><strong>authors:</strong> Oliver Stritzel, Nick Hühnerbein, Simon Rauch, Itzel Zarate, Lukas Fleischmann, Moike Buck, Attila Lischka, Christian Frey</li>
<li class=""><strong>institution:</strong> Fraunhofer IIS, University of Technology Nuremberg, Chalmers University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16715" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16715</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SPICE, a Python framework that reimplements three popular deep-learning-based methods for Predictive Process Mining in PyTorch within a common, configurable base. It aims to address reproducibility and benchmarking issues in the field. The framework is validated by comparing its performance to original reported metrics on 11 datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Polyharmonic Spline Packages: Composition, Efficient Procedures for Computation and Differentiation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [polyharmonic spline, cascade architecture, random function theory, symmetry principles, matrix procedures, end-to-end differentiation]</li>
<li class=""><strong>authors:</strong> Yuriy N. Bakhvalov</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16718" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16718</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a cascade architecture composed of packages of polyharmonic splines to address the scalability and high-dimensionality limitations of a prior kernel-based regression solution derived from random function theory. It presents efficient matrix procedures for both forward computation and end-to-end differentiation through this cascade. The method is theoretically justified for problems with unknown intrinsic low dimensionality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] KOSS: Kalman-Optimal Selective State Spaces for Long-Term Sequence Modeling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [sequence modeling], [selective state space models, Kalman gain, spectral differentiation, segment-wise scan, long-term forecasting]</li>
<li class=""><strong>authors:</strong> Lei Wang, Xin Tan, Mingwei Wang, Ying Zhang</li>
<li class=""><strong>institution:</strong> School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16723" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16723</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes KOSS, a Kalman-optimal Selective State Space model that formulates selection as latent state uncertainty minimization, using a Kalman gain for closed-loop, context-aware information modulation. It demonstrates superior performance in long-term sequence modeling tasks, significantly outperforming existing baselines in accuracy and stability on forecasting benchmarks and a selective copying task.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Phishing Detection System: An Ensemble Approach Using Character-Level CNN and Feature Engineering</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [ensemble learning, character-level CNN, LightGBM, feature engineering, URL analysis, FastAPI]</li>
<li class=""><strong>authors:</strong> Rudra Dubey, Arpit Mani Tripathi, Archit Srivastava, Sarvpal Singh</li>
<li class=""><strong>institution:</strong> Madan Mohan Malaviya University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16717" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16717</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a phishing detection system using an ensemble model that combines a character-level Convolutional Neural Network (CNN) with a LightGBM classifier based on engineered URL features. The system achieves high performance metrics, such as 99.819% accuracy, and is deployed as a real-time service via FastAPI. The results show the ensemble approach outperforms individual models, effectively identifying modern phishing techniques with low false positive rates.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Machine Learning Algorithms: Detection Official Hajj and Umrah Travel Agency Based on Text and Metadata Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Support Vector Machine, Random Forest, Naive Bayes, TF-IDF, metadata analysis, feature extraction, classification]</li>
<li class=""><strong>authors:</strong> Wisnu Uriawan, Muhamad Veva Ramadhan, Firman Adi Nugraha, Hasbi Nur Wahid, M Dantha Arianvasya, Muhammad Zaki Alghifari</li>
<li class=""><strong>institution:</strong> UIN Sunan Gunung Djati Bandung</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16742" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16742</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper implements machine learning classifiers (SVM, RF, NB) using a hybrid feature extraction method combining TF-IDF text analysis of app descriptions and metadata analysis of permissions to detect fraudulent Hajj and Umrah travel agency applications. The SVM algorithm achieved the best performance with 92.3% accuracy, identifying keywords related to legality and high-risk permissions as key discriminators. The system is proposed as a scalable solution to enhance digital trust in religious tourism.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] NRGPT: An Energy-based Alternative for GPT</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [language modeling], [energy-based modeling, transformer, GPT, gradient descent, autoregressive]</li>
<li class=""><strong>authors:</strong> Nima Dehmamy, Benjamin Hoover, Bishwajit Saha, Leo Kozachkov, Jean-Jacques Slotine, Dmitry Krotov</li>
<li class=""><strong>institution:</strong> IBM Research, Georgia Tech, Brown University, MIT</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16762" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16762</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes NRGPT, a minimal modification to the GPT architecture that unifies it with an energy-based modeling framework, conceptualizing inference as exploration on an energy landscape. It demonstrates that this approach performs well on tasks from simple language to OpenWebText modeling and may offer increased resistance to overfitting.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Pattern recognition in complex systems via vector-field representations of spatio-temporal data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [complex systems analysis], [vector fields, discrete measure spaces, multidimensional scaling, dimensionality reduction, phase-space reconstruction, attractor characterisation]</li>
<li class=""><strong>authors:</strong> Ingrid Amaranta Membrillo Solis, Maria van Rossem, Tristan Madeleine, Tetiana Orlova, Nina Podoliak, Giampaolo D&#x27;Alessandro, Jacek Brodzki, Malgosia Kaczmarek</li>
<li class=""><strong>institution:</strong> Queen Mary University of London, University of Southampton</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16763" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16763</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a geometric framework based on vector fields over discrete measure spaces for analyzing spatio-temporal data from complex systems. It proposes a two-parameter family of metrics, validated with simulations, which when combined with multidimensional scaling enables dimensionality reduction and attractor characterization. The method provides a robust data-driven pathway for understanding complex dynamical systems where traditional modeling is impractical.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-agent reinforcement learning, QMIX, reactive jamming, channel hopping, power control, Upper Confidence Bound]</li>
<li class=""><strong>authors:</strong> Bahman Abolhassani, Tugba Erpek, Kemal Davaslioglu, Yalin E. Sagduyu, Sastry Kompella</li>
<li class=""><strong>institution:</strong> Nexcepta</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16813" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16813</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a multi-agent reinforcement learning framework based on the QMIX algorithm to coordinate anti-jamming strategies in swarm networks. The method enables agents to jointly select transmission channels and power levels to counter an adaptive reactive jammer. The results show that QMIX achieves near-optimal performance, higher throughput, and lower jamming incidence compared to baseline policies, demonstrating its effectiveness for securing swarm communications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Tiny Recursive Control: Iterative Reasoning for Efficient Optimal Control</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Tiny Recursive Control, iterative reasoning, hierarchical latent structure, neural network controller, optimal control, embedded aerospace systems]</li>
<li class=""><strong>authors:</strong> Amit Jain, Richard Linares</li>
<li class=""><strong>institution:</strong> Massachusetts Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16824" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16824</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Tiny Recursive Control (TRC), a neural architecture that uses a compact, iteratively applied network within a two-level hierarchical latent structure to refine control sequences for optimal control. It achieves near-optimal performance on nonlinear control problems while requiring only millisecond-scale inference and under 10MB of memory, demonstrating that recursive reasoning is effective for continuous control synthesis in resource-constrained systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] MEPIC: Memory Efficient Position Independent Caching for LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [position-independent caching, KV cache reuse, paged storage, block-level recomputation, Rotary Position Embedding (RoPE) fusion]</li>
<li class=""><strong>authors:</strong> Qian Wang, Zahra Yousefijamarani, Morgan Lindsay Heisler, Rongzhi Gu, Bai Xiaolong, Shan Yizhou, Wei Zhang, Wang Lan, Ying Xiong, Yong Zhang, Zhenan Fan</li>
<li class=""><strong>institution:</strong> Huawei Technologies Canada Co., Ltd., Huawei Technologies Co., Ltd.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16822" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16822</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MEPIC is a memory-efficient system for LLM serving that optimizes position-independent caching by aligning KV chunks to paged storage, shifting recomputation to the block level, and fusing positional encodings in the attention kernel. This enables high reuse of cached chunks across requests, significantly reducing HBM memory usage by up to 2-5x compared to prior methods while maintaining latency and accuracy without model modifications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Meta-RL Induces Exploration in Language Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [meta-reinforcement learning], [meta-rl, cross-episode training, in-context policy adaptation, exploration, large language model agents]</li>
<li class=""><strong>authors:</strong> Yulun Jiang, Liangze Jiang, Damien Teney, Michael Moor, Maria Brbic</li>
<li class=""><strong>institution:</strong> EPFL, ETH Zurich, Idiap Research Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16848" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16848</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces LaMer, a Meta-RL framework designed to enhance exploration in LLM-based agents through cross-episode training and in-context policy adaptation via reflection. The method enables agents to actively learn from environmental feedback without gradient updates. Experiments show LaMer outperforms RL baselines and generalizes better to novel or challenging tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [imitation learning, reinforcement learning, task decomposition, motion planning, data generation, skill policies]</li>
<li class=""><strong>authors:</strong> Zihan Zhou, Animesh Garg, Ajay Mandlekar, Caelan Garrett</li>
<li class=""><strong>institution:</strong> University of Toronto, Vector Institute, Georgia Institute of Technology, NVIDIA Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16861" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16861</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ReinforceGen, a hybrid system for long-horizon robot manipulation that first decomposes tasks into skills trained via imitation learning on generated data, and then refines them using reinforcement learning. It achieves an 80% success rate on benchmark tasks, with fine-tuning contributing to an 89% average performance increase.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Sequencing to Mitigate Catastrophic Forgetting in Continual Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [continual learning], [task sequencing, zero-shot scoring, neural architecture search]</li>
<li class=""><strong>authors:</strong> Hesham G. Moussa, Aroosa Hameed, Arashmid Akhavain</li>
<li class=""><strong>institution:</strong> Huawei Technologies Canada</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16871" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16871</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method to mitigate catastrophic forgetting in continual learning by determining an optimal task sequence. The method uses zero-shot scoring algorithms inspired by neural architecture search. The results show that intelligent task sequencing can substantially reduce forgetting and enhance performance when combined with traditional continual learning strategies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [knowledge distillation, active learning, causal reasoning, semi-supervised learning, online learning]</li>
<li class=""><strong>authors:</strong> Jiabin Xue</li>
<li class=""><strong>institution:</strong> University of Strathclyde</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16866" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16866</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Knowledge Transformation (KT), a hybrid method for semi-supervised online learning on edge devices that combines knowledge distillation, active learning, and causal reasoning to generate pseudo-labels from a teacher model for a student model. The experiments show that a student model can achieve its expected maximum performance when provided with a stable teacher model. The method is beneficial when teacher tasks are generic or student task labels are difficult to acquire.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] On the Universal Representation Property of Spiking Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [neuromorphic computing], [spiking neural networks, universal approximation, sequence-to-sequence processing, spike train functions, circuit complexity]</li>
<li class=""><strong>authors:</strong> Shayan Hundrieser, Philipp Tuchel, Insung Kong, Johannes Schmidt-Hieber</li>
<li class=""><strong>institution:</strong> University of Twente, Ruhr University Bochum</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16872" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16872</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes the representational power of Spiking Neural Networks (SNNs) by modeling them as sequence-to-sequence processors of spike trains. It establishes a universal representation property for a class of spike train functions, with quantitative and near-optimal bounds on the required neurons and weights. The main conclusion is that SNNs are particularly well-suited for representing functions with few inputs, low temporal complexity, or compositions thereof, providing a foundation for understanding spike-based neuromorphic systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Pixel Seal: Adversarial-only training for invisible image and video watermarking</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [adversarial-only training, three-stage training schedule, JND-based attenuation, temporal watermark pooling]</li>
<li class=""><strong>authors:</strong> Tomáš Souček, Pierre Fernandez, Hady Elsahar, Sylvestre-Alvise Rebuffi, Valeriu Lacatusu, Tuan Tran, Tom Sander, Alexandre Mourachko</li>
<li class=""><strong>institution:</strong> Meta FAIR</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16874" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16874</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Pixel Seal introduces an adversarial-only training paradigm for invisible watermarking, eliminating unreliable perceptual losses and using a three-stage schedule to decouple robustness and imperceptibility. It addresses high-resolution scaling with JND-based attenuation and adapts to video via temporal pooling. The method achieves state-of-the-art robustness and imperceptibility for image and video watermarking.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, immunofluorescence microscopy, collagen VI-related dystrophies, rare disease diagnosis, decentralized training]</li>
<li class=""><strong>authors:</strong> Astrid Brull, Sara Aguti, Véronique Bolduc, Ying Hu, Daniel M. Jimenez-Gutierrez, Enrique Zuazua, Joaquin Del-Rio, Oleksii Sliusarenko, Haiyan Zhou, Francesco Muntoni, Carsten G. Bönnemann, Xabi Uribe-Etxebarria</li>
<li class=""><strong>institution:</strong> National Institute of Neurological Disorders and Stroke, National Institutes of Health; University College London; Sherpa.ai</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16876" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16876</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper applies Federated Learning (FL) to train a diagnostic model for a rare disease using collagen VI immunofluorescence images from decentralized datasets across multiple institutions. This approach addresses data scarcity and privacy concerns by keeping patient data local. The resulting FL model outperformed single-institution models, demonstrating improved diagnostic accuracy and generalizability for classifying collagen VI-related dystrophies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Learning Confidence Ellipsoids and Applications to Robust Subspace Recovery</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [robust statistics], [confidence ellipsoids, minimum volume estimator, primal-dual structure, geometric Brascamp-Lieb inequality, robust subspace recovery]</li>
<li class=""><strong>authors:</strong> Chao Gao, Liren Shan, Vaidehi Srinivas, Aravindan Vijayaraghavan</li>
<li class=""><strong>institution:</strong> University of Chicago, Toyota Technological Institute at Chicago, Northwestern University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16875" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16875</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a polynomial-time algorithm for finding confidence ellipsoids with bounded condition number, achieving a volume approximation factor of O(β^(γd)) while covering at least 1-O(α/γ) probability mass. The method leverages the primal-dual structure of minimum volume enclosing ellipsoids and the geometric Brascamp-Lieb inequality. As a key application, this yields the first polynomial-time algorithm with approximation guarantees for worst-case instances of the robust subspace recovery problem.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [neural reconstruction, real-to-sim, co-training, simulation environments, policy evaluation]</li>
<li class=""><strong>authors:</strong> Arhan Jain, Mingtong Zhang, Kanav Arora, William Chen, Marcel Torne, Muhammad Zubair Irshad, Sergey Zakharov, Yue Wang, Sergey Levine, Chelsea Finn, Wei-Chiu Ma, Dhruv Shah, Abhishek Gupta, Karl Pertsch</li>
<li class=""><strong>institution:</strong> University of Washington, Princeton University, University of California, Berkeley, Stanford University, Toyota Research Institute, University of Southern California, Cornell University, Physical Intelligence</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16881" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16881</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces PolaRiS, a framework that uses neural reconstruction to convert short video scans of real-world scenes into interactive simulation environments for robot policy evaluation. It also employs a simulation data co-training method to bridge the real-to-sim gap. The authors demonstrate that evaluations in PolaRiS correlate more strongly with real-world performance than existing simulated benchmarks, enabling scalable and democratized testing for generalist robot policies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [cross-layer knowledge fusion MoE, VLLM, world-knowledge representation, token extraction, layer-wise fusion]</li>
<li class=""><strong>authors:</strong> Haichao Zhang, Yao Lu, Lichen Wang, Yunzhe Li, Daiwei Chen, Yunpeng Xu, Yun Fu</li>
<li class=""><strong>institution:</strong> Northeastern University, LinkedIn, University of Wisconsin–Madison</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16891" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16891</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces LinkedOut, a method that extracts world-knowledge-aware tokens directly from video frames using Video Large Language Models (VLLMs) and fuses features across model layers via a Mixture of Experts (MoE) for recommendation. It achieves state-of-the-art results on benchmarks by enabling low-latency, interpretable video recommendation without handcrafted labels. The approach demonstrates that leveraging VLLM priors and visual reasoning through layer-wise fusion is effective for downstream vision tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] In-Context Algebra</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [transformer interpretability], [transformers, in-context learning, algebraic groups, symbolic reasoning, causal tests, attention mechanisms]</li>
<li class=""><strong>authors:</strong> Eric Todd, Jannik Brinkmann, Rohit Gandikota, David Bau</li>
<li class=""><strong>institution:</strong> Northeastern University, TU Clausthal</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16902" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16902</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper trains small transformers on a novel arithmetic task where token meanings are variable and defined only within each sequence. It finds that, instead of learning geometric embeddings, the models develop symbolic reasoning mechanisms like commutative copying and closure-based cancellation. This demonstrates that transformer reasoning strategies shift from geometric to symbolic when deprived of fixed token embeddings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Impacts of Racial Bias in Historical Training Data for News AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [algorithmic auditing], [multi-label classifier, explainable AI, word2vec, New York Times Annotated Corpus]</li>
<li class=""><strong>authors:</strong> Rahul Bhargava, Malene Hornstrup Jespersen, Emily Boardman Ndulue, Vivica Dsouza</li>
<li class=""><strong>institution:</strong> Northeastern University, University of Copenhagen, Media Ecosystems Analysis Group</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16901" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16901</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper investigates racial bias in a multi-label text classifier trained on the New York Times Annotated Corpus using word2vec and explainable AI methods. It finds that a problematic &quot;blacks&quot; label acts as a general &quot;racism detector&quot; but fails on modern examples, demonstrating how historical training data embeds biases into AI models. The study highlights the tension for newsrooms in adopting AI tools while mitigating the reproduction of historical stereotypes in news coverage.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] SFTok: Bridging the Performance Gap in Discrete Tokenizers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [self-forcing guided visual reconstruction, debias-and-fitting training strategy, discrete tokenizer, multi-step iterative mechanism, autoregressive paradigm]</li>
<li class=""><strong>authors:</strong> Qihang Rao, Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu</li>
<li class=""><strong>institution:</strong> Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16910" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16910</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SFTok, a discrete image tokenizer that uses a multi-step iterative mechanism with self-forcing guided visual reconstruction and a debias-and-fitting training strategy to improve reconstruction quality. This approach resolves training-inference inconsistency and achieves state-of-the-art image reconstruction performance at high compression rates, bridging the performance gap with continuous tokenizers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [behavioral cloning, posterior behavioral cloning, policy pretraining, RL finetuning, generative models]</li>
<li class=""><strong>authors:</strong> Andrew Wagenmaker, Perry Dong, Raymond Tsao, Chelsea Finn, Sergey Levine</li>
<li class=""><strong>institution:</strong> UC Berkeley, Stanford</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16911" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16911</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Posterior Behavioral Cloning (PostBC), a method that pretrains a policy by modeling the posterior distribution of demonstrator behavior from a dataset, rather than exactly matching it. This ensures better coverage of demonstrator actions, which serves as a more effective initialization for subsequent reinforcement learning finetuning. The method is shown to improve RL finetuning performance on robotic control benchmarks and real-world manipulation tasks compared to standard behavioral cloning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [RLVR, GRPO, policy entropy, spurious rewards, clipping bias, exploration-exploitation trade-off]</li>
<li class=""><strong>authors:</strong> Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen, Tianyi Lin</li>
<li class=""><strong>institution:</strong> Columbia University, The Chinese University of Hong Kong, Shenzhen, Alibaba DAMO Academy, New York University Stern School of Business</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16912" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16912</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates the exploration-exploitation trade-off in Reinforcement Learning with Verifiable Rewards (RLVR) for improving LLM reasoning. It finds that spurious rewards, combined with clipping bias, reduce policy entropy to produce more confident outputs, which enhances performance, while entropy minimization alone is insufficient. The authors propose a reward-misalignment model to explain why spurious rewards can be beneficial beyond simple data contamination.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [adversarial reinforcement learning, process reward models, step-level rewards, reasoning chain partitioning, joint training, discriminator]</li>
<li class=""><strong>authors:</strong> Qihao Liu, Luoxin Ye, Wufei Ma, Yu-Cheng Chou, Alan Yuille</li>
<li class=""><strong>institution:</strong> Johns Hopkins University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16917" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16917</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Generative Adversarial Reasoner, a framework that jointly trains an LLM reasoner and an LLM-based discriminator using adversarial reinforcement learning to provide dense, step-level rewards for improving reasoning. This method enhances sample efficiency and reasoning quality by co-evolving the models to detect and correct process errors. It demonstrates consistent performance gains on mathematical benchmarks over standard RL post-training baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Random matrix theory of sparse neuronal networks with heterogeneous timescales</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational neuroscience], [random matrix theory, statistical field theory, supersymmetry, Jacobian analysis, sparse networks, spectral edge]</li>
<li class=""><strong>authors:</strong> Thiparat Chotibut, Oleg Evnin, Weerawit Horinouchi</li>
<li class=""><strong>institution:</strong> Chulalongkorn University, Vrije Universiteit Brussel, International Solvay Institutes, Jagiellonian University, King&#x27;s College London</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12767" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12767</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper develops a random matrix theory to analyze the Jacobian matrices of trained sparse neuronal networks with heterogeneous timescales. Using statistical field theory and supersymmetry methods, it analytically derives the spectral edge, linking network parameters to near-critical dynamics. The main conclusion is that this framework explains how specific network motifs and parameter distributions produce the marginally stable equilibria essential for robust working memory computation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [transformer encoder, self-supervised learning, foundation model, edge deployment, ultra-low-power microcontroller, gesture classification, kinematic regression]</li>
<li class=""><strong>authors:</strong> Matteo Fasulo, Giusy Spacone, Thorir Mar Ingolfsson, Yawei Li, Luca Benini, Andrea Cossettini</li>
<li class=""><strong>institution:</strong> ETH Zurich, University of Bologna</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15729" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15729</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces TinyMyo, a lightweight Transformer-based foundation model pre-trained with self-supervised learning for EMG signal processing. It demonstrates strong generalization across multiple tasks like gesture classification and speech recognition while being deployable on an ultra-low-power microcontroller. The work provides an open-source, efficient model for edge-based EMG applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] The Red Queen&#x27;s Trap: Limits of Deep Evolution in High-Frequency Trading</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [deep reinforcement learning, evolutionary computation, LSTM, Transformer, genetic algorithm, high-frequency trading, multi-agent simulation]</li>
<li class=""><strong>authors:</strong> Yijia Chen</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15732" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15732</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes the failure of a hybrid trading system called &quot;Galaxy Empire,&quot; which combines deep learning (LSTM/Transformer) for perception with evolutionary algorithms for agent survival in a high-frequency cryptocurrency market. Despite promising training results, the system suffered catastrophic live performance losses due to overfitting, survivor bias, and microstructure friction. The main conclusion is that increasing model complexity without true information asymmetry leads to systemic fragility in adaptive markets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Consensus dimension reduction via multi-view learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [dimension reduction], [consensus approach, multi-view learning, principal component analysis, multidimensional scaling, Isomap, locally linear embedding, t-SNE]</li>
<li class=""><strong>authors:</strong> Bingxue An, Tiffany M. Tang</li>
<li class=""><strong>institution:</strong> University of Notre Dame</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15802" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15802</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a consensus dimension reduction method that uses multi-view learning to combine multiple, potentially conflicting visualizations from different dimension reduction techniques into a single, more robust low-dimensional plot. The method identifies and preserves the shared, stable patterns across these different &quot;views.&quot; The authors demonstrate that this approach is robust to the choice of specific dimension reduction method and its hyperparameters, leading to more trustworthy and reproducible visualizations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Bayesian Modeling for Uncertainty Management in Financial Risk Forecasting and Compliance</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [financial risk management], [Bayesian modeling, LSTM, GARCH, Value-at-Risk, logistic regression, state-space model]</li>
<li class=""><strong>authors:</strong> Sharif Al Mamun, Rakib Hossain, Md. Jobayer Rahman, Malay Kumar Devnath, Farhana Afroz, Lisan Al Amin</li>
<li class=""><strong>institution:</strong> iLynx Inc., Cognitive Links, United International University, University of Maryland Baltimore County, Washington University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15739" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15739</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a Bayesian analytics framework for financial risk management, integrating models like discount-factor DLM, Bayesian logistic regression, and hierarchical Beta state-space for uncertainty quantification in volatility forecasting, fraud detection, and compliance monitoring. It shows that the framework improves risk assessment accuracy and interpretability, with GPU acceleration offering significant speedups, though challenges like sparse data remain.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Foundation Models in Biomedical Imaging: Turning Hype into Reality</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [biomedical imaging], [foundation models, clinical reasoning, causal inference, multimodal data, trustworthiness, safety, bias]</li>
<li class=""><strong>authors:</strong> Amgad Muneer, Kai Zhang, Ibraheem Hamdi, Rizwan Qureshi, Muhammad Waqas, Shereen Fouad, Hazrat Ali, Syed Muhammad Anwar, Jia Wu</li>
<li class=""><strong>institution:</strong> The University of Texas MD Anderson Cancer Center, UTHealth Houston, Massachusetts Institute of Technology, Aston University, University of Stirling, George Washington University, Children&#x27;s National Hospital</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15808" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15808</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper critically analyzes the state of foundation models in biomedical imaging, examining their capabilities in clinical reasoning, spatial understanding, and multimodal integration. It concludes that while these models are powerful assistive tools, the field must move beyond scale to develop hybrid, causally aware, and verifiably safe systems that augment human expertise, as autonomous AI-doctors remain a distant vision.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Concurrence: A dependence criterion for time series, applied to biological data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [time series analysis], [concurrence, dependence criterion, classifier, Hilbert-Schmidt Independence Criterion, canonical correlation analysis, Fourier decomposition, wavelet decomposition]</li>
<li class=""><strong>authors:</strong> Evangelos Sariyanidi, John D. Herrington, Lisa Yankowitz, Pratik Chaudhari, Theodore D. Satterthwaite, Casey J. Zampella, Jeffrey S. Morris, Edward Gunning, Robert T. Schultz, Russell T. Shinohara, Birkan Tunc</li>
<li class=""><strong>institution:</strong> The Children&#x27;s Hospital of Philadelphia, University of Pennsylvania</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16001" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16001</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces &quot;concurrence,&quot; a dependence criterion for time series that uses a classifier to distinguish between temporally aligned and misaligned segments. It is designed to capture complex non-linear interactions in biological data without requiring large datasets or prior knowledge. The method is shown to be theoretically linked with dependence and applicable across various signal types like fMRI and physiological data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Graph Neural Networks for Interferometer Simulations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [graph neural networks, interferometer simulation, optical physics, instrumentation design]</li>
<li class=""><strong>authors:</strong> Sidharth Kannan, Pooyan Goodarzi, Evangelos E. Papalexakis, Jonathan W. Richardson</li>
<li class=""><strong>institution:</strong> University of California, Santa Barbara, University of California, Riverside</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16051" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16051</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper applies Graph Neural Networks (GNNs) to simulate the complex optical physics of interferometers like LIGO for instrumentation design. The authors demonstrate that their GNN-based approach can accurately model these systems while achieving runtimes 815 times faster than state-of-the-art simulation packages. They also release a dataset of high-fidelity simulations for benchmarking future work in this area.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] BayesSum: Bayesian Quadrature in Discrete Spaces</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [probabilistic inference], [Bayesian quadrature, Gaussian process, Monte Carlo, Russian Roulette, Conway-Maxwell-Poisson, Potts model]</li>
<li class=""><strong>authors:</strong> Sophia Seulkee Kang, François-Xavier Briol, Toni Karvonen, Zonghao Chen</li>
<li class=""><strong>institution:</strong> Independent Researcher, University College London, Lappeenranta–Lahti University of Technology LUT</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16105" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16105</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces BayesSum, a novel estimator that extends Bayesian quadrature to discrete domains for computing intractable expectations. It uses a Gaussian process to incorporate prior information about the integrand, achieving higher sample efficiency than Monte Carlo methods. Theoretical and empirical results show it converges faster and requires fewer samples for tasks like parameter estimation in statistical models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Artificial Intelligence-Enabled Holistic Design of Catalysts Tailored for Semiconducting Carbon Nanotube Growth</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [machine learning, natural language processing, electronic structure databases, predictive modeling, catalyst screening, high-throughput experiments]</li>
<li class=""><strong>authors:</strong> Liu Qian, Yue Li, Ying Xie, Jian Zhang, Pai Li, Yue Yu, Zhe Liu, Feng Ding, Jin Zhang</li>
<li class=""><strong>institution:</strong> Peking University, National Center for Nanoscience and Technology, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Suzhou Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16151" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16151</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an AI-enabled holistic framework that integrates machine learning, including NLP-based embedding models and data-driven predictive models, with traditional catalyst design for synthesizing semiconducting carbon nanotubes. The method successfully screened candidate catalysts, leading to the experimental validation of highly selective catalysts, such as FeTiO3, achieving over 98% semiconducting selectivity. This approach provides a generalizable methodology for precise catalyst design and nanomaterials synthesis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Physics-Informed Neural Networks for Modeling the Martian Induced Magnetosphere</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Physics-Informed Neural Networks (PINN), data-driven model, MAVEN observations, solar wind parameters, induced magnetosphere]</li>
<li class=""><strong>authors:</strong> Jiawei Gao, Chuanfei Dong, Chi Zhang, Yilan Qin, Simin Shekarpaz, Xinmin Li, Liang Wang, Hongyang Zhou, Abigail Tadlock</li>
<li class=""><strong>institution:</strong> Boston University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16175" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16175</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper develops a data-driven model of the Martian induced magnetosphere using Physics-Informed Neural Networks (PINNs) combined with MAVEN spacecraft observations. It demonstrates that PINNs can accurately reconstruct the three-dimensional magnetic field configuration and its response to solar wind conditions, offering a new tool for studying solar wind-Mars interactions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Global universal approximation with Brownian signatures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [stochastic processes], [rough path theory, Brownian motion, signature methods, universal approximation theorems, stochastic differential equations]</li>
<li class=""><strong>authors:</strong> Mihriban Ceylan, David J. Prömel</li>
<li class=""><strong>institution:</strong> Not specified (inferred from arXiv submission)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16396" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16396</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper establishes L^p-type universal approximation theorems for functionals on rough path spaces using linear functionals acting on path signatures. The main conclusion is that these methods can approximate any p-integrable stochastic process adapted to Brownian filtration, including solutions to stochastic differential equations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] DAG Learning from Zero-Inflated Count Data Using Continuous Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [statistical machine learning], [zero-inflated generalized linear model, continuous optimization, differentiable acyclicity constraint, sparsity regularization, score-based learning]</li>
<li class=""><strong>authors:</strong> Noriaki Sato, Marco Scutari, Shuichi Kawano, Rui Yamaguchi, Seiya Imoto</li>
<li class=""><strong>institution:</strong> The University of Tokyo, Istituto Dalle Molle di Studi sull’Intelligenza Artificiale (IDSIA), Kyushu University, Aichi Cancer Center Research Institute, Nagoya University Graduate School of Medicine</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16233" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16233</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ZICO, a method for learning Directed Acyclic Graphs (DAGs) from zero-inflated count data by modeling each node with a zero-inflated generalized linear model and using continuous optimization with a differentiable acyclicity constraint. It demonstrates superior performance and faster runtimes on simulated data and performs comparably or better than existing methods for reverse engineering gene regulatory networks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [finance], [neural network, consensus-bottleneck, asset pricing, interpretable deep learning, belief aggregation]</li>
<li class=""><strong>authors:</strong> Bong-Gyu Jang, Younwoo Jeong, Changeun Kim</li>
<li class=""><strong>institution:</strong> POSTECH (Pohang University of Science and Technology)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16251" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16251</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Consensus-Bottleneck Asset Pricing Model (CB-APM), an interpretable neural network that models how dispersed investor beliefs are compressed into a consensus to predict stock returns. It demonstrates improved predictive accuracy and explanatory power over standard deep learning methods, uncovering belief-driven return dynamics not captured by traditional factor models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Advantages and limitations in the use of transfer learning for individual treatment effects in causal machine learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [causal machine learning], [transfer learning, TARNet, TL-TARNet, individual treatment effects, causal inference]</li>
<li class=""><strong>authors:</strong> Seyda Betul Aydin, Holger Brandt</li>
<li class=""><strong>institution:</strong> University of Tübingen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16489" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16489</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes TL-TARNet, a transfer learning extension of the TARNet model, to improve the estimation of individual treatment effects (ITEs) in small target datasets by leveraging knowledge from larger source datasets. Through simulations and an empirical application, the method is shown to reduce ITE estimation error and bias when a large, unbiased source is available. The results demonstrate that transfer learning can enhance causal model performance in data-scarce settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Can Transformers overcome the lack of data in the simulation of history-dependent flows?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Transformers, metriplectic neural network, physics-enhanced machine learning, Oldroyd-B, FENE model]</li>
<li class=""><strong>authors:</strong> P. Urdeitx, I. Alfaro, D. Gonzalez, F. Chinesta, E. Cueto</li>
<li class=""><strong>institution:</strong> Universidad de Zaragoza, ENSAM Arts et Métiers Institute of Technology, CNRS@CREATE LTD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16305" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16305</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates the use of Transformer architectures to simulate history-dependent fluid flows when experimental data for key phenomenological variables is missing. It compares Transformers against a thermodynamically consistent metriplectic neural network on three benchmark problems. The results show that Transformers outperform the physics-biased model in data-scarce scenarios, while the metriplectic model is superior when all state variables are fully known.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Non-Asymptotic Global Convergence of PPO-Clip</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [PPO-Clip, f-divergence regularization, Łojasiewicz inequality, non-asymptotic convergence, softmax policy]</li>
<li class=""><strong>authors:</strong> Yin Liu, Qiming Dai, Junyu Zhang, Zaiwen Wen</li>
<li class=""><strong>institution:</strong> Peking University, National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16565" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16565</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper provides a theoretical analysis of the deterministic actor-only PPO-Clip algorithm with f-divergence regularization under softmax parameterization. It establishes non-uniform Lipschitz smoothness and a Łojasiewicz inequality, proving non-asymptotic linear global convergence for forward KL regularization and local linear convergence for reverse KL regularization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Predictive Inorganic Synthesis based on Machine Learning using Small Data sets: a case study of size-controlled Cu Nanoparticles</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [ensemble regression, Latin Hypercube Sampling, AMADEUS framework, small data sets, predictive inorganic synthesis, microwave-assisted polyol synthesis]</li>
<li class=""><strong>authors:</strong> Brent Motmans, Digvijay Ghogare, Thijs G.I. van Wijk, An Hardy, Danny E.P. Vanpoucke</li>
<li class=""><strong>institution:</strong> Hasselt University, imec, Energyville, VITO (Flemish Institute for Technological Research)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16545" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16545</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This study uses machine learning with a small dataset of 25 syntheses to predict the size of copper nanoparticles. By employing Latin Hypercube Sampling and ensemble regression models via the AMADEUS framework, it achieves accurate predictions (R²=0.74) that outperform classical statistical methods. The work demonstrates that high-quality small datasets combined with interpretable ML models are sufficient for quantitative synthesis prediction in lab-scale inorganic synthesis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Muon is Provably Faster with Momentum Variance Reduction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [momentum variance reduction, linear minimization oracle, non-euclidean norm balls, gluon framework, muon, scion]</li>
<li class=""><strong>authors:</strong> Xun Qian, Hussein Rammal, Dmitry Kovalev, Peter Richtárik</li>
<li class=""><strong>institution:</strong> KAUST, Yandex Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16598" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16598</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces momentum variance reduction (MVR) into the Gluon framework, which generalizes LMO-based optimizers like Muon and Scion, to accelerate training of large language models. The proposed methods improve the convergence rate from O(1/K^{1/4}) to O(1/K^{1/3}) for non-convex problems. Numerical experiments confirm the superior iteration complexity of the new algorithms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Riemannian Stochastic Interpolants for Amorphous Particle Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [generative modeling for physics], [Riemannian stochastic interpolants, equivariant flow matching, graph neural networks, periodic boundary conditions]</li>
<li class=""><strong>authors:</strong> Louis Grenioux, Leonardo Galliano, Ludovic Berthier, Giulio Biroli, Marylou Gabrié</li>
<li class=""><strong>institution:</strong> École normale supérieure (ENS), CNRS, Sorbonne Université, Université de Paris, École polytechnique, Institut Polytechnique de Paris, Università di Trieste, ESPCI Paris</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16607" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16607</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a generative framework combining Riemannian stochastic interpolants and equivariant flow matching to sample equilibrium configurations of amorphous particle systems. The method incorporates periodic boundary conditions and particle symmetries by adapting an equivariant graph neural network to operate on a torus. The authors demonstrate that enforcing these geometric and symmetry constraints significantly improves generative performance for model glass-forming materials.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] How accurate are foundational machine learning interatomic potentials for heterogeneous catalysis?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational chemistry], [machine learning interatomic potentials, zero-shot performance, heterogeneous catalysis, density functional theory, graph neural networks, adsorption energies, transition state energies]</li>
<li class=""><strong>authors:</strong> Luuk H. E. Kempen, Raffaele Cheula, Mie Andersen</li>
<li class=""><strong>institution:</strong> Aarhus University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16702" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16702</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically evaluates the zero-shot performance of 80 foundational machine learning interatomic potentials (MLIPs) on tasks relevant to heterogeneous catalysis, such as predicting adsorption and reaction energies on surfaces. It finds that while current MLIPs can achieve high accuracy for some applications, they often fail on magnetic materials and no single model performs best universally, requiring careful selection for specific catalytic systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Non-Linear Strong Data-Processing for Quantum Hockey-Stick Divergences</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [quantum information theory], [strong data-processing inequalities, hockey-stick divergence, quantum channels, contraction coefficients, Fγ curves, reverse-Pinsker inequalities]</li>
<li class=""><strong>authors:</strong> Theshani Nuradha, Ian George, Christoph Hirche</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign, National University of Singapore, Leibniz Universität Hannover</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16778" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16778</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper establishes non-linear strong data-processing inequalities (SDPI) for quantum hockey-stick divergences, improving upon existing linear bounds. The method involves defining Fγ curves to characterize SDPI for sequential compositions of noisy quantum channels. The results enable tighter finite mixing time bounds and stronger privacy guarantees for sequential private quantum channels.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] On The Hidden Biases of Flow Matching Samplers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [generative modeling], [flow matching, optimal transport, kinetic energy, empirical minimization, gradient field]</li>
<li class=""><strong>authors:</strong> Soon Hoe Lim</li>
<li class=""><strong>institution:</strong> KTH Royal Institute of Technology, Nordita, Stockholm University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16768" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16768</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes the implicit biases in flow matching (FM) samplers when trained on finite empirical data. It shows that, unlike the population version, the empirical FM minimizer is almost never a gradient field, leading to intrinsic energetic suboptimality. The study concludes that the kinetic energy of generated samples concentrates exponentially for Gaussian sources and exhibits polynomial tails for heavy-tailed sources, with this behavior governed more by the source distribution than the target data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Few-Shot Specific Emitter Identification via Integrated Complex Variational Mode Decomposition and Spatial Attention Transfer</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [signal processing], [integrated complex variational mode decomposition, temporal convolutional network, spatial attention mechanism, few-shot learning]</li>
<li class=""><strong>authors:</strong> Chenyu Zhu, Zeyang Li, Ziyi Xie, Jie Zhang</li>
<li class=""><strong>institution:</strong> Harbin Institute of Technology (Shenzhen), KTH Royal Institute of Technology, Ranplan Wireless Network Design Ltd.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16786" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16786</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a few-shot specific emitter identification method that integrates complex variational mode decomposition for signal reconstruction and a temporal convolutional network with a spatial attention mechanism for feature extraction. The approach achieves 96% accuracy using only 10 symbols on a public dataset, demonstrating effectiveness in low-data scenarios without prior knowledge.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Cartesian-nj: Extending e3nn to Irreducible Cartesian Tensor Product and Contracion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [equivariant machine learning], [irreducible Cartesian tensors, Cartesian-3j symbol, Cartesian-nj symbol, e3nn, tensor product, MACE, NequIP, Allegro]</li>
<li class=""><strong>authors:</strong> Zemin Xu, Chenyu Wu, Wenbo Xie, Daiqian Xie, P. Hu</li>
<li class=""><strong>institution:</strong> ShanghaiTech University, Nanjing University, The Queen’s University of Belfast</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16882" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16882</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the Cartesian-3j and Cartesian-nj symbols, which are analogues of the Wigner symbols, to enable irreducible Cartesian tensor products and contractions. It extends the e3nn library to support these operations and releases the cartnn package, implementing Cartesian versions of models like MACE and NequIP. The work aims to systematically compare Cartesian and spherical tensor models to assess potential advantages and foundational soundness of Cartesian architectures.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-23T10:56:01.000Z" itemprop="dateModified">Dec 23, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/cslg"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">cs.LG</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/daily/cslg/20251222-20251228"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251222-20251228 (cs.LG)</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-18" class="table-of-contents__link toc-highlight">2025-12-18</a></li><li><a href="#2025-12-19" class="table-of-contents__link toc-highlight">2025-12-19</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/cs_LG/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_LG/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>