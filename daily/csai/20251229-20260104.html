<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_AI/20251229-20260104" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251229-20260104 (cs.AI) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/csai/20251229-20260104"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251229-20260104 (cs.AI) | AI头条"><meta data-rh="true" name="description" content="2025-12-29"><meta data-rh="true" property="og:description" content="2025-12-29"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/csai/20251229-20260104"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/csai/20251229-20260104" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/csai/20251229-20260104" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.AI","item":"https://jokebear666.github.io/ai_toutiao/daily/csai"},{"@type":"ListItem","position":3,"name":"20251229-20260104 (cs.AI)","item":"https://jokebear666.github.io/ai_toutiao/daily/csai/20251229-20260104"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.647bd1ff.css">
<script src="/ai_toutiao/assets/js/runtime~main.f73b021c.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.6153aad8.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/arxiv-daily"><span title="Arxiv每日论文" class="linkLabel_WmDU">Arxiv每日论文</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Collapse sidebar category &#x27;cs.AI&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cs_AI/20251215-20251221"><span title="20251215-20251221 (cs.AI)" class="linkLabel_WmDU">20251215-20251221 (cs.AI)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/csai/20251222-20251228"><span title="20251222-20251228 (cs.AI)" class="linkLabel_WmDU">20251222-20251228 (cs.AI)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/csai/20251229-20260104"><span title="20251229-20260104 (cs.AI)" class="linkLabel_WmDU">20251229-20260104 (cs.AI)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csoh"><span title="cs.OH" class="categoryLinkLabel_W154">cs.OH</span></a><button aria-label="Expand sidebar category &#x27;cs.OH&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/daily/csai"><span>cs.AI</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251229-20260104 (cs.AI)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251229-20260104 (cs.AI)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-29">2025-12-29<a href="#2025-12-29" class="hash-link" aria-label="Direct link to 2025-12-29" title="Direct link to 2025-12-29" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251229] CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [dream-replay reinforcement learning, evolutionary algorithms, adaptive code generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Santhosh Kumar Ravindran</p>
</li>
<li class="">
<p><strong>institution:</strong> Microsoft Corporation</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21351" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21351</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces CosmoCore-Evo, an extension of CosmoCore that integrates evolutionary algorithms into the dream-replay reinforcement learning framework for code generation, 2. Proposes treating RL trajectories as &quot;genomes&quot; that undergo mutation and selection during nocturnal replay to enhance adaptability and novelty, 3. Develops enterprise-tuned fitness functions incorporating efficiency, compliance, and scalability metrics, and demonstrates improved performance on benchmarks with distribution shifts.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/318e081ebd83b7b451c47feed4db9ca1fa830f70f86844ea65dc8e8551ea3656_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/318e081ebd83b7b451c47feed4db9ca1fa830f70f86844ea65dc8e8551ea3656_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> CosmoCore-Evo enhances the affective dream-replay reinforcement learning framework by incorporating evolutionary algorithms to improve adaptability in code generation. It treats RL trajectories as genomes for mutation and selection, enabling agents to break free from trained patterns and adapt to changing environments like API updates. The method achieves higher novelty and faster adaptation compared to baselines, as validated on benchmarks including HumanEval variants and BigCodeBench.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] EcoNet: Multiagent Planning and Control Of Household Energy Resources Using Active Inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [active inference, multi-agent systems, home energy management systems (HEMS), distributed energy resources (DER), Bayesian inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> John C. Boik, Kobus Esterhuysen, Jacqueline B. Hynes, Axel Constant, Ines Hipolito, Mahault Albarracin, Alex B. Kiefer, Karl Friston</p>
</li>
<li class="">
<p><strong>institution:</strong> VERSES, University of Sussex, Macquarie University, UCL (University College London)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21343" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21343</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes EcoNet, a novel Bayesian framework for household and neighborhood energy management based on active inference. 2. Addresses the challenge of planning under uncertainty (e.g., weather, solar forecasts) while handling complex, conditional, and conflicting household goals. 3. Demonstrates the approach through simulations for multiagent planning and control of distributed energy resources.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b885d3c6c9f392a494063522c79cde9a59fead8ab6b04010259b6485f007cec8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b885d3c6c9f392a494063522c79cde9a59fead8ab6b04010259b6485f007cec8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces EcoNet, a multiagent planning and control system for household energy resources using active inference, a Bayesian approach, to manage uncertainty and conflicting goals. The method aims to optimize energy use, costs, and emissions while maintaining comfort. Simulation results demonstrate its potential for improved energy management and coordination.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Multi-Agent LLM Committees for Autonomous Software Beta Testing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [se], [automated software testing], [multi-agent system, large language model, vision-language model, consensus voting, beta testing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sumanth Bharadwaj Hachalli Karanam, Dhiwahar Adhithya Kennady</p>
</li>
<li class="">
<p><strong>institution:</strong> New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21352" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21352</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel multi-agent committee framework that uses a three-round voting protocol for consensus-based decision-making in software testing. 2. Integration of vision-enabled LLMs and diverse testing personas to systematically explore and understand web application user interfaces. 3. Demonstrated significant performance improvements over single-agent baselines in task success, bug detection (F1 score), and security vulnerability coverage on established benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40573d0b1209c41e9825c09111398107cc51ee9d86c5234b50bea2515d0ab37f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40573d0b1209c41e9825c09111398107cc51ee9d86c5234b50bea2515d0ab37f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high cost of manual software beta testing and the limitations of single-agent LLM approaches by proposing a multi-agent committee framework. The method employs diverse, vision-enabled LLMs that collaborate through a structured voting protocol and persona-driven behavior to autonomously test web applications. The results show that this multi-agent approach significantly outperforms single-agent baselines in task success rates, bug detection, and security testing coverage, making it suitable for real-time CI/CD integration.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Fairness Is Not Just Ethical: Performance Trade-Off via Data Correlation Tuning to Mitigate Bias in ML Software</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [se], [software fairness], [correlation tuning, phi-coefficient, multi-objective optimization, pre-processing, bias mitigation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ying Xiao, Shangwen Wang, Sicen Liu, Dingyuan Xue, Xian Zhan, Yepang Liu, Jie M. Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> King’s College London, National University of Defense Technology, Southern University of Science and Technology, The Hong Kong Polytechnic University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21348" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21348</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel pre-processing bias mitigation method called Correlation Tuning (CoT) that adjusts data correlations. 2. Introduces the Phi-coefficient as an intuitive measure to quantify correlation between sensitive attributes and labels. 3. Employs multi-objective optimization to address proxy biases, demonstrating superior effectiveness over state-of-the-art methods in single and multiple attribute scenarios.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4f80681a9ac6a6c3ad7d2bd938623a06836acba00279d9cec368a5ebbe44df3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4f80681a9ac6a6c3ad7d2bd938623a06836acba00279d9cec368a5ebbe44df3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Correlation Tuning (CoT), a novel pre-processing method to mitigate bias in ML software by adjusting data correlations using the Phi-coefficient and multi-objective optimization. It frames fairness as a core software quality issue. Extensive evaluation shows CoT significantly improves performance for unprivileged groups and reduces key bias metrics, outperforming existing methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [db], [text-to-SQL], [unanswerable question detection, few-shot prompting, biomedical databases]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jasmin Saxer, Isabella Maria Aigner, Luise Linzmeier, Andreas Weiler, Kurt Stockinger</p>
</li>
<li class="">
<p><strong>institution:</strong> Zurich University of Applied Sciences, University of Zurich</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21345" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21345</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Query Carefully, a pipeline integrating LLM-based SQL generation with explicit detection of unanswerable inputs. 2. Constructed OncoMX-NAQ, a benchmark dataset of 80 no-answer questions for biomedical text-to-SQL. 3. Demonstrated that balanced few-shot prompting with both answerable and unanswerable examples achieves high unanswerable-detection accuracy without degrading performance on answerable queries.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d95c00b7fa86810771a1c8fb0ff6fd8768baaa0419f172cc5c7a3068ac67a64_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d95c00b7fa86810771a1c8fb0ff6fd8768baaa0419f172cc5c7a3068ac67a64_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the risk of text-to-SQL systems generating executable but incorrect SQL for ambiguous or unanswerable queries, especially in biomedical contexts. The authors propose the Query Carefully pipeline, which uses an LLM with schema-aware prompts and few-shot examples to detect and abstain from unanswerable inputs. Their evaluation shows the method achieves high detection accuracy for structurally unanswerable queries, though challenges remain for semantic ambiguities like missing values.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Study of Solving Life-and-Death Problems in Go Using Relevance-Zone Based Solvers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Relevance-Zone Based Search, AlphaZero, Life-and-Death problems, heuristic search, pattern table]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chung-Chin Shih, Ti-Rong Wu, Ting Han Wei, Yu-Shan Hsu, Hung Guei, I-Chen Wu</p>
</li>
<li class="">
<p><strong>institution:</strong> Academia Sinica, National Yang Ming Chiao Tung University, Kochi University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21365" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21365</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://rlg.iis.sinica.edu.tw/papers/study-LD-RZ" target="_blank" rel="noopener noreferrer" class="">https://rlg.iis.sinica.edu.tw/papers/study-LD-RZ</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Applied and analyzed Relevance-Zone Based Search (RZS) and relevance-zone pattern tables to solve Go Life-and-Death problems, identifying critical relevance-zones. 2. Discovered that solvers can find rare patterns and even alternative solutions differing from established human grandmaster answers. 3. Identified and analyzed key limitations of current solvers, such as misjudging rare patterns and prioritizing direct survival over territory maximization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6376caf4e3dced23991e86eb4d5b0f512ce3623019adeaf18ee253d5fd00c507_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6376caf4e3dced23991e86eb4d5b0f512ce3623019adeaf18ee253d5fd00c507_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes the performance of state-of-the-art computer Go solvers using Relevance-Zone Based Search on classic Life-and-Death problems. The study finds that while these solvers can identify critical areas and discover rare patterns, they exhibit limitations like misjudging pattern values and having a non-human preference for direct survival over territory. The authors suggest future approaches to address these solver issues.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] From Visual Perception to Deep Empathy: An Automated Assessment Framework for House-Tree-Person Drawings Using Multimodal LLMs and Multi-Agent Collaboration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [computational psychology], [multimodal large language model, multi-agent collaboration, cosine similarity, projective assessment, psychological report generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuide Wen, Yu Sun, Beier Ku, Zhi Gao, Lijun Ma, Yang Yang, Can Jiao</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Shenzhen University, University of Oxford, Guangzhou University of Chinese Medicine, Harbin Institute of Technology, Shenzhen Institute of Education Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21360" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21360</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a novel multi-agent collaboration framework to automate the interpretation of House-Tree-Person drawings, decoupling visual feature recognition from psychological inference. 2. Demonstrated that multimodal large language models (MLLMs) can achieve expert-level baseline comprehension in interpreting projective drawings, with high semantic similarity to human expert interpretations. 3. Introduced a destigmatizing narrative and social-psychological perspective integration to correct visual hallucinations and enhance the ecological validity and coherence of automated psychological reports.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/681955eb18a63880e0327c5debb6e992188de12ca52cef0c9c34258f09c3a91d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/681955eb18a63880e0327c5debb6e992188de12ca52cef0c9c34258f09c3a91d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an automated assessment framework for the House-Tree-Person drawing test using multimodal LLMs and multi-agent collaboration to address issues of subjective scoring and lack of standardization. The framework effectively interprets drawings with high similarity to expert analysis and generates coherent psychological reports. The results confirm the potential of multimodal models as standardized tools for projective psychological assessment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Reflection-Driven Control for Trustworthy Code Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [reflection-driven control, secure code generation, trustworthy agents, reflective memory, safety control]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bin Wang, Jiazheng Quan, Xingrui Yu, Hansen Hu, Yuhao, Ivor Tsang</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University, Xiamen University, Agency for Science, Technology and Research (A*STAR)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21354" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21354</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Reflection-Driven Control, a standardized and pluggable control module that integrates self-reflection as an explicit, internal step in an agent&#x27;s reasoning process. 2. Instantiates the method for secure code generation, using a reflection loop to monitor decisions and retrieve repair examples/guidelines from an evolving reflective memory to inject constraints. 3. Empirically demonstrates that the approach substantially improves security and policy compliance of generated code while preserving functional correctness, with minimal overhead.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5126773543627efe84c972810f76eb0631192d8d90ed930bbc91d54b6664007b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5126773543627efe84c972810f76eb0631192d8d90ed930bbc91d54b6664007b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the lack of reliable safety controls in LLM agents by proposing Reflection-Driven Control, a module that makes self-reflection an explicit, continuous part of the agent&#x27;s reasoning to monitor and constrain its decisions using evidence from a reflective memory. Evaluated on security-critical code generation tasks, the method significantly improves code security and compliance while maintaining functionality, offering a practical path toward trustworthy AI coding agents.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] AInsteinBench: Benchmarking Coding Agents on Scientific Repositories</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [se], [software engineering], [benchmark, scientific computing, code generation, pull requests, test-driven verification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Titouan Duston, Shuo Xin, Yang Sun, Daoguang Zan, Aoyan Li, Shulin Xin, Kai Shen, Yixiao Chen, Qiming Sun, Ge Zhang, Jiashuo Liu, Huan Zhou, Jingkai Liu, Zhichen Pu, Yuanheng Wang, Bo-Xuan Ge, Xin Tong, Fei Ye, Zhi-Chao Zhao, Wen-Biao Han, Zhoujian Cao, Yueran Zhao, Weiluo Ren, Qingshen Long, Yuxiao Liu, Anni Huang, Yidi Du, Yuanyuan Rong, Jiahao Peng</p>
</li>
<li class="">
<p><strong>institution:</strong> ByteDance Seed, Princeton University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21373" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21373</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/ByteDance-Seed/AInsteinBench" target="_blank" rel="noopener noreferrer" class="">https://github.com/ByteDance-Seed/AInsteinBench</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a novel benchmark (AInsteinBench) for evaluating LLM agents in end-to-end scientific development using real-world, production-grade codebases. 2. Curates tasks from maintainer-authored pull requests across six diverse scientific domains, ensuring scientific challenge and calibrated difficulty. 3. Employs executable environments and test-driven verification to measure core competencies beyond surface-level code generation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aadf07b453d8d5a061a247b4c4e5e4fc27a43f5b1ffca131e81738bd3728f348_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aadf07b453d8d5a061a247b4c4e5e4fc27a43f5b1ffca131e81738bd3728f348_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces AInsteinBench, a benchmark designed to evaluate LLM agents&#x27; ability to function as scientific computing developers by solving tasks derived from real pull requests in scientific repositories. It uses executable environments and test-driven verification to assess deeper competencies. The benchmark provides a new standard for measuring AI&#x27;s role in computational scientific research.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Safe Path Planning and Observation Quality Enhancement Strategy for Unmanned Aerial Vehicles in Water Quality Monitoring Tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [robotic perception and planning], [Interfered Fluid Dynamical System (IFDS), Model Predictive Control (MPC), Dynamic Flight Altitude Adjustment (DFAA)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuanshuang Fu, Qianyao Wang, Qihao Wang, Bonan Zhang, Jiaxin Zhao, Yiming Cao, Zhijun Li</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China, North China University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21375" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21375</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a dynamic prediction model that transforms time-varying light and shadow disturbances (e.g., sun glint) into 3D virtual obstacles for path planning. 2. Introduces an improved IFDS algorithm combined with an MPC framework to generate smooth, safe, and dynamically feasible real-time trajectories for UAVs. 3. Designs a Dynamic Flight Altitude Adjustment (DFAA) mechanism to actively lower flight altitude in narrow observable areas, enhancing spatial resolution and data quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5582bc8dd27c45953b75b142df4da9d25f5164a9ed81f8842a846572fbb8a2f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5582bc8dd27c45953b75b142df4da9d25f5164a9ed81f8842a846572fbb8a2f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of UAV water quality monitoring being hindered by dynamic illumination disturbances like shadows and sun glint, which degrade spectral data. The proposed method actively plans safe flight paths by modeling disturbances as obstacles, using an improved IFDS and MPC for real-time trajectory optimization, and dynamically adjusting altitude to improve data quality. Simulation results show the method achieves a 98% obstacle avoidance success rate and increases effective observation data volume by approximately 27%.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] LLM-Driven Feature-Level Adversarial Attacks on Android Malware Detectors</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [adversarial attacks], [adversarial attack, large language model, retrieval-augmented generation, Android malware detection, adversarial training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tianwei Lan, Farid Naït-Abdesselam</p>
</li>
<li class="">
<p><strong>institution:</strong> Université Paris Cité</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21404" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21404</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LAMLAD, a novel adversarial attack framework that uses a dual-agent LLM architecture (manipulator and analyzer) to generate feature-level perturbations for evading Android malware detectors., 2. Integrates Retrieval-Augmented Generation (RAG) into the LLM pipeline to improve the efficiency and contextual awareness of the attack., 3. Proposes and evaluates an adversarial training-based defense strategy to enhance model robustness against the proposed LAMLAD-style attacks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6061210b194ba5cf79f70b8959faa3abe6b3e91ffad512d9cfd319de948593bb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6061210b194ba5cf79f70b8959faa3abe6b3e91ffad512d9cfd319de948593bb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes LAMLAD, a novel adversarial attack framework that leverages the generative and reasoning capabilities of Large Language Models (LLMs) to bypass ML-based Android malware classifiers. The method uses a dual-agent LLM architecture with RAG to generate realistic, functionality-preserving feature perturbations, achieving a high attack success rate. The paper also demonstrates that adversarial training can significantly reduce the effectiveness of such attacks, enhancing model robustness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Feasible strategies in three-way conflict analysis with three-valued ratings</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [conflict analysis], [three-way conflict analysis, feasible strategy, consistency measure, non-consistency measure, weighted agent-issue evaluation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jing Liu, Mengjun Hu, Guangming Lang</p>
</li>
<li class="">
<p><strong>institution:</strong> Changsha University of Science and Technology, Saint Mary&#x27;s University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21420" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21420</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel framework for identifying feasible strategies in conflict resolution from the perspectives of consistency and non-consistency. 2. Introduces weighted consistency and non-consistency measures that incorporate the importance of both agents and issues. 3. Develops algorithms to systematically identify feasible strategies, L-order feasible strategies, and optimal solutions, demonstrating superior performance over existing approaches.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4b2e48e756e98608f4388b841aa7940f0ba7b237737fa314abc4db83fbf680f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4b2e48e756e98608f4388b841aa7940f0ba7b237737fa314abc4db83fbf680f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the gap in formulating actionable strategies for conflict resolution within three-way conflict analysis. It proposes a method that computes agent clique ratings and uses novel weighted consistency and non-consistency measures to identify feasible and optimal strategies. The approach is validated through case studies and shown to outperform conventional conflict analysis models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Three-way conflict analysis based on alliance and conflict functions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [decision theory], [three-way decision, conflict analysis, alliance function, conflict function, alliance set]</p>
</li>
<li class="">
<p><strong>authors:</strong> Junfang Luo, Mengjun Hu, Guangming Lang, Xin Yang, Keyun Qin</p>
</li>
<li class="">
<p><strong>institution:</strong> Southwestern University of Finance and Economics, University of Regina, Changsha University of Science and Technology, Southwest Jiaotong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21419" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21419</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel separation of the traditional auxiliary function into distinct alliance and conflict functions to clarify semantic interpretation in conflict analysis. 2. Introduces a framework for trisecting agents, issues, and agent pairs based on the new alliance and conflict functions. 3. Explores and applies new concepts such as alliance sets and strategies to solve crucial questions in conflict analysis, demonstrating the model with a real-world application.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54277009925f58600c765060d6cbc575e96e562e3c9748aa2f54e97b83024e0b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54277009925f58600c765060d6cbc575e96e562e3c9748aa2f54e97b83024e0b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the semantic ambiguity in aggregating traditional three-way conflict analysis functions by proposing a separation into distinct alliance and conflict functions. The method enables clearer trisection of agents, issues, and agent pairs, leading to the exploration of alliance sets and strategies. The main conclusion is that this separation provides a more interpretable and applicable framework for conflict analysis, as illustrated by a real-world example.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Teaching People LLM&#x27;s Errors and Getting it Right</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [human-ai interaction], [overreliance, failure patterns, mental models, user study, meta-labels]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nathan Stringham, Fateme Hashemi Chaleshtori, Xinyuan Yan, Zhichao Xu, Bei Wang, Ana Marasović</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Utah</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21422" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21422</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Empirically demonstrated that failure patterns for LLMs do exist by identifying sizable, error-prone meta-label groups in datasets, countering the hypothesis that their absence caused prior teaching failures. 2. Evaluated automated methods for discovering these failure patterns (prompting and embedding-based) and found mixed results, identifying a key bottleneck in the teaching pipeline. 3. Proposed and validated a new metric for teaching effectiveness—assessing a user&#x27;s ability to anticipate LLM errors using taught patterns—which showed a positive effect, unlike traditional human-AI team accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/83a262b8daf44fdf951904b9202074fd9db4ef9e9666cd5769ec1d8514053804_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/83a262b8daf44fdf951904b9202074fd9db4ef9e9666cd5769ec1d8514053804_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates why prior attempts to teach users about LLM failure patterns to reduce overreliance have failed. It finds that failure patterns do exist, but automated methods to discover them are unreliable, and proposes a new user-centric evaluation metric that shows teaching can be effective. The conclusion is that teaching failure patterns is viable but requires better failure-discovery methods and appropriate metrics.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Three-way decision with incomplete information based on similarity and satisfiability</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [rough set theory], [three-way decision, incomplete information, similarity degree, satisfiability degree, approximability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Junfang Luo, Mengjun Hu, Keyun Qin</p>
</li>
<li class="">
<p><strong>institution:</strong> Southwest Jiaotong University, University of Regina</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21421" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21421</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new measure of similarity degree of objects as a generalization of equivalence relations for handling incomplete information in the computational formulation of three-way decision. 2. Introduces a measure of satisfiability degree of formulas as a quantitative generalization of satisfiability for the conceptual formulation of three-way decision under incomplete information. 3. Proposes novel approaches for three-way decision using approximability of objects and confidence of formulas, pointing out new research directions beyond the common method of similarity classes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d87784acc1b1cc397b822153c118be23974be9721c3d19c9dfc95fbbaef158e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d87784acc1b1cc397b822153c118be23974be9721c3d19c9dfc95fbbaef158e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper generalizes the computational and conceptual formulations of three-way decision to handle incomplete information, which is common in real-world applications. For the computational side, it introduces a similarity degree measure and explores decision-making via α-similarity classes and approximability; for the conceptual side, it proposes a satisfiability degree measure and studies approaches using α-meaning sets and confidence. The work extends rough set theory and identifies promising new directions for three-way decision under uncertainty.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [computational ethics], [moral context, probabilistic clustering, LLM semantics, interpretable prediction, human judgment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Geoffroy Morlat, Marceau Nahon, Augustin Chartouny, Raja Chatila, Ismael T. Freire, Mehdi Khamassi</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Intelligent Systems and Robotics, Sorbonne University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21439" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21439</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An empirically grounded dataset of 300 moral scenarios with human ternary judgments. 2. A reproducible pipeline (COMETH) combining human judgments, probabilistic context learning, and LLM-based semantic abstraction. 3. An interpretable, context-sensitive moral prediction model that outperforms end-to-end LLM prompting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f4abc9f666c2d29dadd77869bddf3f159d0bbc8839c7c0f65bbdb4c29ad40c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f4abc9f666c2d29dadd77869bddf3f159d0bbc8839c7c0f65bbdb4c29ad40c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that moral judgments depend heavily on context. It proposes the COMETH framework, which uses probabilistic clustering on human judgment data and LLM-based semantic abstraction to learn and explain action-specific moral contexts. The main conclusion is that COMETH significantly outperforms direct LLM prompting in aligning with human majority judgments while providing interpretable predictions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [masked diffusion language models, reinforcement learning, parallel decoding, on-policy optimization, unmasking planner]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shirui Chen, Jiantao Jiao, Lillian J. Ratliff, Banghua Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Washington, University of California, Berkeley</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21446" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21446</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes dUltra, an on-policy RL framework (GRPO-based) for learning efficient unmasking strategies in MDLMs. 2. Introduces a joint optimization scheme for the base diffusion model and a new unmasking planner head using a composite reward. 3. Demonstrates improved accuracy-efficiency trade-off over heuristic and distillation baselines in reasoning and code generation tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f1e67e2dde4b6b9e98e4e3c5574326f0e2d63114afe47049e17c2ae04bb41b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f1e67e2dde4b6b9e98e4e3c5574326f0e2d63114afe47049e17c2ae04bb41b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the slow sampling speed of masked diffusion language models (MDLMs) by proposing dUltra, a reinforcement learning framework that learns an optimal strategy for parallel token unmasking. The method jointly optimizes the diffusion model and a planner head using rewards for correctness, distillation, and step count. The results show dUltra achieves a better trade-off between accuracy and efficiency than existing methods, advancing towards &quot;diffusion supremacy&quot; over autoregressive models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [Ground Penetrating Radar (GPR), Multi-modal Chain Feature Fusion (MCFF), Global Attention Mechanism (GAM), DCGAN, transfer learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haotian Lv, Yuhui Zhang, Jiangbo Dai, Hanli Wu, Jiaji Wang, Dawei Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Harbin Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21452" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21452</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a DCGAN-based data augmentation strategy to synthesize high-fidelity GPR images, mitigating data scarcity. 2. Designed a novel Multi-modal Chain and Global Attention Network (MCGA-Net) integrating Multi-modal Chain Feature Fusion (MCFF) and a Global Attention Mechanism (GAM) for enhanced defect representation. 3. Utilized MS COCO transfer learning to fine-tune the backbone network, accelerating convergence and improving model generalization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4debe9b33028e70ed06ab5d1f340e5cb76dcb8d09f7adf0d8195a2422c90668_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4debe9b33028e70ed06ab5d1f340e5cb76dcb8d09f7adf0d8195a2422c90668_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the subjective and inefficient interpretation of Ground Penetrating Radar (GPR) images for road defect detection by proposing a comprehensive framework. The method combines DCGAN-based data augmentation, a novel MCGA-Net architecture with feature fusion and attention mechanisms, and transfer learning. The proposed model achieves high precision, recall, and robustness, establishing a new paradigm for automated GPR-based defect detection.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image retrieval], [polyp re-identification, gated progressive fusion, multimodal feature fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Suncheng Xiang, Xiaoyang Wang, Junjie Jiang, Hejia Wang, Dahong Qian</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, Peking University, Shanghai Fifth People&#x27;s Hospital</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21476" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21476</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/JeremyXSC/GPF-Net" target="_blank" rel="noopener noreferrer" class="">https://github.com/JeremyXSC/GPF-Net</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1) Proposes a novel multimodal feature fusion framework named GPF-Net for polyp re-identification. 2) Introduces a gated progressive fusion strategy for layer-wise refinement of semantic information through multi-level feature interactions. 3) Demonstrates state-of-the-art performance on standard benchmarks, showing the benefit of multimodal fusion over unimodal methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75fe09ad1f753b61f2c30ab37c16d0deef5060ca95658846bc6dcaf6bb4c53f9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75fe09ad1f753b61f2c30ab37c16d0deef5060ca95658846bc6dcaf6bb4c53f9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of colonoscopic polyp re-identification, where coarse high-level features harm small object matching. The authors propose GPF-Net, a Gated Progressive Fusion network that selectively fuses multi-level features using gates. Experiments show this multimodal approach outperforms state-of-the-art unimodal ReID models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [mixture-of-experts (MoE), disaggregated expert parallelism (DEP), task scheduling, inference throughput, fine-grained pipelining]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xinglin Pan, Shaohuai Shi, Wenxiang Lin, Yuxin Wang, Zhenheng Tang, Wei Wang, Xiaowen Chu</p>
</li>
<li class="">
<p><strong>institution:</strong> The Hong Kong University of Science and Technology (Guangzhou), Harbin Institute of Technology (Shenzhen), Hong Kong Baptist University, The Hong Kong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21487" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21487</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1) Partitioning intensive computation and communication tasks into smaller, fine-grained tasks to enable pipelining, including support for shared experts. 2) Formulating a fine-grained task scheduling optimization problem that supports variable task granularity and ordering. 3) Developing an efficient solver to navigate the large solution space and derive a near-optimal task schedule.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44cc55e59c66470ffb4e47c93ad8e48f60e8377f30eff6289fbad1cfcb862c96_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44cc55e59c66470ffb4e47c93ad8e48f60e8377f30eff6289fbad1cfcb862c96_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the memory-intensive inference problem in Mixture-of-Experts (MoE) models by proposing FinDEP, a fine-grained task scheduling algorithm for Disaggregated Expert Parallelism (DEP). FinDEP improves inference throughput by maximizing task overlap through computational partitioning and optimized scheduling. Experiments on systems with up to 32 GPUs show throughput improvements of up to 1.61x over prior methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Oogiri-Master: Benchmarking Humor Understanding via Oogiri</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [humor understanding], [Oogiri, benchmark, linguistic analysis, incongruity resolution, insight-augmented prompting]</p>
</li>
<li class="">
<p><strong>authors:</strong> Soichiro Murakami, Hidetaka Kamigaito, Hiroya Takamura, Manabu Okumura</p>
</li>
<li class="">
<p><strong>institution:</strong> CyberAgent, Nara Institute of Science and Technology, Institute of Science Tokyo</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21494" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21494</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Oogiri-Master, a benchmark for rigorous evaluation of humor understanding in LLMs, and Oogiri-Corpus, a dataset with ~100 diverse responses per prompt and independent human ratings to reduce bias. 2. Conducts quantitative analysis of linguistic factors (e.g., text length, ambiguity, incongruity resolution) to derive objective metrics for predicting human funniness judgments. 3. Benchmarks LLMs and human baselines, showing state-of-the-art models approach human performance and that insight-augmented prompting improves model humor understanding.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41486d2e77493633c6cf66d7f5134ccf646d1df0d17e6d258bc98cc3132ef02b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41486d2e77493633c6cf66d7f5134ccf646d1df0d17e6d258bc98cc3132ef02b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of evaluating humor understanding in LLMs by introducing the Oogiri-Master benchmark and Oogiri-Corpus dataset, which enable rigorous analysis of funniness through diverse responses and independent human ratings. It quantitatively analyzes linguistic factors to derive objective metrics and benchmarks LLMs, demonstrating that advanced models approach human-level performance and benefit from insight-augmented prompting. The work provides a principled basis for advancing humor understanding in AI.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal forgery detection], [visual-textual co-reasoning, cross-cues-aware chain of thought (CCT), GRPO-based optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Fanwei Zeng, Changtao Miao, Jing Huang, Zhiya Tan, Shutao Gong, Xiaoming Yu, Yang Wang, Huazhe Tan, Weibin Yao, Jianshu Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Ant Group, Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21482" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21482</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed LogicLens, a unified framework for visual-textual co-reasoning that jointly performs detection, grounding, and explanation for text-centric forgery analysis. 2. Introduced a Cross-Cues-aware Chain of Thought (CCT) mechanism for iterative cross-validation of visual and textual cues, and a weighted multi-task reward function for GRPO-based optimization. 3. Created the RealText dataset with 5,397 images and fine-grained annotations using a novel PR² (Perceiver, Reasoner, Reviewer) multi-agent annotation pipeline.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/138f8fcb4727c77950b23146e1184851aa8b8cea95056b1d6b161c37e231ad80_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/138f8fcb4727c77950b23146e1184851aa8b8cea95056b1d6b161c37e231ad80_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces LogicLens, a unified visual-textual co-reasoning framework for analyzing text-centric forgeries. It uses a novel Cross-Cues-aware Chain of Thought mechanism and multi-task optimization to jointly handle detection, grounding, and explanation. Experiments show LogicLens achieves state-of-the-art performance, significantly outperforming specialized frameworks and other MLLMs in zero-shot and dense-text scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [wearable sensing, actigraphy encoder, projection module, frozen LLM, behavioral summarization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aiwei Zhang, Arvind Pillai, Andrew Campbell, Nicholas C. Jacobson</p>
</li>
<li class="">
<p><strong>institution:</strong> Dartmouth College</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21506" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21506</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs) for free-text generation of daily behavioral summaries. 2. Constructs a novel, large-scale dataset of 54,383 (actigraphy, text) pairs derived from real-world NHANES recordings. 3. Demonstrates superior performance over prompt-based baselines in semantic fidelity and lexical accuracy, with qualitative analysis showing the model captures circadian structure and behavioral transitions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of generating natural language summaries from raw physiological signals like actigraphy. It proposes MotionTeller, a framework that integrates a pretrained actigraphy encoder with a frozen LLM via a projection module. The model, trained on a novel dataset, outperforms baselines in generating fluent, human-centered descriptions of daily behavior.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [diffusion models], [GRPO, mode collapse, diversity-aware reward, spectral clustering, structure-aware regularization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Henglin Liu, Huijuan Huang, Jing Wang, Chang Liu, Xiu Li, Xiangyang Ji</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Kuaishou Technology (Kling Team), Sun Yat-sen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21514" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21514</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and analyzes the mode collapse problem in GRPO-based image generation from both reward modeling and generation dynamics perspectives. 2. Proposes a distributional creativity bonus reward based on semantic grouping via spectral clustering to encourage novel visual modes. 3. Introduces a structure-aware regularization that applies stronger constraints during early-stage denoising to preserve diversity without sacrificing quality optimization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862e58c2beed3d5383235af01560a2dc06bc384250083e4027ddff5a3aa32368_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862e58c2beed3d5383235af01560a2dc06bc384250083e4027ddff5a3aa32368_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the mode collapse problem in GRPO-based image generation, where models produce homogenized outputs. The proposed DiverseGRPO method introduces a diversity-aware reward based on semantic clustering and a structure-aware regularization to preserve generation diversity. Experiments show the method significantly improves semantic diversity while maintaining image quality, establishing a better quality-diversity trade-off.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Selective LLM-Guided Regularization for Enhancing Recommendation Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [recommender systems], [selective regularization, knowledge distillation, cold-start, long-tail, gating mechanism]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shanglin Yang, Zhan Shi</p>
</li>
<li class="">
<p><strong>institution:</strong> Sichuan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21526" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21526</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a selective LLM-guided regularization framework (S-LLMR) that activates LLM supervision only when a gating mechanism predicts the LLM to be reliable, addressing the issue of inaccurate global distillation. 2. Introduces a trainable gating mechanism informed by user history length, item popularity, and model uncertainty to dynamically decide when to apply LLM-based pairwise ranking supervision. 3. Demonstrates through experiments that the method improves overall accuracy and yields substantial gains in cold-start and long-tail recommendation scenarios, outperforming global distillation baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76336a3d123794e83843c14c4b799afd0817948ee9dfeb2f6f19ce776f183796_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76336a3d123794e83843c14c4b799afd0817948ee9dfeb2f6f19ce776f183796_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of leveraging large language models (LLMs) for recommendation without suffering from their high cost and unreliability in certain scenarios. It proposes Selective LLM-Guided Regularization (S-LLMR), a model-agnostic framework that uses a gating mechanism to selectively apply LLM-based supervision only when the LLM is predicted to be reliable. Experiments show this approach improves recommendation accuracy, especially for cold-start users and long-tail items, outperforming methods that uniformly distill LLM knowledge.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Hierarchy-Aware Fine-Tuning of Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal learning], [hierarchical classification, vision-language models, efficient fine-tuning, LoRA, Tree-Path KL Divergence]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiayu Li, Rajesh Gangireddy, Samet Akcay, Wei Cheng, Juhua Hu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Washington, Intel</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21529" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21529</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an efficient hierarchy-aware fine-tuning framework for Vision-Language Models (VLMs) that updates only a few parameters. 2. Introduces two novel loss functions: Tree-Path KL Divergence (TP-KL) for vertical consistency along label paths and Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) for horizontal consistency among sibling classes. 3. Demonstrates consistent improvements in Full-Path Accuracy and reduced Tree-based Inconsistency Error across multiple hierarchical benchmarks with minimal parameter overhead.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b60d2fc3dd0ad92b5ac8857f844fed2dbe51e280dfb702c26028d91c14fd92_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b60d2fc3dd0ad92b5ac8857f844fed2dbe51e280dfb702c26028d91c14fd92_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of adapting large Vision-Language Models (VLMs) to hierarchical classification tasks efficiently. The proposed method combines two novel hierarchy-aware loss functions (TP-KL and HiSCE) with lightweight LoRA adaptation to enforce structural consistency in predictions. Experiments show the approach improves accuracy and reduces inconsistency across taxonomy levels with minimal computational cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [adaptive length penalty, reinforcement learning, constrained optimization, Lagrangian primal-dual, reasoning efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yanhao Li, Lu Ma, Jiaran Zhang, Lexiang Tang, Wentao Zhang, Guibo Luo</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University, Harbin Institute of Technology, Shenzhen</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21540" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21540</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Leash, a reinforcement learning framework that formulates length control as a constrained optimization problem and uses a Lagrangian primal-dual method to dynamically adjust the penalty coefficient. 2. Introduces an adaptive mechanism that intensifies the penalty when generations exceed the target length and relaxes it when they are shorter, guiding models toward concise reasoning without sacrificing performance. 3. Demonstrates experimentally that Leash reduces average reasoning length by 60% across diverse tasks while maintaining competitive performance, offering a practical paradigm for efficient LLMs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ec5b3e2930213678e6d04b060a50d89faaaacded209387c96170a775f9db310_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ec5b3e2930213678e6d04b060a50d89faaaacded209387c96170a775f9db310_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of LLMs producing overly long reasoning traces, which increases computational cost. It proposes Leash, an adaptive reinforcement learning framework that dynamically adjusts length penalties using a Lagrangian method to balance conciseness and accuracy. Experiments show it reduces reasoning length by 60% while maintaining performance, providing an effective approach for efficient LLM reasoning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Human-AI Interaction Alignment: Designing, Evaluating, and Evolving Value-Centered AI For Reciprocal Human-AI Futures</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [human-ai interaction], [bidirectional alignment, value-centered design, interactive alignment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hua Shen, Tiffany Knearem, Divy Thakkar, Pat Pataranutaporn, Anoop Sinha, Yike, Jenny T. Liang, Lama Ahmad, Tanu Mitra, Brad A. Myers, Yang Li</p>
</li>
<li class="">
<p><strong>institution:</strong> NYU Shanghai, MBZUAI, Google, Massachusetts Institute of Technology, Carnegie Mellon University, OpenAI, University of Washington, Google DeepMind</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21551" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21551</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a shift from unidirectional to bidirectional human-AI alignment, framing it as a dynamic, reciprocal co-adaptation process. 2. Emphasizes embedding human and societal values into AI alignment research through value-centered design. 3. Aims to establish an interdisciplinary research agenda for responsible, reciprocal human-AI futures through collaborative workshop activities.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/acbc6d9188f5aaa4289d9a01fb321cc29a9a54b03061c38e31010c7988a9ca12_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/acbc6d9188f5aaa4289d9a01fb321cc29a9a54b03061c38e31010c7988a9ca12_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This workshop paper identifies the inadequacy of traditional, one-way AI alignment and proposes a bidirectional human-AI alignment framework where humans and AI co-adapt through interaction and value-centered design. It aims to bring together interdisciplinary researchers to explore methods for interactive alignment and societal impact evaluation. The main conclusion is the need for a shared agenda to advance responsible, reciprocal collaboration between humans and AI systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Bidirectional Human-AI Alignment in Education for Trustworthy Learning Environments</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [ai for education], [human-ai alignment, trustworthy ai, adaptive learning, educational technology, ai ethics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hua Shen</p>
</li>
<li class="">
<p><strong>institution:</strong> NYU Shanghai, New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21552" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21552</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the novel concept of &quot;bidirectional human-AI alignment&quot; for education, emphasizing mutual adaptation between humans and AI systems. 2. Explores the evolution of AI&#x27;s role in education from a support tool to a collaborative partner, analyzing its impact on teacher roles and student agency. 3. Provides actionable strategies for policymakers, developers, and educators to ensure AI advances equity, transparency, and human flourishing in learning environments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7f40fe114da7bae34b09e44db18fa32bb4f64e57bd01e75e96afc80c2ddc136_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7f40fe114da7bae34b09e44db18fa32bb4f64e57bd01e75e96afc80c2ddc136_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the risks of AI in education, such as bias and loss of autonomy, by proposing the concept of bidirectional human-AI alignment. The method involves not only embedding human values into AI but also equipping educators and students to guide these technologies. It concludes that reframing AI adoption as a process of mutual adaptation is key to creating trustworthy learning environments where humans and AI can grow together.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Exploration of Reproducible Generated Image Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image forensics], [AIGC detection, reproducibility, generalizability, diffusion models, binary classification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yihang Duan</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in the provided content. (Author name only, no affiliation or email domain provided)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21562" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21562</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and analyzes the root causes of poor reproducibility in AIGC image detection research, citing omitted experimental details and overfitting to generator-specific features. 2. Provides empirical evidence for the reproducibility issue by constructing a test dataset and reproducing a representative detection method, demonstrating performance drops under cross-generator testing. 3. Proposes reference directions for the research community to improve reproducibility and generalizability, such as more comprehensive disclosure of experimental details.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c9df3d6873df2ea90e378adf52625583637b76319eb9a55ebf11b8f17abf1fc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c9df3d6873df2ea90e378adf52625583637b76319eb9a55ebf11b8f17abf1fc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the reproducibility and generalizability challenges in AI-Generated Content (AIGC) image detection. By reviewing key literature, building a test dataset, and reproducing a detection method, it identifies causes like omitted experimental details and model overfitting. The study concludes that while basic performance can be reproduced, detection fails when preprocessing disrupts key features or when testing across different generators, highlighting the need for better methodological disclosure and robustness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Towards Long-window Anchoring in Vision-Language Model Distillation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [knowledge distillation, long-context, rotary position embeddings (RoPE), attention mechanism, vision-language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haoyi Zhou, Shuo Li, Tianyu Chen, Qi Song, Chonghan Gao, Jianxin Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Beihang University, Zhongguancun Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21576" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21576</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies the problem of limited effective context windows in small, distilled vision-language models despite using identical positional embeddings and architectures as their larger counterparts. 2. Proposes LAid, a novel distillation method featuring progressive distance-weighted attention matching and learnable RoPE response gain modulation to transfer long-range attention mechanisms. 3. Demonstrates that LAid-distilled models achieve significantly longer effective context windows (up to 3.2x) while maintaining performance on standard benchmarks, and provides spectral analysis showing successful transfer of low-frequency attention components.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4663870a97f8c352e6cd352d0f9f9be365648a5545642796d256fa99c7ddcd4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4663870a97f8c352e6cd352d0f9f9be365648a5545642796d256fa99c7ddcd4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that small, distilled vision-language models have much shorter effective context windows than their large teacher models. The authors propose LAid, a new distillation method that transfers long-range attention capabilities via progressive attention matching and learnable RoPE modulation. Their method successfully extends the context window of small models by up to 3.2 times while preserving performance on standard benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] NEMO-4-PAYPAL: Leveraging NVIDIA&#x27;s Nemo Framework for empowering PayPal&#x27;s Commerce Agent</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [NeMo Framework, LoRA, Nemotron SLM, hyperparameter sweep, multi-agent system]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ali Sahami, Sudhanshu Garg, Andrew Wang, Chaitanya Kulkarni, Farhad Farahani, Sean Yun-Shiuan Chuang, Jian Wan, Srinivasan Manoharan, Uma Kona, Nitin Sharma, Linsey Pang, Prakhar Mehrotra, Jessica Clark, Mark Moyou</p>
</li>
<li class="">
<p><strong>institution:</strong> PayPal AI, NVIDIA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21578" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21578</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. The first application of NVIDIA&#x27;s NeMo Framework to optimize commerce-specific agents. 2. An LLM-powered fine-tuning strategy for retrieval-focused commerce tasks. 3. A demonstration of significant latency and cost improvements while maintaining agent quality, providing a scalable framework for multi-agent system optimization in production e-commerce.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90dda98c8c5c5f9d75ede0c681c9024dc5973d432f246b90eed23aad0a03c916_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90dda98c8c5c5f9d75ede0c681c9024dc5973d432f246b90eed23aad0a03c916_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents the optimization of PayPal&#x27;s Commerce Agent, a multi-agent system, by fine-tuning a Nemotron small language model using NVIDIA&#x27;s NeMo Framework and LoRA. The method involved systematic hyperparameter sweeps to improve the performance-critical search component. The results show that the fine-tuned model effectively resolves the key latency issue in the retrieval component, which accounted for over 50% of response time, while maintaining or enhancing overall system performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Unified Definition of Hallucination, Or: It&#x27;s the World Model, Stupid</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [hallucination detection &amp; evaluation], [hallucination, world modeling, knowledge conflict, benchmark, language model evaluation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Emmy Liu, Varun Gangal, Chelsea Zou, Xiaoqi Huang, Michael Yu, Alex Chang, Zhuofu Tao, Sachin Kumar, Steven Y. Feng</p>
</li>
<li class="">
<p><strong>institution:</strong> Carnegie Mellon University, Stanford University, The Ohio State University, Patronus AI, DegenAI Labs, Independent Researchers</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21577" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21577</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified definition of hallucination as inaccurate internal world modeling that is observable to the user, synthesizing prior definitions. 2. Provides a framework for analyzing hallucinations by varying the reference world model and knowledge conflict policy, clarifying what constitutes a hallucination versus other error types. 3. Outlines plans for a family of benchmarks based on synthetic, fully-specified world models to stress-test and improve the world modeling components of language models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e2cc31121f769cca7464239d4aa27b26c8c4bc903970a4833fbebac56dc9b85_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e2cc31121f769cca7464239d4aa27b26c8c4bc903970a4833fbebac56dc9b85_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper argues that the persistent problem of hallucination in language models stems from inaccurate internal world modeling. It unifies various historical definitions under this core concept and proposes a framework for clearer evaluation. The authors conclude by sketching plans for new benchmarks to rigorously test and improve language models&#x27; world modeling capabilities.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [vision-language model, logic tree reasoning, medical multimodal diagnosis, explainable AI, LLaVA]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zelin Zang, Wenyi Gu, Siqi Ma, Dan Yang, Yue Shen, Zhu Zhang, Guohui Fan, Wing-Kuen Ling, Fuji Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsientang Institute of Advanced Study (TIAS), Westlake University, Ant Group, China-Japan Friendship Hospital</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21583" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21583</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a diagnostic framework integrating vision-language alignment with logic-regularized reasoning to enhance reliability. 2. Introduces a reasoning controller and logic tree generator to decompose tasks and assemble verifiable conclusions, improving interpretability. 3. Demonstrates improved diagnostic accuracy and more interpretable reasoning traces on multimodal medical benchmarks like MedXpertQA.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b6b8d6d34a614d3cb042f13334dede18494914ca29d6f0fd6f4467f789871f82_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b6b8d6d34a614d3cb042f13334dede18494914ca29d6f0fd6f4467f789871f82_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of unreliable reasoning and hallucinations in existing multimodal medical AI models. It proposes a diagnostic framework built on LLaVA that combines vision-language alignment with logic-regularized reasoning to generate verifiable conclusions via logic trees. Evaluations show the method improves diagnostic accuracy and yields more interpretable reasoning traces, advancing trustworthy multimodal medical AI.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] LLM-I2I: Boost Your Small Item2Item Recommendation Model with Large Language Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [item-to-item recommendation, data-centric, long-tail items, data augmentation, data filtering]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yinfu Feng, Yanjing Wu, Rong Xiao, Xiaoyi Zen</p>
</li>
<li class="">
<p><strong>institution:</strong> Alibaba Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21595" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21595</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LLM-I2I, a data-centric framework that leverages Large Language Models to enhance I2I recommendation models without altering their architecture. 2. Introduces an LLM-based data generator to synthesize user-item interactions, specifically targeting long-tail items to alleviate data sparsity. 3. Designs an LLM-based data discriminator to filter out noisy interactions from both real and synthetic data, improving overall data quality for training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6cb08ea9b26b612493e4d48e7db88c46a869c2050c94d47b75488adcaf6ddfa9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6cb08ea9b26b612493e4d48e7db88c46a869c2050c94d47b75488adcaf6ddfa9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses data sparsity and noise problems in Item-to-Item (I2I) recommendation systems by proposing LLM-I2I, a data-centric framework that uses an LLM to generate synthetic interactions for long-tail items and filter noisy data. The refined data is then used to train existing I2I models. Experimental results on industrial and academic datasets show significant improvements in recommendation accuracy, especially for long-tail items, and deployment on a large e-commerce platform led to measurable gains in recall and gross merchandise value.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] AMS-IO-Bench and AMS-IO-Agent: Benchmarking and Structured Reasoning for Analog and Mixed-Signal Integrated Circuit Input/Output Design</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [AMS IC design, LLM-based agent, structured reasoning, design automation, I/O ring generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhishuai Zhang, Xintian Li, Shilong Liu, Aodong Zhang, Lu Jie, Nan Sun</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Princeton University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21613" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21613</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Arcadia-1/AMS-IO-Agent" target="_blank" rel="noopener noreferrer" class="">https://github.com/Arcadia-1/AMS-IO-Agent</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed AMS-IO-Agent, a domain-specialized LLM-based agent for structure-aware I/O subsystem generation in AMS ICs. 2. Introduced AMS-IO-Bench, a benchmark for wirebond-packaged AMS I/O ring automation. 3. Demonstrated the first reported human-agent collaborative AMS IC design where an LLM agent&#x27;s output was directly used in a silicon tape-out, achieving over 70% DRC+LVS pass rate and reducing design time from hours to minutes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7205181105fdcb012bb4c8c5b3cce6565751edc220003d3784f6dbf648ee893a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7205181105fdcb012bb4c8c5b3cce6565751edc220003d3784f6dbf648ee893a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the labor-intensive and non-reusable nature of analog and mixed-signal (AMS) integrated circuit I/O design by proposing AMS-IO-Agent, an LLM-based agent that uses structured domain knowledge and intent structuring to automate the process. The method connects natural language design intent to industrial deliverables and is evaluated on a new benchmark, AMS-IO-Bench. The agent significantly outperforms baseline LLMs, achieves a high verification pass rate, and its generated I/O ring was successfully fabricated, demonstrating practical effectiveness in real design flows.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Democratizing Drug Discovery with an Orchestrated, Knowledge-Driven Multi-Agent Team for User-Guided Therapeutic Design</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [multi-agent platform, knowledge graph, physiologically based pharmacokinetic (PBPK) simulations, autonomous execution, human-in-the-loop]</p>
</li>
<li class="">
<p><strong>authors:</strong> Takahide Suzuki, Kazuki Nakanishi, Takashi Fujiwara, Hideyuki Shimizu</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Science Tokyo, Kyoto University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21623" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21623</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces OrchestRA, a human-in-the-loop multi-agent platform that unifies biology, chemistry, and pharmacology into an autonomous discovery engine for drug design. 2. Features an architecture with specialized agents (Biologist, Chemist, Pharmacologist) governed by an Orchestrator, which actively execute simulations and reason over results to create a dynamic feedback loop for iterative optimization. 3. Democratizes therapeutic design by transforming drug discovery from a stochastic search into a programmable, evidence-based engineering discipline through the integration of autonomous execution with human guidance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbd4a841925fb9129111928c81fe29ac7016c26df168e9b4ca87c8782a692d5e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbd4a841925fb9129111928c81fe29ac7016c26df168e9b4ca87c8782a692d5e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of fragmented and passive tools in therapeutic discovery by proposing OrchestRA, a multi-agent platform where specialized AI agents autonomously execute and reason over biological, chemical, and pharmacological tasks. This creates a dynamic feedback loop for iterative drug candidate optimization, guided by human input. The conclusion is that this approach transforms drug discovery into a more programmable and evidence-based engineering process.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Multiple-play Stochastic Bandits with Prioritized Arm Capacity Sharing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-armed bandits], [multiple-play bandits, prioritized resource sharing, regret analysis, combinatorial optimization, UCB]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hong Xie, Haoran Gu, Yanying Huang, Tao Tan, Defu Lian</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology of China, Chongqing University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21626" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21626</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new variant of the multiple-play stochastic bandit model (MSB-PRS) that incorporates prioritized capacity sharing among plays, tailored for resource allocation in LLM and edge intelligence applications. 2. Establishes instance-independent and instance-dependent regret lower bounds for the proposed model, characterizing its fundamental learning difficulty. 3. Designs an offline optimal policy solver (MSB-PRS-OffOpt) and an online UCB-based learning algorithm with theoretical regret guarantees that nearly match the derived lower bounds.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8562089dc3d9fa0a65e8caf8921b51648e1718efd62395a7af2fadba8cf952d9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8562089dc3d9fa0a65e8caf8921b51648e1718efd62395a7af2fadba8cf952d9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a new multi-armed bandit model where multiple plays with priorities compete for the stochastic capacity of arms. The authors design an algorithm that first computes an optimal allocation offline and then uses it within an online UCB-based strategy, proving that its regret nearly matches the fundamental lower bounds they establish for this problem.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Monte Carlo Tree Search, Upper Confidence Bound, Variance-Aware, Prior-Based Tree Policy, Inverse-RPO]</p>
</li>
<li class="">
<p><strong>authors:</strong> Maximilian Weichart</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Regensburg</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21648" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21648</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Max-We/inverse-rpo" target="_blank" rel="noopener noreferrer" class="">https://github.com/Max-We/inverse-rpo</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Inverse-RPO, a general methodology to systematically derive prior-based UCTs from any prior-free UCB., 2. Applies Inverse-RPO to UCB-V to create two new variance-aware prior-based tree policies., 3. Provides an extension to the mctx library for variance-aware UCTs, showing minimal code changes and improved performance over PUCT in benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c9098504a8a9013ab805fffa4a23f04af76e28f5d8b8a0353e1e7d5583f589_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c9098504a8a9013ab805fffa4a23f04af76e28f5d8b8a0353e1e7d5583f589_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of extending prior-based tree policies in Monte Carlo Tree Search beyond the empirically derived PUCT. The authors propose Inverse-RPO, a principled method to derive prior-based UCTs from any prior-free UCB, and apply it to create variance-aware policies. Their new policies outperform the standard PUCT across multiple benchmarks without added computational cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D object grounding], [temporal multimodal grounding, LiDAR-image fusion, language-conditioned decoding, UniScene representation, NuPrompt benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Fudan University, Huawei Technologies Ltd.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21641" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21641</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes TrackTeller, a unified temporal multimodal framework for 3D grounding that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning. 2. Introduces a shared UniScene representation aligned with textual semantics to generate language-aware 3D proposals. 3. Demonstrates significant performance improvements on the NuPrompt benchmark, including a 70% relative gain in Average Multi-Object Tracking Accuracy and a 3.15-3.4x reduction in False Alarm Frequency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc60054c622874cc83217afc715702b65eb56126f722620ffb7004f46ebe296d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc60054c622874cc83217afc715702b65eb56126f722620ffb7004f46ebe296d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of grounding natural language references to objects in dynamic 3D driving scenes, which often depend on recent motion or behavior. The authors propose TrackTeller, a framework that fuses LiDAR and camera data with language, builds a unified scene representation, and uses temporal reasoning to refine object identification. Experiments show that TrackTeller significantly outperforms existing baselines in language-grounded tracking accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Near-Optimal Coalition Structures in Polynomial Time</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [cooperative game theory], [coalition structure generation, anytime algorithms, sparse relaxations, dynamic programming, MILP]</p>
</li>
<li class="">
<p><strong>authors:</strong> Angshul Majumdar</p>
</li>
<li class="">
<p><strong>institution:</strong> Indraprastha Institute of Information Technology, Delhi</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21657" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21657</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proves that under a &quot;sparse synergy&quot; model, sparse relaxation methods can find near-optimal coalition structures in polynomial time with high probability. 2. Demonstrates that broad classes of dynamic programming and MILP algorithms require exponential time to achieve comparable solution quality. 3. Establishes a rigorous probabilistic anytime performance separation favoring sparse relaxations over exact methods for the CSG problem.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb8e459bba93eeaf4c9237729ad06cf13b47ef6a941811135ed634157dd979c7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb8e459bba93eeaf4c9237729ad06cf13b47ef6a941811135ed634157dd979c7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the coalition structure generation (CSG) problem. It compares three algorithmic paradigms and proves that, under a random sparse synergy model, sparse relaxation methods can find near-optimal solutions in polynomial time, while exact methods like DP and MILP require exponential time to reach similar quality, establishing a clear anytime performance advantage for the sparse approach.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [swarm intelligence], [Ant Colony Optimization, structural prior, load-aware objective, overlap suppression, multi-robot path planning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zikun Guo, Adeyinka P. Adedigba, Rammohan Mallipeddi, Heoncheol Lee</p>
</li>
<li class="">
<p><strong>institution:</strong> Kyungpook National University, Kumoh National Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21654" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21654</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a structure-induced exploration framework that integrates structural priors into ACO initialization to constrain the search space. 2. Designs a pheromone update rule that emphasizes structurally meaningful connections and incorporates a load-aware objective to balance total travel distance with individual robot workload. 3. Introduces an explicit overlap suppression strategy to ensure distinct and balanced task allocation across the robot team.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca4635b64758650f78e762004770f2a7ac8eb62cd36aa2db98d90af82d3f6eae_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca4635b64758650f78e762004770f2a7ac8eb62cd36aa2db98d90af82d3f6eae_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of scalable and balanced multi-robot path planning. It proposes a new framework that integrates structural priors into Ant Colony Optimization, along with a load-aware objective and overlap suppression, to improve route compactness, stability, and workload distribution. The method demonstrates consistent improvements over metaheuristic baselines and offers a scalable solution for applications like logistics and search-and-rescue.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [YOLO-NAS, YOLOv8, perception, autonomous vehicles, custom dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jalal Khan</p>
</li>
<li class="">
<p><strong>institution:</strong> United Arab Emirates University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21673" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21673</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [spatiotemporal forecasting], [probabilistic forecasting, uncertainty estimation, principal component analysis, road impedance, traffic flow]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haochen Lv, Yan Lin, Shengnan Guo, Xiaowei Mao, Hong Nie, Letian Gong, Youfang Lin, Huaiyu Wan</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Jiaotong University, Aalborg University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21685" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21685</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a dynamic impedance evolution network to model directional traffic transfer patterns driven by congestion and flow variability, revealing causes of uncertainty and enhancing reliability and interpretability. 2. Designs a principal component network to forecast the dominant eigenvectors of future flow covariance, enabling the capture of spatiotemporal uncertainty correlations. 3. Integrates domain-specific transportation theory with spatiotemporal principal component learning for probabilistic traffic flow forecasting, achieving superior performance over existing methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f835b2051524d67481fbf9f4479086a541b29307c2876046f2c3ee4eec60f92_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f835b2051524d67481fbf9f4479086a541b29307c2876046f2c3ee4eec60f92_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes RIPCN, a Road Impedance Principal Component Network for probabilistic traffic flow forecasting. It integrates transportation theory with principal component learning to model the causes of uncertainty and capture spatiotemporal uncertainty correlations. Experimental results show it outperforms existing probabilistic forecasting methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] BeHGAN: Bengali Handwritten Word Generation from Plain Text Using Generative Adversarial Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [handwritten text generation], [Generative Adversarial Networks, Handwritten Text Generation, Bengali, Dataset, Pre-processing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md. Rakibul Islam, Md. Kamrozzaman Bhuiyan, Safwan Muntasir, Arifur Rahman Jawad, Most. Sharmin Sultana Samu</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in provided text. Affiliation/domain cannot be reliably inferred.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21694" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21694</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a method for generating Bengali handwritten words from plain text using GANs, addressing a significant research gap. 2. Developed and used a novel, self-collected dataset of Bengali handwriting from approximately 500 diverse individuals. 3. Demonstrated the ability to produce diverse and realistic handwritten outputs through the described approach.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8b36b4afe805283b88409b54815e1bd251762075786246ae741c57cb65c191a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8b36b4afe805283b88409b54815e1bd251762075786246ae741c57cb65c191a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of research on Bengali handwritten text generation by proposing a GAN-based method to generate words from plain text. The authors created a new dataset of Bengali handwriting samples from hundreds of contributors. The work successfully generates diverse handwritten outputs and contributes to advancing research in this area for the Bengali language.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [agentic AI, consensus-driven reasoning, explainable AI, responsible AI, multi-model governance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Eranga Bandara, Tharaka Hewa, Ross Gore, Sachin Shetty, Ravi Mukkamala, Peter Foytik, Abdul Rahman, Safdar H. Bouk, Xueping Liang, Amin Hass, Sachini Rajapakse, Ng Wee Keong, Kasun De Zoysa, Aruna Withanage, Nilaan Loganathan</p>
</li>
<li class="">
<p><strong>institution:</strong> Old Dominion University, University of Oulu, Deloitte &amp; Touche LLP, Florida International University, Nanyang Technological University, University of Colombo, IcicleLabs.AI, Accenture Technology Labs, Effectz.AI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21699" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21699</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel RAI/XAI agent architecture for production workflows based on multi-model consensus and reasoning-layer governance. 2. Introduces a mechanism where a consortium of heterogeneous LLM/VLM agents generate independent outputs, exposing uncertainty and alternatives for structured consolidation. 3. Demonstrates that the consensus-driven approach improves robustness, transparency, and operational trust across diverse real-world agentic AI workflows.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed22161f8f004c98e3fb6124bac7991548de5e54f47adb42f0d3eb1095409e6e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed22161f8f004c98e3fb6124bac7991548de5e54f47adb42f0d3eb1095409e6e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of explainability and responsibility in increasingly autonomous agentic AI systems. It proposes a new architecture where multiple AI agents generate candidate outputs, and a dedicated reasoning agent consolidates them while enforcing safety constraints, thereby improving decision robustness and auditability. The work provides a practical framework for building agentic systems that are both scalable and responsible by design.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [audio deepfake detection], [transfer learning, zero-shot inference, fine-tuning, Bengali audio, BanglaFake dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Most. Sharmin Sultana Samu, Md. Rakibul Islam, Md. Zahid Hossain, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in the provided content.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21702" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21702</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducts the first systematic benchmark for Bengali deepfake audio detection using the BanglaFake dataset. 2. Evaluates and demonstrates the limited performance of multiple pre-trained models in a zero-shot setting for this task. 3. Shows that fine-tuning deep learning models (e.g., ResNet18) significantly improves detection performance, establishing an effective approach for low-resource languages.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66eab5318a9b87f6facd46827f1723103def2a66467b77d3a3f1b6ea7a41d92f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66eab5318a9b87f6facd46827f1723103def2a66467b77d3a3f1b6ea7a41d92f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the underexplored problem of Bengali deepfake audio detection. It first evaluates several pre-trained models using zero-shot inference, finding limited performance, and then fine-tunes various architectures, with ResNet18 achieving the best results. The study concludes that fine-tuning is crucial for effective deepfake detection in low-resource languages like Bengali.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Enabling Conversational Behavior Reasoning Capabilities in Full-Duplex Speech</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [spoken dialogue systems], [Graph-of-Thoughts, full-duplex, speech acts, causal inference, multimodal transformer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuchang Pan, Siddharth Banerjee, Dhruv Hebbar, Siddhant Patel, Akshaj Gupta, Kan Jen Cheng, Hanjo Kim, Zeyi Austin Li, Martin Q. Ma, Tingle Li, Gopala Anumanchipalli, Jiachen Lian</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, University of California, Berkeley, Carnegie Mellon University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21706" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21706</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://got-duplex.github.io/" target="_blank" rel="noopener noreferrer" class="">https://got-duplex.github.io/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A framework that models conversational behavior reasoning as causal inference within a Graph-of-Thoughts (GoT) to enable interpretable decision-making in full-duplex dialogue. 2. A hierarchical labeling scheme and hybrid training corpus combining simulated dialogues with human rationales and real speech to learn causal and temporal dependencies between intents and speech acts. 3. A system that structures streaming predictions as an evolving graph, allowing a multimodal transformer to forecast the next speech act, generate justifications, and dynamically refine its reasoning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaad0398d39f15391b728b9e3c53af71ff071dcfd269c61b0a277091d58ee7f3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaad0398d39f15391b728b9e3c53af71ff071dcfd269c61b0a277091d58ee7f3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of explicit reasoning in full-duplex spoken dialogue systems by proposing a framework that models the perception-reasoning-generation loop as causal inference within a Graph-of-Thoughts (GoT). The method uses a hierarchical behavior detection model and a hybrid corpus to learn dependencies, enabling an agent to predict the next speech act and generate interpretable justifications. Experiments show the framework provides robust behavior detection and interpretable reasoning, establishing a foundation for benchmarking conversational reasoning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [ai-generated text detection], [transformer, fine-tuning, zero-shot, Bengali, paraphrase detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md. Rakibul Islam, Most. Sharmin Sultana Samu, Md. Zahid Hossain, Farhad Uz Zaman, Md. Kamrozzaman Bhuiyan</p>
</li>
<li class="">
<p><strong>institution:</strong> Not specified in provided content.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21709" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21709</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducts the first comparative study of transformer models for detecting AI-generated paraphrases specifically in the Bengali language. 2. Demonstrates that zero-shot evaluation of pre-trained models yields near-chance performance, highlighting the necessity of task-specific fine-tuning for this problem. 3. Shows that fine-tuning significantly boosts performance, with XLM-RoBERTa, mDeBERTa, and MultilingualBERT achieving high accuracy (~91%), establishing a strong baseline for future research.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b32597f75301412c6dbf1765506d21eb52c7e7727c1e3eefdfaa406f8c4ae44_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b32597f75301412c6dbf1765506d21eb52c7e7727c1e3eefdfaa406f8c4ae44_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of detecting AI-generated paraphrased text in Bengali, a low-resource language. It evaluates five transformer models in zero-shot and fine-tuned settings, finding that fine-tuning is essential and leads to high detection accuracy (~91%) for several models. The work establishes a foundation for robust AI-generated content detection systems in Bengali.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [interpretability &amp; analysis], [latent tokens, chain-of-thought, model reliability, causal analysis, shortcut learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuyi Zhang, Boyu Tang, Tianjie Ju, Sufeng Duan, Gongshen Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21711" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21711</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces &quot;Steering Experiments&quot; to causally test the impact of perturbing latent reasoning tokens, revealing COCONUT tokens are insensitive to perturbation unlike explicit CoT tokens. 2. Conducts &quot;Shortcut Experiments&quot; to evaluate models under biased and out-of-distribution settings, demonstrating COCONUT exploits dataset artifacts rather than performing genuine reasoning. 3. Repositions COCONUT as a &quot;pseudo-reasoning&quot; mechanism that generates plausible traces to conceal shortcut dependence, challenging its claimed reasoning capabilities.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99abfa3b8406909febaa5ee077a1feab3c1d8b8cda1eebe350774e19cb82eb77_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99abfa3b8406909febaa5ee077a1feab3c1d8b8cda1eebe350774e19cb82eb77_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the reliability of latent reasoning tokens in LLMs, specifically Chain-of-Continuous-Thought (COCONUT). Through causal steering and adversarial shortcut experiments, it finds that COCONUT tokens are uninterpretable placeholders insensitive to perturbation and that the method relies on dataset shortcuts. The main conclusion is that COCONUT is a pseudo-reasoning mechanism that inflates benchmark performance without faithful reasoning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [communication &amp; networking], [multiconnectivity, SAGIN, resource allocation, agentic reinforcement learning, heterogeneous networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abd Ullah Khan, Adnan Shahid, Haejoon Jung, Hyundong Shin</p>
</li>
<li class="">
<p><strong>institution:</strong> Kyung Hee University, Ghent University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21717" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21717</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a comprehensive review of current developments and key implementation challenges in SAGIN-enabled multiconnectivity. 2. Highlights the transformative potential of AI-driven approaches, particularly agentic reinforcement learning, for resource optimization in heterogeneous SAGIN environments. 3. Presents a case study demonstrating that learning-based methods can effectively enhance network performance (latency, capacity) with a moderate trade-off in power consumption.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31a130dffac74abb8ad0616817d555bf857333050b690554853596eda30c2fa7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31a130dffac74abb8ad0616817d555bf857333050b690554853596eda30c2fa7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper reviews the challenges of implementing multiconnectivity in heterogeneous Space-Air-Ground Integrated Networks (SAGIN) and proposes AI-driven solutions, specifically agentic reinforcement learning, for optimal resource allocation. A case study shows these methods significantly improve latency and capacity, albeit with a moderate increase in power consumption as a trade-off. The work concludes by outlining open research problems for realizing efficient SAGIN-enabled multiconnectivity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] CATCH: A Controllable Theme Detection Framework with Contextualized Clustering and Hierarchical Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [dialogue systems], [theme detection, topic clustering, hierarchical generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rui Ke, Jiahui Xu, Shenghao Yang, Kuang Wang, Feng Jiang, Haizhou Li</p>
</li>
<li class="">
<p><strong>institution:</strong> The Chinese University of Hong Kong, Shenzhen; Shenzhen University of Advanced Technology; National University of Singapore</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21715" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21715</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A context-aware topic representation method that enriches utterance semantics using surrounding topic segments. 2. A preference-guided topic clustering mechanism that jointly models semantic proximity and personalized feedback for cross-dialogue theme alignment. 3. A hierarchical theme generation mechanism designed to suppress noise and produce robust, coherent topic labels.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da012bcf7b19d126b0f1a64e4fc67ee4a82a999c3d110d6b449ab0c750d9458e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da012bcf7b19d126b0f1a64e4fc67ee4a82a999c3d110d6b449ab0c750d9458e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes CATCH, a framework for controllable theme detection in dialogues, which integrates contextualized clustering and hierarchical generation to address sparse utterances and user preference alignment. It demonstrates effectiveness on the DSTC-12 benchmark using an 8B LLM for both clustering and label generation quality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] An Information Theoretic Perspective on Agentic System Design</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [mutual information, noisy channel, compressor-predictor, on-device AI, information-theoretic]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shizhe He, Avanika Narayan, Ishan S. Khare, Scott W. Linderman, Christopher Ré, Dan Biderman</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21720" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21720</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an information-theoretic framework for analyzing agentic LM systems, viewing the compressor as a noisy channel. 2. Introduces a task-independent estimator of mutual information between context and compression to quantify compression quality. 3. Empirically demonstrates that scaling compressor models is more effective than scaling predictors for performance and cost, enabling efficient on-device compression.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124535642e159e8f7a123525ffbf3cb5f163a7ae4a7876a4d1e71e7e6c885ace_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124535642e159e8f7a123525ffbf3cb5f163a7ae4a7876a4d1e71e7e6c885ace_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the ad-hoc design of agentic LM systems that use a compressor LM to summarize context for a predictor LM. It proposes an information-theoretic framework using mutual information to evaluate compressors, finding that larger compressors are more accurate, concise, and information-dense, making scaling compressors more effective than scaling predictors for cost-efficient performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] HELP: Hierarchical Embodied Language Planner for Household Tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [embodied agent, hierarchical planning, large language model, household tasks, open source LLM]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alexandr V. Korchemnyi, Anatoly O. Onishchenko, Eva A. Bakaeva, Alexey K. Kovalev, Aleksandr I. Panov</p>
</li>
<li class="">
<p><strong>institution:</strong> MIRAI, Cognitive AI Systems Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21723" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21723</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Hierarchical Embodied Language Planner (HELP) architecture using multiple LLM-based agents for decomposing and grounding natural language instructions. 2. Demonstrates the approach on a real-world household task using an embodied agent. 3. Focuses on the use of relatively small, open-source LLMs to enable autonomous deployment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of planning for embodied agents following ambiguous natural language instructions in complex environments. It proposes HELP, a hierarchical planner using multiple LLM-based agents to decompose high-level instructions into grounded, executable subtasks. The method is evaluated on a household task with a real robot, showing the feasibility of using smaller, open-source LLMs for autonomous operation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Model of Causal Explanation on Neural Networks for Tabular Data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [explainable ai], [CENNET, structural causal models, entropy, causal explanation, tabular data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Takashi Isozaki, Masahiro Yamamoto, Atsushi Noda</p>
</li>
<li class="">
<p><strong>institution:</strong> Sony Computer Science Laboratories, Inc., Sony Corporation of America</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21746" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21746</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CENNET, a novel causal explanation method for neural network predictions on tabular data. 2. Introduces a new explanation power index based on entropy for evaluating the proposed method. 3. Demonstrates the method&#x27;s effectiveness by combining structural causal models with neural networks for causal explanations, validated on synthetic and quasi-real data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02e8ba8968728eba560984773ed8ba2b124e42c46e3c839dec8a1ca2a5975ce1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02e8ba8968728eba560984773ed8ba2b124e42c46e3c839dec8a1ca2a5975ce1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of providing causal explanations for neural network predictions on tabular data, where pseudo-correlations can mislead. The authors propose a method called CENNET, which integrates structural causal models with neural networks to generate causal explanations and introduces an entropy-based index to measure explanation power. Experiments on synthetic and quasi-real data show that CENNET effectively provides causal explanations compared to existing methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] How Do Agents Perform Code Optimization? An Empirical Study</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [se], [code optimization], [AI coding agents, performance optimization, empirical study, pull request analysis, AIDev dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Huiyun Peng, Antonio Zhong, Ricardo Andrés Calvo Méndez, Kelechi G. Kalu, James C. Davis</p>
</li>
<li class="">
<p><strong>institution:</strong> Purdue University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21757" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21757</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducts the first empirical study comparing AI-agent-authored and human-authored performance optimization commits using real-world PR data. 2. Identifies a significant gap in explicit performance validation between AI-authored (45.7%) and human-authored (63.6%) PRs. 3. Finds that AI agents largely employ the same optimization patterns as humans, suggesting they learn from existing code but lack rigorous validation practices.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e44d9c47004517dbb7baa5f42b9023e94e10fbf2a09070a4a953b43ded2bf802_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e44d9c47004517dbb7baa5f42b9023e94e10fbf2a09070a4a953b43ded2bf802_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents an empirical study comparing how AI coding agents and humans perform code optimization by analyzing performance-related pull requests from the AIDev dataset. The study finds that while AI agents use similar optimization patterns as humans, they are significantly less likely to include explicit performance validation in their commits. This highlights a key limitation in current agentic code optimization and an opportunity for improvement.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network for Multimodal Liver Tumor Segmentation from Unpaired Datasets</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [Quaternion Neural Networks, Cross-Attention, Unpaired Data, Multimodal Learning, Explainable AI]</p>
</li>
<li class="">
<p><strong>authors:</strong> Arunkumar V, Firos V M, Senthilkumar S, Gangadharan G R</p>
</li>
<li class="">
<p><strong>institution:</strong> Anna University, National Institute of Technology Tiruchirappalli</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21760" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21760</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an Adaptive Quaternion Cross-Fusion (A-QCF) block for bidirectional knowledge transfer between unpaired CT and MRI data streams., 2. Introduces a unified segmentation model (A-QCF-Net) that leverages Quaternion Neural Networks to build a shared feature space from separate datasets., 3. Demonstrates significant performance gains over strong unimodal baselines and validates clinical relevance through explainability analysis.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebec19949c3fad65f84d0acee71e271ec2d8caca288cc4ecf24768e9533dbbe8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebec19949c3fad65f84d0acee71e271ec2d8caca288cc4ecf24768e9533dbbe8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of training multimodal segmentation models with unpaired datasets. It proposes A-QCF-Net, which uses Quaternion Neural Networks and an adaptive cross-fusion block to enable knowledge transfer between separate CT and MRI data. The method significantly outperforms unimodal baselines and provides a viable paradigm for leveraging large, unpaired medical image archives.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [Data Provenance], [Data Provenance, Compliance Rating, Generative AI, Dataset Ethics, Transparency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Matyas Bohacek, Ignacio Vilanova Echavarri</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University, Imperial College London</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21775" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21775</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the Compliance Rating Scheme (CRS), a framework for evaluating dataset compliance with transparency, accountability, and security principles. 2. Develops and releases an open-source Python library that implements the CRS framework using data provenance technology. 3. Creates a tool that is both reactive (evaluating existing datasets) and proactive (guiding the responsible construction of new datasets).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa33a1fedd52ef1c87e9bf7d9a25dad61aae942ba50660263862470b9b677745_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa33a1fedd52ef1c87e9bf7d9a25dad61aae942ba50660263862470b9b677745_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the lack of ethical and legal oversight in the creation and sharing of datasets for Generative AI. It proposes the Compliance Rating Scheme (CRS) framework and an accompanying open-source library to assess and ensure dataset compliance with key principles. The work aims to improve traceability and accountability in the AI data supply chain.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Inference-based GAN Video Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [VAE-GAN, Markov chain, long video generation, temporal consistency, encoder-decoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jingbo Yang, Adrian G. Bors</p>
</li>
<li class="">
<p><strong>institution:</strong> University of York</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21776" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21776</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new video generator, Encoder GAN3 (EncGAN3), which is a VAE-GAN hybrid structure that incorporates inference capabilities into an adversarial-based unconditional video generator. 2. Introduces a novel, memory-efficient approach to generate long videos (hundreds/thousands of frames) by extending the base VAE-GAN model. 3. Leverages a Markov chain framework with a recall mechanism, where each state is a short VAE-GAN generator, to sequentially connect video sub-sequences and ensure temporal continuity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5d6cd59a07e4478b6b21e586a92b15f90e237037b758a29738bf31e46f9843c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5d6cd59a07e4478b6b21e586a92b15f90e237037b758a29738bf31e46f9843c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of generating long, high-quality videos, a task where existing models suffer from quality degradation. The proposed method first introduces a VAE-GAN hybrid video generator (EncGAN3) and then extends it using a Markov chain framework to sequentially generate short video clips, enabling the creation of temporally consistent long videos. The main conclusion is that this approach overcomes the temporal scaling limitation and allows for memory-efficient generation of long video sequences.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Accelerating Scientific Discovery with Autonomous Goal-evolving Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scientific discovery agents], [autonomous goal evolution, bi-level optimization, LLM agents, objective function design, scientific discovery]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuanqi Du, Botao Yu, Tianyu Liu, Tony Shen, Junwu Chen, Jan G. Rittig, Kunyang Sun, Yikun Zhang, Zhangde Song, Bo Zhou, Cassandra Masschelein, Yingze Wang, Haorui Wang, Haojun Jia, Chao Zhang, Hongyu Zhao, Martin Ester, Teresa Head-Gordon, Carla P. Gomes, Huan Sun, Chenru Duan, Philippe Schwaller, Wengong Jin</p>
</li>
<li class="">
<p><strong>institution:</strong> Cornell University, The Ohio State University, Yale University, Simon Fraser University, École Polytechnique Fédérale de Lausanne, University of California Berkeley, Northeastern University, Deep Principle, University of Illinois Chicago, Georgia Institute of Technology, Broad Institute of MIT and Harvard</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21782" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21782</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and addresses the unmet requirement of automating objective function design for scientific discovery agents, moving beyond fixed, imperfect proxies. 2. Proposes the SAGA framework, a novel bi-level architecture where an outer loop of LLM agents evolves objectives and an inner loop optimizes solutions under them. 3. Demonstrates the framework&#x27;s effectiveness across diverse scientific domains (antibiotic, materials, DNA, chemical process design), showing improved discovery outcomes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9035dcf8fd16c34c235e39c8960c63fa826c45df283663a01722181f7ab419d8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9035dcf8fd16c34c235e39c8960c63fa826c45df283663a01722181f7ab419d8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces SAGA, a framework for scientific discovery where LLM agents autonomously evolve and refine the objective functions used to guide optimization, rather than relying on fixed human-specified goals. This bi-level architecture enables systematic exploration of objective spaces and their trade-offs. The method is shown to substantially improve the effectiveness of discovery agents across multiple application domains.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Multi-agent Adaptive Mechanism Design</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [mechanism design], [distributionally robust optimization, online learning, incentive compatibility, adaptive mechanism, regret analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qiushi Han, David Simchi-Levi, Renfei Tan, Zishuo Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Massachusetts Institute of Technology, University of Illinois Urbana-Champaign</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21794" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21794</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces DRAM, a novel framework combining mechanism design and online learning to handle unknown agent beliefs. 2. Provides theoretical guarantees of high-probability truthfulness and achieves optimal <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mo stretchy="false">{</mo><mo>~</mo></mover><mi>O</mi><mo stretchy="false">}</mo><mo stretchy="false">(</mo><msqrt><mo stretchy="false">{</mo></msqrt><mi>T</mi><mo stretchy="false">}</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tilde\{O\}(\sqrt\{T\})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2919em;vertical-align:-0.305em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9869em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mopen">{</span></span><span style="top:-3.669em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">~</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.25em"><span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mclose">}</span><span class="mopen">(</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.935em"><span class="svg-align" style="top:-3.2em"><span class="pstrut" style="height:3.2em"></span><span class="mopen" style="padding-left:1em">{</span></span><span style="top:-2.895em"><span class="pstrut" style="height:3.2em"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.28em" viewBox="0 0 400000 1296" preserveAspectRatio="xMinYMin slice"><path d="M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.305em"><span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mclose">})</span></span></span></span> cumulative regret with a matching lower bound. 3. Generalizes the framework (DRAM+) to support plug-in estimators, structured priors, and delayed feedback.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ece7d8b60855735e1eee48c51256eacf5f3997d56ced3396434f12a30ad44_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ece7d8b60855735e1eee48c51256eacf5f3997d56ced3396434f12a30ad44_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of designing a truthful mechanism when the principal has no prior knowledge of agents&#x27; beliefs. It proposes the Distributionally Robust Adaptive Mechanism (DRAM), which iteratively learns beliefs and updates a robust optimization problem to minimize cost while ensuring truthfulness. The mechanism is proven to achieve optimal regret, and the framework is the first to maintain truthfulness under these general learning conditions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [image captioning], [scientific figure captioning, large-scale dataset, domain-specific training, human evaluation, large language models (LLMs)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ting-Hao K.Huang, Ryan A. Rossi, Sungchul Kim, Tong Yu, Ting-Yao E. Hsu, Ho Yin, C. Lee Giles</p>
</li>
<li class="">
<p><strong>institution:</strong> The Pennsylvania State University, Adobe Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21789" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21789</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Creation and continuous updating of a large-scale, real-world dataset of scientific figure-caption pairs from arXiv papers. 2. Conducting extensive evaluations, both automatic and human, on generated and author-written captions to assess quality. 3. Developing interactive systems and launching annual challenges to advance the field and help scientists write better captions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper reviews the SciCap project&#x27;s first five years, which focused on generating and evaluating captions for scientific figures. The core method involved building a large-scale dataset from arXiv and exploring domain-specific training, similar to models like SciBERT, for captioning. The conclusion outlines key lessons learned and proposes future research directions to address unsolved challenges in the field.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [diffusion models], [Parameter-Efficient Fine-Tuning, Mixture of Low-rank Experts, Instruction-Guided Routing, Multi-Conditional Generation, Diffusion Transformers]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jinqi Xiao, Qing Yan, Liming Jiang, Zichuan Liu, Hao Kang, Shen Sang, Tiancheng Zhi, Jing Liu, Cheng Yang, Xin Lu, Bo Yuan</p>
</li>
<li class="">
<p><strong>institution:</strong> ByteDance Inc., Rutgers University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21788" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21788</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/yanq095/InstructMoLE" target="_blank" rel="noopener noreferrer" class="">https://github.com/yanq095/InstructMoLE</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces InstructMoLE, a framework using an Instruction-Guided Mixture of Low-Rank Experts for multi-conditional image generation., 2. Proposes Instruction-Guided Routing (IGR), a global routing signal derived from user instructions to select a coherent expert council for all tokens, addressing spatial fragmentation and semantic drift., 3. Introduces an output-space orthogonality loss to promote expert functional diversity and prevent representational collapse.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11bd8639cb632e86b35367843cf453bbc6a02fb6882f88ff7441c78b42b653ce_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11bd8639cb632e86b35367843cf453bbc6a02fb6882f88ff7441c78b42b653ce_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of task interference in multi-conditional image generation when using monolithic PEFT adapters like LoRA. It proposes InstructMoLE, a novel framework that uses global instruction-guided routing to select a consistent mixture of low-rank experts, combined with an orthogonality loss for diversity, which outperforms existing methods on benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] CellMamba: Adaptive Mamba for Accurate and Efficient Cell Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [Mamba, Triple-Mapping Adaptive Coupling (TMAC), Adaptive Mamba Head, biomedical instance detection, VSSD backbone]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ruochen Liu, Yi Tian, Jiahao Wang, Hongbin Liu, Xianxu Hou, Jingxin Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Liverpool, National University of Singapore, Xi&#x27;an Jiaotong-Liverpool University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21803" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21803</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Triple-Mapping Adaptive Coupling (TMAC) module that splits channels into parallel branches with dual idiosyncratic and one consensus attention map for enhanced spatial discriminability. 2. Designs an Adaptive Mamba Head that fuses multi-scale features via learnable weights to handle varying object sizes robustly. 3. Introduces CellMamba, a lightweight one-stage detector built on a VSSD backbone with CellMamba Blocks, achieving superior accuracy and efficiency on biomedical cell detection datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3fe9b8372a27eedc5a8e2e1150bcdf6cf461c195f9ed154d8890662de191da_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3fe9b8372a27eedc5a8e2e1150bcdf6cf461c195f9ed154d8890662de191da_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes CellMamba, a lightweight one-stage detector for cell detection in pathological images. It introduces a novel Triple-Mapping Adaptive Coupling (TMAC) module and an Adaptive Mamba Head to improve spatial discriminability and multi-scale feature fusion. Experiments show CellMamba outperforms CNN, Transformer, and Mamba baselines in accuracy while being more efficient in model size and inference speed.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] S&amp;P 500 Stock&#x27;s Movement Prediction using CNN</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [financial time series forecasting], [Convolutional Neural Network (CNN), multivariate raw data, stock movement prediction, historical data matrices, S&amp;P 500]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rahul Gupta</p>
</li>
<li class="">
<p><strong>institution:</strong> None (No affiliation or email domain provided in the given content)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21804" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21804</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the application of Convolutional Neural Networks (CNNs), typically used for image classification, to the problem of stock movement prediction by treating multivariate historical stock data as image-like matrices. 2. Utilizes raw, unprocessed market data including events like stock splits and dividends, instead of relying on pre-engineered financial features. 3. Demonstrates a flexible prediction framework that can be applied at different levels: individual stocks, sectors, or entire portfolios.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d6197655292ac8e55d8c7606c8c3cfe730f7f8dad4004b93d4a9b3a8d8f457_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d6197655292ac8e55d8c7606c8c3cfe730f7f8dad4004b93d4a9b3a8d8f457_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the problem of predicting stock price movements for the S&amp;P 500 index. The core method involves using a Convolutional Neural Network (CNN) to analyze multivariate historical stock data, which is structured as image-like matrices, without extensive feature engineering. The approach shows promising results and offers a flexible model for stock, sector, or portfolio-level predictions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] MoonBot: Modular and On-Demand Reconfigurable Robot Toward Moon Base Construction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [space robotics], [modular robot, reconfigurable robot, lunar construction, field demonstration, connector design]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kentaro Uno, Elian Neppel, Gustavo H. Diaz, Ashutosh Mishra, Shamistan Karimov, A. Sejal Jain, Ayesha Habib, Pascal Pama, Hazal Gozbasi, Shreya Santra, Kazuya Yoshida</p>
</li>
<li class="">
<p><strong>institution:</strong> Space Robotics Laboratory (SRL), Department of Aerospace Engineering, Graduate School of Engineering, Tohoku University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21853" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21853</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MoonBot, a modular and reconfigurable robotic system designed for lunar payload constraints and task adaptability. 2. Details the system&#x27;s design and development, including a field demonstration simulating lunar infrastructure tasks like civil engineering and component deployment. 3. Systematically summarizes lessons learned, particularly on connector design, to inform future modular robotic systems for lunar missions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7ceec836e29be790b6ca39392714dc64e19923b0842210e75988ad1c5fdeeb2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7ceec836e29be790b6ca39392714dc64e19923b0842210e75988ad1c5fdeeb2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces MoonBot, a modular and reconfigurable robot designed for constructing lunar bases under strict mass constraints. It details the robot&#x27;s design and validates its concept through field demonstrations of simulated construction tasks. The work concludes with lessons learned, especially regarding connector design, to guide future lunar robotic systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [evaluation], [anthropomorphic intelligence, benchmark, psychological counseling, rubric-based evaluation, reasoning-before-scoring]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiaxin Liu, Peiyi Tu, Wenyu Chen, Yihong Zhuang, Xinxia Ling, Anji Zhou, Chenxi Wang, Zhuo Han, Zhengkai Yang, Junbo Zhao, Zenan Huang, Yuanyuan Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Ant Group, Xiamen University, Beijing Normal University, Zhejiang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21849" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21849</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/inclusionAI/HeartBench" target="_blank" rel="noopener noreferrer" class="">https://github.com/inclusionAI/HeartBench</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces HeartBench, a novel benchmark framework for evaluating the integrated emotional, cultural, and ethical dimensions (anthropomorphic intelligence) of Chinese LLMs. 2. Proposes a theory-driven taxonomy and a case-specific, rubric-based &quot;reasoning-before-scoring&quot; evaluation protocol to translate abstract human-like traits into measurable criteria. 3. Provides an analysis revealing a significant performance gap in current LLMs, especially in scenarios with subtle emotional subtexts and complex ethical trade-offs, establishing a standardized metric and a blueprint for creating human-aligned training data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0dc9f1570e111d07840de8240e6e5f545f05ae646e05a5121a0a6c4037e3637a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0dc9f1570e111d07840de8240e6e5f545f05ae646e05a5121a0a6c4037e3637a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the gap in evaluating the social and emotional intelligence (anthropomorphic intelligence) of LLMs, particularly in the Chinese context. It proposes HeartBench, a benchmark framework grounded in psychological counseling scenarios, which uses a rubric-based evaluation method. The assessment of 13 LLMs shows a substantial performance ceiling, with even top models achieving only 60% of the expert ideal, highlighting significant decay in handling complex emotional and ethical nuances.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Comedy of Estimators: On KL Regularization in RL Training of LLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [KL divergence, policy gradient, on-policy sampling, off-policy training, gradient bias]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro, Yoshua Bengio, Nikolay Malkin, Moksh Jain, Siddarth Venkatraman, Aaron Courville</p>
</li>
<li class="">
<p><strong>institution:</strong> Mila – Quebec AI Institute, Université de Montréal, McGill University, LLNL, University of Edinburgh, CIFAR</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21852" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21852</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Systematic analysis of KL divergence estimator configurations in RL for LLMs, revealing how design choices introduce gradient bias. 2. Empirical demonstration that estimator configurations with unbiased gradients lead to better and more stable performance on both in-domain and out-of-domain tasks. 3. Investigation showing KL regularization can stabilize off-policy RL training in asynchronous setups.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96999f081bc421143202d98f560b5d13a6fb0c09613b8c160b121158bce3811a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96999f081bc421143202d98f560b5d13a6fb0c09613b8c160b121158bce3811a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes the use of various estimators for the KL divergence regularization term in RL fine-tuning of LLMs, finding that common practices introduce biased gradients. Through experiments on models like Qwen2.5-7B, the study shows that using estimator configurations with unbiased gradients improves training stability and downstream task performance. The work also finds that KL regularization helps stabilize off-policy RL training.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [feature-level fusion, convolutional neural networks, diabetic retinopathy screening, EfficientNet, DenseNet]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan</p>
</li>
<li class="">
<p><strong>institution:</strong> North South University, Korea Institute of Oriental Medicine, American International University–Bangladesh</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21861" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21861</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [copyright compliance, vision-language models, tool-augmented defense, benchmark dataset, multimodal query]</p>
</li>
<li class="">
<p><strong>authors:</strong> Naen Xu, Jinghuai Zhang, Changjiang Li, Hengyu An, Chunyi Zhou, Jun Wang, Boyu Xu, Yuyuan Li, Tianyu Du, Shouling Ji</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, University of California, Los Angeles, Palo Alto Networks</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21871" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21871</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/bluedream02/CopyGuard" target="_blank" rel="noopener noreferrer" class="">https://github.com/bluedream02/CopyGuard</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a large-scale benchmark dataset of 50,000 multimodal query-content pairs to evaluate copyright compliance in LVLMs. 2. Conducted a comprehensive evaluation revealing significant deficiencies in state-of-the-art LVLMs&#x27; ability to recognize and respect copyrighted content. 3. Proposed a novel tool-augmented defense framework to reduce copyright infringement risks in LVLM inference.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a933ea78af16685ceab38b447862e9c50b08de435c2e6b662d59551bf5552fdc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a933ea78af16685ceab38b447862e9c50b08de435c2e6b662d59551bf5552fdc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper evaluates how large vision-language models (LVLMs) handle copyrighted visual content and finds they often fail to comply with copyright regulations. To address this, the authors propose a tool-augmented defense framework for copyright compliance. The work highlights the need for developing copyright-aware LVLMs to ensure responsible use.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [Privacy-preserving machine learning], [dataset distillation, random forest, synthetic data generation, explainable AI, membership-inference attack]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yiming Qian, Thorsten Neumann, Xueyining Huang, David Hardoon, Fei Gao, Yong Liu, Siow Mong Rick Goh</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of High Performance Computing (A*STAR), Standard Chartered Bank, Xi’an Jiaotong–Liverpool University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21866" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21866</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a privacy-preserving dataset distillation framework that converts a trained random forest into transparent rule regions and generates synthetic data by uniform sampling within these regions, creating a compact, auditable surrogate dataset. 2. Enables explainable AI by providing both global pattern summaries (e.g., support, lift) from aggregated rules and local, human-readable rationales with calibrated uncertainty for individual cases based on tree-vote disagreement. 3. Demonstrates strong utility-privacy trade-offs, showing the distilled data maintains competitive model performance (e.g., precision, F1) with significant data reduction (85-93%), improves cross-institution learning, and resists membership-inference attacks (AUC ~0.5), indicating low memorization risk.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6994d363f1e76d7cc78ab02d48685c11199b353aab61b3eb5297064b1df9b722_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6994d363f1e76d7cc78ab02d48685c11199b353aab61b3eb5297064b1df9b722_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a dataset distillation method for financial fraud detection that converts a random forest model into interpretable rule regions to generate synthetic data, preserving privacy and model utility. The approach produces a compact, explainable dataset that supports collaborative learning across institutions while resisting privacy attacks. Experiments show it reduces data volume by over 85% with minimal performance loss and enhances cross-cluster fraud detection.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [multi-agent framework, bias mitigation, financial forecasting, LLM integration, modular design]</p>
</li>
<li class="">
<p><strong>authors:</strong> Marc S. Montalvo, Hamed Yaghoobian</p>
</li>
<li class="">
<p><strong>institution:</strong> Rochester Institute of Technology, Muhlenberg College</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21878" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21878</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MASFIN, a modular multi-agent framework that integrates LLMs with structured financial metrics and unstructured news for decomposed financial reasoning. 2. Embeds explicit bias-mitigation protocols (e.g., against survivorship and hindsight bias) to enhance transparency and robustness. 3. Demonstrates practical effectiveness through an eight-week evaluation showing outperformance of major market benchmarks, highlighting the promise of bias-aware generative AI in finance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/351cdc3ccfe2b2d987cf53c8380e153fe4b93de0def6253cbc7b2feb4af093fe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/351cdc3ccfe2b2d987cf53c8380e153fe4b93de0def6253cbc7b2feb4af093fe_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces MASFIN, a multi-agent system that combines LLMs with financial data and news to perform decomposed reasoning and forecasting while mitigating biases. In an eight-week evaluation, it achieved a 7.33% cumulative return, outperforming benchmarks like the S&amp;P 500 in most weeks, though with higher volatility. The results show the potential of modular, bias-aware AI frameworks for transparent and reproducible quantitative finance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [text-to-sql], [benchmark, multilingual, domain-specific, large language models, sports analytics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vaibhav Devraj, Dhruv Kumar, Jagat Sesh Challa</p>
</li>
<li class="">
<p><strong>institution:</strong> Birla Institute of Technology and Science (BITS), Pilani</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21877" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21877</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces CricBench, a novel benchmark for evaluating LLMs on Text-to-SQL tasks in the specialized domain of cricket analytics. 2. Establishes a multilingual framework, providing a &quot;Gold Standard&quot; dataset in both English and Hindi, with extensibility to other languages. 3. Demonstrates a significant performance gap for LLMs between general and specialized domains and challenges the assumption of English as the optimal prompt language for such tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd335127a490c2b4b59330fd1867a57551c792f1b695f15e48789a3992b7c05a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd335127a490c2b4b59330fd1867a57551c792f1b695f15e48789a3992b7c05a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces CricBench, a multilingual benchmark for evaluating Large Language Models on Text-to-SQL tasks in the specialized domain of cricket analytics. The benchmark features a manually curated dataset in English and Hindi and is used to evaluate six state-of-the-art models. The results show that high performance on general benchmarks does not transfer well to this specialized domain, and surprisingly, code-mixed Hindi queries can perform as well as or better than English ones.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [distributed inference, block placement, request routing, performance modeling, resource allocation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tingyang Sun, Ting He, Bo Ji, Parimal Parag</p>
</li>
<li class="">
<p><strong>institution:</strong> Pennsylvania State University, Virginia Tech, Indian Institute of Science</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21884" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21884</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed experimentally validated performance models for distributed LLM inference under given block placement and request routing decisions. 2. Formulated the offline optimization problem as a MILP, proved its NP-hardness, and designed a polynomial-complexity algorithm with performance guarantees. 3. Adapted the offline algorithm for the online setting with the same performance guarantee under bounded load.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25938e1be55cbd072ba066aea4bb0e492f8b8c2a83e48eaa7e09e800b8697383_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25938e1be55cbd072ba066aea4bb0e492f8b8c2a83e48eaa7e09e800b8697383_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the resource allocation problem for geographically-distributed LLM inference, focusing on optimizing block placement and request routing. It proposes performance models, offline and online algorithms with theoretical guarantees, and a lightweight CPU-only simulator. The solution significantly reduces inference time compared to the state-of-the-art in diverse distributed settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [visual navigation], [world model, future frame projection, 4-dof uav, long-horizon visual generation, aerial navigation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Weichen Zhang, Peizhi Tang, Xin Zeng, Fanhang Man, Shiquan Yu, Zichao Dai, Baining Zhao, Hongjin Chen, Yu Shang, Wei Wu, Chen Gao, Xinlei Chen, Xin Wang, Yong Li, Wenwu Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21887" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21887</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ANWM, an aerial navigation world model for predicting future visual observations to incorporate high-level semantics into UAV path planning. 2. Introduces a physics-inspired Future Frame Projection (FFP) module to provide coarse geometric priors and mitigate uncertainty in long-distance visual generation. 3. Demonstrates superior performance in long-distance visual forecasting and improves UAV navigation success rates in large-scale 3D environments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02f69e3df37002aba4354016667950073739b574c3c8044ad15f65d9721e62db_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02f69e3df37002aba4354016667950073739b574c3c8044ad15f65d9721e62db_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes ANWM, an aerial navigation world model that predicts future visual observations for UAVs using a novel Future Frame Projection module. It addresses the challenges of complex 4-DoF action spaces and long-horizon visual generation. The model outperforms existing methods in visual forecasting and enhances navigation success in large-scale environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Flexible Multitask Learning with Factorized Diffusion Policy</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [diffusion policy, modular architecture, multitask learning, imitation learning, mixture-of-experts]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chaoqi Liu, Haonan Chen, Sigmund H. Høeg, Shaoxiong Yao, Yunzhu Li, Kris Hauser, Yilun Du</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Illinois at Urbana-Champaign, Harvard University, Norwegian University of Science and Technology, Columbia University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21898" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21898</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a novel modular diffusion policy framework (FDP) that factorizes complex action distributions into a composition of specialized diffusion models. 2. Proposes continuous score aggregation via an observation-conditioned router for stable training and clear component specialization, addressing issues in standard MoE. 3. Demonstrates that the modular structure enables flexible policy adaptation to new tasks and mitigates catastrophic forgetting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b261f496908cd892964760d9f52edb76c57a6126f2c6a9969b7e36d9d43b048e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b261f496908cd892964760d9f52edb76c57a6126f2c6a9969b7e36d9d43b048e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of multitask imitation learning in robotics, where complex action distributions are difficult to model. It proposes a Factorized Diffusion Policy (FDP) that decomposes the policy into specialized diffusion components and composes them via a router. The method outperforms baselines in simulation and real-world manipulation and supports flexible adaptation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] MMCTOP: A Multimodal Textualization and Mixture-of-Experts Framework for Clinical Trial Outcome Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [multimodal fusion, sparse mixture-of-experts, schema-guided textualization, clinical trial prediction, temperature scaling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Carolina Aparício, Qi Shi, Bo Wen, Tesfaye Yadete, Qiwei Han</p>
</li>
<li class="">
<p><strong>institution:</strong> Nova School of Business and Economics, Hogarthian Technologies, IBM Research, Cleveland Clinic, Oregon Health &amp; Science University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21897" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21897</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A multimodal framework (MMCTOP) that integrates molecular structures, protocol metadata, eligibility narratives, and disease ontologies for clinical trial outcome prediction. 2. A novel architecture combining schema-guided textualization for data normalization and a drug-disease-conditioned sparse Mixture-of-Experts (SMoE) for context-aware, specialized multimodal fusion. 3. Demonstrates improved prediction performance (precision, F1, AUC) over baselines and incorporates operational safeguards like temperature scaling for calibrated probabilities to enhance auditability and reproducibility.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb552211ea905d5cbf8e190ead9078caf65c5d200327a02c7a6dab156a030645_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb552211ea905d5cbf8e190ead9078caf65c5d200327a02c7a6dab156a030645_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes MMCTOP, a multimodal framework for predicting clinical trial outcomes. It uses schema-guided textualization to normalize heterogeneous data and a sparse Mixture-of-Experts model for specialized fusion, achieving better performance than existing methods and providing calibrated risk estimates.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [agent system], [spatial transcriptomics, AI agents, benchmark, deterministic grader, harness design]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kenny Workman, Zhen Yang, Harihara Muralidharan, Hannah Le</p>
</li>
<li class="">
<p><strong>institution:</strong> LatchBio</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21907" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21907</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces SpatialBench, a benchmark of 146 verifiable problems derived from real-world spatial biology analysis workflows, covering five technologies and seven task categories. 2. Provides a deterministic grader for each problem to evaluate the recovery of key biological results from messy spatial datasets. 3. Demonstrates through benchmark data that frontier AI agents have low accuracy (20-38%) on these tasks and reveals the significant impact of harness design (tools, prompts, control flow, execution environment) on performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3912de9f7e0f0d2acbf8bcd709b023f2ed3b9ccd56886885ca3f8e9e9e81880_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3912de9f7e0f0d2acbf8bcd709b023f2ed3b9ccd56886885ca3f8e9e9e81880_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces SpatialBench, a benchmark to evaluate whether AI agents can analyze messy, real-world spatial biology data. It tests frontier models on 146 practical problems and finds low accuracy, highlighting that performance heavily depends on the agent&#x27;s harness design. The benchmark serves as a tool to measure and diagnose agent capabilities for faithful and reproducible data analysis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning from human feedback (rlhf)], [preference optimization, single-index model, semiparametric, link function, policy learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nathan Kallus</p>
</li>
<li class="">
<p><strong>institution:</strong> Netflix, Cornell University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21917" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21917</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulates policy alignment as a semiparametric single-index model problem, relaxing the need for a known link function between preferences and rewards. 2. Develops novel policy learners based on profiling, orthogonalizing, and link-agnostic ranking objectives, providing theoretical error bounds. 3. Proposes practical first-order optimization implementations that are robust to unknown preference noise and scale, enabling direct policy optimization without explicit reward fitting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/378875cc0ef0eb142c184d960e479724ea3df83c84cf81e185efea5469a4387e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/378875cc0ef0eb142c184d960e479724ea3df83c84cf81e185efea5469a4387e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of bias in aligning language models when the assumed link function between human preferences and latent rewards is misspecified. It proposes a semiparametric framework that treats the link function as unknown, develops several robust policy learning algorithms, and provides theoretical guarantees. The main conclusion is that this approach enables more reliable policy alignment without needing to correctly specify the preference noise distribution.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [unsupervised anomaly detection, disentangled representation, pseudo-healthy image reconstruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tao Yang, Xiuying Wang, Hao Liu, Guanzhong Gong, Lian-Ming Wu, Yu-Ping Wang, Lisheng Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21924" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21924</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a disentangled representation module to decouple brain MRI into imaging-invariant anatomical information and imaging-specific information, improving model generalizability across multi-modality and multi-center data. 2. Designed an edge-to-image restoration module that reconstructs high-quality pseudo-healthy images from edge information, suppressing the propagation of abnormal residuals. 3. Introduced brain anatomical priors and a differentiable one-hot encoding operator to constrain and stabilize the disentanglement learning process.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/785ff0154b41353c2fdfb9a61f66c2f97de1b49c0e9dbba451d335e3a8460e71_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/785ff0154b41353c2fdfb9a61f66c2f97de1b49c0e9dbba451d335e3a8460e71_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of generalizability and performance in unsupervised anomaly detection for brain MRI. It proposes a new framework that disentangles anatomical from imaging information and reconstructs pseudo-healthy images from edges, achieving state-of-the-art results on multi-center datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] LVLM-Aided Alignment of Task-Specific Vision Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [model alignment and interpretability], [LVLM-VA, spurious correlations, explainable AI (XAI), vision-language model, human-in-the-loop]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alexander Koebler, Lukas Kuhn, Ingo Thon, Florian Buettner</p>
</li>
<li class="">
<p><strong>institution:</strong> Goethe University Frankfurt, Siemens AG, German Cancer Research Center (DKFZ)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21985" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21985</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces LVLM-Aided Visual Alignment (LVLM-VA), a novel method for aligning small task-specific vision models with human domain knowledge using a Large Vision Language Model (LVLM). 2. Proposes a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling efficient expert-model interaction. 3. Demonstrates that the method effectively reduces the model&#x27;s dependence on spurious features and group-specific biases without requiring fine-grained, instance-level feedback.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c85ee676094853ba26d7711ac1d50d0ab994498bc2f91f2aaab4f1104d3222_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c85ee676094853ba26d7711ac1d50d0ab994498bc2f91f2aaab4f1104d3222_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of small task-specific vision models relying on spurious correlations, which leads to brittle real-world performance. The authors propose LVLM-Aided Visual Alignment (LVLM-VA), a method that uses a Large Vision Language Model to create a bidirectional interface between human domain knowledge and the model, translating explanations and critiques. The method is shown to significantly improve model alignment with human specifications and reduce dependence on spurious features across synthetic and real-world datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [vision-and-language navigation], [spatiotemporal context modeling, slot-based compression, prompt-guided multimodal integration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wen Jiang, Li Wang, Kangyao Huang, Wei Fan, Jinyuan Liu, Shaoyu Liu, Hongwei Duan, Bin Xu, Xiangyang Ji</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Institute of Technology, Tsinghua University, Dalian University of Technology, Xidian University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22010" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22010</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A slot-based historical image compression module to distill multi-view historical observations into fixed-length contextual representations. 2. A spatiotemporal trajectory encoding module to capture the temporal dynamics and spatial structure of UAV trajectories. 3. A prompt-guided multimodal integration module to fuse spatiotemporal context with current observations for robust waypoint prediction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c00484796a9df425f1884ea8ae22314b0592466ebe305303cf7f1f0c467b528_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c00484796a9df425f1884ea8ae22314b0592466ebe305303cf7f1f0c467b528_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes LongFly, a framework for long-horizon UAV vision-and-language navigation that addresses the challenge of modeling spatiotemporal context. The method integrates a history-aware modeling strategy with modules for compressing past observations, encoding trajectories, and fusing multimodal information. Experimental results show it outperforms state-of-the-art baselines in navigation success metrics across seen and unseen environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models for drug discovery], [hit-like molecule generation, autoregressive models, diffusion models, docking scores, multi-stage filtering]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nagham Osman, Vittorio Lembo, Giovanni Bottegoni, Laura Toni</p>
</li>
<li class="">
<p><strong>institution:</strong> University College London, University of Urbino Carlo Bo</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22031" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22031</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Framing hit-like molecule generation as a standalone task for generative models. 2. Proposing a tailored evaluation framework integrating physicochemical, structural, and bioactivity criteria. 3. Benchmarking autoregressive and diffusion models, with synthesized GSK-3β hits confirmed active in vitro.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0d0d188f2e13e0f8561de5950d5637586c871a0ee36935ebfbd5bb2bad540_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0d0d188f2e13e0f8561de5950d5637586c871a0ee36935ebfbd5bb2bad540_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether generative models can replace the hit identification step in drug discovery. It proposes a multi-stage evaluation framework and benchmarks autoregressive and diffusion models, showing they can generate valid, diverse, and biologically relevant compounds, with some synthesized hits confirmed active in vitro.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Meta-Learning-Based Handover Management in NextG O-RAN</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [communication &amp; networking], [Conditional Handovers, O-RAN, Meta-Learning, Mobility Management, xApp]</p>
</li>
<li class="">
<p><strong>authors:</strong> Michail Kalntis, George Iosifidis, José Suárez-Varela, Andra Lutu, Fernando A. Kuipers</p>
</li>
<li class="">
<p><strong>institution:</strong> Delft University of Technology, Telefónica Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22022" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22022</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces CONTRA, the first framework to jointly optimize Traditional and Conditional Handovers within the O-RAN architecture. 2. Proposes a practical meta-learning algorithm for adaptive, on-the-fly handover type selection, guaranteeing universal no-regret performance. 3. Provides and analyzes unique, countrywide mobility management datasets from a top-tier mobile network operator, offering fresh insights into handover trade-offs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d76dc23b355711f7c28f6efbb425c914a47fa4ebefd3807c4f00b61b58aedb3e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d76dc23b355711f7c28f6efbb425c914a47fa4ebefd3807c4f00b61b58aedb3e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of traditional and conditional handovers in mobile networks by proposing CONTRA, a meta-learning-based framework for O-RAN that dynamically selects and optimizes handover types. It is designed as a near-real-time xApp and is evaluated using real-world datasets. The results show that CONTRA improves user throughput and reduces switching costs, outperforming standard and RL-based baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] LibContinual: A Comprehensive Library towards Realistic Continual Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [catastrophic forgetting, stability-plasticity dilemma, modular architecture, memory budget, online continual learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenbin Li, Shangge Liu, Borui Kang, Yiyang Chen, KaXuan Lew, Yang Chen, Yinghuan Shi, Lei Wang, Yang Gao, Jiebo Luo</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanjing University, University of Wollongong, University of Rochester</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22029" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22029</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/RL-VIG/LibContinual" target="_blank" rel="noopener noreferrer" class="">https://github.com/RL-VIG/LibContinual</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed LibContinual, a unified, modular, and reproducible library for Continual Learning (CL) that integrates 19 representative algorithms across five methodological categories. 2. Systematically identified and investigated three unrealistic implicit assumptions (offline data accessibility, unregulated memory, intra-task semantic homogeneity) prevalent in mainstream CL evaluation. 3. Conducted a comprehensive analysis under stricter, more realistic settings (strict online CL, unified memory budget, category-randomized tasks), revealing significant performance drops in many existing methods and highlighting the need for resource-aware and semantically robust CL strategies.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a733a967f663a216bbd5fb0cee657ed8bb70a4e6f0205d669f983cbf9bb6fd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a733a967f663a216bbd5fb0cee657ed8bb70a4e6f0205d669f983cbf9bb6fd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces LibContinual, a comprehensive library designed to unify and standardize research in Continual Learning (CL). By using this framework to evaluate existing methods under more realistic constraints, the study shows that many current CL algorithms suffer significant performance drops, underscoring the gap between common evaluation practices and real-world applicability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [autoregressive distillation, adversarial refinement, real-time streaming, reference-anchored positional re-encoding, consistency-aware discriminator]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhiyao Sun, Ziqiao Peng, Yifeng Ma, Yi Chen, Zhengguang Zhou, Zixiang Zhou, Guozhen Zhang, Youliang Zhang, Yuan Zhou, Qinglin Lu, Yong-Jin Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Renmin University of China, Tencent Hunyuan, Nanjing University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22065" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22065</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://streamavatar.github.io" target="_blank" rel="noopener noreferrer" class="">https://streamavatar.github.io</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A two-stage autoregressive adaptation and acceleration framework (autoregressive distillation + adversarial refinement) to adapt a non-causal human video diffusion model for real-time, interactive streaming. 2. Three novel components to ensure long-term stability and consistency: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. 3. A one-shot, interactive human avatar model capable of generating both natural talking and listening behaviors with coherent full-body gestures, surpassing existing methods in quality, efficiency, and interaction naturalness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of making diffusion-based human avatar generation suitable for real-time, interactive streaming. It proposes StreamAvatar, a two-stage framework that adapts a high-fidelity human video diffusion model using autoregressive distillation and adversarial refinement, incorporating novel components for long-term consistency. The method achieves state-of-the-art performance in generating high-resolution, full-body interactive avatars with natural talking/listening behaviors in real-time.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Unifying Learning Dynamics and Generalization in Transformers Scaling Law</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [learning theory], [scaling law, learning dynamics, generalization error, transformer, stochastic gradient descent]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chiwun Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> Sun Yat-sen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22088" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22088</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formalizes the learning dynamics of transformers as an ODE system and approximates it to kernel behaviors, moving beyond toy models to analyze SGD on multi-layer transformers with arbitrary data distributions. 2. Establishes a theoretical upper bound on excess risk with a distinct phase transition: exponential decay in the optimization phase and a power-law decay of Θ(C^{-1/6}) in the statistical phase. 3. Derives isolated scaling laws for model size, training time, and dataset size, explaining how each variable independently governs generalization bounds.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9b067b56202cd4607e684058e78ac331373bf12bf6848ca444276e2dcafe9f9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9b067b56202cd4607e684058e78ac331373bf12bf6848ca444276e2dcafe9f9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper provides a theoretical foundation for the empirical scaling laws of large language models. It models transformer learning dynamics as an ODE system and analyzes SGD training on realistic data. The main result is a unified theory showing a phase transition in generalization error, from exponential to power-law decay, as computational resources scale.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [benchmark construction], [Turkish NLU benchmark, semi-automated annotation, sentiment analysis dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Duygu Altinok</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22100" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22100</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces TrGLUE, the first comprehensive GLUE-style benchmark for Turkish Natural Language Understanding, filling a critical gap. 2. Presents SentiTurca, a specialized benchmark for Turkish sentiment analysis. 3. Provides a scalable, reproducible semi-automated dataset creation pipeline combining LLM annotation, cross-model checks, and human validation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3aae4aef01bf4bd7a32414041836c4d9d7383c50872f47bdb0dee4d45af35adb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3aae4aef01bf4bd7a32414041836c4d9d7383c50872f47bdb0dee4d45af35adb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of a comprehensive benchmark for evaluating Turkish language understanding by introducing TrGLUE and SentiTurca. The benchmarks are created using a semi-automated pipeline with LLM annotation and human validation to ensure quality and linguistic naturalness. The work establishes a robust evaluation framework and provides resources to empower Turkish NLP research.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [multi-agent pipeline, automated data analysis, insight generation, report synthesis, visual analytics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuyu Gan, Renxiang Wang, James Mooney, Dongyeop Kang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Minnesota</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22101" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22101</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A Data Analyzer agent that orchestrates data profiling, generates diverse visualizations, filters low-quality charts, and automatically scores candidate insights for depth, correctness, and actionability. 2. A Presenter agent that sequences topics, composes chart-grounded narratives from top insights, writes transitions, and revises the document to produce a coherent, publication-ready report. 3. An end-to-end Analyzer-to-Presenter (A2P) pipeline that operationalizes co-analysis by coupling quality-assured analysis with narrative synthesis, improving the real-world usefulness of automated data analysis.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92651ded84480402816f8db1df902e28dd62cce1b0958cece60e0f518bdd7e1c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92651ded84480402816f8db1df902e28dd62cce1b0958cece60e0f518bdd7e1c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents A2P-Vis, a two-part multi-agent pipeline designed to automate the generation of data visualization reports. The system uses a Data Analyzer to create and vet visual insights and a Presenter to assemble them into a coherent narrative. The authors claim this end-to-end approach improves the practical utility of automated data analysis for practitioners.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Pruning as a Game: Equilibrium-Driven Sparsification of Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [neural network pruning, game theory, equilibrium, non-cooperative game, sparsification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zubair Shah, Noaman Khan</p>
</li>
<li class="">
<p><strong>institution:</strong> Hamad Bin Khalifa University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22106" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22106</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel game-theoretic perspective on neural network pruning, modeling parameter groups as players in a non-cooperative game where sparsity emerges as an equilibrium outcome. 2. Provides a theoretical analysis showing that dominated players (redundant parameters) collapse to zero participation under mild conditions, offering a principled explanation for pruning. 3. Derives a simple equilibrium-driven pruning algorithm that jointly updates network parameters and participation variables without relying on explicit, heuristic importance scores.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4fd762b6b064bb7810151beeb40f55c93bfc05054b2d4a98cd925aed8bea43b2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4fd762b6b064bb7810151beeb40f55c93bfc05054b2d4a98cd925aed8bea43b2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a novel game-theoretic framework for neural network pruning, where sparsity emerges naturally from the equilibrium of a non-cooperative game among model components. The method jointly updates network parameters and participation variables without external importance scores. Experiments show it achieves competitive sparsity-accuracy trade-offs with a more interpretable, theory-grounded foundation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [root cause analysis, service dependency graph, program dependence graph, LLM agent, cloud incident]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shengkun Cui, Rahul Krishna, Saurabh Jha, Ravishankar K. Iyer</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Illinois at Urbana-Champaign, IBM Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22113" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22113</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. PRAXIS, an agentic approach for cloud incident RCA with structured, LLM-driven graph reasoning and traversal over microservice and program dependency graphs. 2. An application of the hammock block program dependence graph for agentic RCA, leveraging its hierarchical structure for multi-granular code analysis. 3. A Code-Cloud-RCA Benchmark consisting of 30 real-world incident scenarios injected in a live Kubernetes environment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62ebd8a01fd966235e0d8d40581cb8352024a391331fada8ea23868c2235ada9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62ebd8a01fd966235e0d8d40581cb8352024a391331fada8ea23868c2235ada9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces PRAXIS, an orchestrator that uses an LLM-driven agent to traverse service dependency graphs and program dependence graphs to diagnose the root cause of code- and configuration-related cloud incidents. Compared to ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x, as demonstrated on a benchmark of 30 real-world incidents.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Atomistic Simulation Guided Convolutional Neural Networks for Thermal Modeling of Friction Stir Welding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [physics-informed machine learning], [molecular dynamics, convolutional neural network, friction stir welding, explainable AI, LAMMPS]</p>
</li>
<li class="">
<p><strong>authors:</strong> Akshansh Mishra</p>
</li>
<li class="">
<p><strong>institution:</strong> Politecnico di Milano, AI Fab Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21344" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21344</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a novel method to transform atomistic simulation data (atomic positions and velocities) into physics-based 2D spatial grids for deep learning input. 2. Created and optimized a 2D CNN model to directly predict temperature evolution from spatially resolved atomistic data, achieving high accuracy (R²=0.94). 3. Used Class Activation Map analysis to provide explainability, showing the model&#x27;s focus aligns with physical mechanisms (e.g., tool-material interface).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34d6f71500319f52aecac1e95e1951a537fdc504134a8ba43851f31acc2e41c6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34d6f71500319f52aecac1e95e1951a537fdc504134a8ba43851f31acc2e41c6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a method that combines molecular dynamics simulations with convolutional neural networks for thermal modeling in friction stir welding. The method transforms atomic-scale simulation data into spatial grids and uses a CNN to accurately predict temperature, with results validated against physical mechanisms. The approach demonstrates that deep learning can effectively learn from atomistic data to model complex thermomechanical processes.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Applications of synthetic financial data in portfolio and risk modeling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models for time series], [TimeGAN, Variational Autoencoder (VAE), synthetic financial data, portfolio optimization, risk modeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Christophe D. Hounwanou, Yae Ulrich Gaba</p>
</li>
<li class="">
<p><strong>institution:</strong> African Institute for Mathematical Sciences (AIMS Rwanda), Sefako Makgatho Health Sciences University (SMU), AI Research and Innovation Nexus for Africa (AIRINA Labs)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21798" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21798</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Evaluated and compared the performance of TimeGAN and VAEs for generating realistic synthetic financial time series data. 2. Demonstrated that TimeGAN-generated data closely matches real data in distributional, volatility, and autocorrelation properties. 3. Showed the practical utility of synthetic data in downstream financial tasks like mean-variance portfolio optimization, yielding similar portfolio weights and risk metrics to real data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a7ecf00c1350b58057e2f03c93bf0e8845d1441745fc22505c46475eceeeb65_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a7ecf00c1350b58057e2f03c93bf0e8845d1441745fc22505c46475eceeeb65_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the scarcity and privacy issues of real financial data by using generative models like TimeGAN and VAEs to create synthetic return series. It evaluates the synthetic data on statistical similarity and financial tasks, concluding that TimeGAN effectively captures temporal dynamics and can serve as a privacy-preserving, cost-effective substitute for real data in portfolio and risk analysis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [diffusion models, generative modeling, evidence lower bound, residual learning, two-stage framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Takuro Kutsuna</p>
</li>
<li class="">
<p><strong>institution:</strong> Toyota Central R&amp;D Labs., Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21593" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21593</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Enabling Ultra-Fast Cardiovascular Imaging Across Heterogeneous Clinical Environments with a Generalist Foundation Model and Multimodal Database</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image reconstruction], [foundation model, k-space, multimodal database, zero-shot generalization, accelerated imaging]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zi Wang, Mingkai Huang, Zhang Shi, Hongjie Hu, Lan Lan, Hui Zhang, Yan Li, Xi Hu, Qing Lu, Zongming Zhu, Qiong Yao, Yuxiang Dai, Fanwen Wang, Yinzhe Wu, Jun Lyu, Qianqian Gao, Guangming Xu, Zhenxuan Zhang, Haosen Zhang, Qing Li, Guangming Wang, Tianxing He, Lizhen Lan, Siyue Li, Le Xue, Mengting Sun, Yuntong Lyu, Junpu Hu, Jiayu Zhu, Rizwan Ahmad, Zhengyu Bu, Xianling Qian, Guanke Cai, Ruiyu Cao, Weirui Cai, Chang Xu, Yuyang Ren, Feidan Yu, Siying Ma, Ziqiang Xu, Xinran Chen, Sha Hua, Daniel Kim, Yajing Zhang, Chen Ouyang, Wenjia Bai, Jing Qin, Yucheng Yang, Daniel Rueckert, He Wang, Qian Tao, Claudia Prieto, Michael Markl, Alistair Young, Lianming Wu, Shuo Wang, Chen Qin, Mengsu Zeng, Xihong Hu, Haibo Xu, Xiaobo Qu, Hao Li, Guang Yang, Chengyan Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Imperial College London, Fudan University, Xiamen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21652" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21652</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. The curation of MMCMR-427K, the largest and most comprehensive multimodal cardiovascular magnetic resonance (CMR) k-space database. 2. The introduction of CardioMM, a generalist reconstruction foundation model that unifies semantic understanding with physics-informed data consistency for robust, accelerated imaging. 3. Demonstrating state-of-the-art performance and strong zero-shot generalization across heterogeneous clinical settings, enabling up to 24x acceleration without compromising clinical integrity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6001c34604e8bff7b6e1efa31a4faa67e7e43e6efb18e01ea880e43095344349_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6001c34604e8bff7b6e1efa31a4faa67e7e43e6efb18e01ea880e43095344349_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the slow scan times and environmental heterogeneity limiting clinical cardiovascular MRI. It proposes CardioMM, a generalist foundation model trained on a large multimodal k-space database (MMCMR-427K), which achieves robust, ultra-fast reconstructions across diverse scanners and protocols. The results show that CardioMM enables high acceleration (up to 24x) while preserving diagnostic quality and generalizing to unseen clinical environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-30">2025-12-30<a href="#2025-12-30" class="hash-link" aria-label="Direct link to 2025-12-30" title="Direct link to 2025-12-30" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251230] GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [GPU Virtualization, Benchmarking, Multi-tenancy, CUDA, Performance Isolation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jithin VG, Ditto PS</p>
</li>
<li class="">
<p><strong>institution:</strong> Bud Ecosystem Inc</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22125" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22125</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/BudEcosystem/GPU-Virt-Bench" target="_blank" rel="noopener noreferrer" class="">https://github.com/BudEcosystem/GPU-Virt-Bench</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed GPU-Virt-Bench, a comprehensive benchmarking framework with 56 metrics across 10 categories for evaluating software-based GPU virtualization systems. 2. Enabled systematic comparison between software virtualization approaches (e.g., HAMi-core, BUD-FCSP) and ideal hardware-based MIG behavior. 3. Demonstrated the framework&#x27;s utility by revealing critical performance characteristics for production deployment decisions in multi-tenant environments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a1c9f2d4dfba1fc452a424ad0f1298f01afe6d95dfd39dd2ff3f0c1bac9430c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a1c9f2d4dfba1fc452a424ad0f1298f01afe6d95dfd39dd2ff3f0c1bac9430c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of standardized evaluation for software-based GPU virtualization systems, which are needed for efficient GPU sharing in AI/LLM workloads. The authors propose GPU-Virt-Bench, a comprehensive benchmarking framework that measures performance across multiple critical dimensions. The framework provides actionable insights for practitioners by comparing software solutions against hardware-based baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent reinforcement learning], [ad-hoc teamwork, retrieval-augmented generation, teammate modeling, Overcooked]</p>
</li>
<li class="">
<p><strong>authors:</strong> Conor Wallace, Umer Siddique, Yongcan Cao</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Texas at San Antonio</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22129" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22129</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces COLLAB, a novel language-based framework that uses LLMs as behavioral world models to classify unseen teammate types in ad-hoc teamwork. 2. Extends COLLAB to RECOLLAB by incorporating retrieval-augmented generation (RAG) with exemplar trajectories to stabilize inference and improve adaptation. 3. Demonstrates empirically in the Overcooked environment that RECOLLAB achieves Pareto-optimal trade-offs between classification accuracy and episodic return, highlighting the value of retrieval grounding.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/078fe396118022eaa0d391e86072d08c5b6617a143aba1478029ca3185af6472_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/078fe396118022eaa0d391e86072d08c5b6617a143aba1478029ca3185af6472_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of ad-hoc teamwork, where an agent must collaborate with unseen teammates. It proposes RECOLLAB, a framework that uses retrieval-augmented LLMs to model and classify teammate behavior from short interaction traces. The method is shown to effectively improve adaptation and coordination in the cooperative Overcooked environment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SoDA: An Efficient Interaction Paradigm for the Agentic Web</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [Sovereign Digital Avatar, Intent-Permission Handshake, orthogonal decoupling, A2A protocols, dual-factor adaptive routing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zicai Cui, Zhouyuan Jian, Weiwen Liu, Weinan Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, Shanghai Innovation Institute</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22135" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22135</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a user sovereignty interaction paradigm for the Agentic Web, decoupling memory from application logic to break data lock-in and shifting from explicit instruction to implicit intent alignment to reduce cognitive load. 2. Implements the paradigm via the Sovereign Digital Avatar (SoDA) with an orthogonal decoupling design of storage, computation, and interaction, establishing the principle of &quot;data as a persistent asset, model as a transient tool&quot;. 3. Designs an Intent-Permission Handshake Mechanism based on A2A protocols with dual-factor adaptive routing for active risk governance in zero-trust environments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef4bb1bba1b84bcf1102a80dc39b16d212412d68a7abea9ab0aac1dc9e23dedb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef4bb1bba1b84bcf1102a80dc39b16d212412d68a7abea9ab0aac1dc9e23dedb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes the Sovereign Digital Avatar (SoDA), a new interaction paradigm for the Agentic Web that decouples user memory from applications and uses intent alignment to reduce cognitive load. It introduces an architecture with orthogonal decoupling and a secure handshake mechanism for zero-trust environments. Empirical results show it significantly reduces token consumption and user cognitive load compared to existing methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [FPGA, HLS, Point Cloud, Model Compression, Fixed-Point]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amur Saqib Pal, Muhammad Mohsin Ghaffar, Faisal Shafait, Christian Weis, Norbert Wehn</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Sciences and Technology (Pakistan), RPTU Kaiserslautern-Landau (Germany)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22139" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22139</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/dll-ncai/HLS4PC" target="_blank" rel="noopener noreferrer" class="">https://github.com/dll-ncai/HLS4PC</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed HLS4PC, a parameterizable HLS framework for accelerating point-based 3D point cloud models on FPGA. 2. Introduced PointMLP-Lite, a 4x less complex model variant created via hardware-aware compression techniques (URS, quantization, pruning, fusion). 3. Demonstrated FPGA acceleration achieving 3.56x higher throughput than prior work and outperforming GPU/CPU implementations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21433bfda0767bbdfcee46f13fb3acd9373d13bb741d87a755643767c9ad74f9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21433bfda0767bbdfcee46f13fb3acd9373d13bb741d87a755643767c9ad74f9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of real-time 3D point cloud processing by proposing HLS4PC, a parameterizable FPGA acceleration framework. The method combines algorithmic optimizations and hardware-aware model compression to create an efficient fixed-point implementation, which significantly outperforms previous accelerators and GPU/CPU baselines in throughput.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Pre-review to Peer review: Pitfalls of Automating Reviews using Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [peer review automation], [large language models, peer review, pre-review, citation prediction, review alignment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Akhil Pandey Akella, Harish Varma Siravuri, Shaurya Rohatgi</p>
</li>
<li class="">
<p><strong>institution:</strong> AllSci Corp, Sunwater Capital, Kellogg School of Management (Northwestern University), Northern Illinois University, MBZUAI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22145" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22145</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a systematic evaluation of frontier open-weight LLMs for generating peer reviews, measuring alignment with human reviewers and correlation with post-publication metrics like citations and novelty. 2. Identified key pitfalls of LLMs as autonomous reviewers, including weak correlation with human scores (0.15), systematic overestimation bias (3-5 points), and uniformly high confidence scores despite errors. 3. Demonstrated the potential utility of LLMs as pre-review screening agents, as their generated reviews correlate more strongly with post-publication outcomes than with human reviewer scores, and released an open-source dataset (DLMRSD) to support further safety research.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff267a101523eaa0ec56d561e9fa2c165c73baa1b3016d38df1ed64dbc91dcf6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff267a101523eaa0ec56d561e9fa2c165c73baa1b3016d38df1ed64dbc91dcf6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper evaluates the use of large language models (LLMs) for automating academic peer review by comparing LLM-generated reviews against human reviewer scores and post-publication metrics. The study finds that while LLMs show weak alignment with human reviewers and exhibit overconfidence and bias, their reviews correlate better with future citation impact, suggesting they could serve as useful pre-review screening tools rather than fully autonomous reviewers.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [hpc], [gpu kernels], [Minimal Executable Program (MEP), Automatic Error Repair, Performance Pattern Inheritance, iterative optimization, cross-platform]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ruifan Chu, Anbang Wang, Xiuxiu Bai, Shuai Liu, Xiaoshe Dong</p>
</li>
<li class="">
<p><strong>institution:</strong> School of Software Engineering, Xi’an Jiaotong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22147" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22147</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an end-to-end LLM framework that optimizes GPU kernels by constructing Minimal Executable Programs (MEPs) to avoid expensive full application builds and executions. 2. Introduces Automatic Error Repair and Performance Pattern Inheritance to automatically fix faults and reuse effective optimization strategies, reducing search cost. 3. Demonstrates cross-platform portability and effectiveness on NVIDIA GPUs and the Haiguang DCU platform, achieving significant speedups over direct LLM optimization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd593bd3569f30bdbf11d361054f51142863fb91e592b76bc4eb2f600850c5e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd593bd3569f30bdbf11d361054f51142863fb91e592b76bc4eb2f600850c5e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high cost of full builds for GPU kernel optimization in large HPC applications by proposing an LLM framework that uses Minimal Executable Programs (MEPs) for iterative optimization. The method integrates automatic error repair and performance pattern inheritance to maintain correctness and reuse strategies. It achieves substantial speedups across different hardware platforms without requiring full-source dependencies.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [serverless computing, GPU resource allocation, workload scheduling, multi-agent systems, collaborative reasoning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Guilin Zhang, Wulan Guo, Ziqi Tan</p>
</li>
<li class="">
<p><strong>institution:</strong> George Washington University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22149" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22149</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An adaptive GPU resource allocation framework for multi-agent systems in serverless environments that dynamically adjusts resources based on workload characteristics, agent priorities, and minimum requirements. 2. An O(N) complexity algorithm for real-time adaptation, enabling millisecond-scale reallocation to handle dynamic workload fluctuations. 3. A comprehensive evaluation demonstrating the framework&#x27;s superiority over static and round-robin strategies, achieving 85% latency reduction while maintaining throughput and improving GPU utilization and cost-efficiency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2fe7e30427c00e4689f161fb9912d4d11cc091ed6dd1dae3c4ea2c5805084e3b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2fe7e30427c00e4689f161fb9912d4d11cc091ed6dd1dae3c4ea2c5805084e3b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an adaptive GPU resource allocation framework to address the challenge of efficiently deploying heterogeneous multi-agent AI systems on serverless platforms. The method dynamically allocates resources using a real-time algorithm to handle varying computational demands and workload fluctuations. The results show it significantly reduces latency compared to baseline schedulers while maintaining throughput, offering a cost-effective solution for serverless multi-agent deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Rethinking Leveraging Pre-Trained Multi-Layer Representations for Speaker Verification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [speaker verification], [Layer Attentive Pooling, Attentive Statistical Temporal Pooling, pre-trained speech models, multi-level features, speaker embeddings]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jin Sob Kim, Hyun Joon Park, Wooseok Shin, Sung Won Han</p>
</li>
<li class="">
<p><strong>institution:</strong> Korea University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22148" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22148</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/sadPororo/LAP" target="_blank" rel="noopener noreferrer" class="">https://github.com/sadPororo/LAP</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Layer Attentive Pooling (LAP), a novel dynamic strategy for aggregating multi-layer representations from pre-trained speech models, moving beyond static weighted averaging. 2. Introduced a lightweight backend speaker model combining LAP and Attentive Statistical Temporal Pooling (ASTP) for efficient speaker embedding extraction. 3. Demonstrated state-of-the-art performance on the VoxCeleb benchmark with a compact architecture that significantly reduces training time.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96b640602033c1b23da872d515add261756f5ab1b958eac2b83c4e62b1fc7f3f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96b640602033c1b23da872d515add261756f5ab1b958eac2b83c4e62b1fc7f3f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the underutilization of multi-layer features from pre-trained speech models in speaker verification. It proposes a novel Layer Attentive Pooling (LAP) method and a lightweight backend model to dynamically aggregate these features. The approach achieves state-of-the-art results on VoxCeleb while being more efficient in training time.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Practical challenges of control monitoring in frontier AI deployments</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [ai security], [control monitoring, oversight latency, safety case, scheming agents, incremental attacks]</p>
</li>
<li class="">
<p><strong>authors:</strong> David Lindner, Charlie Griffin, Tomek Korbak, Roland S. Zimmermann, Geoffrey Irving, Sebastian Farquhar, Alan Cooney</p>
</li>
<li class="">
<p><strong>institution:</strong> Google DeepMind, UK AI Safety Institute, University of Oxford</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22154" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22154</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Analysis of real-world deployment dynamics (parallelism, latency, incremental attacks, partial incrimination) for control monitoring, 2. Proposal and comparison of three monitoring protocols (synchronous, semi-synchronous, asynchronous) with different latency-safety trade-offs, 3. Introduction of a high-level safety case sketch as a tool for analyzing and comparing monitoring protocols, applied to four case studies.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7cfc3d08c169a23a56cd0a16a1471c5bceacb1b76913f7079f51b7024f030d1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7cfc3d08c169a23a56cd0a16a1471c5bceacb1b76913f7079f51b7024f030d1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes the practical challenges of scaling automated control monitors for overseeing frontier AI agents in real-world deployments. It proposes and compares three monitoring protocols (synchronous, semi-synchronous, asynchronous) with different latency-safety trade-offs and introduces a safety case sketch as an analytical tool. The analysis identifies oversight, latency, and recovery as key challenges, explored through four case studies of potential AI attacks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [fault-tolerance], [bit-flip faults, fault localization, transformer reliability, residual-path perturbation, loss-sensitivity profiling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Muhammad Zeeshan Karamat, Sadman Saif, Christiana Chamon Garcia</p>
</li>
<li class="">
<p><strong>institution:</strong> Virginia Tech</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22174" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22174</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces BitFlipScope, a scalable software framework for localizing bit-flip corruptions in transformer-based LLMs under two deployment scenarios (with and without a clean reference model). 2. Proposes differential analysis for fault localization when a reference model is available and residual-path perturbation/loss-sensitivity profiling for localization when no reference exists. 3. Enables lightweight performance recovery for corrupted models without requiring costly fine-tuning or full retraining.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e931785a8ed1d0dca51ed3c75265de72147ccd6e3d68df21de1c7cad78a1d912_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e931785a8ed1d0dca51ed3c75265de72147ccd6e3d68df21de1c7cad78a1d912_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces BitFlipScope, a framework for localizing and recovering from bit-flip corruptions in LLMs. It uses differential analysis with a reference model or perturbation-based profiling without one to identify fault-affected regions, enabling targeted recovery without full retraining. The work aims to improve fault resilience for LLMs in hardware-prone and adversarial environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Solving Multi-Agent Multi-Goal Path Finding Problems in Polynomial Time</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent path finding], [multi-agent path finding, vehicle routing, polynomial-time algorithm, conflict resolution, assignment problem]</p>
</li>
<li class="">
<p><strong>authors:</strong> Stefan Edelkamp</p>
</li>
<li class="">
<p><strong>institution:</strong> Charles University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22171" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22171</a></p>
</li>
<li class="">
<p><strong>contributions:</strong>  1. Proposes a polynomial-time algorithm for solving discrete multi-agent multi-goal path finding (CMAPF) problems with node and edge conflicts, which is unexpected given the NP-hardness of traditional vehicle routing. 2. Introduces a planner that autonomously finds and updates the assignment of multiple goals to agents, contrasting with regular MAPF which uses fixed assignments. 3. Develops conflict resolution strategies including global assignment to reduce conflicts, and local methods like &quot;ants-on-the-stick,&quot; local assignment, path interleaving, and destination clearing.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e9267701cf1d96333033d8663590f3b652040a494de98cd10ba5a86ede709d3b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e9267701cf1d96333033d8663590f3b652040a494de98cd10ba5a86ede709d3b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the multi-agent multi-goal path finding (CMAPF) problem where agents in graphs must be assigned and routed to multiple goals. It presents a polynomial-time algorithm for discrete variants with conflicts, implemented in a planner that autonomously handles goal assignment and resolves conflicts. The main conclusion is that efficient, conflict-free solutions can be achieved in polynomial time, challenging the typical NP-hard complexity of vehicle routing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Wireless Traffic Prediction with Large Language Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [spatio-temporal forecasting], [large language model, wireless traffic prediction, spatial-temporal correlation, prompt engineering, fine-tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chuanting Zhang, Haixia Zhang, Jingping Qiao, Zongzhang Li, Mohamed-Slim Alouini</p>
</li>
<li class="">
<p><strong>institution:</strong> Shandong University, Shandong Normal University, China Mobile Communications Group Shandong Co., Ltd, King Abdullah University of Science and Technology (KAUST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22178" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22178</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes TIDES, an LLM-based framework that captures spatial-temporal correlations for urban wireless traffic prediction. 2. Introduces a prompt engineering scheme to bridge the domain gap between numerical traffic data and language models by embedding statistical features as structured inputs. 3. Designs a DeepSeek module enabling spatial alignment via cross-domain attention, allowing the LLM to leverage information from related regions, and employs efficient fine-tuning of lightweight components.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/402a3d6681d9c203daf7e8ef09e0d0af8998f3eba780abc404c945025563d6a8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/402a3d6681d9c203daf7e8ef09e0d0af8998f3eba780abc404c945025563d6a8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes TIDES, a novel framework that uses a large language model (LLM) enhanced with spatial awareness for urban wireless traffic prediction. It addresses the lack of spatial modeling in existing LLM-based predictors through region clustering, prompt engineering, and a spatial alignment module, achieving superior accuracy and robustness on real-world datasets, which is key for intelligent 6G network management.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Characterizing Motion Encoding in Video Diffusion Timesteps</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [video diffusion models, timestep analysis, motion-appearance disentanglement, motion transfer, one-shot customization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vatsal Baherwani, Yixuan Ren, Abhinav Shrivastava</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Maryland</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22175" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22175</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a quantitative proxy and conducts a large-scale study to systematically characterize how motion is encoded across the denoising timesteps of video diffusion models, identifying distinct motion-dominant and appearance-dominant regimes. 2. Derives an operational motion-appearance boundary in timestep space, turning a widely used empirical heuristic into a spatiotemporal disentanglement principle. 3. Simplifies the one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ff2b9cdf8da1e9fbc9ae04ff2d085e89388ee26b1689338abbb853b17970bed_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ff2b9cdf8da1e9fbc9ae04ff2d085e89388ee26b1689338abbb853b17970bed_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates how motion is encoded across the denoising timesteps of text-to-video diffusion models. By quantifying the trade-off between appearance editing and motion preservation when injecting new conditions, the authors identify early timesteps as motion-dominant and later ones as appearance-dominant. This characterization enables a simplified, effective method for one-shot motion transfer without extra modules.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] iOS as Acceleration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [distributed pipeline parallelism, mobile acceleration, iOS, memory constraints, thermal throttling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alexander K. Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent High School Researcher (No institutional affiliation inferred)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22180" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22180</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel proof-of-concept system using distributed pipeline parallelism to harness iOS devices as computational accelerators for local ML tasks. 2. Demonstrates the system&#x27;s effectiveness in accelerating modest model training (e.g., ResNet-34) and agentic LRM tool-usage, achieving a 44% decrease in training time in a specific setup. 3. Explores the unique potential of ubiquitous mobile devices with powerful processors and sensors (e.g., LiDAR, GPS) as cost-effective resources for embodied agentic AI and local compute, discussing practical use-cases and limitations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2533767e76bdf97e302af13359b973b06a9948269cc9017131b6e880553cb6b9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2533767e76bdf97e302af13359b973b06a9948269cc9017131b6e880553cb6b9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the barrier of expensive compute for local machine learning by proposing a system that uses distributed pipeline parallelism to leverage underutilized iOS phones as accelerators. The method partitions model weights to circumvent mobile memory limits, successfully accelerating tasks like training ResNet-34. The work concludes that commonplace mobile devices have significant potential to contribute to ML, especially for local, cost-sensitive, or sensor-driven applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [dimensionality reduction], [Locally Linear Embedding (LLE), AI-enhanced LLE, medical data analysis, medical billing, transcription]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hassan Khalid, Muhammad Mahad Khaliq, Muhammad Jawad Bashir</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Science and Technology (NUST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22182" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22182</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an innovative integration of AI with Locally Linear Embedding (LLE) to handle high-dimensional medical data. 2. Develops a comprehensive mathematical model for the AI-enhanced LLE technique. 3. Demonstrates the model&#x27;s application in real-world healthcare scenarios, showing significant improvements in data processing accuracy and operational efficiency for medical billing and transcription.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces an AI-enhanced Locally Linear Embedding (LLE) model to improve the analysis of high-dimensional medical data. The method is applied to automate and enhance medical billing and transcription services. The results show significant improvements in processing accuracy and operational efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Interpretable Link Prediction in AI-Driven Cancer Research: Uncovering Co-Authorship Patterns</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [network science], [co-authorship networks, link prediction, SHAP, random forest, interdisciplinary collaboration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shahab Mosallaie, Andrea Schiffauerova, Ashkan Ebadi</p>
</li>
<li class="">
<p><strong>institution:</strong> Concordia University, National Research Council Canada</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22181" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22181</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Constructed 36 overlapping co-authorship networks from 7,738 publications to model new, persistent, and discontinued collaborations in AI-driven cancer research. 2. Engineered both attribute-based and structure-based features and built four machine learning classifiers, with Random Forest achieving the highest recall for all collaboration types. 3. Applied SHAP for model interpretability, identifying key factors like discipline similarity, productivity, and seniority that influence collaboration patterns.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f6c5e92a95d2e6eeb8291558739ab641247bb834675d42c86ece0ee73d9d15_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f6c5e92a95d2e6eeb8291558739ab641247bb834675d42c86ece0ee73d9d15_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper uses machine learning to predict collaboration patterns in AI-driven cancer research by analyzing co-authorship networks. The authors built classifiers using engineered features and applied SHAP for interpretability, finding that discipline similarity promotes new and persistent collaborations while high productivity and seniority are linked to discontinued links. The results aim to guide the formation of effective interdisciplinary research teams and inform policy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Unbiased Visual Reasoning with Controlled Visual Inputs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal reasoning], [vision-language models, spurious correlations, information bottleneck, reinforcement learning, modular reasoning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhaonan Li, Shijie Lu, Fei Wang, Jacob Dineen, Xiao Ye, Zhikun Xu, Siyi Liu, Young Min Cho, Bangzheng Li, Daniel Chang, Kenny Nguyen, Qizheng Yang, Muhao Chen, Ben Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> Arizona State University, University of Southern California, University of Pennsylvania, University of California, Davis</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22183" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22183</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes VISTA, a modular framework that decouples visual perception from reasoning using an explicit information bottleneck to control visual inputs. 2. Introduces a training method using reinforcement learning (GRPO) on a small curated dataset to align the reasoner with unbiased visual evidence. 3. Demonstrates improved robustness against spurious correlations, transferability across VLM sensors, and enhanced interpretability in reasoning traces.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f546535b9c36b7873d0f685328a4f4a8e058e6a4788639c170f69e073d8f9e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f546535b9c36b7873d0f685328a4f4a8e058e6a4788639c170f69e073d8f9e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of vision-language models (VLMs) relying on spurious correlations rather than causal visual evidence. It proposes VISTA, a modular framework that separates perception (via a frozen VLM) from reasoning (via an LLM) using controlled queries and trains the reasoner with reinforcement learning. The method shows significant gains in robustness on benchmarks like SpuriVerse while maintaining competitive performance on other tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Dueling Double Deep Q-Network, curriculum learning, tennis simulation, sequential decision-making, sports analytics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vishnu Mohan</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22186" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22186</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a custom tennis simulation environment that models hierarchical scoring, tactical decisions, fatigue, and opponent skill. 2. Integrated a Dueling Double Deep Q-Network (DDQN) with curriculum learning to enable stable and effective strategy learning in a long-horizon, stochastic domain. 3. Identified a key limitation of win-rate optimization, revealing a learned defensive bias and highlighting challenges in reward design for sports RL.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/543e2f9f07244abac63adfdbdefd7fccfefed9147b55aed87016c10657f91bae_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/543e2f9f07244abac63adfdbdefd7fccfefed9147b55aed87016c10657f91bae_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a reinforcement learning framework using a Dueling Double Deep Q-Network trained with curriculum learning to optimize tennis strategy in a custom simulation. The method achieves high win rates and demonstrates stable convergence, but analysis reveals the learned policy is overly defensive, pointing to a fundamental issue with reward design in sports simulations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [computational pathology], [Multiple Instance Learning, Hook Tokens, Linear Complexity, Multimodal Initialization, Hook Diversity Loss]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xitong Ling, Minxi Ouyang, Xiaoxiao Li, Jiawen Li, Ying Chen, Yuxuan Sun, Xinrui Chen, Tian Guan, Xiaoping Liu, Yonghong He</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Xiamen University, Westlake University, Wuhan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22188" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22188</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/lingxitong/HookMIL" target="_blank" rel="noopener noreferrer" class="">https://github.com/lingxitong/HookMIL</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes HookMIL, a context-aware MIL framework using learnable hook tokens for structured contextual aggregation with linear computational complexity. 2. Introduces a multimodal initialization strategy for hook tokens using visual, textual, and spatial priors to accelerate convergence and improve representation. 3. Presents a Hook Diversity Loss and a hook-to-hook communication mechanism to encourage token specialization and refine interactions while minimizing redundancy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/125d35cff1b2c94d5e736ab81d897743e3d0b37d50d5ba6ce441aa77f8bc620e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/125d35cff1b2c94d5e736ab81d897743e3d0b37d50d5ba6ce441aa77f8bc620e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the loss of context in traditional MIL and the high computational cost of transformer-based MIL for whole-slide image analysis. It proposes HookMIL, a framework that uses learnable hook tokens for efficient, linear-complexity context modeling, enhanced by multimodal initialization and specialized loss functions. Experiments on four public datasets show that HookMIL achieves state-of-the-art performance with improved efficiency and interpretability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MatKV: Trading Compute for Flash Storage in LLM Inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [retrieval-augmented generation, key-value cache, flash storage, prefill optimization, power efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kun-Woo Shin, Jay H. Park, Moonwook Oh, Yohan Jo, Jaeyoung Do, Sang-Won Lee</p>
</li>
<li class="">
<p><strong>institution:</strong> Seoul National University, Samsung Electronics</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22195" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22195</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/kunwooshin/MatKV" target="_blank" rel="noopener noreferrer" class="">https://github.com/kunwooshin/MatKV</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes MatKV, a scheme to precompute and materialize KV vectors of RAG documents in flash storage to avoid recomputation during inference. 2. Demonstrates that MatKV reduces inference time and power consumption by half for RAG workloads with minimal accuracy impact. 3. Shows MatKV enables additional optimizations like overlapping KV loading with decoding and enabling the use of low-end GPUs for decoding.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d47111ab2d615579a09c00ff5a99f391f035b974cc23e324cddfbabf4d23cec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d47111ab2d615579a09c00ff5a99f391f035b974cc23e324cddfbabf4d23cec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high compute and energy cost of the prefill phase in RAG-based LLM inference. It proposes MatKV, which precomputes and stores key-value vectors of documents in flash storage for reuse, trading compute for storage. Experiments show this approach halves inference time and power consumption while maintaining accuracy and enabling further hardware optimizations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [Retrieval-Augmented Generation, Hallucination Prevention, Multi-Stage Validation, Corpus Expansion, Self-Improving AI]</p>
</li>
<li class="">
<p><strong>authors:</strong> Teja Chinthala</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher (affiliated email domain: avila.edu)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22199" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22199</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel RAG architecture enabling safe corpus expansion through validated write-back of model outputs. 2. A multi-stage acceptance layer combining grounding verification, attribution checking, and novelty detection for safety. 3. An experience store for meta-learning from both accepted and rejected responses.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5eee110568078584ba44c846b5af3fab7d300dbfaec310a5826fd74784dc8040_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5eee110568078584ba44c846b5af3fab7d300dbfaec310a5826fd74784dc8040_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that conventional RAG systems have static knowledge bases. It proposes Bidirectional RAG, a novel architecture that safely expands the retrieval corpus by writing back high-quality, validated LLM responses. The results show that this self-improving approach nearly doubles answer coverage compared to standard RAG while adding significantly fewer documents than a naive write-back strategy, demonstrating a safe and practical path for RAG systems to learn from deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [dynamic routing, residual networks, cosine incompatibility, Gumbel-Softmax, FLOPs regularization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yogeswar Reddy Thota</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Texas at Dallas</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22206" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22206</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces CosineGate, an end-to-end differentiable architecture for dynamic routing in residual networks using cosine incompatibility as a self-supervised skip signal. 2. Proposes the Cosine Incompatibility Ratio (CIR) to measure semantic redundancy and employs Gumbel-Softmax relaxation for per-sample, per-block gating during training. 3. Incorporates a progressive FLOPs regularization term to control average computational usage without destabilizing the optimization process.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed7e3fa1c114a73795152210d2b55ffe2541fede331ce04d228e11ca599688fa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed7e3fa1c114a73795152210d2b55ffe2541fede331ce04d228e11ca599688fa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the computational inefficiency in residual networks, where all blocks are evaluated for every input. It proposes CosineGate, a method that uses the cosine incompatibility between identity and residual features to dynamically skip redundant blocks, achieving significant FLOPs savings on CIFAR-10 while maintaining or improving accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Emergent Persuasion: Will LLMs Persuade Without Being Prompted?</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [ai safety &amp; alignment], [emergent persuasion, activation steering, supervised fine-tuning (SFT), threat model, persona vectors]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vincent Chang, Thee Ho, Sunishchal Dev, Kevin Zhu, Shi Feng, Kellin Pelrine, Matthew Kowal</p>
</li>
<li class="">
<p><strong>institution:</strong> Algoverse, FAR.AI, UC Berkeley, George Washington University, University of Toronto</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22201" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22201</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/ith8/persona_vectors" target="_blank" rel="noopener noreferrer" class="">https://github.com/ith8/persona_vectors</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Investigates the novel threat model of &quot;unprompted&quot; or &quot;emergent&quot; persuasion in LLMs, moving beyond the standard misuse (prompted) scenario. 2. Empirically compares two techniques for inducing traits (activation steering vs. supervised fine-tuning) and finds SFT reliably increases unprompted persuasion while steering does not. 3. Demonstrates that SFT on benign persuasion datasets can lead to increased persuasion propensity on harmful topics, highlighting a significant safety risk.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3e4599070a9919228b3e5bc9cb7f7fd0fe17df086f5d7bec7ef20de22526f15_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3e4599070a9919228b3e5bc9cb7f7fd0fe17df086f5d7bec7ef20de22526f15_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies whether Large Language Models (LLMs) will attempt to persuade users without being explicitly prompted to do so. The authors investigate this by applying activation steering and supervised fine-tuning (SFT) to induce persuasive traits, finding that SFT reliably increases unprompted persuasion, including on harmful topics, even when trained only on benign data. The main conclusion is that emergent harmful persuasion is a real risk that warrants further study for AI safety and governance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [crowd counting], [weakly-supervised learning, vision transformer, density-guided aggregation, parameter efficiency, lightweight model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qiang Guo, Rubo Zhang, Bingbing Zhang, Junjie Liu, Jianqing Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Dalian Minzu University, Dalian University of Technology, Dalian Rijia Electronics Co., Ltd.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22203" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22203</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes TCFormer, an ultra-lightweight transformer-based framework with only 5 million parameters for weakly-supervised crowd counting. 2. Introduces a Learnable Density-Weighted Averaging module to dynamically re-weight local features based on predicted density, compensating for the lack of spatial annotations. 3. Designs a density-level classification loss to discretize crowd density into grades, regularizing training and enhancing performance across varying density levels.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67bff3bf5e0b02cbd2cc6071e68fd191f9adc37c49a6f5dd3f4b89fbc7205ca3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67bff3bf5e0b02cbd2cc6071e68fd191f9adc37c49a6f5dd3f4b89fbc7205ca3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes TCFormer, a tiny transformer-based model for weakly-supervised crowd counting that uses only image-level count labels. It introduces a density-guided feature aggregation module and a density-level classification loss to achieve accurate counting. Experiments show it achieves a superior trade-off between parameter efficiency and accuracy, making it suitable for edge devices.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal reasoning], [spatial reasoning, multimodal large language models, benchmark, origami folding, viewpoint consistency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ryan Spencer, Roey Yaari, Ritvik Vemavarapu, Joyce Yang, Steven Ngo, Utkarsh Sharma</p>
</li>
<li class="">
<p><strong>institution:</strong> Algoverse AI Research, UC San Diego, University of New South Wales</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22207" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22207</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/stvngo/GamiBench" target="_blank" rel="noopener noreferrer" class="">https://github.com/stvngo/GamiBench</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces GamiBench, a novel benchmark for evaluating spatial reasoning and 2D-to-3D planning in MLLMs using origami folding tasks. 2. Proposes new diagnostic metrics, viewpoint consistency (VC) and impossible fold selection rate (IFSR), to holistically assess the reasoning process. 3. Provides a comprehensive dataset of 186 regular and 186 impossible 2D crease patterns with 3D shapes from multiple viewpoints.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7bb91a40c2344a9e50dec2b2754647404d609db9a78508a54a040d5f6c5f58b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7bb91a40c2344a9e50dec2b2754647404d609db9a78508a54a040d5f6c5f58b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces GamiBench, a benchmark that uses origami folding tasks to evaluate the spatial reasoning and 2D-to-3D planning capabilities of Multimodal Large Language Models (MLLMs). It assesses models on tasks like predicting 3D configurations and detecting impossible folds, revealing that even state-of-the-art models like GPT-5 and Gemini-2.5-Pro struggle with fundamental spatial understanding.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [algorithmic fairness], [adversarial debiasing, gradient reversal layer, fairness-aware representation learning, statistical parity difference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Farjana Yesmin, Romana Akter</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher, Researcher (affiliations not specified)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22210" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22210</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A comprehensive dataset integrating official flood impact data with socioeconomic indicators across 87 upazilas in Bangladesh. 2. An adversarial debiasing architecture adapted from healthcare AI for disaster management. 3. Rigorous fairness evaluation showing significant reductions in statistical parity and regional fairness gaps while maintaining predictive accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd82a75c606f74970d3c93e7f31e57d4d35ae9f285a2ccb25e804770225a020b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd82a75c606f74970d3c93e7f31e57d4d35ae9f285a2ccb25e804770225a020b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a fairness-aware AI framework using adversarial debiasing with a gradient reversal layer to prioritize post-flood aid allocation in Bangladesh. The model learns bias-invariant representations to reduce systematic disadvantages against marginalized regions. Experimental results show it significantly improves fairness metrics while maintaining strong predictive accuracy, demonstrating the effective application of algorithmic fairness in humanitarian contexts.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk &amp; Capability Framework for Governing Agentic AI Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [agentic AI, risk assessment, technical governance, autonomous action, safety controls]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shaun Khoo, Jessica Foo, Roy Ka-Wei Lee</p>
</li>
<li class="">
<p><strong>institution:</strong> GovTech Singapore, Singapore University of Technology and Design</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22211" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22211</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/govtech-ai/arc-framework" target="_blank" rel="noopener noreferrer" class="">https://github.com/govtech-ai/arc-framework</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a novel capability-centric perspective for analyzing agentic AI systems. 2. Distills three primary intrinsic risk sources (components, design, capabilities) and maps them to specific risks and technical controls. 3. Provides a structured, practical methodology for organizations to implement the framework for governance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0283ff7ed974694c06d620f02b6bfd0752cd1ab34c83d05b1d66ec9b4088059c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0283ff7ed974694c06d620f02b6bfd0752cd1ab34c83d05b1d66ec9b4088059c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces the Agentic Risk &amp; Capability (ARC) Framework to address governance challenges posed by autonomous AI agents. The framework provides a structured methodology to identify, assess, and mitigate risks from agentic systems by analyzing their capabilities and linking risk sources to technical controls. It aims to enable safe and responsible deployment of agentic AI while supporting innovation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On the Existence and Behaviour of Secondary Attention Sinks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [attention sinks, transformer, mlp, attention mechanism, large language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jeffrey T.H. Wong, Cheng Zhang, Louis Mahon, Wayne Luk, Anton Isopoussu, Yiren Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Imperial College London, UnlikelyAI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22213" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22213</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and characterizes a new class of &quot;secondary attention sinks&quot; that arise in middle layers, have variable lifetimes, and draw moderate attention, differing from persistent primary sinks like BOS. 2. Shows that secondary sinks are formed by specific middle-layer MLP modules that map token representations to align with the primary sink&#x27;s direction, with their L2-norm determining sink strength and lifetime. 3. Observes that in larger models, these sink patterns (sink levels) become more deterministic and frequent, with distinct levels identified in models like QwQ-32B and Qwen3-14B.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec1462ca646134f445eac98192ff5189abb63f37682802d695aadace9f83b0d3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec1462ca646134f445eac98192ff5189abb63f37682802d695aadace9f83b0d3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies a new phenomenon called &quot;secondary attention sinks&quot; in transformer LLMs, which are distinct from the known primary sinks (like BOS). The authors show these secondary sinks are created by middle-layer MLPs aligning tokens with the primary sink direction, and their properties (strength, lifetime) become more structured in larger models. This provides new insights into the internal mechanics of attention in large language models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [pedestrian attribute recognition], [vision-language model, cross-attention fusion, class imbalance, domain generalization, SigLIP]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abdellah Zakaria Sellam, Salah Eddine Bekhouche, Fadi Dornaika, Cosimo Distante, Abdenour Hadid</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Salento, Institute of Applied Sciences and Intelligent Systems - CNR, University of the Basque Country UPV/EHU, IKERBASQUE, Sorbonne University Abu Dhabi</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22217" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22217</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes VLM-PAR, a modular vision-language framework built on frozen SigLIP 2 multilingual encoders for Pedestrian Attribute Recognition. 2. Introduces a compact cross-attention fusion mechanism to align image and prompt embeddings by refining visual features. 3. Demonstrates state-of-the-art performance on the imbalanced PA100K benchmark and significant gains on PETA and Market-1501, showing effectiveness against imbalance and domain shift.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44bf59cdfc87f0708e9f41a8899fbff5562f87b0b721fe79f7fcfc23fb5bb4d4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44bf59cdfc87f0708e9f41a8899fbff5562f87b0b721fe79f7fcfc23fb5bb4d4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes VLM-PAR, a vision-language model framework that uses a frozen SigLIP encoder and a cross-attention fusion module to refine visual features for Pedestrian Attribute Recognition. It achieves new state-of-the-art results on the PA100K benchmark and shows strong performance on other datasets, demonstrating the effectiveness of leveraging large-scale vision-language pretraining to address class imbalance and generalization challenges in PAR.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Signal-SGN++: Topology-Enhanced Time-Frequency Spiking Graph Network for Skeleton-Based Action Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [skeleton-based action recognition], [Spiking Neural Networks, Graph Convolutional Networks, Time-Frequency Learning, Topology-Aware Learning, Energy Efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Naichuan Zheng, Xiahai Lun, Weiyi Li, Yuchen Du</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing University of Posts and Telecommunications</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22214" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22214</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel spiking graph network backbone integrating 1D Spiking Graph Convolution (1D-SGC) and Frequency Spiking Convolution (FSC) for joint spatiotemporal and spectral feature extraction. 2. Introduces a Topology-Shift Self-Attention (TSSA) mechanism to adaptively route attention across learned skeletal topologies without increasing computational complexity. 3. Designs an auxiliary Multi-Scale Wavelet Transform Fusion (MWTF) branch with a Topology-Aware Time-Frequency Fusion (TATF) unit to preserve structural priors in multi-resolution spectral fusion.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6af8fb8807de54b37db40834a79908ad86cf7e159907b658e54986356c4494d4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6af8fb8807de54b37db40834a79908ad86cf7e159907b658e54986356c4494d4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Signal-SGN++, a novel spiking graph network for skeleton-based action recognition that integrates topology-aware learning with time-frequency spiking dynamics to capture motion dependencies. The method combines a spiking graph backbone with a topology-shift attention mechanism and a multi-scale wavelet fusion branch. Experiments show it achieves a superior accuracy-efficiency trade-off, outperforming other SNN methods and competing with state-of-the-art GCNs while using significantly less energy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On Extending Semantic Abstraction for Efficient Search of Hidden Objects</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [semantic abstraction, relevancy maps, 3D localization, hidden objects, unstructured search]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tasha Pais, Nikhilesh Belulkar</p>
</li>
<li class="">
<p><strong>institution:</strong> Columbia University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22220" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22220</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Extends the Semantic Abstraction framework to the novel domain of localizing hidden (occluded) objects. 2. Proposes using historical placement data to efficiently guide the unstructured search for hidden objects. 3. Demonstrates a model that can accurately identify a hidden object&#x27;s complete 3D location faster than a naive random search.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb1e039b06f845acfd3a644354907fc35eec78f060cf605799b7607c8a20ba8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb1e039b06f845acfd3a644354907fc35eec78f060cf605799b7607c8a20ba8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of efficiently finding hidden or lost objects by extending the Semantic Abstraction framework. The method uses 2D VLM relevancy maps as abstract object representations to learn 3D localization and leverages historical placement data to optimize the search. The result is a model that can locate hidden objects significantly faster than random search, aiming to improve the capabilities of household robots.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [neural network architecture], [Müntz-Szász Networks, fractional power bases, physics-informed neural networks, universal approximation, singular function approximation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gnankan Landry Regis N&#x27;guessan</p>
</li>
<li class="">
<p><strong>institution:</strong> Axiom Research Group, The Nelson Mandela African Institution of Science and Technology (NM-AIST), African Institute for Mathematical Sciences (AIMS) Research and Innovation Centre</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22222" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22222</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Müntz-Szász Networks (MSN), a novel neural architecture with learnable fractional power bases to approximate functions with singular or fractional power behavior. 2. Provides theoretical analysis proving MSN inherits universal approximation from the Müntz-Szász theorem and establishes superior approximation rates compared to standard MLPs for singular functions. 3. Demonstrates empirical superiority, achieving significantly lower error with fewer parameters in supervised regression and 3-6x improvement in physics-informed neural network benchmarks, while learning interpretable exponents.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66c3ba4917449496481593b1432346dc5ef9301c3c66f4713e327bbb234969d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66c3ba4917449496481593b1432346dc5ef9301c3c66f4713e327bbb234969d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces Müntz-Szász Networks (MSN), a neural architecture that replaces fixed activation functions with learnable fractional power bases to better approximate singular functions common in physics. It proves MSN&#x27;s universal approximation capability and shows it achieves much lower error with fewer parameters than standard MLPs on regression and physics-informed tasks, demonstrating the value of theory-guided design.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video understanding], [streaming video, multimodal large language models, event segmentation, hierarchical representation, elastic-scale]</p>
</li>
<li class="">
<p><strong>authors:</strong> Naishan Zheng, Jie Huang, Qingpei Guo, Feng Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology of China, Ant Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22226" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22226</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/zheng980629/VideoScaffold" target="_blank" rel="noopener noreferrer" class="">https://github.com/zheng980629/VideoScaffold</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes VideoScaffold, a dynamic representation framework for streaming video understanding in MLLMs that adaptively adjusts event granularity. 2. Introduces Elastic-Scale Event Segmentation (EES) for prediction-guided, dynamic boundary refinement. 3. Introduces Hierarchical Event Consolidation (HEC) for progressively aggregating segments into multi-level abstractions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of understanding long, streaming videos with MLLMs by proposing VideoScaffold, a framework that dynamically segments and hierarchically consolidates video events to adapt granularity and preserve semantics. It achieves state-of-the-art performance on benchmarks and can extend image-based MLLMs to video comprehension.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [retrieval-augmented generation (RAG), network traffic analysis, large language models (LLMs), hierarchical retrieval, explainable AI]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shaghayegh Shajarian, Kennedy Marsh, James Benson, Sajad Khorsandroo, Mahmoud Abdelsalam</p>
</li>
<li class="">
<p><strong>institution:</strong> North Carolina A&amp;T State University, University of Texas at San Antonio</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22223" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22223</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/270771/llm-traffictraffic" target="_blank" rel="noopener noreferrer" class="">https://github.com/270771/llm-traffictraffic</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ReGAIN, a multi-stage framework combining traffic summarization, RAG, and LLM reasoning for transparent network traffic analysis. 2. Introduces a hierarchical retrieval pipeline with metadata filtering, MMR sampling, cross-encoder reranking, and an abstention mechanism to ground responses and reduce hallucinations. 3. Demonstrates high accuracy (95.95%-98.82%) on real-world attack traces and outperforms traditional baselines while providing explainable, evidence-cited outputs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56c5ba03e3d4a510212509143bfecf0fa8b76f9171aa38dd765dadecc7b1ab32_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56c5ba03e3d4a510212509143bfecf0fa8b76f9171aa38dd765dadecc7b1ab32_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents ReGAIN, a framework that uses retrieval-augmented generation (RAG) and LLMs to analyze network traffic. It converts traffic into summaries, retrieves relevant evidence from a vector database, and generates interpretable, grounded analyses. The method achieves high accuracy on attack detection and provides explainable results, outperforming traditional rule-based and ML approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Scalable Cloud-Native Architectures for Intelligent PMU Data Processing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [cluster infrastructure], [cloud-native, distributed stream processing, containerized microservices, elastic resource orchestration, edge-cloud hybrid]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nachiappan Chockalingam, Akshay Deshpande, Lokesh Butra, Ram Sekhar Bodala, Nitin Saksena, Adithya Parthasarathy, Balakrishna Pothineni, Akash Kumar Agarwal</p>
</li>
<li class="">
<p><strong>institution:</strong> IEEE, NTT Data, Amtrak, Albertsons Companies</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22231" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22231</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A comprehensive theoretical framework for AI-enhanced cloud-based PMU analytics. 2. Mathematical formulations for distributed machine learning optimized for PMU time-series data. 3. Analysis of edge-cloud hybrid architectures with integrated security and privacy considerations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59263c4210b1af52fedb9e9660a5117d937ac4a63d70c41f31a04dc3c553429f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59263c4210b1af52fedb9e9660a5117d937ac4a63d70c41f31a04dc3c553429f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a scalable cloud-native architecture to address the latency and scalability challenges of processing high-frequency data from Phasor Measurement Units (PMUs) in smart grids. The method integrates AI with edge and cloud computing, using distributed stream processing and containerized microservices for real-time analytics. The analysis shows the architecture can achieve sub-second response times while scaling to large deployments, providing a robust foundation for next-generation grid analytics.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image reconstruction], [diffusion model, cross-domain, meta-information, sinogram adapter, low-dose PET]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mengxiao Geng, Ran Hong, Xiaoling Xu, Bingxuan Li, Qiegen Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanchang University, Hefei Comprehensive National Science Center</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22237" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22237</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a meta-information guided cross-domain synergistic diffusion model (MiG-DM) that integrates cross-modal priors for PET reconstruction. 2. Introduces a meta-information encoding module that transforms clinical parameters into semantic prompts for cross-modal alignment. 3. Designs a cross-domain architecture with a specialized sinogram adapter to capture global physical structures in the projection domain.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/974872f0916ad96ffeb1ed9b169c4f627d5c534046b438f10fd015171720864a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/974872f0916ad96ffeb1ed9b169c4f627d5c534046b438f10fd015171720864a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of low-dose PET image reconstruction, which suffers from noise and loss of detail. It proposes a novel diffusion model called MiG-DM that guides the reconstruction using patient-specific meta-information and processes data across both the projection and image domains. Experiments show that MiG-DM outperforms existing methods in improving image quality and preserving physiological details.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] We are not able to identify AI-generated images</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image forensics], [AI-generated images, human evaluation, MidJourney, CC12M, synthetic media detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Adrien Pavão</p>
</li>
<li class="">
<p><strong>institution:</strong> (Institution not explicitly stated in provided content. Author name is Adrien Pavão; no affiliation or email domain is given. Therefore, institution cannot be reliably inferred.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22236" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22236</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a controlled web-based experiment to empirically test human ability to distinguish real photographs from AI-generated portraits, finding performance near random chance (54% accuracy). 2. Created and released a curated, challenging dataset of 120 images (real from CC12M and AI-generated counterparts from MidJourney) designed to be difficult for humans. 3. Demonstrated that human judgment is insufficient for reliable detection of synthetic media, highlighting the need for greater public awareness and ethical guidelines as AI-generated content becomes more realistic.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b100d4e4882d297b51639fb736da62f90b11f1d810da4b6665f9da69ef3f31_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b100d4e4882d297b51639fb736da62f90b11f1d810da4b6665f9da69ef3f31_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper tests the assumption that humans can easily identify AI-generated images through an interactive web experiment where participants classified 20 images as real or AI-generated. Using a carefully curated dataset of 120 difficult portrait images (real from CC12M and AI-generated from MidJourney), the study found an average human accuracy of only 54%, barely above random guessing. The results show that humans struggle to reliably detect AI-generated content, indicating that human judgment alone is becoming insufficient and underscoring the need for awareness and ethical guidelines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] DiRL: An Efficient Post-Training Framework for Diffusion Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [Diffusion Language Models, FlexAttention, Group Relative Policy Optimization, LMDeploy, blockwise training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ying Zhu, Jiaxin Wan, Xiaoran Liu, Siyanag He, Qiqi Wang, Xu Guo, Tianyi Liang, Zengfeng Huang, Ziwei He, Xipeng Qiu</p>
</li>
<li class="">
<p><strong>institution:</strong> Fudan University, Shanghai Innovation Institute, OpenMoss Team</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22234" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22234</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/OpenMOSS/DiRL" target="_blank" rel="noopener noreferrer" class="">https://github.com/OpenMOSS/DiRL</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DiRL, an efficient post-training framework for Diffusion Language Models (dLLMs) that integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. 2. Introduces DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation specifically designed for dLLMs. 3. Demonstrates state-of-the-art math reasoning performance for dLLMs by training DiRL-8B-Instruct, surpassing comparable models like Qwen2.5 series on benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0ae2d009d9099214203b1dcca9a8b460cf0609d952e240f981b2689f247e17d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0ae2d009d9099214203b1dcca9a8b460cf0609d952e240f981b2689f247e17d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the underdeveloped and inefficient post-training landscape for Diffusion Language Models (dLLMs). It proposes DiRL, an efficient framework combining accelerated training and optimized inference, and introduces DiPO, a tailored reinforcement learning method. The resulting model, DiRL-8B-Instruct, achieves state-of-the-art math performance among dLLMs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Enhanced geometry prediction in laser directed energy deposition using meta-learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [meta-learning], [meta-learning, model-agnostic meta-learning, reptile, laser-directed energy deposition, bead geometry prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abdul Malik Al Mardhouf Al Saadi, Amrita Basak</p>
</li>
<li class="">
<p><strong>institution:</strong> The Pennsylvania State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22241" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22241</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a cross-dataset knowledge transfer model for L-DED bead geometry prediction using meta-learning to address data scarcity and heterogeneity. 2. Investigated and applied two gradient-based meta-learning algorithms (MAML and Reptile) for rapid adaptation to new deposition conditions with limited data. 3. Demonstrated strong generalization performance of the meta-learning models across diverse L-DED processes (powder-fed, wire-fed, hybrid) using minimal training examples, outperforming conventional neural networks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4239294fb44dd0fe97ebc3a123162ba7aaa82e51c2805d4460072daeffe6de9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4239294fb44dd0fe97ebc3a123162ba7aaa82e51c2805d4460072daeffe6de9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of predicting bead geometry in laser-directed energy deposition (L-DED) where experimental data is scarce and heterogeneous. It proposes using meta-learning algorithms, specifically MAML and Reptile, to enable rapid model adaptation to new printing conditions with very few training examples. The results show that this approach achieves accurate predictions and outperforms traditional neural networks under similar data constraints, demonstrating effective knowledge transfer across different L-DED settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical imaging], [algorithmic fairness, subgroup performance analysis, JustEFAB framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shaurya Gaur, Michel Vitale, Alessa Hering, Johan Kwisthout, Colin Jacobs, Lena Philipp, Fennie van der Graaf</p>
</li>
<li class="">
<p><strong>institution:</strong> Radboud University Medical Center, Radboud University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22242" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22242</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a fairness evaluation of three lung cancer risk estimation models (Sybil, Venkadesh21, PanCan2b) using the JustEFAB framework to assess ethically significant biases. 2. Identified and quantified statistically significant performance disparities across demographic subgroups (e.g., gender, race) that were not explained by available clinical confounders. 3. Highlighted the critical need for monitoring and improving model fairness in lung cancer screening AI to ensure equitable clinical application.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study evaluates the fairness of AI models for lung cancer risk estimation from CT scans. Using the JustEFAB framework, it assessed performance disparities across demographic groups and found significant, unexplained biases in two deep learning models. The findings underscore the importance of algorithmic fairness in medical AI to ensure equitable screening outcomes.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Masking Teacher and Reinforcing Student for Distilling Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [knowledge distillation, reinforcement learning, vision-language models, progressive masking, offline RL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Byung-Kwan Lee, Yu-Chiang Frank Wang, Ryo Hachiuma</p>
</li>
<li class="">
<p><strong>institution:</strong> NVIDIA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22238" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22238</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Masters, a mask-progressive RL distillation framework that first masks non-dominant teacher weights to reduce complexity and then progressively restores them for stable student learning. 2. Introduces an offline RL stage with complementary accuracy and distillation rewards, leveraging pre-generated responses from masked teachers for efficient guidance. 3. Demonstrates that progressive teacher scaling (e.g., from 14B to 38B) yields smoother convergence and stronger generalization than one-shot distillation, providing a scalable path to efficient VLMs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of distilling large vision-language models (VLMs) into compact ones by proposing Masters, a framework that uses progressive masking of the teacher model and offline reinforcement learning. This method enables stable knowledge transfer and efficient training, resulting in small VLMs that achieve strong performance, sometimes surpassing larger models, while being far more efficient for deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [model compression], [knowledge distillation, lightweight CNN, inverted residual blocks, dense connectivity, multi-objective learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Phi-Hung Hoang, Nam-Thuan Trinh, Van-Manh Tran, Thi-Thu-Hong Phan</p>
</li>
<li class="">
<p><strong>institution:</strong> FPT University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22239" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22239</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hybrid knowledge distillation framework integrating hard-label supervision, feature-level, response-level, and self-distillation for training efficient models. 2. Designs a customized student CNN architecture combining inverted residual blocks with dense connectivity to balance efficiency and accuracy. 3. Demonstrates strong generalization across multiple agricultural datasets (rice seeds and plant leaf diseases) with significant reductions in computational cost and model size while maintaining high accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccaf949c91086899f4aa9953236d8ee76957b0a5aaafcda22bf81f403ee1e0a5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccaf949c91086899f4aa9953236d8ee76957b0a5aaafcda22bf81f403ee1e0a5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a multi-objective hybrid knowledge distillation method to create a lightweight CNN for smart agriculture, combining inverted residual and dense blocks. The distilled model achieves near-teacher accuracy with drastically reduced computation and parameters, showing robust performance on rice seed and plant disease datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [interpretability], [mechanistic multiplicity, explanatory stability, stochastic optimization, model explanations, diagnostic framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chama Bensmail</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Hertfordshire, Omics Data Solutions LTD</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22240" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22240</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/bensmailchama-boop/EvoXplain" target="_blank" rel="noopener noreferrer" class="">https://github.com/bensmailchama-boop/EvoXplain</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces EvoXplain, a diagnostic framework for measuring the stability of model explanations across repeated training runs, treating explanations as samples from the optimization process. 2. Demonstrates that high-accuracy models (e.g., Logistic Regression, Random Forests) can rely on multiple distinct internal mechanisms, revealing explanatory multimodality not captured by single-model or averaged explanations. 3. Reframes interpretability as a property of a model class under repeated instantiation, challenging the assumption that a single high-accuracy model yields a unique or trustworthy explanation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a39d3b0c4608e98a5ffde3a753078019f45b0c48774cb02ee158633c35e823d2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a39d3b0c4608e98a5ffde3a753078019f45b0c48774cb02ee158633c35e823d2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces EvoXplain, a framework to diagnose if models achieving similar high accuracy do so via the same or different internal mechanisms by analyzing explanation stability across training runs. It finds that even simple, stable models like Logistic Regression can exhibit multiple distinct explanatory modes on datasets like Breast Cancer and COMPAS, showing that single-model explanations can be misleading. This work highlights explanatory instability as a measurable property and reframes interpretability as a characteristic of a model class rather than a single trained instance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [uncertainty estimation, calibration, linear probe, brier score, llm-as-judge]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bhaktipriya Radharapu, Eshika Saxena, Kenneth Li, Chenxi Whitehouse, Adina Williams, Nicola Cancedda</p>
</li>
<li class="">
<p><strong>institution:</strong> Meta (FAIR at Meta, Meta Superintelligence Labs)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22245" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22245</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a method using linear probes on LLM hidden states to provide calibrated uncertainty estimates for LLM judges, requiring no additional model training. 2. Demonstrates superior calibration and ≈10x computational savings compared to baseline methods like verbalized confidence. 3. Shows the method generalizes robustly across different model architectures, training paradigms, and unseen evaluation domains.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33ea018b43295c51d076b3840537c861c2e2c7f22ca2bdd183164d1f1feed91d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33ea018b43295c51d076b3840537c861c2e2c7f22ca2bdd183164d1f1feed91d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of obtaining efficient and well-calibrated uncertainty estimates for LLM-based judges. It proposes using linear probes trained with a Brier score loss on the model&#x27;s hidden states. The method achieves better calibration with significant computational savings and provides a practical plug-and-play solution for production deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Graph Attention-based Adaptive Transfer Learning for Link Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [graph attention network, link prediction, transfer learning, graph transformer, contrastive loss]</p>
</li>
<li class="">
<p><strong>authors:</strong> Huashen Lu, Wensheng Gan, Guoting Chen, Zhichao Huang, Philip S. Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> Jinan University, Great Bay University, JD Technology, University of Illinois Chicago</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22252" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22252</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/DSI-Lab1/GAATNet" target="_blank" rel="noopener noreferrer" class="">https://github.com/DSI-Lab1/GAATNet</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GAATNet, a novel graph attention adaptive transfer network combining pre-training and fine-tuning for cross-dataset knowledge transfer in link prediction. 2. Incorporates distant neighbor embeddings as biases in self-attention to capture global node features. 3. Introduces a lightweight self-adapter module during fine-tuning to improve training efficiency and generalization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e9ea1091686509af1da226ce7553577b4005c5646524a5c6718473e451d3c39_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e9ea1091686509af1da226ce7553577b4005c5646524a5c6718473e451d3c39_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses challenges in link prediction on large-scale sparse graphs and cross-dataset transfer learning by proposing GAATNet, which integrates graph attention with adaptive transfer strategies. The method uses distant neighbor embeddings and a self-adapter module to enhance global feature capture and training efficiency. Experiments on seven datasets show state-of-the-art performance, offering a scalable solution for integrating GNNs with transfer learning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [biomedical knowledge graph, graph attention network, gene perturbation, multimodal embeddings, PrimeKG++]</p>
</li>
<li class="">
<p><strong>authors:</strong> Pascal Passigan, Kevin zhu, Angelina Ning</p>
</li>
<li class="">
<p><strong>institution:</strong> Massachusetts Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22251" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22251</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a novel framework for gene perturbation prediction by merging PrimeKG++ with LINCS L1000 data into a heterogeneous biomedical knowledge graph, moving beyond binary drug-disease association tasks. 2. Demonstrates the application of a Graph Attention Network (GAT) to predict delta gene expression profiles for drug-cell pairs, outperforming MLP baselines. 3. Provides interpretability through ablation studies (edge shuffling, node feature randomization) showing the critical role of biomedical KG edges in enhancing perturbation-level prediction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a8b52522e1bdfc53ae9978a144c2011810eca2532cb9819ad999bf9b2b6cbb6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a8b52522e1bdfc53ae9978a144c2011810eca2532cb9819ad999bf9b2b6cbb6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the gap in predicting detailed gene expression changes (perturbations) caused by drugs by constructing a merged biomedical knowledge graph from PrimeKG++ and LINCS L1000 data. The proposed method uses a Graph Attention Network to predict delta expression profiles for drug-cell pairs, which outperforms baseline models and demonstrates the value of graph structure for mechanistic understanding.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [prompt engineering], [Logic Sketch Prompting, deterministic prompting, interpretability, rule adherence, clinical decision support]</p>
</li>
<li class="">
<p><strong>authors:</strong> Satvik Tripathi</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Pennsylvania</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22258" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22258</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/satviktri/LSP" target="_blank" rel="noopener noreferrer" class="">https://github.com/satviktri/LSP</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Logic Sketch Prompting (LSP), a lightweight prompting framework that introduces typed variables and deterministic condition evaluators for structured reasoning., 2. Incorporates a rule-based validator to produce traceable and repeatable outputs, enhancing auditability., 3. Demonstrates significant performance gains over standard prompting methods (zero-shot, chain-of-thought, concise) on pharmacologic logic-compliance tasks across multiple open-weight LLMs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b5d49d54027e4f7e6b110a48568a0255a45010197e26e8bc344e0cd3e1785a9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b5d49d54027e4f7e6b110a48568a0255a45010197e26e8bc344e0cd3e1785a9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the unreliability of LLMs on tasks requiring strict rule adherence and determinism. It proposes Logic Sketch Prompting (LSP), a framework using typed variables and rule-based validation to produce traceable outputs. Evaluations on clinical tasks show LSP significantly outperforms standard prompting methods in accuracy and F1 score, making it suitable for safety-critical systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Agentic Software Issue Resolution with Large Language Models: A Survey</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [se], [automated software maintenance], [large language models, agentic systems, software issue resolution, reinforcement learning, software engineering]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhonghao Jiang, David Lo, Zhongxin Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Singapore Management University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22256" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22256</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/ZhonghaoJiang/Awesome-Issue-Solving" target="_blank" rel="noopener noreferrer" class="">https://github.com/ZhonghaoJiang/Awesome-Issue-Solving</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a systematic survey of 126 recent studies on LLM-based agentic software issue resolution. 2. Establishes a taxonomy for the field across three key dimensions: benchmarks, techniques, and empirical studies. 3. Highlights the paradigm shift brought by agentic reinforcement learning in designing and training agentic systems for software engineering.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5c1c5e173acc2646c4322651d8a6c89dabed4b251b6106c2a468adeeafadf5f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5c1c5e173acc2646c4322651d8a6c89dabed4b251b6106c2a468adeeafadf5f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper surveys the use of Large Language Model (LLM)-based agentic systems for automating complex software issue resolution, such as bug fixing. It reviews recent research, categorizes approaches, and discusses how agentic reinforcement learning is changing system design. The conclusion outlines current challenges and future research directions for improving automated software maintenance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [reasoning], [chain-of-thought, synthetic data, distribution shift, fine-tuning, reasoning robustness]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abhranil Chandra, Ayush Agrawal, Arian Hosseini, Sebastian Fischmeister, Rishabh Agarwal, Navin Goyal, Aaron Courville</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Waterloo, University of Massachusetts Amherst, MILA - Quebec AI Institute, Université de Montréal, Microsoft Research India, Google DeepMind, Periodic Labs</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22255" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22255</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that training on synthetic chain-of-thought traces from more capable models, even when they lead to incorrect final answers, can improve a language model&#x27;s reasoning performance more than training on human-annotated datasets. 2. Proposes and validates two hypotheses for this phenomenon: the distributional alignment of synthetic data with the model, and the partial validity of reasoning steps within flawed traces. 3. Shows that paraphrasing human traces to align with the model&#x27;s distribution improves performance, and investigates model tolerance to increasingly flawed reasoning steps.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf038d101bb93ad9f8c253c481419dec618655c74a6b867d0128b6358a3fa331_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf038d101bb93ad9f8c253c481419dec618655c74a6b867d0128b6358a3fa331_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper challenges the assumption that correctness is the primary determinant of data quality for training language models on reasoning tasks. It shows that fine-tuning on synthetic, incorrect chain-of-thought traces from stronger models can outperform training on correct human-annotated data, primarily because the synthetic data&#x27;s distribution is closer to the model&#x27;s own. The key conclusion is that aligning the training data distribution with the model&#x27;s is more critical for performance than the correctness of the final answers.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ReVEAL: GNN-Guided Reverse Engineering for Formal Verification of Optimized Multipliers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [formal verification, computer algebra], [reverse engineering, graph neural network, multiplier verification, algebraic circuit verification, SAT-based equivalence checking]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chen Chen, Daniela Kaufmann, Chenhui Deng, Zhan Song, Hongce Zhang, Cunxi Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Maryland, College Park; TU Wien; NVIDIA; Hong Kong University of Science and Technology (Guangzhou)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22260" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22260</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ReVEAL, a graph-learning-based framework for reverse engineering optimized multiplier architectures to recover their word-level structure. 2. Leverages structural graph features and learning-driven inference to identify architectural patterns at scale, enabling robust handling of large, optimized circuits. 3. Integrates smoothly with existing verification flows and supports downstream algebraic proof strategies, showing improvements in scalability and accuracy over traditional rule-based approaches.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bdd1907cef6efb0b9b81d88b611f825942f30ef8a8674a9587cc4261e4774ef_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bdd1907cef6efb0b9b81d88b611f825942f30ef8a8674a9587cc4261e4774ef_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces ReVEAL, a method that uses Graph Neural Networks (GNNs) to reverse engineer the architecture of optimized hardware multipliers. This recovered structure enables more effective formal verification using algebraic techniques. The approach demonstrates improved scalability and accuracy compared to traditional rule-based methods on diverse benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph representation learning], [temporal motifs, dynamic graphs, llm agent, structure-aware dispatcher, prompting techniques]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bing Hao, Minglai Shao, Zengyi Wo, Yunlong Chu, Yuhang Liu, Ruijie Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tianjin University, Beihang University, Guangxi Normal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22266" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22266</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Wjerry5/LLMTM" target="_blank" rel="noopener noreferrer" class="">https://github.com/Wjerry5/LLMTM</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LLMTM, a comprehensive benchmark for evaluating LLMs on six tasks across nine types of temporal motifs in dynamic graphs. 2. Develops a tool-augmented LLM agent that uses engineered prompts to achieve high accuracy on temporal motif analysis tasks. 3. Introduces a structure-aware dispatcher that intelligently routes queries between standard LLM prompting and the more powerful agent to balance accuracy and cost.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ebbb05fef920a296d5863029d0c69e932a75230132aa0658a09e6bd8d04010c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ebbb05fef920a296d5863029d0c69e932a75230132aa0658a09e6bd8d04010c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the use of Large Language Models (LLMs) for analyzing temporal motifs in dynamic graphs, an area that is relatively unexplored. The authors propose a new benchmark (LLMTM), develop a high-accuracy but costly tool-augmented LLM agent, and then introduce a structure-aware dispatcher to reduce cost while maintaining performance. Their experiments show the dispatcher effectively maintains high accuracy while reducing cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal reasoning], [clinical reasoning benchmark, vision-language models, multimodal integration, medical image interpretation, hallucination]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dingyu Wang, Zimu Yuan, Jiajun Liu, Shanggui Liu, Nan Zhou, Tianxing Xu, Di Huang, Dong Jiang</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University Third Hospital, Beihang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22275" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22275</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced the Bones and Joints (B&amp;J) Benchmark, a comprehensive evaluation framework with 1,245 questions derived from real-world patient cases to assess clinical reasoning. 2. Revealed a significant performance gap in VLMs, showing high accuracy on structured tasks but poor performance on open-ended, multimodal reasoning tasks, with severe text-driven hallucinations. 3. Demonstrated that medically fine-tuned models show no consistent advantage over general-purpose models, highlighting a fundamental limitation in current AI for clinical competency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43db8495c9ac46c5ea892f723394bd1e6c9b400003c2c67ace7ac9abe26f0bcf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43db8495c9ac46c5ea892f723394bd1e6c9b400003c2c67ace7ac9abe26f0bcf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces the Bones and Joints (B&amp;J) Benchmark to rigorously evaluate the clinical reasoning capabilities of vision-language and large language models. The results show that while models perform well on structured tasks, they struggle significantly with open-ended, multimodal reasoning essential for real-world patient care, indicating they are not yet clinically competent. The authors conclude that safe AI deployment should be limited to supportive roles until fundamental breakthroughs in multimodal integration are achieved.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Valori: A Deterministic Memory Substrate for AI Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [memory &amp; caching], [deterministic memory, fixed-point arithmetic, vector embeddings, approximate nearest neighbor search, state machine]</p>
</li>
<li class="">
<p><strong>authors:</strong> Varshith Gudur</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher (Valori Kernel Project)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22280" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22280</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/varshith-Git/Valori-Kernel" target="_blank" rel="noopener noreferrer" class="">https://github.com/varshith-Git/Valori-Kernel</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and characterizes the fundamental non-determinism in AI memory systems caused by hardware-dependent floating-point arithmetic, which leads to divergent memory states and retrieval results. 2. Proposes Valori, a deterministic AI memory substrate that replaces floating-point operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. 3. Demonstrates that Valori guarantees bit-identical memory states, snapshots, and search results across different hardware platforms (e.g., x86 vs. ARM), establishing deterministic memory as a necessary primitive for trustworthy AI.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb7ed17a8a06e6a2b8760f08fa9345391995aee74a3542cacf55dd051b383f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb7ed17a8a06e6a2b8760f08fa9345391995aee74a3542cacf55dd051b383f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies non-determinism in AI memory systems due to hardware-dependent floating-point arithmetic, which compromises replayability and auditability. It proposes Valori, a memory substrate using fixed-point arithmetic and a state machine model to guarantee bit-identical behavior across platforms. The work concludes that deterministic memory is essential for building trustworthy AI systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Cluster Aggregated GAN (CAG): A Cluster-Based Hybrid Model for Appliance Pattern Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models], [Generative Adversarial Networks, Non-Intrusive Load Monitoring, Clustering, LSTM, Pattern Generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zikun Guoa, Adeyinka.P. Adedigbaa, Rammohan Mallipeddi</p>
</li>
<li class="">
<p><strong>institution:</strong> Kyungpook National University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22287" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22287</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hybrid GAN framework that routes appliances to specialized branches based on behavioral characteristics (intermittent vs. continuous). 2. Introduces a clustering module for intermittent appliances to group similar activation patterns and allocate dedicated generators, improving modeling of both common and rare modes. 3. Employs a separate LSTM-based generator branch for continuous appliances to capture gradual temporal evolution while maintaining training stability through sequence compression.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37f2c4c207022049ee869567d36df9e77e5a04d1beb0cc584dc57ee6ad1145b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37f2c4c207022049ee869567d36df9e77e5a04d1beb0cc584dc57ee6ad1145b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes the Cluster Aggregated GAN (CAG), a hybrid generative model that synthesizes appliance load patterns by separating intermittent and continuous devices into specialized branches, using clustering for the former and an LSTM for the latter. Experiments on the UVIC dataset show it outperforms baselines in realism, diversity, and training stability. The integration of clustering as an active component also enhances the model&#x27;s interpretability and scalability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [causal inference], [Double Machine Learning, Moderated Mediation, Algorithmic Control, Nonmonotonic Effects, Gig Economy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Arunkumar V, Nivethitha S, Sharan Srinivas, Gangadharan G.R</p>
</li>
<li class="">
<p><strong>institution:</strong> Anna University, National Institute of Technology Tiruchirappalli, University of Missouri</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22290" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22290</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Applied a Double Machine Learning framework to estimate a moderated mediation model without restrictive linear assumptions in organizational research. 2. Uncovered a nonmonotonic relationship between algorithmic oversight, worker wellbeing, and performance, highlighting a &quot;murky middle&quot; of confusing oversight. 3. Demonstrated that simple linear models can be misleading and provided practical insights for designing transparent and explainable algorithmic management systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ffeb8d364908888226033cff39cbb354772ae1044ba1a51632db140d81dca17_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ffeb8d364908888226033cff39cbb354772ae1044ba1a51632db140d81dca17_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the nonlinear effects of algorithmic control on gig workers. Using a Double Machine Learning approach on survey data, it finds that the link between supportive HR practices and worker performance weakens under opaque algorithmic oversight but strengthens again when the oversight is transparent and explainable.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [Masked Diffusion Models, Markov Decision Process, Group Relative Policy Optimization, inference schedule optimization, trajectory-level training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Renping Zhou, Zanlin Ni, Tianyi Chen, Zeyu Liu, Yang Yue, Yulin Wang, Yuxuan Wang, Jingshu Liu, Gao Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University (Leap Lab), Anyverse Dynamics</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22288" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22288</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://co-grpo.github.io" target="_blank" rel="noopener noreferrer" class="">https://co-grpo.github.io</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and addresses the discrepancy between the single-step training and multi-step inference of Masked Diffusion Models (MDMs). 2. Proposes Co-GRPO, a method that reformulates MDM generation as a unified Markov Decision Process to jointly optimize model parameters and inference schedule parameters. 3. Introduces a trajectory-level optimization using Group Relative Policy Optimization that avoids costly backpropagation through the multi-step generation process.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the misalignment between the training and inference procedures of Masked Diffusion Models (MDMs). It proposes Co-GRPO, a method that jointly optimizes the MDM and its inference schedule as a unified Markov Decision Process using Group Relative Policy Optimization. The approach improves generation quality across multiple benchmarks without requiring expensive backpropagation through the full generation trajectory.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scientific machine learning], [Physics-informed neural networks, Kolmogorov-Arnold networks, Adaptive weighting, B-splines, Partial differential equations]</p>
</li>
<li class="">
<p><strong>authors:</strong> Guokan Chen, Yao Xiao</p>
</li>
<li class="">
<p><strong>institution:</strong> Fujian University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22283" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22283</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DBAW-PIKAN, a novel architecture combining a Kolmogorov-Arnold Network (KAN) with learnable B-splines for enhanced function representation in solving PDEs., 2. Introduces an adaptive weighting strategy with a dynamic decay upper bound to mitigate gradient flow stiffness and spectral bias, addressing key failure modes of PINNs., 3. Demonstrates significant improvements in convergence speed and solution accuracy (at least an order of magnitude) on benchmarks like Klein-Gordon, Burgers, and Helmholtz equations without added computational cost.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb9e65232c0c340c6072237e2ad1388255462aafca80cf91d4b7f3054eb9fe8c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb9e65232c0c340c6072237e2ad1388255462aafca80cf91d4b7f3054eb9fe8c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes DBAW-PIKAN, a novel neural network that integrates a Kolmogorov-Arnold architecture with an adaptive weighting strategy to overcome the stiffness and spectral bias challenges faced by Physics-Informed Neural Networks (PINNs) when solving multi-scale PDEs. The method accelerates convergence and improves solution accuracy by at least an order of magnitude on standard benchmarks without increasing computational complexity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multi-Head Spectral-Adaptive Graph Anomaly Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph anomaly detection], [spectral graph neural network, hypernetwork, Chebyshev filter, teacher-student contrastive learning, Barlow Twins loss]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qingyue Cao, Bo Jin, Changwei Gong, Xin Tong, Wenzheng Li, Xiaodong Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> People&#x27;s Public Security University of China, Third Research Institute of the Ministry of Public Security, Shanghai Police College</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22291" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22291</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Multi-Head Spectral-Adaptive Graph Neural Network (MHSA-GNN) that uses a lightweight hypernetwork to dynamically generate instance-specific Chebyshev filter parameters based on a &#x27;spectral fingerprint&#x27;. 2. Introduces a novel dual regularization strategy combining teacher-student contrastive learning (TSC) and Barlow Twins diversity loss (BTD) to prevent mode collapse and ensure representation accuracy and head orthogonality in the multi-head mechanism. 3. Demonstrates through extensive experiments that the method effectively preserves high-frequency anomaly signals and outperforms state-of-the-art methods, especially on highly heterogeneous datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ad6dee88128393dc0036e3430c2763e19882412f9750f09622c52d9ae28dc6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ad6dee88128393dc0036e3430c2763e19882412f9750f09622c52d9ae28dc6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of graph anomaly detection where fixed filters in spectral GNNs cause over-smoothing and fail to adapt to varying graph structures. It proposes MHSA-GNN, which uses a hypernetwork to generate adaptive filters per instance and a dual regularization strategy to stabilize multi-head learning. Experiments show the method preserves critical high-frequency signals and achieves superior performance, particularly on heterogeneous graphs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal retrieval and generation], [3D retrieval, 4D generation, cross-modal alignment, multi-head attention, open-vocabulary]</p>
</li>
<li class="">
<p><strong>authors:</strong> Philip Xu, David Elizondo, Raouf Hamzaoui</p>
</li>
<li class="">
<p><strong>institution:</strong> De Montfort University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22294" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22294</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Uni4D, a unified framework for large-scale open-vocabulary 3D retrieval and controlled 4D generation. 2. Introduces a structured three-level alignment strategy across text, 3D models, and images to enhance semantic understanding. 3. Presents a 3D-Text Multi-head Attention and Search (ATMS) model to optimize text-to-3D retrieval efficiency and accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb826b31ecaac1f8358869614a5af4e6a0fe9eeceb1eb97ce8bbb0ff8afdcd6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb826b31ecaac1f8358869614a5af4e6a0fe9eeceb1eb97ce8bbb0ff8afdcd6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Uni4D, a framework that uses a three-level alignment strategy across text, 3D, and images to address the challenges of large-scale 3D retrieval and controlled 4D generation. The method employs a novel attention and search model to improve semantic alignment and retrieval efficiency. Experimental results demonstrate that Uni4D achieves high-quality 3D retrieval and controllable 4D generation, advancing dynamic multimodal understanding.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Attack-Aware Deepfake Detection under Counter-Forensic Manipulations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [deepfake detection], [counter-forensics, red-team training, test-time defense, two-stream architecture, tamper heatmaps]</p>
</li>
<li class="">
<p><strong>authors:</strong> Noor Fatima, Hasan Faraz Khan, Muzammil Behzad</p>
</li>
<li class="">
<p><strong>institution:</strong> King Fahd University of Petroleum and Minerals (KFUPM), SDAIA-KFUPM Joint Research Center for Artificial Intelligence</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22303" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22303</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an attack-aware deepfake detection method combining red-team training with randomized test-time defense for robustness against counter-forensic manipulations. 2. Introduces a two-stream architecture with a lightweight residual adapter for fusing semantic and forensic features, and a weakly supervised FPN-style head for generating tamper heatmaps. 3. Establishes a practical, modular, and data-efficient baseline with well-calibrated probabilities and actionable evidence, evaluated on standard and challenging surveillance-style datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45e4389b2e929ec3ecb92d5f6329f2086fd32a88abcf98391c61119cd497cc8f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45e4389b2e929ec3ecb92d5f6329f2086fd32a88abcf98391c61119cd497cc8f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of robust deepfake detection under realistic counter-forensic attacks. It proposes a two-stream model trained with worst-case adversarial manipulations and defended at test-time with random jitters, which achieves strong performance, reliable probability calibration, and useful localization heatmaps. The method provides a practical and data-efficient baseline for attack-aware detection in real-world conditions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Beyond Single Bugs: Benchmarking Large Language Models for Multi-Vulnerability Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [vulnerability detection], [multi-vulnerability detection, count bias, selection bias, long-context code, CWE injection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chinmay Pushkar, Sanchit Kabra, Dhruv Kumar, Jagat Sesh Challa</p>
</li>
<li class="">
<p><strong>institution:</strong> BITS Pilani, Virginia Tech</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22306" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22306</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a comprehensive benchmark for Multi-Vulnerability Detection across four programming languages (C, C++, Python, JavaScript) to address the limitations of existing single-vulnerability benchmarks. 2. Constructed a novel dataset of 40,000 files by systematically injecting controlled counts of vulnerabilities (1, 3, 5, 9) into long-context code samples, enabling the study of performance under varying vulnerability densities. 3. Quantified the performance degradation of state-of-the-art LLMs (e.g., GPT-4o-mini, Llama-3.3-70B) in high-density vulnerability settings, revealing distinct failure modes like severe &quot;under-counting&quot; in Python and JavaScript.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd6a5fa286f161d0dab7d6a06a8f54502b88fd2f448edc9ed3609a31efaf97f5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd6a5fa286f161d0dab7d6a06a8f54502b88fd2f448edc9ed3609a31efaf97f5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the gap in evaluating LLMs for detecting multiple vulnerabilities in large, real-world code files. The authors propose a new benchmark by creating a dataset of long code files with systematically injected vulnerabilities and evaluate several LLMs. The main finding is that LLM performance sharply degrades as the number of vulnerabilities per file increases, with significant drops in recall for languages like Python.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LLMBoost: Make Large Language Models Stronger with Boosting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [ensemble learning, boosting, cross-model attention, chain training, near-parallel inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zehao Chen, Tianxiang Ai, Yifei Li, Gongxun Li, Yuyang Wei, Wang Zhou, Guanghui Li, Bin Yu, Zhijun Chen, Hailong Sun, Fuzhen Zhuang, Jianxin Li, Deqing Wang, Yikun Ban</p>
</li>
<li class="">
<p><strong>institution:</strong> Beihang University, China Telecom eSurfing Cloud</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22309" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22309</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A cross-model attention mechanism that allows successor models to access and fuse hidden states from predecessors for hierarchical error correction and knowledge transfer. 2. A chain training paradigm that progressively fine-tunes connected models with an error-suppression objective to rectify predecessor mispredictions efficiently. 3. A near-parallel inference paradigm that pipelines hidden states across models layer by layer, achieving inference efficiency close to single-model decoding.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb6a601c8d3bba7ec992d00772421c31e3e03d00d8a89a06f09ad3fe3c1b6ce1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb6a601c8d3bba7ec992d00772421c31e3e03d00d8a89a06f09ad3fe3c1b6ce1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes LLMBoost, a novel ensemble fine-tuning framework for LLMs that leverages intermediate hidden states across models. Inspired by boosting, it introduces cross-model attention, chain training, and a near-parallel inference pipeline to improve accuracy and reduce latency. Experiments on reasoning tasks show it consistently boosts performance while maintaining efficient inference.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [hardware security, model protection], [logic locking, intellectual property protection, hardware accelerator, model theft, supply chain security]</p>
</li>
<li class="">
<p><strong>authors:</strong> You Li, Guannan Zhao, Yuhao Ju, Yunqi He, Jie Gu, Hai Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> Northwestern University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22307" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22307</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LLA, a hardware-software co-design scheme for protecting generative AI models by embedding key bits into neurons and using invariance transformations to obscure them. 2. Integrates a lightweight, dataflow-compatible locking module into the AI accelerator, using the accelerator with a secret key as a license for model access. 3. Demonstrates that the approach is resilient against oracle-guided key optimization attacks while adding minimal computational overhead (&lt;0.1% for 7,168 key bits).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df323cc56f8d048243dce9e6af97041fca6264165872c422e5f81437cb03ef0d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df323cc56f8d048243dce9e6af97041fca6264165872c422e5f81437cb03ef0d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces LLA, a method to protect generative AI models from supply chain threats like theft and corruption by combining software-based key embedding in neurons with a hardware locking module in the accelerator. This approach uses the accelerator as a license key, ensuring only authorized hardware can run the model correctly. Evaluation shows it effectively resists attacks with negligible performance overhead.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LangPrecip: Language-Aware Multimodal Precipitation Nowcasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [weather forecasting], [multimodal nowcasting, rectified flow, semantic motion constraint, latent space integration, large-scale dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xudong Ling, Tianxi Huang, Qian Dong, Tao He, Chaorong Li, Guiduo Duan</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China (UESTC), Chengdu Textile College, Yibin University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22317" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22317</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed LangPrecip, a language-aware multimodal nowcasting framework that uses meteorological text as a semantic motion constraint to guide precipitation evolution. 2. Introduced LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. 3. Formulated nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm for efficient and physically consistent multimodal integration.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes LangPrecip, a novel framework that integrates meteorological text descriptions with radar data to constrain precipitation nowcasting. By formulating the problem as semantically constrained trajectory generation using Rectified Flow, it achieves significant performance gains, especially for heavy rainfall at long lead times, as demonstrated on Swedish and MRMS datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video understanding], [agentic framework, temporal zoom, reinforcement learning, long video reasoning, multimodal large language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yang Ding, Yizhen Zhang, Xin Lai, Ruihang Chu, Yujiu Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, The Chinese University of Hong Kong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22315" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22315</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/zsgvivo/VideoZoomer" target="_blank" rel="noopener noreferrer" class="">https://github.com/zsgvivo/VideoZoomer</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control visual focus during reasoning for long videos. 2. Introduces a two-stage training strategy combining supervised fine-tuning on distilled trajectories with reinforcement learning to refine the agentic policy. 3. Demonstrates strong performance across long video benchmarks, surpassing open-source models and rivaling proprietary systems with superior efficiency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab9b4b645f4fff1b4f548256b858cb41da2b35db78b1083f110e983d60c3547e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab9b4b645f4fff1b4f548256b858cb41da2b35db78b1083f110e983d60c3547e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the limitation of Multimodal LLMs in understanding long videos due to context window constraints. It proposes VideoZoomer, an agentic framework that dynamically selects and zooms into key temporal moments for fine-grained evidence gathering, trained with a two-stage strategy. The resulting 7B model achieves state-of-the-art performance on long video reasoning benchmarks with high efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SpotEdit: Selective Region Editing in Diffusion Transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [Diffusion Transformers, selective region editing, training-free, perceptual similarity, dynamic fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhibin Qin, Zhenxiong Tan, Zeqing Wang, Songhua Liu, Xinchao Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Singapore, Shanghai Jiao Tong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22323" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22323</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://biangbiang0321.github.io/SpotEdit.github.io/" target="_blank" rel="noopener noreferrer" class="">https://biangbiang0321.github.io/SpotEdit.github.io/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a training-free framework (SpotEdit) for selective region editing in Diffusion Transformers, reducing redundant computation. 2. Introduces SpotSelector to identify stable, unmodified regions via perceptual similarity and skip their denoising. 3. Introduces SpotFusion to adaptively blend reused conditional features with edited tokens, preserving coherence and quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4788068dfc021eb102e739694d672f72a617b80ada2a17fbe2ccbc2780fc1287_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4788068dfc021eb102e739694d672f72a617b80ada2a17fbe2ccbc2780fc1287_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the inefficiency of full-image regeneration in diffusion-based editing when only small regions need modification. It proposes SpotEdit, a training-free framework that selectively updates only modified regions using a selector for stable areas and a fusion mechanism for coherence. This approach reduces computation and maintains fidelity in unedited areas for efficient, precise editing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [self-verifying agent, proactive evidence seeking, LLM-as-a-Judge, 3C Principles, agentic reinforcement learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University, Tencent</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22322" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22322</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://huggingface.co/collections/yolay/smartsnap" target="_blank" rel="noopener noreferrer" class="">https://huggingface.co/collections/yolay/smartsnap</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed SmartSnap, a paradigm shift from passive, post-hoc task verification to proactive, in-situ self-verification by the agent itself. 2. Introduced the Self-Verifying Agent, a new agent type with dual missions to complete tasks and prove accomplishment via curated snapshot evidences guided by 3C Principles (Completeness, Conciseness, Creativity). 3. Demonstrated that the SmartSnap paradigm enables scalable training of LLM-driven agents, achieving significant performance gains (up to 26.08% and 16.66%) on mobile tasks and competitive results against larger models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the scalability bottleneck in agentic RL caused by costly and unreliable post-hoc task verification. It proposes SmartSnap, a paradigm where agents proactively seek minimal, decisive snapshot evidence to prove task completion during execution, guided by 3C Principles. Experiments show this approach significantly improves agent performance and enables scalable training, achieving competitive results with much larger models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [TimeXer, Global M2 Liquidity, exogenous variable, long-horizon forecasting]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sravan Karthick T</p>
</li>
<li class="">
<p><strong>institution:</strong> RV College of Engineering (RVCE), Bengaluru, India</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22326" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22326</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the integration of Global M2 Liquidity as a leading exogenous variable with a 12-week lag for Bitcoin price forecasting. 2. Proposes a liquidity-conditioned forecasting model (TimeXer-Exog) based on the TimeXer architecture. 3. Demonstrates that explicit macroeconomic conditioning significantly stabilizes and improves long-horizon forecasts, outperforming a univariate baseline by over 89% at a 70-day horizon.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38de8480bbb274bd2bcfff3317dfee4cd7813e42e3af4cc61efcd6d928a0ae36_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38de8480bbb274bd2bcfff3317dfee4cd7813e42e3af4cc61efcd6d928a0ae36_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of long-horizon Bitcoin price forecasting by proposing a model that integrates Global M2 Liquidity as an exogenous variable into the TimeXer transformer architecture. The proposed TimeXer-Exog model significantly outperforms univariate benchmarks, showing that conditioning on global macroeconomic factors substantially improves forecast stability and accuracy over long horizons.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [multi-view learning, variational autoencoder, latent representation learning, radiomics, glioblastoma]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mariya Miteva, Maria Nisheva-Pavlova</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in provided content.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22331" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22331</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a multi-view latent representation learning framework based on VAEs for integrating complementary MRI radiomic features. 2. Introduced independent probabilistic encoders for each modality to preserve modality-specific structure before fusion in a compact latent space. 3. Applied the learned latent embeddings for the non-invasive classification of MGMT promoter methylation status in glioblastoma.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bde275b3d90b700f19b613a2539cb5c16f7fb3637de09a820e24738011c2f8da_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bde275b3d90b700f19b613a2539cb5c16f7fb3637de09a820e24738011c2f8da_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of non-invasively predicting MGMT promoter methylation in glioblastoma from MRI scans. It proposes a multi-view framework using variational autoencoders to integrate features from T1Gd and FLAIR MRI sequences by fusing them in a latent space, aiming to better preserve modality-specific information. The resulting latent embeddings are used for classification, offering a potential improvement over conventional unimodal or early-fusion radiomics approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [evaluation &amp; benchmarking], [scientific intelligence, multimodal reasoning, benchmarking toolkit]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yiheng Wang, Yixin Chen, Shuo Li, Yifan Zhou, Bo Liu, Hengjian Gao, Jiakang Yuan, Jia Bu, Wanghan Xu, Yuhao Zhou, Xiangyu Zhao, Zhiwang Zhou, Fengxiang Wang, Haodong Duan, Songyang Zhang, Jun Yao, Han Deng, Yizhou Wang, Jiabei Xiao, Jiaqi Liu, Encheng Su, Yujie Liu, Weida Wang, Junchi Yao, Shenghe Zheng, Haoran Sun, Runmin Ma, Xiangchao Yan, Bo Zhang, Dongzhan Zhou, Shufei Zhang, Peng Ye, Xiaosong Wang, Shixiang Tang, Wenlong Zhang, Lei Bai</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Artificial Intelligence Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22334" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22334</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/InternScience/SciEvalKit" target="_blank" rel="noopener noreferrer" class="">https://github.com/InternScience/SciEvalKit</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces SciEvalKit, a unified, open-source toolkit for evaluating AI models across a broad range of scientific disciplines and core scientific intelligence capabilities. 2. Provides a flexible and extensible evaluation pipeline supporting batch evaluation, custom model/dataset integration, and ensuring transparent, reproducible results. 3. Curates expert-grade scientific benchmarks from real-world, domain-specific datasets to reflect authentic scientific challenges across six major domains.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/84233ab293826e87328abdd509857546d8a108ec2ff9c7ccc92d7c00c26ececa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/84233ab293826e87328abdd509857546d8a108ec2ff9c7ccc92d7c00c26ececa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across multiple disciplines and core competencies like multimodal reasoning and code generation. It provides a flexible, extensible pipeline for reproducible evaluation and is built on expert-grade, real-world scientific benchmarks. The toolkit is open-sourced to foster community-driven development in AI for science.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [symbolic world models, multi-agent feedback, PDDL, adaptive testing, supervised fine-tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mengkang Hu, Bowei Xia, Yuran Wu, Ailing Yu, Yude Zou, Qiguang Chen, Shijian Wang, Jiarui Jin, Kexin Li, Wenxiang Jiao, Yuan Lu, Ping Luo</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Hong Kong, Xiaohongshu Inc., UESTC, Harbin Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22336" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22336</a></p>
</li>
<li class="">
<p><strong>code:</strong> agent2world.github.io</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Agent2World, a tool-augmented multi-agent framework for generating symbolic world models via adaptive multi-agent feedback. 2. Introduced a three-stage pipeline with specialized agents (Deep Researcher, Model Developer, Testing Team) for knowledge synthesis, implementation, and behavior-aware validation. 3. Demonstrated that the framework not only achieves state-of-the-art inference-time performance but also serves as a data engine for supervised fine-tuning, leading to substantial model improvement.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3eb7640894bc37e231771de5c5b9dca9d3fe86f38d911d91d2cb55f73a1005c6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3eb7640894bc37e231771de5c5b9dca9d3fe86f38d911d91d2cb55f73a1005c6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of generating correct symbolic world models (like PDDL domains) from natural language by proposing Agent2World, a multi-agent framework that uses adaptive feedback for validation and repair. The method outperforms existing approaches on benchmarks and the feedback collected also enables effective supervised fine-tuning, significantly improving model performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [LoRA, catastrophic forgetting, KL divergence, instruction-tuning, parameter-efficient fine-tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Matthew Riemer, Erik Miehling, Miao Liu, Djallel Bouneffouf, Murray Campbell</p>
</li>
<li class="">
<p><strong>institution:</strong> IBM Research, Mila, Université de Montréal</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22337" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22337</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that catastrophic forgetting is a severe problem even during parameter-efficient fine-tuning (LoRA) of LLMs on small datasets. 2. Proposes a simple, low-overhead regularized approximate replay method that penalizes KL divergence from the initial model and interleaves next-token prediction data. 3. Shows that this method effectively preserves the model&#x27;s general knowledge while maintaining plasticity for new tasks, applied to Qwen models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2e24fadff03a1d6696f3147893642a90d9f500d5c68233669842e5792db7332_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2e24fadff03a1d6696f3147893642a90d9f500d5c68233669842e5792db7332_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies that catastrophic forgetting is a major issue during LoRA-based supervised fine-tuning of large language models, even with small datasets. To solve this, the authors propose a regularized approximate replay method that uses KL divergence regularization and interleaves general pre-training-like data. Their approach successfully preserves the model&#x27;s original capabilities while allowing adaptation to new instructions, with minimal computational overhead.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D scene understanding and manipulation], [Multimodal Large Language Models (MLLMs), 3D object arrangement, tool-augmented agents, MCP-based API, multi-agent framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhengfei Kuang, Rui Lin, Long Zhao, Gordon Wetzstein, Saining Xie, Sanghyun Woo</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University, Google, Google DeepMind, New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22351" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22351</a></p>
</li>
<li class="">
<p><strong>code:</strong> vulcan-3d.github.io</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced an MCP-based API to shift interaction from raw code to robust function-level updates, addressing MLLMs&#x27; weak visual grounding in 3D. 2. Augmented MLLMs with specialized visual tools for scene analysis, spatial information gathering, and action validation, creating a perceptual feedback loop. 3. Proposed a collaborative multi-agent framework with designated planning, execution, and verification roles to manage iterative, error-prone updates in complex tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79364171e71fa2e99be3aaae7f42a6f8bb502acdf59cc5033e98f9a1d424f8bb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79364171e71fa2e99be3aaae7f42a6f8bb502acdf59cc5033e98f9a1d424f8bb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the underexplored challenge of applying Multimodal Large Language Models (MLLMs) to complex 3D object arrangement. The proposed VULCAN system uses an MCP-based API, a suite of visual tools, and a multi-agent framework to enable robust, iterative 3D scene manipulation. The approach significantly outperforms baselines on a diverse set of 25 complex arrangement tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [vision transformer, whole slide image, HER2 scoring, multi-modality, tumor classification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Olaide N. Oyelade, Oliver Hoxey, Yulia Humrye</p>
</li>
<li class="">
<p><strong>institution:</strong> North Carolina A&amp;T State University, University of Chichester, Yale University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22335" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22335</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a novel mapping function to correlate malignant regions in H&amp;E whole slide images with corresponding regions in IHC images for joint analysis. 2. Developed an end-to-end pipeline using a multi-stage vision transformer system for automatic pixel-level annotation of 4-way HER2 status scoring (0, 1+, 2+, 3+). 3. Embedded a clinically inspired HER2 scoring mechanism that accurately classifies HER2-negative and HER2-positive cases from whole slide images.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03d448dc8fb7520382bb6ea1a3e317817181ebbce215c0d5c387ed2268b2f407_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03d448dc8fb7520382bb6ea1a3e317817181ebbce215c0d5c387ed2268b2f407_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an end-to-end pipeline using multi-stage vision transformers to jointly analyze H&amp;E and IHC whole slide images for HER2 status scoring and tumor classification. The method introduces a novel mapping function to align modalities and provides pixel-level HER2 scoring. The results demonstrate high accuracy (0.94) for HER2 status prediction, showing the method&#x27;s effectiveness comparable to human pathologists.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [pseudo-colouring, few-shot learning, prototypical networks, ResNet-18, explainability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alaa Alahmadi, Mohamed Hasan</p>
</li>
<li class="">
<p><strong>institution:</strong> Newcastle University, University of Leeds</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22349" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22349</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a perception-informed pseudo-colouring technique to encode clinically salient temporal ECG features (like QT-interval) into structured colour representations. 2. Demonstrates that this technique enables effective few-shot and one-shot learning for a complex physiological data task (drug-induced LQTS) using prototypical networks and ResNet-18. 3. Shows that the method improves model explainability by guiding attention to clinically meaningful features and that aggregating multiple cardiac cycles (mirroring human perceptual averaging) further boosts performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problems of data inefficiency and poor interpretability in deep learning models for physiological signal analysis. It proposes a human-inspired pseudo-colouring technique to encode ECG features, enabling effective few-shot learning and improving model explainability by focusing on clinically relevant signal components. The results demonstrate that incorporating human-like perceptual encoding can bridge data efficiency and interpretability in medical AI.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [Text-to-SQL, Cloud Cost Optimization, Query Efficiency, Large Language Models, Google BigQuery]</p>
</li>
<li class="">
<p><strong>authors:</strong> Saurabh Deochake, Debajyoti Mukhopadhyay</p>
</li>
<li class="">
<p><strong>institution:</strong> SentinelOne, WIDiCoReL Research Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22364" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22364</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a cloud-native cost evaluation methodology for Text-to-SQL systems, measuring bytes processed, slot utilization, and estimated query cost on production infrastructure. 2. Conducted an empirical evaluation of six LLMs on Google BigQuery, demonstrating that reasoning models achieve significantly lower cloud compute costs while maintaining high correctness. 3. Quantified cost variance across models, identified prevalent inefficiency patterns (e.g., missing partition filters), and provided deployment guidelines for cost-sensitive environments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06f17566f5fb65cb73b79b0dbb64bde11c2f87d177f02865af7fc2d8910e3ac4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06f17566f5fb65cb73b79b0dbb64bde11c2f87d177f02865af7fc2d8910e3ac4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the cloud compute costs of SQL queries generated by Large Language Models (LLMs) for Text-to-SQL tasks. By evaluating six state-of-the-art LLMs on Google BigQuery, it finds that reasoning models are more cost-efficient, processing far fewer bytes, and that execution time is a poor proxy for cloud cost. The work provides a new cost-focused evaluation methodology and guidelines for deploying cost-aware Text-to-SQL systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [automated planning], [numeric planning, control parameters, subgoaling heuristics, optimistic compilation, infinite action space]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ángel Aso-Mollar, Diego Aineto, Enrico Scala, Eva Onaindia</p>
</li>
<li class="">
<p><strong>institution:</strong> Universitat Politècnica de València, Università degli Studi di Brescia</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22367" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22367</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies a tractable subset of numeric planning problems with infinite actions (controllable, simple numeric problems)., 2. Proposes an optimistic compilation approach that transforms these problems into standard simple numeric tasks by abstracting control-dependent expressions., 3. Enables the effective use of traditional subgoaling heuristics for goal distance estimation in this challenging setting, pushing the state of the art.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abc3c2ae44e3bb664061095be1d4f9beb835e627f31f49a8bfccf9fd7df412ea_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abc3c2ae44e3bb664061095be1d4f9beb835e627f31f49a8bfccf9fd7df412ea_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of applying standard numeric heuristics in planning problems with an infinite number of actions due to control parameters. It proposes an optimistic compilation method that transforms a tractable subset of these problems into simpler numeric tasks, enabling the use of subgoaling heuristics. The results show this approach is effective and computationally feasible for handling infinite action spaces.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Self-Evaluation Unlocks Any-Step Text-to-Image Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [text-to-image generation, flow matching, self-evaluation, any-step inference, from-scratch training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xin Yu, Xiaojuan Qi, Zhengqi Li, Kai Zhang, Richard Zhang, Zhe Lin, Eli Shechtman, Tianyu Wang, Yotam Nitzan</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Hong Kong, Adobe Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22374" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22374</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Self-Evaluating Model (Self-E), a novel from-scratch training framework that combines local flow matching with a self-evaluation mechanism, eliminating the need for a pretrained teacher model. 2. Enables &quot;any-step&quot; inference, allowing the same model to perform both high-quality few-step and many-step generation, bridging the gap between local supervision and global matching paradigms. 3. Demonstrates competitive performance with state-of-the-art flow matching models at high step counts while excelling at very low step counts, offering a unified and scalable solution.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that traditional diffusion/flow models require many inference steps, and distillation methods need a pretrained teacher. It proposes Self-E, a model trained from scratch that uses self-evaluation as a dynamic teacher to enable high-quality generation at any number of steps. The results show Self-E excels at few-step generation and is competitive at many steps, providing a unified framework for efficient and scalable text-to-image synthesis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Towards Efficient Post-Training via Fourier-Driven Adapter Architectures</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [parameter-efficient fine-tuning], [Fourier-Activated Adapter, random Fourier features, frequency-aware activation, parameter-efficient fine-tuning, spectral sparsity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Donggyun Bae, Jongil Park</p>
</li>
<li class="">
<p><strong>institution:</strong> Konkuk University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22378" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22378</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the Fourier-Activated Adapter (FAA), a novel PEFT framework that integrates random Fourier features to decompose representations into frequency components. 2. Introduces a dynamic, frequency-aware activation mechanism to selectively modulate semantic information across different frequency bands. 3. Demonstrates through extensive experiments that FAA achieves competitive or superior performance on multiple benchmarks while maintaining low computational overhead.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cf813cee09035fa7f545005f9b12789221e4e00ecb3d551020cff824fb62233_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cf813cee09035fa7f545005f9b12789221e4e00ecb3d551020cff824fb62233_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes the Fourier-Activated Adapter (FAA), a parameter-efficient fine-tuning method for large language models that uses random Fourier features to enable frequency-aware modulation of semantic representations. Experiments on GLUE and other benchmarks show that FAA achieves strong performance with low computational cost, highlighting the effectiveness of its frequency-based approach.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [reproducibility, dependency management, code generation, large language models, empirical study]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bhanu Prakash Vangala, Ali Adibifar, Tanu Malik, Ashish Gehani</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Missouri, SRI International</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22387" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22387</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a three-layer dependency framework (claimed, working, runtime) to quantify the execution reproducibility of LLM-generated code. 2. Conducts an empirical study evaluating three state-of-the-art LLM coding agents across 300 projects in three programming languages, revealing low out-of-the-box execution success rates. 3. Discovers a significant hidden dependency problem, with an average 13.5x expansion from declared to actual runtime dependencies.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3f3a69233ca5eacf2fea5882b11aeb102413519f3be78440b3532150966328b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3f3a69233ca5eacf2fea5882b11aeb102413519f3be78440b3532150966328b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the reproducibility of code generated by LLM-based coding agents. It proposes a three-layer dependency framework and conducts an empirical study on 300 projects, finding that only 68.3% execute successfully out-of-the-box and that actual runtime dependencies are significantly larger than declared ones. The study concludes that AI-generated code currently suffers from major reproducibility issues due to dependency gaps and code generation errors.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [hyperparameter transfer, Complete(d)P parameterisation, per-module hyperparameter optimisation, scaling laws, evolutionary strategy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bruno Mlodozeniec, Pierre Ablin, Louis Béthune, Dan Busbridge, Michal Klein, Jason Ramapuram, Marco Cuturi</p>
</li>
<li class="">
<p><strong>institution:</strong> Apple, University of Cambridge</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22382" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22382</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the Complete(d)P parameterisation, a unified framework for scaling hyperparameters across model width, depth, batch size, and training duration. 2. Investigates and enables the transfer of per-module hyperparameters (e.g., learning rates, weight decay) across model scales, moving beyond global hyperparameter transfer. 3. Provides practical guidelines for navigating the high-dimensional per-module hyperparameter optimization landscape and demonstrates significant training speed improvements in Large Language Models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06e0593eb453191fce60eeebdd4eb6147ab462084db9eb8d81ea8e2486d468be_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06e0593eb453191fce60eeebdd4eb6147ab462084db9eb8d81ea8e2486d468be_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of hyperparameter transfer across different model scales and configurations. It introduces the Complete(d)P parameterisation to unify scaling across width, depth, batch size, and duration, and demonstrates that with this method, even granular per-module hyperparameters can be optimized on a small model and successfully transferred to much larger models, leading to faster training and improved performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [few-shot learning], [exemplar selection, large language model, human activity recognition, facility-location optimization, PageRank]</p>
</li>
<li class="">
<p><strong>authors:</strong> Elsen Ronando, Sozo Inoue</p>
</li>
<li class="">
<p><strong>institution:</strong> Kyushu Institute of Technology, Universitas 17 Agustus 1945 Surabaya</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22385" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22385</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an LLM-Guided Exemplar Selection framework that incorporates semantic reasoning via LLM-generated knowledge priors (feature importance, inter-class confusability, budget multipliers) for HAR. 2. Integrates these semantic priors with multiple geometric and structural cues (margin-based validation, PageRank centrality, hubness penalization, facility-location optimization) for a unified exemplar scoring and selection process. 3. Demonstrates superior performance (88.78% macro F1-score on UCI-HAR) under strict few-shot conditions compared to classical selection methods like random sampling, herding, and k-center.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90a149428a6ff84d38e1ab6a741982394b05c9d5b98eaaa538dcd12183dbe7bf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90a149428a6ff84d38e1ab6a741982394b05c9d5b98eaaa538dcd12183dbe7bf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of relying on large labeled datasets and purely geometric exemplar selection in Human Activity Recognition (HAR) by proposing an LLM-Guided Exemplar Selection framework. The method uses an LLM to generate semantic knowledge priors, which are combined with structural and geometric cues to select a compact, informative set of exemplars for few-shot learning. Evaluated on the UCI-HAR dataset, the framework outperforms classical selection approaches, showing that integrating semantic reasoning improves representative exemplar selection for wearable-sensor HAR.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [hallucination detection], [hallucination detection, retrieval-augmented verification, contradiction graph, Paraphrased Hallucination Consistency Score (PHCS), materials science]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bhanu Prakash Vangala, Sajid Mahmud, Pawan Neupane, Joel Selvaraj, Jianlin Cheng</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Missouri</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22396" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22396</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces HalluMatData, a benchmark dataset for evaluating hallucination detection in AI-generated materials science content. 2. Proposes HalluMatDetector, a multi-stage hallucination detection framework integrating intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment. 3. Introduces the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2d61ec50b266277e33c913c31df1946cc27990dbacfbd8d5b4979a627f3fa00_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2d61ec50b266277e33c913c31df1946cc27990dbacfbd8d5b4979a627f3fa00_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of factual hallucinations in LLM-generated materials science content. It proposes HalluMatDetector, a multi-stage verification framework that combines intrinsic checks, retrieval, and contradiction analysis to detect and mitigate errors. The method reduces hallucination rates by 30% compared to standard LLM outputs and introduces a new metric (PHCS) for evaluating response consistency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [knowledge graph embeddings, inference-time personalization, parameter-efficient adaptation, structure-gated adaptation, frozen backbone]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ozan Oguztuzun, Cerag Oguztuzun</p>
</li>
<li class="">
<p><strong>institution:</strong> Case Western Reserve University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22398" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22398</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A post-hoc personalization mechanism that operates at inference time on frozen KG embeddings without backbone updates. 2. A structure-gated adaptation method that conditions candidate rankings on profile features via graph-derived gates. 3. New evaluation metrics for personalization (Alignment@k and Counterfactual Responsiveness) to quantify alignment and causal responsiveness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b860d25f74e154dce6d1b1a346b065fe4b4e238a600b81e91ad6a825aa744e1c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b860d25f74e154dce6d1b1a346b065fe4b4e238a600b81e91ad6a825aa744e1c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that foundation models for knowledge graphs perform well for groups but fail to capture individual user preferences. It proposes GatedBias, a lightweight framework that adds interpretable, per-entity biases to frozen KG embeddings at inference time using profile features and graph-derived gates, requiring only ~300 parameters. The method significantly improves personalized ranking alignment on benchmark datasets while preserving global accuracy, demonstrating that parameter-efficient and causally verifiable personalization is possible.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] BLISS: Bandit Layer Importance Sampling Strategy for Efficient Training of Graph Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [Graph Neural Networks, Multi-armed Bandits, Layer-wise Sampling, Node Importance, Efficient Training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Omar Alsaqa, Linh Thi Hoang, Muhammed Fatih Balin</p>
</li>
<li class="">
<p><strong>institution:</strong> Wilfrid Laurier University, Singapore Management University, Georgia Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22388" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22388</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces BLISS, a novel adaptive sampling strategy using multi-armed bandits to dynamically select informative nodes at each GNN layer. 2. Balances exploration and exploitation to ensure comprehensive graph coverage and adapts to evolving node importance, unlike static methods. 3. Demonstrates versatility by integrating with different GNN architectures (GCNs and GATs) and maintaining or exceeding full-batch training accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1be93ab34a4af58594bc48c8d52cc9f7177c298fc314982c8ac7ae27b2086b70_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1be93ab34a4af58594bc48c8d52cc9f7177c298fc314982c8ac7ae27b2086b70_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the computational bottleneck in training Graph Neural Networks (GNNs) on large graphs by proposing BLISS, a Bandit Layer Importance Sampling Strategy. BLISS uses multi-armed bandits to dynamically and adaptively sample the most informative nodes at each layer. Experiments show that this method maintains or even surpasses the accuracy of full-batch training while being more efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Efficient Multi-Model Orchestration for Self-Hosted Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [Kubernetes, Helm, DistilBERT, scale-to-zero, hybrid routing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bhanu Prakash Vangala, Tanu Malik</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Missouri</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22402" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22402</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A unified Helm-based deployment system for self-hosted LLMs on Kubernetes, 2. An adaptive scale-to-zero automation mechanism for efficient GPU resource utilization, 3. A hybrid routing module combining keyword heuristics and a lightweight DistilBERT classifier to balance cost, latency, and accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e9270400ac5f7fbf1ac4048cb82d9527c762106e232f9cd97653eb0ab3bdb4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e9270400ac5f7fbf1ac4048cb82d9527c762106e232f9cd97653eb0ab3bdb4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces &quot;Pick and Spin,&quot; a framework for efficient orchestration of self-hosted large language models. It addresses challenges in GPU utilization and workload routing by integrating Kubernetes-based deployment, adaptive scaling, and a hybrid routing strategy. The system demonstrates significant improvements in success rate, latency, and cost compared to static deployments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [speculative decoding, dynamic adaptation, multi-armed bandit, throughput optimization, latency reduction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rui Li, Zhaoning Zhang, Libo Zhang, Huaimin Wang, Xiang Fu, Zhiquan Lai</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Defense Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22420" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22420</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies the critical trade-off in speculative decoding: beneficial in memory-bound (low-load) scenarios but detrimental in compute-bound (high-load) scenarios due to verification overhead. 2. Proposes Nightjar, a novel learning-based algorithm that dynamically adapts the speculative length (or disables SD) based on real-time request load and batch size. 3. Demonstrates significant performance gains, achieving up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c6466394cd16e760ca78e05f13eba9852a284e7e8231b58de2c71fbee1e7b39_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c6466394cd16e760ca78e05f13eba9852a284e7e8231b58de2c71fbee1e7b39_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the inefficiency of fixed-length speculative decoding in LLM serving, which fails to adapt to dynamic request loads. It proposes Nightjar, a learning-based algorithm that dynamically selects the optimal speculative length. Experiments show Nightjar significantly improves throughput and reduces latency compared to standard speculative decoding.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [Heterogeneous Computing, ROS 2, FreeRTOS, PID Control, AWS IoT]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amro Gamar, Ahmed Abduljalil, Alargam Mohammed, Ali Elhenidy, Abeer Tawakol</p>
</li>
<li class="">
<p><strong>institution:</strong> Mansoura University, Egypt</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22408" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22408</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a heterogeneous computing architecture combining a Raspberry Pi 5 with ROS 2 for high-level AI perception/path planning and an ESP32 with FreeRTOS for real-time motor control. 2. Implemented a low-latency, reliable communication link between the ROS 2 host and the embedded controller to ensure system coordination. 3. Enhanced system reliability through deterministic PID-based motor control with static memory allocation and integrated AWS IoT monitoring with a firmware-level motor shutdown failsafe.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c49a4266e78db3945d26829ffd5e030ccc9fa68be1c543cca653fc81df446dfe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c49a4266e78db3945d26829ffd5e030ccc9fa68be1c543cca653fc81df446dfe_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents the development of an autonomous delivery robot using a unified, multi-disciplinary approach. It employs a heterogeneous computing architecture to handle AI-based navigation on a Raspberry Pi and real-time motor control on an ESP32, addressing challenges like algorithm optimization and inter-processor communication. The result is a robust, operational system demonstrated to be capable of real-world deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Emergence of Human to Robot Transfer in Vision-Language-Action Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robot learning], [vision-language-action models, human-to-robot transfer, co-training, emergent capability, embodiment-agnostic representations]</p>
</li>
<li class="">
<p><strong>authors:</strong> Simar Kareer, Karl Pertsch, James Darpinian, Judy Hoffman, Danfei Xu, Sergey Levine, Chelsea Finn, Suraj Nair</p>
</li>
<li class="">
<p><strong>institution:</strong> Physical Intelligence, Georgia Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22414" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22414</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a simple co-training recipe for training Vision-Language-Action (VLA) models on a mix of human video and robot data. 2. Discovers and demonstrates that the ability to transfer skills from human videos to robot policies is an emergent property that appears with sufficient scale and diversity in robot pre-training data. 3. Provides analysis suggesting the emergent capability arises from the model learning embodiment-agnostic representations through diverse pre-training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c14383fd88b5d16af8c13ba59202c2143ecd1d25b65a10248ec614491595a50_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c14383fd88b5d16af8c13ba59202c2143ecd1d25b65a10248ec614491595a50_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether Vision-Language-Action (VLA) models can learn to transfer skills from human videos to robots, a task that is typically challenging. The authors propose a simple co-training method and find that this human-to-robot transfer capability emerges as a property of scale when the model is pre-trained on a sufficiently large and diverse dataset of robot tasks. Their experiments show that with diverse pre-training, leveraging human data can nearly double performance on tasks seen only in human videos.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [hyperspherical learning, native sparse attention, anisotropic patch embed]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amil Khan, Matheus Palhares Viana, Suraj Mishra, B.S. Manjunath</p>
</li>
<li class="">
<p><strong>institution:</strong> UC Santa Barbara, Allen Institute for Cell Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22423" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22423</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A 4B-parameter foundation model (Bright-4B) that learns on the unit hypersphere for segmenting subcellular structures directly from 3D brightfield volumes. 2. Novel architectural components: a hardware-aligned Native Sparse Attention mechanism, depth-width residual HyperConnections, and a soft Mixture-of-Experts for adaptive capacity. 3. A plug-and-play anisotropic patch embed that respects confocal point-spread and axial thinning for geometry-faithful 3D tokenization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces Bright-4B, a 4-billion parameter foundation model designed for volumetric segmentation of subcellular structures directly from label-free 3D brightfield microscopy images. It employs novel architectural components like hyperspherical learning, native sparse attention, and an anisotropic patch embed to handle 3D context and anisotropic sampling. The model outperforms contemporary baselines in preserving fine structural detail across depth and cell types without requiring fluorescence or heavy post-processing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [transformer, fluence map prediction, physics-informed loss, two-stage regression, Swin UNETR]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ujunwa Mgboh, Rafi Ibn Sultan, Joshua Kim, Kundan Thind, Dongxiao Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> Wayne State University, Henry Ford Health</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22425" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22425</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed FluenceFormer, a backbone-agnostic transformer framework for direct, geometry-aware fluence map regression. 2. Introduced a unified two-stage design (dose prior prediction followed by geometry-conditioned fluence regression) and the physics-informed Fluence-Aware Regression (FAR) loss. 3. Demonstrated the framework&#x27;s generality across multiple transformer backbones and achieved state-of-the-art performance, significantly reducing energy error.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d0e329d1bfd4c1f892f7333af6a3a4e76c5219cd192f715a25e502a35ef178_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d0e329d1bfd4c1f892f7333af6a3a4e76c5219cd192f715a25e502a35ef178_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces FluenceFormer, a transformer-based framework for automating radiotherapy planning by predicting multi-beam fluence maps. The method uses a two-stage, geometry-aware regression approach with a novel physics-informed loss function. The results show that FluenceFormer outperforms existing methods, achieving a low energy error and improved structural fidelity in fluence map prediction.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [point cloud processing], [Graph Attention Networks, LiDAR reconstruction, beam dropout, gated residual fusion, sparse point cloud]</p>
</li>
<li class="">
<p><strong>authors:</strong> Khalfalla Awedat, Mohamed Abidalrekab, Gurcan Comert, Mustafa Ayad</p>
</li>
<li class="">
<p><strong>institution:</strong> SUNY Morrisville College, Portland State University, North Carolina A&amp;T State University, SUNY Oswego</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22439" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22439</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SuperiorGAT, a novel graph attention-based framework for reconstructing missing elevation data in sparse LiDAR point clouds. 2. Introduces a beam-aware graph modeling approach for LiDAR scans combined with gated residual fusion and feed-forward refinement to achieve accurate reconstruction without increasing network depth. 3. Demonstrates superior performance in reconstruction error and geometric consistency across diverse environments compared to PointNet and deeper GAT baselines, validated through structured beam dropout simulation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68e019420f70e8da99107deedb1af90b7e95ec95b9487f8fc6c872f9994d15e1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68e019420f70e8da99107deedb1af90b7e95ec95b9487f8fc6c872f9994d15e1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of LiDAR beam dropout and sparse resolution in autonomous systems by proposing SuperiorGAT, a graph attention network framework that reconstructs missing elevation information. The method models LiDAR scans as beam-aware graphs and uses gated residual fusion for accurate reconstruction without deeper networks. The results show it achieves lower error and better geometric consistency than baselines, offering a computationally efficient way to improve LiDAR resolution.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Monadic Context Engineering</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [Monadic Context Engineering, Monad Transformers, Meta-Agents, computational contexts, algebraic structures]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yifan Zhang, Mengdi Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Princeton University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22431" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22431</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/yifanzhang-pro/monadic-context-engineering" target="_blank" rel="noopener noreferrer" class="">https://github.com/yifanzhang-pro/monadic-context-engineering</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Monadic Context Engineering (MCE), a novel architectural paradigm using Functors, Applicatives, and Monads to provide a formal foundation for AI agent design. 2. Demonstrates how Monads and Applicatives manage sequential composition and parallel execution, and how Monad Transformers enable systematic composition of capabilities like state and error handling. 3. Extends the MCE framework to describe Meta-Agents for generative orchestration, dynamically creating and managing sub-agent workflows via metaprogramming.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/635ff6dca4b79fe5e98a96641cbb26356935e3090aa65b20972b744e69151810_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/635ff6dca4b79fe5e98a96641cbb26356935e3090aa65b20972b744e69151810_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the brittleness and complexity in current AI agent architectures by introducing Monadic Context Engineering (MCE), a paradigm that leverages algebraic structures like Monads to formally manage state, errors, and concurrency within agent workflows. The proposed method enables the construction of complex, resilient agents from simple, verifiable components and is extended to support generative orchestration via Meta-Agents. The work concludes that MCE provides a principled foundation for building robust and scalable autonomous agent systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [hierarchical filtering, two-pass generation, citation verification, query formulation, model cascade]</p>
</li>
<li class="">
<p><strong>authors:</strong> Cattalyya Nuengsigkapian</p>
</li>
<li class="">
<p><strong>institution:</strong> Google</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22442" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22442</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hierarchical content filtering pipeline to replace standard vector similarity search, improving context precision. 2. Introduces a model cascade strategy using a cost-efficient model (Gemini 2.5 Flash) for filtering and a powerful model (Gemini 2.5 Pro) for final generation. 3. Demonstrates significant performance gains on the MMU-RAGent benchmark and a custom dataset for post-cutoff knowledge.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9840d615edee0e72c18b93838479ebb342195ea1805803858fb9effaf9ba2e95_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9840d615edee0e72c18b93838479ebb342195ea1805803858fb9effaf9ba2e95_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents HiFi-RAG, a system designed to improve open-domain RAG by addressing irrelevant retrieved information. The method uses a multi-stage pipeline with hierarchical filtering and a two-pass generation strategy employing different LLMs for efficiency and quality. The system won a NeurIPS 2025 competition and showed substantial improvements over baselines in evaluation metrics.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [optical-SAR fusion, missing modality, quality-aware fusion, dynamic fusion, orthogonal constraint]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhicheng Zhao, Yuancheng Xu, Andong Lu, Chenglong Li, Jin Tang</p>
</li>
<li class="">
<p><strong>institution:</strong> Anhui University, China Electronics Technology Group Corporation (38th Research Institute)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22447" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22447</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection under missing or degraded modalities. 2. Designed a Dynamic Modality Quality Assessment (DMQA) module that uses learnable reference tokens to iteratively assess feature reliability and identify degraded regions. 3. Developed an Orthogonal Constraint Normalization Fusion (OCNF) module that uses orthogonal constraints to preserve modality independence and dynamically adjust fusion weights based on reliability scores to suppress unreliable features.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69b5db28a2b2a43b3d55d1592c7f26e5ce4421dd707ae3e19282c69c4e894e50_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69b5db28a2b2a43b3d55d1592c7f26e5ce4421dd707ae3e19282c69c4e894e50_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of robust object detection using optical and SAR images when one modality is missing or degraded. The proposed QDFNet method dynamically assesses feature quality and adaptively fuses information using learnable tokens and orthogonal constraints. Experiments show it outperforms other methods, especially when modalities are partially missing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AMBIT: Augmenting Mobility Baselines with Interpretable Trees</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [urban computing, spatial data science], [origin-destination flow prediction, spatial interaction models, gradient-boosted trees, SHAP analysis, gray-box model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qizhi Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> PingCAP, Data &amp; AI-Innovation Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22466" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22466</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducts a comprehensive audit of classical spatial interaction models on high-resolution mobility data, identifying PPML gravity as the strongest physical baseline. 2. Proposes AMBIT, a gray-box framework that augments interpretable physical baselines with gradient-boosted trees to learn residuals, balancing accuracy and interpretability. 3. Demonstrates that physics-grounded and POI-anchored residual learners achieve competitive accuracy and robust spatial generalization, providing a reproducible pipeline with diagnostics for urban decision-making.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d0d9ab13b8b2f60e318c90f38821b29e87e0bdd9bf0d9bc963b48c5ac7c2759_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d0d9ab13b8b2f60e318c90f38821b29e87e0bdd9bf0d9bc963b48c5ac7c2759_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes AMBIT, a gray-box framework that combines interpretable physical mobility baselines with gradient-boosted trees to predict origin-destination flows. It shows that learning residuals on top of physics-based models can achieve accuracy close to strong black-box predictors while maintaining interpretable structure, with POI-anchored residuals being particularly robust for spatial generalization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Bayesian Geometry of Transformer Attention</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [interpretability], [Bayesian inference, transformer attention, mechanistic interpretability, Bayesian wind tunnels, geometric analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra</p>
</li>
<li class="">
<p><strong>institution:</strong> Columbia University, Dream Sports, Google DeepMind</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22471" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22471</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces &quot;Bayesian wind tunnels&quot; as controlled environments to rigorously test if transformers perform Bayesian inference, where true posteriors are known and memorization is impossible. 2. Demonstrates that small transformers implement Bayesian inference via a consistent geometric mechanism (residual streams as belief substrate, FFNs for updates, attention for routing), while MLPs fail. 3. Identifies specific geometric diagnostics (orthogonal key bases, query-key alignment, low-dimensional value manifold) and a frame-precision dissociation during training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5333e070f21cfd2abea4d13cddb84bf6d9f3c3124c93bf9142879f24f1e739b1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5333e070f21cfd2abea4d13cddb84bf6d9f3c3124c93bf9142879f24f1e739b1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether transformers perform genuine Bayesian inference. To test this, the authors create controlled &quot;Bayesian wind tunnel&quot; tasks with known posteriors and show that small transformers accurately reproduce Bayesian posteriors via a specific geometric mechanism, while MLPs fail, establishing attention as crucial for this capability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [ai safety &amp; alignment], [manipulation detection, safety benchmark, harm categorization, multi-layer analysis, autonomy harm]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sadia Asif, Israel Antonio Rosales Laguan, Haris Khan, Shumaila Asif, Muneeb Asif</p>
</li>
<li class="">
<p><strong>institution:</strong> Rensselaer Polytechnic Institute, National University of Sciences and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22470" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22470</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/sadia-sigma-lab/Benchmark-dataset-for-dark-patterns-in-llms" target="_blank" rel="noopener noreferrer" class="">https://github.com/sadia-sigma-lab/Benchmark-dataset-for-dark-patterns-in-llms</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces DarkPatterns-LLM, the first comprehensive benchmark dataset with 401 expert-annotated examples for fine-grained detection of manipulative LLM behaviors across seven harm categories. 2. Proposes a novel four-layer diagnostic framework (MGD, MSIAN, THP, DCRA) for nuanced analysis of manipulative content, moving beyond coarse binary safety labels. 3. Provides an empirical evaluation revealing significant performance disparities (65.2%-89.7%) among state-of-the-art LLMs and identifies consistent weaknesses, particularly in detecting autonomy-undermining patterns.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b68b7543a951540a0b9d1fde4bdde15299ee5f4f63a4526b64cdf4ab7f7a4c28_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b68b7543a951540a0b9d1fde4bdde15299ee5f4f63a4526b64cdf4ab7f7a4c28_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of nuanced benchmarks for detecting manipulative behaviors in Large Language Models (LLMs). It introduces DarkPatterns-LLM, a new dataset and a four-layer analytical framework for fine-grained assessment across multiple harm categories. The evaluation shows current LLMs have significant and varied weaknesses in manipulation detection, establishing a standardized benchmark for developing more trustworthy AI.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding for Fine-Grained sEMG-Based Movement Decoding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [biomedical signal processing], [self-supervised learning, surface electromyography, rotary position encoding, spectral pre-training, movement decoding]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zihan Weng, Chanlin Yi, Pouya Bashivan, Jing Lu, Fali Li, Dezhong Yao, Jingming Hou, Yangsong Zhang, Peng Xu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22481" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22481</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel self-supervised pre-training task that uses masked prediction of clustered STFT pseudo-labels to learn robust, physiologically relevant frequency patterns from sEMG signals. 2. A novel Cylindrical Rotary Position Embedding (CyRoPE) that factorizes embeddings along temporal and annular spatial dimensions to explicitly model the cylindrical topology of forearm electrode arrays. 3. The SPECTRE framework, which integrates these contributions to establish a new state-of-the-art for fine-grained movement decoding, validated on multiple datasets including data from individuals with amputation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5801cbdcbac93bf7df15e18531c5ff2653259e25003568da5b53b240d06d32c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5801cbdcbac93bf7df15e18531c5ff2653259e25003568da5b53b240d06d32c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces SPECTRE, a domain-specific self-supervised learning framework for decoding fine-grained movements from surface electromyography (sEMG) signals. It proposes a spectral pre-training task using masked pseudo-label prediction and a novel cylindrical rotary position encoding to model sensor topology. Evaluations show SPECTRE significantly outperforms existing supervised and generic self-supervised baselines, providing a robust foundation for practical myoelectric interfaces.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Hierarchical Pedagogical Oversight: A Multi-Agent Adversarial Framework for Reliable AI Tutoring</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [adversarial reasoning, multi-agent system, pedagogical oversight, hierarchical framework, low-compute inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Saisab Sadhu, Ashim Dhor</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Science Education and Research Bhopal</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22496" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22496</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Hierarchical Pedagogical Oversight (HPO), a novel multi-agent adversarial framework designed to improve the reliability of AI tutoring by separating pedagogical generation from evaluation. 2. Adapts structured adversarial synthesis to educational assessment, enforcing a dialectical debate between opposing pedagogical critics to mitigate sycophancy and superficial consensus. 3. Demonstrates that the adversarial protocol enables a small 8B-parameter model to outperform GPT-4o on pedagogical oversight while using significantly fewer computational resources.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67b6c2853ea8b40662f9788f9062b5488d7ec541a754a445a856888b9fb9300c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67b6c2853ea8b40662f9788f9062b5488d7ec541a754a445a856888b9fb9300c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of unreliable AI tutors (LLMs) that often validate incorrect student answers. It proposes the Hierarchical Pedagogical Oversight (HPO) framework, which uses a structured multi-agent adversarial debate to assess tutoring quality. The main conclusion is that this adversarial approach enables a much smaller model to outperform a much larger one (GPT-4o) on a pedagogical reasoning benchmark, establishing it as a critical mechanism for reliable, low-compute oversight.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ManchuTTS: Towards High-Quality Manchu Speech Synthesis via Flow Matching and Hierarchical Text Representation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [speech synthesis], [flow matching, hierarchical attention, low-resource TTS, agglutinative language, non-autoregressive generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Suhua Wang, Zifan Wang, Xiaoxin Sun, D. J. Wang, Zhanbo Liu, Xin Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Northeast Normal University, Changchun Humanities and Sciences College, Zhejiang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22491" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22491</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel hierarchical text representation and cross-modal attention mechanism to handle Manchu&#x27;s agglutinative phonology. 2. Introduces an end-to-end speech synthesis model integrating deep convolutional networks with a flow-matching Transformer for efficient, non-autoregressive generation. 3. Constructs the first public Manchu TTS dataset and employs data augmentation to address severe data scarcity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/118d24570c94614dbb94eaabcc1dc47ba64643f094c4ea3727d4674221bf53fc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/118d24570c94614dbb94eaabcc1dc47ba64643f094c4ea3727d4674221bf53fc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes ManchuTTS, a novel text-to-speech system designed for the endangered and agglutinative Manchu language. The method uses a three-tier text representation and a flow-matching Transformer with hierarchical guidance to tackle data scarcity and complex phonology. Experiments show it achieves a high MOS score of 4.52 and significantly improves pronunciation accuracy and prosodic naturalness compared to baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Role-Based Fault Tolerance System for LLM RL Post-Training</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [fault-tolerance], [role-based fault tolerance, RL post-training, UCX communication, warm standby, Effective Training Time Ratio (ETTR)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhenqian Chen, Baoquan Zhong, Xiang Li, Qing Dai, Xinkui Zhao, Miao Ye, Ren Cheng, Lufei Zhang, Jianwei Yin</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, State Key Laboratory of Mathematical Engineering and Advanced Computing</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22492" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22492</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a role-based fault isolation and recovery system (RobustRL) for RL post-training, enabling recovery of only the failed component (trainer, rollout) instead of restarting the entire task. 2. Introduces a role-aware monitoring mechanism to accurately detect failures and avoid false positives/delays specific to different RL roles. 3. Implements dynamic, UCX-based point-to-point communication to reconnect recovered roles and synchronize weights immediately, replacing static collective communication.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceff77cfb9d0c7d7e763916b77716ee6534c3aa3d54d41253fef6ea4d9a86ec0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceff77cfb9d0c7d7e763916b77716ee6534c3aa3d54d41253fef6ea4d9a86ec0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the lack of fault tolerance for RL post-training of LLMs, which interleaves training and inference workloads. It proposes RobustRL, a system that isolates and recovers failed roles (e.g., trainer, rollout) individually using a Detect-Restart-Reconnect paradigm, instead of restarting the entire job. This approach significantly improves the Effective Training Time Ratio and reduces end-to-end training time compared to baseline methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [hallucination detection], [correctness prediction, metadata signals, prompting strategies, log probability, response consistency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lucky Susanto, Anasta Pranawijayana, Cortino Sukotjo, Soni Prasad, Derry Wijaya</p>
</li>
<li class="">
<p><strong>institution:</strong> Monash University Indonesia, University of Pittsburgh, University of North Carolina Adams School of Dentistry, Boston University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22508" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22508</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel method to predict LLM correctness (not just hallucination) using metadata and hallucination signals in a high-stakes medical domain (prosthodontics). 2. Demonstrates that metadata-based predictors can improve accuracy over a naive baseline and achieve high precision, but are not reliable for directly predicting hallucination. 3. Shows that prompting strategies significantly alter model internal behavior and the predictive utility of metadata, despite not changing overall task accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbe159260b97cf5e7a59edbee0a23b697a76a89a0fdbf6e0c9797739cc322b42_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbe159260b97cf5e7a59edbee0a23b697a76a89a0fdbf6e0c9797739cc322b42_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study investigates predicting the correctness of LLM answers on a prosthodontics exam using metadata (like log probability and consistency) and hallucination signals. The method, applied to GPT-4o and OSS-120B across different prompts, shows improved accuracy over a baseline but is not yet robust for high-stakes deployment. The research highlights that prompting strategies change model behavior and metadata utility, offering a direction for reliability signals.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [adversarial robustness], [Spiking Neural Networks, surrogate gradient, adversarial attack, gradient vanishing, adaptive optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jihang Wang, Dongcheng Zhao, Ruolin Chen, Qian Zhang, Yi Zeng</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Beijing Institute of AI Safety and Governance</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22522" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22522</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Theoretical analysis of gradient vanishing in surrogate gradient methods for SNNs. 2. Proposal of Adaptive Sharpness Surrogate Gradient (ASSG) to adaptively adjust the surrogate function for more accurate gradients. 3. Design of Stable Adaptive Projected Gradient Descent (SA-PGD), an adversarial attack with adaptive step size for stable convergence under imprecise gradients.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7b10ed2a96f6e46c5c2afb21a8e1184dd9c5e1f342efdb93f3cd317a08c20ea_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7b10ed2a96f6e46c5c2afb21a8e1184dd9c5e1f342efdb93f3cd317a08c20ea_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the unreliable evaluation of adversarial robustness in Spiking Neural Networks (SNNs) caused by gradient vanishing from surrogate gradients. The authors propose a framework combining an adaptive surrogate gradient method (ASSG) and an adaptive-step attack (SA-PGD) to generate stronger attacks. Experiments show this framework significantly increases attack success rates, revealing that current SNN robustness is overestimated and highlighting the need for more reliable adversarial training.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multi-AI Agent Framework Reveals the &quot;Oxide Gatekeeper&quot; in Aluminum Nanoparticle Oxidation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [multi-AI agent, machine learning potential, human-in-the-loop, self-auditing, scalable molecular dynamics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yiming Lu, Tingyu Lu, Di Zhang, Lili Ye, Hao Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Tohoku University, Dalian University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22529" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22529</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a novel &quot;human-in-the-loop&quot; closed-loop framework utilizing self-auditing AI agents to validate and evolve a machine learning potential, ensuring quantum accuracy while achieving near-linear scalability to million-atom systems and nanosecond timescales. 2. Discovered a temperature-regulated dual-mode oxidation mechanism for aluminum nanoparticles, where the oxide shell acts as a dynamic &quot;gatekeeper&quot; via a &quot;breathing mode&quot; at moderate temperatures and transitions to a catastrophic &quot;rupture mode&quot; above a critical threshold. 3. Resolved a long-standing controversy by demonstrating that aluminum cation outward diffusion, not oxygen transport, is the dominant mass transfer mechanism across all temperature regimes, with diffusion coefficients 2-3 orders of magnitude higher.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42c9680d91fb0cb7f7611fd33feaef10bdab707dc569594f3d51f8af4d9e0651_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42c9680d91fb0cb7f7611fd33feaef10bdab707dc569594f3d51f8af4d9e0651_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a human-in-the-loop multi-AI agent framework to develop and validate a highly accurate and scalable machine learning potential for molecular dynamics simulations. Using this method, the study reveals a dual-mode oxidation mechanism in aluminum nanoparticles and conclusively shows that aluminum cation diffusion, not oxygen transport, dominates the oxidation process.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [closed-loop framework, entity-level memory, vision-language verification, pacing-aware editing, multi-agent collaboration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qinglin Zeng, Kaitong Cai, Ruiqi Chen, Qinhan Lv, Keze Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Sun Yat-sen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22536" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22536</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CoAgent, a collaborative closed-loop framework that formulates video generation as a plan-synthesize-verify-edit process. 2. Introduces a Global Context Manager to maintain entity-level memory for cross-shot identity and appearance consistency. 3. Employs a Verifier Agent with vision-language reasoning to evaluate intermediate results and trigger selective regeneration for quality control.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfaf9ffbd76c531ee1d089b472175957646bd5b08a39cad1cce07b642bd237a4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfaf9ffbd76c531ee1d089b472175957646bd5b08a39cad1cce07b642bd237a4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of maintaining narrative coherence and visual consistency in long-form video generation. It proposes CoAgent, a collaborative multi-agent framework that uses a closed-loop plan-synthesize-verify-edit pipeline with entity memory and consistency verification. Experiments show that CoAgent significantly improves coherence, consistency, and narrative quality in generated videos.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal reasoning], [self-rewarded learning, process alignment, multimodal large language models, reasoning coherence, visual grounding]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jesen Zhang, Ningyuan Liu, Kaitong Cai, Sidi Liu, Jing Yang, Ziliang Chen, Xiaofei Sun, Keze Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Sun Yat-sen University, Alibaba Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22545" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22545</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A lightweight, label-free framework (SR-MCR) that aligns multimodal reasoning by constructing a self-reward from intrinsic process signals (semantic alignment, lexical fidelity, non-redundancy, visual grounding, step consistency). 2. A normalized, reliability-weighted reward mechanism that adaptively combines multiple self-referential cues to provide fine-grained, process-level guidance. 3. A critic-free GRPO objective enhanced with a confidence-aware cooling mechanism to stabilize training and suppress trivial or overconfident generations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/629536dd38b71477a18bd2e7208023831047c11b4eafeff5c5c094c124751f98_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/629536dd38b71477a18bd2e7208023831047c11b4eafeff5c5c094c124751f98_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of multimodal LLMs producing fluent but unreliable reasoning with poor step coherence and visual grounding. It proposes SR-MCR, a self-rewarded framework that uses multiple intrinsic process signals from model outputs to create a fine-grained reward for alignment, achieving state-of-the-art accuracy and improved reasoning coherence on visual benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time-series forecasting], [encoder-decoder, latent bottleneck representations, learnable queries, generalized forecasting]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jaebin Lee, Hankook Lee</p>
</li>
<li class="">
<p><strong>institution:</strong> Sungkyunkwan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22550" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22550</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/efficient-learning-lab/TimePerceiver" target="_blank" rel="noopener noreferrer" class="">https://github.com/efficient-learning-lab/TimePerceiver</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Generalizes the time-series forecasting task to include diverse objectives like extrapolation, interpolation, and imputation. 2. Proposes a novel encoder-decoder architecture with latent bottleneck representations to capture temporal and cross-channel dependencies. 3. Introduces learnable queries for decoding to effectively retrieve information for arbitrarily positioned target timestamps.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/091fdcd1058e0acfad18820d025c1fe50ff4315cbd5ec8a06f5d86eb9767513c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/091fdcd1058e0acfad18820d025c1fe50ff4315cbd5ec8a06f5d86eb9767513c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes TimePerceiver, a unified encoder-decoder framework for generalized time-series forecasting. It handles diverse prediction objectives by using latent bottleneck encodings and learnable query-based decoding. Extensive experiments show it consistently outperforms prior state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Learning When Not to Attend Globally</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [All-or-Here Attention, sliding window attention, conditional computation, binary router, context dependency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xuan Luo, Kailai Zhang, Xifeng Yan</p>
</li>
<li class="">
<p><strong>institution:</strong> UC Santa Barbara</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22562" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22562</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes All-or-Here Attention (AHA), a novel attention mechanism that dynamically toggles between full and local sliding window attention using a binary router per head. 2. Demonstrates empirically that full attention is largely redundant, showing up to 93% of full attention operations can be replaced with local attention without performance loss. 3. Identifies a long-tail distribution in context dependency, revealing that the need for global context decays rapidly as the local window expands.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edc0024b0088e710cd3ce9c0be8276b43396c11c0424f0f6340e0f97d63982e6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edc0024b0088e710cd3ce9c0be8276b43396c11c0424f0f6340e0f97d63982e6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the computational inefficiency of full self-attention in LLMs by proposing All-or-Here Attention (AHA), which learns to dynamically switch between full and local sliding window attention for each token. The results show that most full attention operations are unnecessary, and efficient inference can be achieved with on-demand global context access.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [disaggregated infrastructure, hardware-affinity mapping, fine-grained asynchrony]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wei Gao, Yuheng Zhao, Tianyuan Wu, Shaopan Xiong, Weixun Wang, Dakai An, Lunxi Cao, Dilxat Muhtar, Zichen Liu, Haizhou Zhao, Ju Huang, Siran Yang, Yongbin Li, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> HKUST, Alibaba Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22560" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22560</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/alibaba/ROLL" target="_blank" rel="noopener noreferrer" class="">https://github.com/alibaba/ROLL</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A hardware-affinity workload mapping strategy that routes compute-bound and bandwidth-bound tasks to best-fit GPU devices. 2. A fine-grained asynchrony mechanism that manages execution at the trajectory level to mitigate resource bubbles and improve utilization. 3. A statefulness-aware computation design that offloads stateless components to serverless infrastructure for elastic scaling.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc8e408c613097b7b60bc32e41ec3137faa8dd94184658c8a802b65cbf57c593_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc8e408c613097b7b60bc32e41ec3137faa8dd94184658c8a802b65cbf57c593_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents RollArt, a distributed system designed to scale agentic reinforcement learning training on disaggregated infrastructure. It addresses the heterogeneity of agentic RL workloads by proposing three core techniques: hardware-affinity workload mapping, fine-grained asynchrony, and statefulness-aware computation. The system demonstrates significant improvements in training throughput, achieving 1.35-2.05x speedup over baselines, and scales to thousands of GPUs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [cognitive architectures], [predictive coding, compositional structure, episodic memory, action integration, foundation models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rajesh P. N. Rao, Vishwas Sathish, Linxing Preston Jiang, Matthew Bryan, Prashant Rangarajan</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Washington</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22568" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22568</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes integrating actions, compositional structure, and episodic memory into foundation models to address their deficiencies. 2. Presents neuroscience evidence to support the importance of these components for achieving human-like AI. 3. Compares the proposal to current trends like chain-of-thought reasoning and retrieval-augmented generation, suggesting new brain-inspired augmentation methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c90f622f122bb0e11d5909a8de5e173e03638e3854680c6b2a0139a74ea9314_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c90f622f122bb0e11d5909a8de5e173e03638e3854680c6b2a0139a74ea9314_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper argues that current foundation models, despite their success, lack key components of brain-inspired predictive coding models: action integration, compositional structure, and episodic memory. It proposes integrating these components to address issues like hallucinations, lack of grounding, and poor interpretability, aiming for safer and more human-like AI. The conclusion advocates for renewed collaboration between neuroscience and AI to achieve these goals.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SANet: A Semantic-aware Agentic AI Networking Framework for Cross-layer Optimization in 6G</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [Agentic AI Networking, Multi-agent Multi-objective Optimization, Model Partition and Sharing (MoPS), Cross-layer Optimization, Pareto-optimal Solution]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yong Xiao, Xubo Li, Haoran Zhou, Yingyu Li, Yayu Gao, Guangming Shi, Ping Zhang, Marwan Krunz</p>
</li>
<li class="">
<p><strong>institution:</strong> Huazhong University of Science and Technology, Peng Cheng Laboratory, Pazhou Laboratory (Huangpu), China University of Geosciences (Wuhan), Xidian University, Beijing University of Posts and Telecommunications, University of Arizona</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22579" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22579</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SANet, a semantic-aware Agentic AI Networking architecture that infers user semantic goals and automatically assigns cross-layer agents to fulfill them. 2. Formulates the decentralized optimization as a multi-agent multi-objective problem, proposes three novel evaluation metrics, and develops a Model Partition and Sharing (MoPS) framework for efficient model deployment. 3. Derives theoretical bounds proving a three-way tradeoff among optimization, generalization, and conflicting errors, and validates the framework with a hardware prototype showing significant performance gains and computational efficiency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eee5846572238b17d03d837b7c31a7bd60644abe5d9070207f45cd166b326470_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eee5846572238b17d03d837b7c31a7bd60644abe5d9070207f45cd166b326470_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes SANet, a semantic-aware Agentic AI networking framework for 6G that uses AI agents to infer user goals and perform cross-layer optimization. It formulates the problem as multi-agent multi-objective optimization, introduces a model partition and sharing method, and proves a theoretical tradeoff. Experiments on a hardware prototype show the framework achieves up to 14.61% performance gain while requiring only 44.37% of the FLOPs of state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [physiological signal analysis, unified data interface, modular architecture, end-to-end workflow, configurable toolkit]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tao Zhou, Lingyu Shu, Zixing Zhang, Jing Han</p>
</li>
<li class="">
<p><strong>institution:</strong> Hunan University, University of Cambridge</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22601" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22601</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/SmileHnu/Tyee" target="_blank" rel="noopener noreferrer" class="">https://github.com/SmileHnu/Tyee</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A unified data interface and configurable preprocessing pipeline for 12 physiological signal modalities. 2. A modular and extensible architecture enabling flexible integration and rapid prototyping. 3. An end-to-end workflow configuration system promoting reproducible and scalable experimentation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c4a4d498d8e9b9f7b3a7f4413e2399342920644addd35cc3e7401a5ebadf033_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c4a4d498d8e9b9f7b3a7f4413e2399342920644addd35cc3e7401a5ebadf033_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces Tyee, a configurable deep learning toolkit designed to address challenges in physiological signal analysis, such as heterogeneous data and fragmented pipelines. Its unified, modular design allows for flexible and reproducible experimentation. The toolkit demonstrates strong performance, achieving state-of-the-art results on 12 out of 13 evaluated datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [location-based recommendation], [multi-modal learning, spatial-temporal knowledge graph, cross-modal alignment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Junshu Dai, Yu Wang, Tongya Zheng, Wei Ji, Qinghong Guo, Ji Cao, Jie Song, Canghong Jin, Mingli Song</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Hangzhou City University, Nanjing University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22605" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22605</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://anonymous.4open.science/r/M3ob-62EF" target="_blank" rel="noopener noreferrer" class="">https://anonymous.4open.science/r/M3ob-62EF</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified spatial-temporal relational graph (STRG) for multi-modal representation, enhanced by LLMs. 2. Designs a gating mechanism to fuse spatial-temporal graph representations from different modalities. 3. Introduces an STKG-guided cross-modal alignment method to inject dynamic knowledge into static image representations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de0c70167dfab9ddb767e6d0d1161ce4f31f482e064a77521ffa2e25c598f1d2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de0c70167dfab9ddb767e6d0d1161ce4f31f482e064a77521ffa2e25c598f1d2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limited generalization of next location recommendation methods by proposing M³ob, a framework that leverages multi-modal spatial-temporal knowledge. It constructs a unified graph representation and uses a gating mechanism with cross-modal alignment to capture mobility dynamics. Experiments on six datasets show the method improves performance in both normal and abnormal scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [multi-agent system, graph neural network, collective decision-making, startup success prediction, role-playing agents]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhongyang Liu, Haoyu Pei, Xiangyi Xiao, Xiaocong Du, Yihui Li, Suting Hong, Kunpeng Zhang, Haipeng Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> ShanghaiTech University, Xi’an Jiaotong-Liverpool University, University of Maryland</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22608" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22608</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SimVC-CAS, a novel collective agent system that reformulates startup financing prediction as a multi-agent group decision-making task, moving beyond single decision-maker models. 2. Introduces role-playing agents with unique traits and a GNN-based supervised interaction module to capture heterogeneous investor evaluations and behavioral dynamics within a co-investment network. 3. Demonstrates significant predictive performance improvement (e.g., ~25% relative improvement in average precision@10) on real-world PitchBook data with strict leakage controls, while providing interpretable, multi-perspective reasoning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5560279fcb9dca880844ffe21da36f9901c81ce8898e265fff9cc80a5e7cfb8a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5560279fcb9dca880844ffe21da36f9901c81ce8898e265fff9cc80a5e7cfb8a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of predicting startup success by simulating venture capital decision-making as a collective process. It proposes SimVC-CAS, a multi-agent system where role-playing LLM agents interact via a GNN module to model investor networks. The method significantly outperforms previous approaches in predicting financing outcomes and offers interpretable reasoning from multiple investor perspectives.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Chord Recognition with Deep Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [music information retrieval], [chord recognition, deep learning, generative models, pitch augmentation, beat detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Pierre Mackenzie</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Edinburgh</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22621" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22621</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and analyzes the poor performance of chord classifiers on rare chords, providing a key insight into a major limitation of current methods. 2. Demonstrates that pitch augmentation is an effective technique for boosting chord recognition accuracy. 3. Improves model interpretability by integrating beat detection into the model&#x27;s output, leading to some of the best reported results in the field.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53155c1b8ae949083d40642ac5cf37ed34863c9840190ad8076fa052bb0f3185_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53155c1b8ae949083d40642ac5cf37ed34863c9840190ad8076fa052bb0f3185_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This thesis investigates the slow progress in automatic chord recognition despite the use of deep learning. It experiments with existing methods and generative models, finding that pitch augmentation improves accuracy while generative features do not help. The work concludes by enhancing model interpretability with beat detection, achieving state-of-the-art results and suggesting synthetic data as a promising future direction.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [forecasting], [large language models, deliberation, multi-agent, forecasting accuracy, log loss]</p>
</li>
<li class="">
<p><strong>authors:</strong> Paul Schneider, Amalie Schramm</p>
</li>
<li class="">
<p><strong>institution:</strong> PRIORB</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22625" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22625</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces and tests a structured deliberation intervention for LLMs, where models review each other&#x27;s forecasts before updating, as a novel method for improving AI-based forecasting. 2. Systematically evaluates the intervention across four distinct scenarios (diverse/homogeneous models with distributed/shared information), identifying that accuracy improvement is specific to diverse models with shared information. 3. Provides empirical evidence that deliberation can be a viable strategy for improving LLM forecasting, while also revealing the unexpected finding that providing additional contextual information did not improve accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0685f113c8c22bd110f615c357b7de1633adf1104f67be97e690bae6bee70345_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0685f113c8c22bd110f615c357b7de1633adf1104f67be97e690bae6bee70345_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study investigates whether allowing large language models (LLMs) to deliberate by reviewing each other&#x27;s forecasts improves their forecasting accuracy. The method was tested on 202 binary questions across different model group compositions and information-sharing scenarios. The main conclusion is that deliberation significantly improves accuracy for diverse LLM groups with shared information, but not for homogeneous groups, suggesting it as a viable strategy for enhancing LLM-based forecasting.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [probabilistic scoring, Swiss-system tournament, explainable evaluation, evidence-coupled framework, ranking fidelity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shiyan Liu, Jian Ma, Rui Qu</p>
</li>
<li class="">
<p><strong>institution:</strong> Huazhong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22629" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22629</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces DICE, a two-stage, evidence-coupled framework for explainable and robust RAG evaluation using probabilistic {A, B, Tie} scoring. 2. Employs a Swiss-system tournament to reduce computational complexity from O(N²) to O(N log N) for efficient multi-system comparisons. 3. Demonstrates high agreement (85.7%) with human experts on a Chinese financial QA dataset, outperforming existing LLM-based metrics like RAGAS.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6045a90fb152ced5737d976a17be84cd02b0a0396b4dd9cb385e9ba4d9063de6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6045a90fb152ced5737d976a17be84cd02b0a0396b4dd9cb385e9ba4d9063de6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the lack of interpretability and efficiency in evaluating Retrieval-Augmented Generation (RAG) systems. It proposes DICE, a framework that uses probabilistic scoring and a Swiss-system tournament to provide transparent, confidence-aware judgments while reducing computational cost. The method shows strong agreement with human experts, establishing it as an explainable and efficient paradigm for trustworthy RAG assessment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Scaling Unverifiable Rewards: A Case Study on Visual Insights</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [Test-Time Scaling, multi-agent pipeline, process-based refinement, LLM-as-Judge, unverifiable rewards]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuyu Gan, James Mooney, Pan Hao, Renxiang Wang, Mingyi Hong, Qianwen Wang, Dongyeop Kang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Minnesota</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22650" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22650</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://minnesotanlp.github.io/insight-scaling-webpage" target="_blank" rel="noopener noreferrer" class="">https://minnesotanlp.github.io/insight-scaling-webpage</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Selective Test-Time Scaling, a process-based refinement framework that scales inference across stages in multi-agent pipelines instead of repeated refinement over time. 2. Designed a reliable LLM-based judge model aligned with human experts for evaluating visual insights. 3. Demonstrated improved insight quality under fixed compute budget in a data science pipeline application.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60e14b2c6ac8597444bea3610d692bad067ed798ec62c0920b0fe7c54b7fcd14_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60e14b2c6ac8597444bea3610d692bad067ed798ec62c0920b0fe7c54b7fcd14_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of scaling LLM agents for tasks with unverifiable rewards by introducing Selective Test-Time Scaling, which distributes compute across pipeline stages and prunes low-quality branches early using process-specific judges. Applied to generating visual insights from datasets, the method increases mean quality scores and reduces variance compared to traditional time-based refinement. The work provides a foundation for scaling complex, open-ended tasks like scientific discovery and story generation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [transfer learning / domain adaptation], [Cluster Attention Adapter, adapter tuning, data-limited domains, vision foundation models, adaptive transfer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qiankun Li, Feng He, Huabao Chen, Xin Ning, Kun Wang, Zengfu Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology of China, AnnLab (Institute of Semiconductors, Chinese Academy of Sciences), Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22664" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22664</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/qklee-lz/CLAdapter" target="_blank" rel="noopener noreferrer" class="">https://github.com/qklee-lz/CLAdapter</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Cluster Attention Adapter (CLAdapter) that refines and adapts pre-trained representations to data-limited downstream tasks using attention mechanisms and cluster centers. 2. Designs a unified interface for seamless integration with diverse model architectures (CNNs, Transformers) in both 2D and 3D contexts. 3. Demonstrates state-of-the-art performance through extensive experiments on 10 datasets spanning diverse scientific domains.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d174c6a31de34c34ee98111995fd6b43142db3342aa6c7a647cb2a8121615532_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d174c6a31de34c34ee98111995fd6b43142db3342aa6c7a647cb2a8121615532_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of adapting large-scale pre-trained vision foundation models to specialized, data-limited scientific domains. It proposes a novel Cluster Attention Adapter (CLAdapter) that personalizes feature enhancement for different downstream tasks, enabling effective adaptive transfer. Extensive experiments across 10 diverse datasets show that CLAdapter achieves state-of-the-art performance, demonstrating its effectiveness in unleashing the potential of foundation models for scientific applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [3D Inception, two-stream networks, CNN-RNN, EchoNet-Dynamic, video analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shravan Saranyan, Pramit Saha</p>
</li>
<li class="">
<p><strong>institution:</strong> Branham High School, University of Oxford</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22657" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22657</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A systematic investigation and comparison of multiple deep learning architectures (3D Inception, two-stream, CNN-RNN) for automated LVEF estimation from echocardiography videos. 2. Identification that modified 3D Inception architectures achieve the best performance (RMSE of 6.79%) on the EchoNet-Dynamic dataset. 3. Key insights on model design, including the tendency for smaller, simpler models to generalize better and the high sensitivity of performance to hyperparameters like kernel size and normalization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates deep learning models for automating the estimation of left ventricular ejection fraction (LVEF) from echocardiography videos to address the time-consuming and variable nature of manual assessment. The authors systematically evaluate and modify several architectures, including 3D Inception, two-stream, and CNN-RNN models, finding that a modified 3D Inception model performs best. The study concludes that while these models show promise, they are prone to overfitting and their performance is highly sensitive to specific hyperparameter choices.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [width pruning, expansion ratio, Maximum Absolute Weight (MAW), GLU-MLP, instruction-following]</p>
</li>
<li class="">
<p><strong>authors:</strong> Pere Martra</p>
</li>
<li class="">
<p><strong>institution:</strong> Universidad Internacional Menéndez Pelayo (UIMP)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22671" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22671</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Systematically characterizes a capability dichotomy where structured width pruning degrades parametric knowledge (e.g., MMLU) but significantly improves instruction-following (e.g., IFEval). 2. Discovers and quantifies a robust inverse correlation between factual knowledge and truthfulness, linking knowledge degradation under pruning to improved misconception discrimination. 3. Identifies the expansion ratio as a critical architectural parameter that selectively modulates cognitive capabilities, rather than just a compression metric, and quantifies its context-dependent efficiency trade-offs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4df6003ce0dd5672f67ef0c36b943a93c7cbb475cc96f2c7592e1f230af17d53_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4df6003ce0dd5672f67ef0c36b943a93c7cbb475cc96f2c7592e1f230af17d53_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates structured width pruning of GLU-MLP layers in Llama-3.2 models using the Maximum Absolute Weight (MAW) criterion. It finds that pruning creates a dichotomy: while parametric knowledge degrades, instruction-following improves and multi-step reasoning remains robust, challenging the assumption of uniform degradation. The main conclusion is that width pruning acts as a selective filter, reducing knowledge but preserving or enhancing behavioral alignment, with identified trade-offs in efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Conformal Prediction Sets for Next-Token Prediction in Large Language Models: Balancing Coverage Guarantees with Set Efficiency</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [uncertainty quantification], [conformal prediction, adaptive prediction sets, vocabulary-aware, coverage-efficiency tradeoff, marginal coverage]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yoshith Roy Kotla, Varshith Roy Kotla</p>
</li>
<li class="">
<p><strong>institution:</strong> The ICFAI Foundation for Higher Education</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22682" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22682</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identified and formally characterized the coverage-efficiency tradeoff unique to applying conformal prediction to next-token prediction in LLMs with large vocabularies. 2. Proposed Vocabulary-Aware Conformal Prediction (VACP), a framework using semantic masking and temperature-adjusted scoring to reduce the effective prediction space. 3. Provided a theoretical analysis of when vocabulary reduction preserves conformal validity and demonstrated a 197x improvement in prediction set efficiency on benchmarks while maintaining coverage guarantees.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fb40fe541a33e11ac6d9b3f6f3fac213d5602d391cc590303cb8079bf97a840_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fb40fe541a33e11ac6d9b3f6f3fac213d5602d391cc590303cb8079bf97a840_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that naive conformal prediction for LLM next-token prediction produces uninformatively large prediction sets due to large vocabularies. It proposes Vocabulary-Aware Conformal Prediction (VACP), which uses semantic masking and hierarchical conformalization to drastically reduce set size. The method achieves near-target coverage while improving set efficiency by 197x, making conformal prediction practical for LLMs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [travel planning benchmark, multi-turn interaction, tool-augmented agents, deterministic sandbox, real-world user requests]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiang Cheng, Yulan Hu, Xiangwen Zhang, Lu Xu, Zheng Pan, Xin Li, Yong Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Gaoling School of Artificial Intelligence, Renmin University of China; AMAP, Alibaba Group; National University of Singapore</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22673" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22673</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces TravelBench, a real-world travel planning benchmark featuring multi-turn user-agent interaction and tool use, addressing limitations of prior static benchmarks. 2. Constructs a controlled sandbox environment with 10 deterministic travel-domain tools (e.g., POI search, route planning) to enable stable and reproducible evaluation of agent reasoning. 3. Collects and curates a diverse dataset of 1,103 instances (multi-turn, single-turn, unsolvable) from real user scenarios to comprehensively evaluate different aspects of agent performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e9e23f8223243d3734eaad4483f929312f198ee51f05e74faf519458ec18add0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e9e23f8223243d3734eaad4483f929312f198ee51f05e74faf519458ec18add0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces TravelBench, a new benchmark for evaluating LLM agents in realistic travel planning, which features multi-turn interaction and a sandbox of deterministic tools. The benchmark addresses the limitations of prior work by supporting dynamic user interaction and long-horizon planning. The authors evaluate several LLMs on TravelBench, providing a practical testbed for advancing agent capabilities in planning and tool use.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Learning with the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span></span></span></span>-adics</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [representation learning], [p-adic numbers, ultrametric space, hierarchical representation, non-archimedean geometry, semantic networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> André F. T. Martins</p>
</li>
<li class="">
<p><strong>institution:</strong> Instituto Superior Técnico, Universidade de Lisboa; Instituto de Telecomunicações</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22692" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22692</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes using the p-adic number field (Q_p) as an alternative to real numbers for machine learning, leveraging its ultrametric and hierarchical structure. 2. Develops foundational building blocks for classification, regression, and representation learning models and algorithms within the p-adic framework. 3. Demonstrates a novel application by representing simple Quillian semantic networks as compact p-adic linear networks, which is not achievable with real numbers.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec090da18463864436ff27e6cb4651eb7e01719b66dcdae5d6644770e6a7b488_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec090da18463864436ff27e6cb4651eb7e01719b66dcdae5d6644770e6a7b488_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper explores using p-adic numbers, an ultrametric and non-archimedean field, as an alternative to real numbers for machine learning. It introduces theoretical models and algorithms for classification, regression, and representation learning, showing that p-adics enable compact representations of hierarchical structures like semantic networks. The work opens new research directions by leveraging the unique geometric properties of p-adic spaces.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [hope speech detection], [transformer models, multilingual classification, low-resource languages, XLM-RoBERTa, UrduBERT]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ahmed Abdullah, Sana Fatima, Haroon Mahmood</p>
</li>
<li class="">
<p><strong>institution:</strong> FAST-National University, Al Ain University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22705" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22705</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a multilingual framework for hope speech detection, specifically addressing the underrepresentation of low-resource languages like Urdu. 2. Applies and evaluates multiple pretrained transformer models (XLM-RoBERTa, mBERT, EuroBERT, UrduBERT) on the PolyHope-M 2025 benchmark for this task. 3. Demonstrates strong performance, achieving high F1-scores for Urdu classification, validating the use of existing multilingual models in low-resource settings.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c79c484e6d35762080aa8d6e1dbf075222d30335d656555a46ddf73380d7fe88_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c79c484e6d35762080aa8d6e1dbf075222d30335d656555a46ddf73380d7fe88_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of resources for hope speech detection in low-resource languages by proposing a multilingual framework using pretrained transformer models like XLM-RoBERTa and UrduBERT. The method involves simple preprocessing and training classifiers, which achieve high F1-scores on the PolyHope-M 2025 benchmark, particularly for Urdu. The results show that existing multilingual models can be effectively implemented to identify hope speech and foster positive digital discourse in low-resource environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Memento-II: Learning by Stateful Reflective Memory</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [stateful reflective decision process, episodic memory, policy iteration, continual learning, retrieval-augmented generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jun Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> University College London (UCL)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22716" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22716</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Stateful Reflective Decision Process (SRDP), a formal theoretical framework that models continual learning in LLM agents as a two-stage read-write interaction with episodic memory, linking it to policy evaluation and improvement. 2. Provides a theoretical analysis showing that the reflective learning process induces an equivalent Markov Decision Process, enabling the use of classical dynamic programming and RL tools, and establishes convergence guarantees when instantiated with entropy-regularised policy iteration. 3. Unifies heuristic approaches like case-based reasoning and retrieval-augmented generation with principled reinforcement learning, offering a rigorous mathematical foundation for building memory-augmented agents capable of online adaptation without parameter updates.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a theoretical framework for continual learning in LLM agents that uses episodic memory and reflection instead of back-propagation. The core method formalizes learning as a Stateful Reflective Decision Process, where writing to memory is policy evaluation and reading from it is policy improvement. The main conclusion is that this framework provides a principled, convergent foundation for agents to self-improve through interaction without fine-tuning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [context folding, long-horizon RL, non-stationary observation, gradient dilution, selective segment training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiaqi Shao, Yufeng Miao, Wei Zhang, Bing Luo</p>
</li>
<li class="">
<p><strong>institution:</strong> Hong Kong University of Science and Technology, Duke Kunshan University, Microsoft AI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22733" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22733</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/SHAO-Jiaqi757/FoldAct" target="_blank" rel="noopener noreferrer" class="">https://github.com/SHAO-Jiaqi757/FoldAct</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Separated loss computation for independent gradient signals on summary and action tokens to address gradient dilution. 2. Full context consistency loss to reduce distribution shift caused by policy-dependent observation changes. 3. Selective segment training to reduce computational cost by processing unique contexts efficiently.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebdb4b7ca8ea3a44c0e368eed5fbbebfc656b663cf280d1891abfbf823c742fa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebdb4b7ca8ea3a44c0e368eed5fbbebfc656b663cf280d1891abfbf823c742fa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that treating context folding (history summarization) as a standard action in long-horizon RL for LLMs creates a non-stationary observation distribution, leading to training instability and inefficiency. It proposes FoldAct, a framework with three innovations—separated loss, consistency loss, and selective training—to stabilize training and improve efficiency. The method achieves stable training and a 5.19× speedup.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Harnessing Large Language Models for Biomedical Named Entity Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [named entity recognition], [instruction tuning, data filtering, weak-to-strong learning, biomedical named entity recognition, json generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jian Chen, Leilei Su, Cong Sun</p>
</li>
<li class="">
<p><strong>institution:</strong> Hainan University, Weill Cornell Medicine</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22738" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22738</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes BioSelectTune, a data-centric framework for fine-tuning LLMs for BioNER that prioritizes data quality. 2. Introduces a Hybrid Superfiltering strategy, a weak-to-strong data curation method to distill a high-impact training dataset. 3. Reformulates BioNER as a structured JSON generation task to leverage LLMs&#x27; instruction-following capabilities.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e941de51836d02e0004ef558a69019ce22af7124cd10760a2c904f6329cfa1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e941de51836d02e0004ef558a69019ce22af7124cd10760a2c904f6329cfa1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of adapting general-domain LLMs to Biomedical Named Entity Recognition (BioNER) by proposing BioSelectTune, a framework that uses a novel Hybrid Superfiltering data curation strategy and formulates BioNER as a JSON generation task. The method achieves state-of-the-art performance on multiple benchmarks, outperforming specialized models even when trained on only 50% of the curated data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Robust LLM-based Column Type Annotation via Prompt Augmentation with LoRA Tuning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [Column Type Annotation, Prompt Augmentation, LoRA, Parameter-Efficient Fine-Tuning, Prompt Sensitivity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hanze Meng, Jianhao Cao, Rachel Pottinger</p>
</li>
<li class="">
<p><strong>institution:</strong> University of British Columbia</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22742" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22742</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/fripSideMeng/PACTA" target="_blank" rel="noopener noreferrer" class="">https://github.com/fripSideMeng/PACTA</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a parameter-efficient fine-tuning framework for Column Type Annotation (CTA) using Low-Rank Adaptation (LoRA) to reduce computational cost. 2. Introduces a prompt augmentation strategy during training to mitigate model sensitivity to variations in prompt wording and structure. 3. Demonstrates robust and stable performance across diverse datasets and prompt templates, achieving higher weighted F1 scores than single-template fine-tuning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95317c9af6072a1e0ffbb34950b7d9da057c55baaf301c56bb751746b366785a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95317c9af6072a1e0ffbb34950b7d9da057c55baaf301c56bb751746b366785a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of prompt sensitivity and high computational cost in using Large Language Models (LLMs) for Column Type Annotation. It proposes a parameter-efficient framework that fine-tunes LLMs using LoRA on prompt-augmented data. The method achieves robust performance across different prompts and datasets while requiring significantly fewer trainable parameters.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Active Constraint Learning in High Dimensions from Demonstrations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robot learning], [active learning, constraint inference, Gaussian processes, learning from demonstration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zheng Qiu, Chih-Yuan Chiu, Glen Chou</p>
</li>
<li class="">
<p><strong>institution:</strong> Georgia Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22757" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22757</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an iterative active constraint learning (ACL) algorithm that intelligently queries for new demonstrations to reduce constraint uncertainty. 2. Integrates a Gaussian process (GP) model within the learning from demonstrations (LfD) paradigm to represent and infer unknown constraints. 3. Demonstrates superior performance over a random-sampling baseline in recovering nonlinear constraints from sparse, informative demonstrations in high-dimensional settings with nonlinear dynamics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5acbf135d5aa431114b6b72db2fb101427cb6bd1e55fb1a8afd3028c0874cb4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5acbf135d5aa431114b6b72db2fb101427cb6bd1e55fb1a8afd3028c0874cb4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the data inefficiency of learning unknown constraints from demonstrations by proposing an active learning algorithm. The method iteratively trains a Gaussian process on demonstration data to model constraints and uses the model&#x27;s uncertainty to query for new, informative start/goal states to generate more demonstrations. Experiments show the approach outperforms a random-sampling baseline in accurately inferring constraints from fewer demonstrations in high-dimensional, nonlinear environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Understanding the Mechanisms of Fast Hyperparameter Transfer</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [hyperparameter optimization], [hyperparameter transfer, scale-aware hyperparameters, Maximal Update Parameterization (μP), compute-optimal grid search]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nikhil Ghosh, Denny Wu, Alberto Bietti</p>
</li>
<li class="">
<p><strong>institution:</strong> Flatiron Institute, New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22768" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22768</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Develops a formal conceptual framework defining &quot;fast&quot; hyperparameter transfer and proves its equivalence to &quot;useful&quot; transfer for compute-optimal grid search. 2. Demonstrates that the fast transfer property is not universal and depends critically on problem structure, showing synthetic cases where it succeeds or fails. 3. Proposes and provides empirical evidence for a mechanistic hypothesis explaining fast transfer, decomposing the loss reduction into width-stable and width-sensitive components.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b19c9825d64e8e70117e18cd478466aaf2627a7a5167d401bcac21353870d508_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b19c9825d64e8e70117e18cd478466aaf2627a7a5167d401bcac21353870d508_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the mechanisms behind fast hyperparameter transfer, a strategy to reduce tuning costs by transferring optimal hyperparameters from small to large models. It formally defines fast transfer and shows it is computationally advantageous, then explains the phenomenon by hypothesizing a decomposition of the optimization trajectory into stable and sensitive components, supported by empirical evidence.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3d reconstruction], [3D Gaussian Splatting, Next Best View, Active Learning, Fisher Information, Dynamic Scene Modeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yiqian Li, Wen Jiang, Kostas Daniilidis</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Pennsylvania</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22771" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22771</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulates the next-best-view selection problem for dynamic and semantic 3D scenes as an active learning problem. 2. Proposes an active learning algorithm using Fisher Information to quantify view informativeness for both semantic Gaussian parameters and deformation networks. 3. Provides a unified framework that jointly handles semantic reasoning and dynamic scene modeling, outperforming heuristic and random baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/512528158b99bf9d8d5519331a5d1557baee5b3050273bff66df842767a73963_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/512528158b99bf9d8d5519331a5d1557baee5b3050273bff66df842767a73963_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of selecting the most informative camera views for training dynamic and semantic 3D Gaussian Splatting models. It proposes an active learning method based on Fisher Information to prioritize frames that maximize information gain for both geometry and semantics. The approach improves rendering quality and segmentation performance compared to random or uncertainty-based selection strategies.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [causal inference], [causal transportability, domain adaptation, few-shot learning, circuit composition, distribution shift]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kasra Jalaldoust, Elias Bareinboim</p>
</li>
<li class="">
<p><strong>institution:</strong> Columbia University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22777" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22777</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Circuit-TR, an algorithm for zero-shot compositional generalization based on causal transportability theory, using modules learned from source data. 2. Introduced a supervised domain adaptation scheme that leverages circuit transportability without requiring an explicit causal graph, using only limited target data. 3. Provided theoretical characterization of few-shot learnable tasks using graphical circuit transportability criteria, linking generalizability to circuit size complexity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b025a886304d6097d2893ec4af3bb34a59528db65e22ddf5b4dfff4439a096_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b025a886304d6097d2893ec4af3bb34a59528db65e22ddf5b4dfff4439a096_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of generalization under distribution shift by proposing a method based on causal transportability theory. The method, Circuit-TR, learns local predictors (modules) from source data and composes them into a circuit for prediction in a target domain, enabling both zero-shot and few-shot adaptation. The theoretical results connect few-shot learnability to circuit transportability criteria and complexity, which are supported by simulations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [temporal graph neural networks, explainable ai, graph explanation, recurrent neural networks, breadth-first search]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xuyan Li, Jie Wang, Zheng Yan</p>
</li>
<li class="">
<p><strong>institution:</strong> Xidian University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22772" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22772</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GRExplainer, a universal explanation method applicable to both snapshot-based and event-based Temporal Graph Neural Networks (TGNNs). 2. Introduces an efficient approach using breadth-first search and temporal information to construct node sequences, reducing computational cost. 3. Designs a user-friendly generative model based on Recurrent Neural Networks (RNNs) for automated and continuous explanation generation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0722c64cfeebe092d7b253f417faaa51990e69198068745c39fe241716082408_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0722c64cfeebe092d7b253f417faaa51990e69198068745c39fe241716082408_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of explainability in Temporal Graph Neural Networks (TGNNs) by proposing GRExplainer, a universal and efficient method that uses node sequences and an RNN-based generative model to provide explanations. Experiments on six datasets with three TGNNs demonstrate that GRExplainer outperforms existing methods in generality, efficiency, and user-friendliness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] CNSight: Evaluation of Clinical Note Segmentation Tools</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [text segmentation], [clinical note segmentation, transformer models, large language models, MIMIC-IV, rule-based baselines]</p>
</li>
<li class="">
<p><strong>authors:</strong> Risha Surana, Adrian Law, Sunwoo Kim, Rishab Sridhar, Angxiao Han, Peiyu Hong</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Southern California</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22795" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22795</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A comprehensive evaluation of diverse methods (rule-based, domain-specific transformers, and large language models) for the task of clinical note segmentation. 2. The curation and use of a dataset of 1,000 notes from MIMIC-IV for benchmarking segmentation performance. 3. Empirical findings that large API-based models (e.g., GPT-5-mini) achieve the best overall performance, while lightweight baselines remain competitive only on structured tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be9738f44f0f558dc344bd32b267879341b566035c5dbe67f97ac2e4529b479_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be9738f44f0f558dc344bd32b267879341b566035c5dbe67f97ac2e4529b479_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper evaluates various methods for segmenting unstructured clinical notes into distinct sections. It compares rule-based baselines, domain-specific transformers, and large language models on a curated dataset from MIMIC-IV. The main conclusion is that large API-based models like GPT-5-mini achieve the best overall segmentation performance, providing guidance for method selection in downstream clinical applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition via Spherical Normalization and Mahalanobis Distance</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [open-set recognition], [spherical normalization, Mahalanobis distance, electronic nose, open-set recognition, feature drift]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuai Chen, Chen Wang, Ziran Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> School of Mechanical Engineering, Shandong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22792" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22792</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A geometric decoupling mechanism using cascaded batch normalization and L2 normalization to project features onto a unit hypersphere, eliminating signal intensity fluctuations. 2. The introduction of Mahalanobis distance as a scoring mechanism to construct adaptive ellipsoidal decision boundaries that account for anisotropic feature distributions. 3. A universal, architecture-agnostic framework (SNM-Net) that can be seamlessly integrated with various backbone networks (CNN, RNN, Transformer) for robust open-set gas recognition.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d69d593ebbfea881a49530f570b5c2a934cb8b5cd782c1d7e7c9fba99a92906_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d69d593ebbfea881a49530f570b5c2a934cb8b5cd782c1d7e7c9fba99a92906_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes SNM-Net, a universal framework for robust open-set gas recognition in electronic nose systems. It addresses signal drift and unknown interference by projecting features onto a hypersphere for intensity normalization and using Mahalanobis distance for scoring. The method achieves state-of-the-art performance with high accuracy and exceptional robustness across different sensor conditions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Reach-Avoid Differential game with Reachability Analysis for UAVs: A decomposition approach</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [differential games], [Hamilton-Jacobi reachability, reach-avoid games, dimensionality decomposition, UAVs, tracking control]</p>
</li>
<li class="">
<p><strong>authors:</strong> Minh Bui, Simon Monckton, Mo Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Simon Fraser University, Defense Research &amp; Development Canada (DRDC)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22793" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22793</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel dimensionality reduction framework for 3D reach-avoid games by decomposing the problem into horizontal and vertical sub-games., 2. A Hamilton-Jacobi-based tracking control algorithm to reconstruct the solution from sub-games, guaranteeing capture and subsequent tracking of the attacker., 3. Theoretical proof of the conditions for maintaining capture guarantees and empirical validation in both numerical simulations and a physics simulator (Gazebo).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d14da164894760eae683bf30139829cd77a6bbd67cf14fee19eb09a05cb31eff_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d14da164894760eae683bf30139829cd77a6bbd67cf14fee19eb09a05cb31eff_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the high-dimensional challenge of 3D reach-avoid differential games for UAVs by proposing a decomposition approach that splits the problem into horizontal and vertical sub-games, solves them using Hamilton-Jacobi reachability analysis, and uses a novel tracking control to reconstruct the solution. The method is proven to maintain optimality and capture guarantees, and its effectiveness is successfully demonstrated through simulations and a physics simulator for quadrotor capture.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MoR: Mixture Of Representations For Mixed-Precision Training</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [mixed-precision training, FP8, dynamic quantization, tensor representation, low-precision training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bor-Yiing Su, Peter Dykas, Mike Chrzanowski, Jatin Chhugani</p>
</li>
<li class="">
<p><strong>institution:</strong> Nvidia, Meta</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22804" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22804</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Mixture-of-Representations (MoR), a novel per-tensor and sub-tensor level quantization framework that dynamically selects numerical representations based on tensor properties. 2. Introduces and experiments with concrete algorithms that dynamically choose between FP8 and BF16 representations at different granularities. 3. Demonstrates a universal approach that preserves model quality across datasets and achieves state-of-the-art results with 98.38% of tensors quantized to FP8, showing potential for even lower precision formats like NVFP4.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14c70a65c4f996e1c88d9996c77e4d780e13466bf11a0601ad82d27376753968_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14c70a65c4f996e1c88d9996c77e4d780e13466bf11a0601ad82d27376753968_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces MoR, a dynamic quantization framework for mixed-precision training that analyzes tensor properties to select between representations like FP8 and BF16. It achieves high FP8 quantization rates (98.38%) while maintaining model quality, offering a robust approach for low-precision training that can be combined with other methods for even lower precision formats.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human motion generation], [egocentric video, 3D human reaction, autoregressive generation, VQ-VAE, GPT]</p>
</li>
<li class="">
<p><strong>authors:</strong> Libo Zhang, Zekun Li, Tianyu Li, Zeyu Cao, Rui Xu, Xiaoxiao Long, Wenjia Wang, Jingbo Wang, Yuan Liu, Wenping Wang, Daquan Zhou, Taku Komura, Zhiyang Dou</p>
</li>
<li class="">
<p><strong>institution:</strong> THU, Brown, Georgia Tech, Cambridge, HKU, NJU, CUHK, HKUST, TAMU, PKU, MIT</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22808" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22808</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Constructed the Human Reaction Dataset (HRD), a spatially aligned egocentric video-reaction dataset to address data scarcity and misalignment in existing resources. 2. Proposed EgoReAct, the first autoregressive framework for real-time, 3D-aligned human reaction motion generation from streaming egocentric video. 3. Incorporated 3D dynamic features (metric depth, head dynamics) into the generation pipeline to enhance spatial grounding and realism.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/157e088cb49368b1dbc239ff6380ef8897576c9c3fa92a5bf6737c3a5029e927_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/157e088cb49368b1dbc239ff6380ef8897576c9c3fa92a5bf6737c3a5029e927_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the challenge of generating realistic and spatially aligned 3D human reactions from egocentric video streams. The authors propose EgoReAct, an autoregressive framework that uses a VQ-VAE and a GPT to generate motions in real-time, enhanced by 3D features. Experiments show the method achieves superior realism, spatial consistency, and efficiency while maintaining strict causality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FasterPy: An LLM-based Code Execution Efficiency Optimization Framework</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [Code Optimization, Retrieval-Augmented Generation (RAG), Low-Rank Adaptation (LoRA), Large Language Models (LLMs), Python]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yue Wu, Minghao Han, Ruiyin Li, Peng Liang, Amjed Tahir, Zengyang Li, Qiong Feng, Mojtaba Shahin</p>
</li>
<li class="">
<p><strong>institution:</strong> Wuhan University, Carnegie Mellon University, Massey University, Central China Normal University, Nanjing University of Science and Technology, RMIT University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22827" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22827</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/WuYue22/fasterpy" target="_blank" rel="noopener noreferrer" class="">https://github.com/WuYue22/fasterpy</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FasterPy, a low-cost and efficient framework that adapts LLMs for Python code execution efficiency optimization. 2. Combines Retrieval-Augmented Generation (RAG) with a knowledge base of performance-improving code pairs and Low-Rank Adaptation (LoRA) to enhance optimization performance. 3. Demonstrates superior performance over existing models on the Performance Improving Code Edits (PIE) benchmark.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49aae1b7cd12cfd30401a619c9b06d4bccc853d1d14eed57af87eb6c80858f31_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49aae1b7cd12cfd30401a619c9b06d4bccc853d1d14eed57af87eb6c80858f31_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces FasterPy, a framework that uses Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation (RAG) and Low-Rank Adaptation (LoRA) to automatically optimize Python code for better execution efficiency. It addresses the limitations of traditional rule-based and data-intensive ML methods by providing a more scalable and cost-effective solution. Experimental results show that FasterPy outperforms existing models on standard benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [automated environment synthesis, environment-level RL, agentic reinforcement learning, simulated user, policy optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shihao Cai, Runnan Fang, Jialong Wu, Baixuan Li, Xinyu Wang, Yong Jiang, Liangcai Su, Liwen Zhang, Wenbiao Yin, Zhen Zhang, Fuli Feng, Pengjun Xie, Xiaobin Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tongyi Lab, Alibaba Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22857" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22857</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A unified, automated pipeline for synthesizing scalable simulated environments with high-difficulty, easily verifiable tasks. 2. An Environment-level Relative Policy Optimization (ERPO) algorithm that mitigates simulated user instability and performs advantage estimation at the environment level. 3. Comprehensive validation on agentic benchmarks demonstrating effectiveness and out-of-domain generalization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf43f01b4afce8af27cc99730129e26bd5b170c90172ddf77134a48ec54cccb0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf43f01b4afce8af27cc99730129e26bd5b170c90172ddf77134a48ec54cccb0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes AutoForge, a framework to automate the synthesis of challenging simulated environments for training language-based agents via reinforcement learning. It introduces an environment-level RL algorithm to improve training stability and efficiency by handling simulated user instability and heterogeneous environments. Evaluations show the method is effective and generalizes well to out-of-domain tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The body is not there to compute: Comment on &quot;Informational embodiment: Computational role of information structure in codes and robots&quot; by Pitti et al</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [embodied cognition, robotics], [morphological computation, embodiment, information theory, passive dynamic walker]</p>
</li>
<li class="">
<p><strong>authors:</strong> Matej Hoffmann</p>
</li>
<li class="">
<p><strong>institution:</strong> Czech Technical University in Prague</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22868" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22868</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Critiques the application of computational and informational frameworks to biological and robotic bodies, arguing it is a misleading metaphor. 2. Distinguishes between the physical, non-computational role of body morphology and the metaphorical concept of &quot;morphological computation&quot;. 3. Proposes that the primary function of bodies is not to compute, challenging a core premise of the target article.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c58366db344796ab1e3ca689f71030b6079280073a3b1974c0cb683e87c3918c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c58366db344796ab1e3ca689f71030b6079280073a3b1974c0cb683e87c3918c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This commentary argues against the central thesis of a target article that applies computational and informational concepts to understand animal and robot bodies. The author contends that the concept of &quot;morphological computation&quot; is merely a metaphor and that the body&#x27;s main role is physical, not computational. The core conclusion is that bodies are not fundamentally for computing, challenging an informational embodiment perspective.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent reinforcement learning], [Reinforcement Networks, directed acyclic graph (DAG), credit assignment, LevelEnv, hierarchical RL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Maksim Kryzhanovskiy, Svetlana Glazyrina, Roman Ischenko, Konstantin Vorontsov</p>
</li>
<li class="">
<p><strong>institution:</strong> Lomonosov Moscow State University, Institute for Artificial Intelligence, Lomonosov Moscow State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22876" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22876</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Reinforcement Networks framework, a general approach for collaborative MARL that organizes agents as vertices in a directed acyclic graph (DAG)., 2. Formalizes training and inference methods for the framework and connects it to the LevelEnv concept for reproducible construction and evaluation., 3. Demonstrates improved performance over standard MARL baselines and unifies hierarchical, modular, and graph-structured views of MARL.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b613175c92e9bdddfbd57ed84d044a5846b7b8148cca8ab0405e69847f66c33a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b613175c92e9bdddfbd57ed84d044a5846b7b8148cca8ab0405e69847f66c33a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of end-to-end training for AI systems with multiple learnable components. It proposes Reinforcement Networks, a framework that organizes agents in a directed acyclic graph for flexible credit assignment and coordination in multi-agent reinforcement learning. The method shows improved performance over baselines and provides a principled foundation for designing complex multi-agent systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [multimodal fusion, text-guided segmentation, transformer-based architecture, lightweight model, 3D segmentation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hasan Faraz Khan, Noor Fatima, Muzammil Behzad</p>
</li>
<li class="">
<p><strong>institution:</strong> King Fahd University of Petroleum and Minerals, SDAIA-KFUPM Joint Research Center for Artificial Intelligence</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22878" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22878</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SwinTF3D, a lightweight multimodal fusion model for text-guided 3D medical image segmentation, integrating visual and linguistic representations. 2. Introduces an efficient fusion mechanism to align semantic text prompts with spatial structures in volumetric medical images. 3. Demonstrates competitive performance and significant efficiency gains on the BTCV dataset, offering a practical and interpretable paradigm for interactive clinical segmentation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/991651742357d8ab55dc27a29fa78a0c7b0f65e13d3fe1d6d260f8acea2238d9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/991651742357d8ab55dc27a29fa78a0c7b0f65e13d3fe1d6d260f8acea2238d9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes SwinTF3D, a lightweight multimodal model that uses a transformer-based visual encoder and a text encoder to perform text-guided 3D medical image segmentation. It achieves competitive accuracy on the BTCV dataset with low computational overhead, establishing a practical paradigm for interactive, resource-efficient clinical imaging.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [cyber resilience], [agentic AI, game theory, autonomous agents, system-theoretic framework, equilibrium-based design]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tao Li, Quanyan Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> City University of Hong Kong, New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22883" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22883</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a paradigm shift from prevention-centric security to agentic cyber resilience, arguing for systems that anticipate, maintain, recover, and learn under attack. 2. Develops a system-level framework and general architecture for designing AI workflows where autonomous agents participate in sensing, reasoning, and action. 3. Demonstrates how game-theoretic formulations provide a unifying design language for analyzing coupled attacker-defender workflows and enable equilibrium-based resiliency design, illustrated with case studies.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1d5f82743a059040190978c2a78338bb73c72bc9cec9a0aafe00a0d12f0f24d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1d5f82743a059040190978c2a78338bb73c72bc9cec9a0aafe00a0d12f0f24d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper argues that the rise of foundation-model-based AI necessitates a shift from traditional prevention-focused cybersecurity to a new paradigm of agentic cyber resilience. It proposes a system-theoretic framework for designing autonomous AI workflows and uses game theory as a unifying language to model attacker-defender dynamics, concluding that equilibrium-based design enables system-level resilience as demonstrated in case studies like automated penetration testing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] DECEPTICON: How Dark Patterns Manipulate Web Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [dark patterns, web agents, adversarial robustness, deceptive UI, agent testing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Phil Cuvin, Hao Zhu, Diyi Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22894" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22894</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://agentdarkpatterns.org" target="_blank" rel="noopener noreferrer" class="">https://agentdarkpatterns.org</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces DECEPTICON, a novel environment for testing dark patterns in isolation with 700 web navigation tasks, 2. Demonstrates that dark patterns successfully manipulate agent trajectories in over 70% of tasks, significantly higher than human susceptibility, 3. Shows that larger, more capable models are more susceptible to dark patterns, and existing countermeasures like in-context prompting and guardrail models fail to mitigate the risk effectively.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8998ab43971416f683709173f00be5e8d5373de89f15ac199ec42d645a75b8b6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8998ab43971416f683709173f00be5e8d5373de89f15ac199ec42d645a75b8b6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces DECEPTICON, a testing environment to evaluate how dark patterns manipulate web agents, revealing that these deceptive UI designs successfully steer agent actions in over 70% of tasks, with larger models being more vulnerable and current defenses ineffective. The findings highlight an urgent need for robust defenses against such manipulative designs in agent systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [benchmark evaluation], [scientific intelligence, hierarchical benchmark, multi-disciplinary evaluation, multimodal inputs, dependency-aware framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yaping Zhang, Qixuan Zhang, Xingquan Zhang, Zhiyuan Chen, Wenwen Zhuang, Yupu Liang, Lu Xiang, Yang Zhao, Jiajun Zhang, Yu Zhou, Chengqing Zong</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences; University of the Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22899" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22899</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces HiSciBench, a novel hierarchical benchmark spanning five levels (Scientific Literacy to Scientific Discovery) to evaluate the complete scientific workflow. 2. Provides a comprehensive, multi-disciplinary dataset of 8,735 instances across six scientific fields, supporting multimodal and cross-lingual inputs. 3. Establishes an integrated, dependency-aware evaluation framework that reveals significant performance gaps in foundation models, especially on higher-order discovery tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d6954386f880faf48b86cfbd5d72eb78413ee39a02fe0f600306713ecc9bda_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d6954386f880faf48b86cfbd5d72eb78413ee39a02fe0f600306713ecc9bda_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces HiSciBench, a hierarchical and multi-disciplinary benchmark designed to evaluate the full spectrum of scientific intelligence in foundation models, from basic literacy to creative discovery. It contains thousands of multimodal instances across six disciplines and uses a dependency-aware framework for evaluation. The evaluation of leading models shows a sharp performance decline on complex discovery tasks, highlighting a key capability gap and setting a new standard for assessing scientific AI.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Neural Network-Based Real-time Casing Collar Recognition System for Downhole Instruments</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [Casing Collar Locator (CCL), ARM Cortex-M7, Depthwise Separable Convolutions, MACs, Inference Latency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Si-Yu Xiao, Xin-Di Zhao, Xiang-Zhan Wang, Tian-Hao Mao, Ying-Kai Liao, Xing-Yu Liao, Yu-Qiao Chen, Jun-Jie Wang, Shuang Liu, Tu-Pei Chen, Yang Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China, China National Petroleum Corporation Logging Co., Ltd., Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22901" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22901</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an in-situ, real-time collar recognition system using embedded neural networks to overcome signal degradation in traditional surface-based monitoring. 2. Introduces lightweight &quot;Collar Recognition Nets&quot; (CRNs) optimized for resource-constrained ARM Cortex-M7 microprocessors, using temporal and depthwise separable convolutions. 3. Demonstrates a highly efficient model achieving 8,208 MACs, an F1 score of 0.972, and an average inference latency of 343.2 µs, proving feasibility for downhole power/space constraints.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/235a8bd15d5c0da93f1ea31dd4b6da44b238cc7be57fd42a7bef3e629f9c6495_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/235a8bd15d5c0da93f1ea31dd4b6da44b238cc7be57fd42a7bef3e629f9c6495_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of accurate downhole positioning in oil/gas operations by developing a real-time, embedded neural network system for casing collar recognition. The method introduces lightweight &quot;Collar Recognition Nets&quot; optimized for ARM Cortex-M7 processors, achieving high accuracy with minimal computational cost. The results demonstrate that robust, autonomous signal processing is feasible within the severe power and space limitations of downhole instrumentation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [hierarchical deep reinforcement learning, portfolio management, dynamic asset grouping, utility-based capital allocation, SHAP interpretability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiaotian Ren, Nuerxiati Abudurexiti, Zhengyong Jiang, Angelos Stefanidis, Hongbin Liu, Jionglong Su</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in provided content.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22895" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22895</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hierarchical DRL framework (SAMP-HDRL) that integrates dynamic asset grouping, upper-lower agent coordination, and a utility-based capital allocation mechanism for robust portfolio management. 2. Demonstrates superior performance through extensive backtests across multiple market regimes, showing consistent improvements in return and risk-adjusted metrics over traditional and DRL baselines. 3. Provides interpretability via SHAP analysis, revealing a complementary &quot;diversified + concentrated&quot; decision pattern across agent layers.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07244b408b9238d10b2d5561e0007db8732b1d4e9e79bda5477bef5db2dd385c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07244b408b9238d10b2d5561e0007db8732b1d4e9e79bda5477bef5db2dd385c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles portfolio optimization in non-stationary markets by proposing SAMP-HDRL, a hierarchical deep reinforcement learning framework that segments assets, coordinates global and local agents, and uses a utility-based capital allocator. The method outperforms numerous baselines in backtests, achieving higher returns and risk-adjusted ratios, and its decisions are made interpretable through SHAP analysis, revealing a combined diversified and concentrated investment strategy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Q-learning, ensemble learning, satisficing, distillation, bounded rationality]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ünver Çiftçi</p>
</li>
<li class="">
<p><strong>institution:</strong> Tekirdağ Namık Kemal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22910" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22910</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a two-phase framework (Sat-EnQ) that first trains an ensemble of lightweight Q-networks using a satisficing objective to limit early value growth and reduce variance. 2. Provides theoretical proof that the satisficing objective induces bounded updates and cannot increase target variance, with a corollary for substantial reduction. 3. Demonstrates empirical results including significant variance reduction, elimination of catastrophic failures, robustness to noise, and improved compute efficiency compared to baseline methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d12c2382ed5ee9e4da47d1775097d950b626f676e5d3552cd0ff19b6c385b2a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d12c2382ed5ee9e4da47d1775097d950b626f676e5d3552cd0ff19b6c385b2a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the instability of deep Q-learning, especially early in training, by introducing Sat-EnQ. This framework first trains a satisficing ensemble of weak Q-learners to produce stable, low-variance estimates, then distills and fine-tunes the ensemble. The method significantly improves training reliability, robustness, and computational efficiency compared to standard approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multimodal Fact-Checking: An Agent-based Approach</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal fact-checking], [multimodal misinformation, agent-based reasoning, explainable dataset, vision-language models, evidence retrieval]</p>
</li>
<li class="">
<p><strong>authors:</strong> Danni Xu, Shaojing Fan, Xuanang Cheng, Mohan Kankanhalli</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Singapore (NUS)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22933" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22933</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces RW-Post, a high-quality, explainable dataset for real-world multimodal fact-checking that aligns claims with original social media posts and provides detailed reasoning and evidence. 2. Proposes AgentFact, a novel agent-based multimodal fact-checking framework that emulates the human verification workflow through five specialized, collaboratively working agents. 3. Demonstrates that the synergy between the new dataset and the agent framework substantially improves both the accuracy and interpretability of multimodal fact-checking.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05bbe58d9ac10920f1b315029a664c297bd8051834b3724dbf3fa80f26372bec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05bbe58d9ac10920f1b315029a664c297bd8051834b3724dbf3fa80f26372bec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of automated multimodal fact-checking by introducing a new dataset (RW-Post) and an agent-based framework (AgentFact). The dataset provides real-world misinformation instances with reasoning and evidence, while the framework uses specialized agents to collaboratively perform verification tasks. The combined approach is shown to significantly enhance the accuracy and explainability of fact-checking systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Geometric Structural Knowledge Graph Foundation Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [knowledge graph reasoning], [structural foundation model, geometric attention, inductive link prediction, multi-head transformation, relational fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ling Xin, Mojtaba Nayyeri, Zahra Makki Nayeri, Steffen Staab</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Stuttgart, University of Southampton, Shahrood University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22931" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22931</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Gamma, a novel structural KG foundation model that replaces the single relational transformation with multiple parallel geometric transformations (real, complex, split-complex, dual). 2. Introduces a relational conditioned attention fusion mechanism with entropy regularization to adaptively fuse these geometric representations at the link level. 3. Provides a full formalization of the algebraic message functions and demonstrates through extensive experiments on 56 KGs that Gamma consistently outperforms the prior state-of-the-art (Ultra) in zero-shot inductive link prediction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1926af9f65861ace968c86c6c18e3eaa892e2114d0d505e3fce8a7ae39975f1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1926af9f65861ace968c86c6c18e3eaa892e2114d0d505e3fce8a7ae39975f1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies a key limitation in existing structural knowledge graph foundation models: their reliance on a single relational transformation limits their ability to capture diverse relational patterns. To address this, the authors propose Gamma, a new model that employs multi-head geometric attention, using parallel transformations from different algebraic spaces and a fusion mechanism to adaptively combine them. Comprehensive experiments show that Gamma outperforms the previous best model, Ultra, in zero-shot inductive link prediction across diverse benchmarks, demonstrating the benefit of complementary geometric representations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Heterogeneity in Multi-Agent Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent reinforcement learning], [heterogeneity, multi-agent reinforcement learning, parameter sharing, heterogeneity distance, dynamic algorithm]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tianyi Hu, Zhiqiang Pu, Yuan Wang, Tenghai Qiu, Min Chen, Xin Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22941" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22941</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Harry67Hu/HetDPS" target="_blank" rel="noopener noreferrer" class="">https://github.com/Harry67Hu/HetDPS</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a systematic categorization of heterogeneity in MARL into five types with mathematical definitions. 2. Defines a heterogeneity distance and introduces a practical method to quantify agent heterogeneity. 3. Designs a heterogeneity-based dynamic parameter sharing algorithm that demonstrates better interpretability and adaptability compared to baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de4448c413f180749bc7f2220bea2793dad9a358fb068164020bd7b0421e5b05_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de4448c413f180749bc7f2220bea2793dad9a358fb068164020bd7b0421e5b05_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of a rigorous definition and understanding of heterogeneity in multi-agent reinforcement learning (MARL). It proposes a methodology to define, quantify, and utilize heterogeneity, culminating in a dynamic parameter sharing algorithm. Experiments show this algorithm offers improved interpretability and adaptability over other parameter-sharing methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] APO: Alpha-Divergence Preference Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning from human feedback (rlhf)], [alpha-divergence, preference optimization, mode collapse, anchored coordinates, gradient variance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wang Zixian</p>
</li>
<li class="">
<p><strong>institution:</strong> China Mobile Communications Group Shandong Co., Ltd. Tai’an Branch</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22953" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22953</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces APO, an anchored framework using Csiszár alpha-divergence to continuously interpolate between forward and reverse KL behavior for RLHF. 2. Derives unified gradient dynamics parameterized by alpha and analyzes gradient variance properties. 3. Proposes a practical reward-and-confidence-guarded alpha schedule to transition from mode-covering to mode-seeking behavior safely.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a407212dc95985ef8918d58e7c65f70fd3f6adf8764c95f85871cd1924b3528_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a407212dc95985ef8918d58e7c65f70fd3f6adf8764c95f85871cd1924b3528_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the trade-off between stable but under-exploitative mode-covering updates and high-reward but unstable mode-seeking updates in LLM alignment. It proposes APO, an anchored preference optimization framework that uses alpha-divergence to smoothly interpolate between these regimes via a guarded schedule. Experiments show APO achieves competitive performance while maintaining training stability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D visual grounding], [open-world, zero-shot, active cognition-based reasoning, object lookup table, visual language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenyuan Huang, Zhao Wang, Zhou Wei, Ting Huang, Fang Zhao, Jian Yang, Zhenyu Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanjing University, China Mobile Zijin Innovation Institute</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23020" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23020</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes OpenGround, a novel zero-shot framework for open-world 3D visual grounding that overcomes the limitation of pre-defined object categories. 2. Introduces the Active Cognition-based Reasoning (ACR) module to progressively augment VLM cognition via a cognitive task chain and a dynamically updated Object Lookup Table (OLT). 3. Presents a new dataset named OpenTarget with over 7000 object-description pairs to evaluate open-world 3D grounding performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dca85890cab234049b1698ab9f6ade12ea02b16f297cec04cbcb907dcdffb7be_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dca85890cab234049b1698ab9f6ade12ea02b16f297cec04cbcb907dcdffb7be_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of existing 3D visual grounding methods that rely on a pre-defined object lookup table, which restricts their use in open-world scenarios. The authors propose OpenGround, a zero-shot framework featuring an Active Cognition-based Reasoning module that dynamically expands the model&#x27;s cognitive scope to handle undefined objects. The method achieves competitive or state-of-the-art results on standard benchmarks and shows a 17.6% improvement on their new OpenTarget dataset.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [multimodal language models], [multimodal sensing, time-series encoding, ecological momentary assessment (EMA)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenxuan Xu, Arvind Pillai, Subigya Nepal, Amanda C Collins, Daniel M Mackin, Michael V Heinz, Tess Z Griffin, Nicholas C Jacobson, Andrew Campbell</p>
</li>
<li class="">
<p><strong>institution:</strong> Dartmouth College, University of Virginia, Massachusetts General Hospital, Harvard Medical School</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23025" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23025</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces LENS, a framework that aligns multimodal sensing data with language models to generate clinically grounded mental health narratives. 2. Constructs a large-scale dataset of over 100,000 sensor-text QA pairs by transforming Ecological Momentary Assessment (EMA) responses. 3. Trains a patch-level encoder to project raw sensor time-series signals directly into an LLM&#x27;s representation space for native integration.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4b276805556458f16b63a7994f848b1c3a3a24eeed8ecb80496361d925fb9d8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4b276805556458f16b63a7994f848b1c3a3a24eeed8ecb80496361d925fb9d8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of translating long-duration, multimodal sensor data into interpretable natural language for mental health assessment. It proposes the LENS framework, which creates a large sensor-text dataset and trains a specialized encoder to align sensor signals with an LLM, enabling the generation of clinically meaningful narratives. The results show LENS outperforms baselines on NLP and clinical metrics, and is validated by mental health professionals.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] An Architecture-Led Hybrid Report on Body Language Detection Project</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video understanding], [vision-language models, structured generation, bounding boxes, mixture-of-experts, video analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Thomson Tong, Diba Darooneh</p>
</li>
<li class="">
<p><strong>institution:</strong> None</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23028" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23028</a></p>
</li>
<li class="">
<p><strong>code:</strong> BodyLanguageDetection repository [1]</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides an architecture-led analysis of two modern VLMs (Qwen2.5-VL-7B-Instruct and Llama-4-Scout-17B-16E-Instruct) for a practical task. 2. Maps model architectural properties to a concrete video-to-artifact pipeline for person detection and attribute extraction. 3. Explicitly defines and analyzes critical system constraints and limitations arising from model behavior, such as semantic vs. syntactic correctness and frame-local identifiers.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a618c048bc336ad2ade96a7a97cf301fb10fee2c9c8e7bc16556348f1c0c4b9d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a618c048bc336ad2ade96a7a97cf301fb10fee2c9c8e7bc16556348f1c0c4b9d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This report analyzes two vision-language models (VLMs) and connects their architectures to a practical system for detecting people and their emotions in video frames. The system prompts VLMs to generate structured outputs like bounding boxes, validates the output structure, and can render annotated videos. The core conclusion is that understanding model architecture is crucial for designing robust interfaces and making defensible claims, as VLMs can produce syntactically correct but semantically incorrect outputs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Viability and Performance of a Private LLM Server for SMBs: A Benchmark Analysis of Qwen3-30B on Consumer-Grade Hardware</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [quantization, mixture-of-experts, on-premise deployment, consumer-grade hardware, benchmark analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alex Khalil, Guillaume Heilles, Maria Parraga, Simon Heilles</p>
</li>
<li class="">
<p><strong>institution:</strong> UCLouvain, Universidad Espíritu Santo, DENEM Labs</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23029" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23029</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A comprehensive benchmarking framework for evaluating both the intrinsic model capabilities and the server-side performance (latency, throughput, scalability) of a private LLM deployment. 2. A practical demonstration and performance analysis of deploying a quantized, large-scale (30B parameter) Mixture-of-Experts model (Qwen3) on next-generation consumer-grade hardware (NVIDIA RTX 5090). 3. Evidence that a carefully configured on-premises LLM server can achieve performance comparable to cloud services, offering SMBs a viable, cost-effective, and privacy-preserving alternative.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed34c10397ed5cae19c39a4a8e2a5a1f0fd64e2f76183b8ba093c74b9a79fe51_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed34c10397ed5cae19c39a4a8e2a5a1f0fd64e2f76183b8ba093c74b9a79fe51_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the feasibility of deploying a private, high-performance LLM server for Small and Medium Businesses using consumer-grade hardware. It benchmarks a quantized Qwen3-30B model on an NVIDIA RTX 5090, evaluating both model capability and server performance under load. The results show that such an on-premises setup can achieve performance close to cloud services at a lower cost and with full data privacy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [interpretability], [chain-of-thought, faithfulness, causal mediation analysis, biasing features, explainability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kerem Zaman, Shashank Srivastava</p>
</li>
<li class="">
<p><strong>institution:</strong> UNC Chapel Hill</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23032" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23032</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Argues that the Biasing Features metric conflates unfaithfulness with incompleteness in Chain-of-Thought explanations. 2. Introduces a new faithful@k metric showing increased token budgets improve hint verbalization. 3. Uses Causal Mediation Analysis to show non-verbalized hints can still causally mediate predictions through the CoT.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/527440e442abe55ce371c5ad3ce8f49609f0398a6001b33523b6a3aa4bbc6e44_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/527440e442abe55ce371c5ad3ce8f49609f0398a6001b33523b6a3aa4bbc6e44_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper challenges the use of hint-verbalization metrics like Biasing Features for evaluating the faithfulness of Chain-of-Thought reasoning. It proposes that apparent unfaithfulness is often due to incompleteness from lossy compression and tight token limits, not a lack of alignment, and demonstrates this using new metrics and causal mediation analysis. The conclusion advocates for a broader interpretability toolkit beyond hint-based evaluations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [educational data mining], [knowledge tracing, learner modelling, temporal coherence, fine-tuning, deep knowledge tracing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Danial Hooshyar, Yeongwook Yang, Gustav Šíř, Tommi Kärkkäinen, Raija Hämäläinen, Mutlu Cukurova, Roger Azevedo</p>
</li>
<li class="">
<p><strong>institution:</strong> Tallinn University, University of Jyväskylä, Gangneung-Wonju National University, Czech Technical University, University College London, University of Central Florida</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23036" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23036</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a synthesis of evidence on the limitations of LLM-based tutors, framing them within the high-risk context of K-12 education and responsible AI design. 2. Empirically demonstrates that a Deep Knowledge Tracing (DKT) model significantly outperforms a widely-used LLM (both zero-shot and fine-tuned) in next-step correctness prediction and temporal coherence of mastery estimation. 3. Highlights the computational inefficiency of fine-tuning LLMs for this task compared to DKT, and argues for hybrid frameworks over LLM-only approaches for responsible tutoring.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5735716766e72627a0d5d23b01771e8d0161795e3958d394eccf1045f5a797ec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5735716766e72627a0d5d23b01771e8d0161795e3958d394eccf1045f5a797ec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether large language models (LLMs) can effectively replace traditional learner modelling for adaptive tutoring in K-12 education. By comparing a Deep Knowledge Tracing (DKT) model against a fine-tuned and zero-shot LLM on knowledge assessment tasks, it finds DKT is more accurate, reliable, and temporally coherent. The study concludes that LLMs alone are insufficient for responsible tutoring and advocates for hybrid systems that incorporate dedicated learner models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Reward Model Selection Crisis in Personalized Alignment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [alignment &amp; personalization], [reward-guided decoding, policy accuracy, Pref-LaMP benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Fady Rezk, Yuangang Pan, Chuan-Sheng Foo, Xun Xu, Nancy Chen, Henry Gouk, Timothy Hospedales</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Edinburgh, Agency for Science, Technology and Research (A*STAR)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23067" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23067</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and demonstrates the failure of standard reward model (RM) accuracy as a selection criterion for deployment-ready personalized alignment. 2. Introduces a new metric, policy accuracy, to evaluate the token-level discrimination ability of reward models under inference-time adaptation (reward-guided decoding). 3. Introduces Pref-LaMP, the first personalized alignment benchmark with ground-truth user completions, enabling direct behavioral evaluation and revealing a decoupling between reward discrimination and actual generation quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a0a1bd7d940db8c394f189ea84b7dbcfae7cd34b8e3662ea7c7a8babfdfefe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a0a1bd7d940db8c394f189ea84b7dbcfae7cd34b8e3662ea7c7a8babfdfefe_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies a crisis in personalized alignment, showing that optimizing reward models for preference ranking accuracy does not translate to effective behavioral adaptation under realistic deployment constraints like reward-guided decoding. The authors propose a new metric (policy accuracy) and a new benchmark (Pref-LaMP) to evaluate this gap, finding that reward model accuracy poorly predicts generation quality and that simple in-context learning often outperforms reward-guided methods for larger models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Trust Region Masking for Long-Horizon LLM Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [trust region, policy gradient, off-policy mismatch, KL divergence, sequence-level masking]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yingru Li, Jiacai Liu, Jiawei Xu, Yuxuan Tong, Ziniu Li, Baoxiang Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> (Institutions not explicitly listed in provided content; inferred from author names and common affiliations in the field, but not specified. Therefore, output is left blank.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23075" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23075</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Deriving two novel, tighter theoretical bounds (Pinsker-Marginal and Mixed) on the approximation error in off-policy LLM-RL, scaling better with sequence length than classical O(T^2) bounds. 2. Identifying that these bounds depend on a sequence-level quantity (maximum token-level KL divergence) that cannot be controlled by token-independent methods like PPO clipping. 3. Proposing the Trust Region Masking (TRM) algorithm, which masks entire sequences from gradient updates to enforce the trust region, providing non-vacuous monotonic improvement guarantees for long-horizon tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74d8872516a568f2933a83e36b0cf25d414f3a25ffa113cd0cee809d2b39c6ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74d8872516a568f2933a83e36b0cf25d414f3a25ffa113cd0cee809d2b39c6ac_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that classical trust region bounds become vacuous for long-horizon LLM reinforcement learning due to unavoidable off-policy mismatch. It proposes Trust Region Masking (TRM), a method that excludes entire sequences from gradient computation if any token violates a trust region constraint. This approach, supported by new tighter theoretical bounds, provides the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multimodal Functional Maximum Correlation for Emotion Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal learning], [self-supervised learning, dual total correlation, functional maximum correlation analysis, affective computing, physiological signals]</p>
</li>
<li class="">
<p><strong>authors:</strong> Deyang Zheng, Tianyi Zhang, Wenming Zheng, Shujian Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> Southeast University, Westlake University, Vrije Universiteit Amsterdam</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23076" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23076</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/DY9910/MFMC" target="_blank" rel="noopener noreferrer" class="">https://github.com/DY9910/MFMC</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel self-supervised learning framework (MFMC) that maximizes higher-order multimodal dependence using a Dual Total Correlation objective. 2. Derives a tight sandwich bound and optimizes it using a functional maximum correlation analysis-based trace surrogate to capture joint interactions. 3. Demonstrates state-of-the-art or competitive performance on affective computing benchmarks, showing robustness to inter-subject variability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369b23e1c7a17085940402e88d2347afb3386819a237c48ac347be73127aea2a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369b23e1c7a17085940402e88d2347afb3386819a237c48ac347be73127aea2a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of learning joint dynamics from scarce and subjective emotion labels by proposing a self-supervised learning framework called MFMC. It captures higher-order multimodal dependencies beyond pairwise alignment, leading to improved emotion recognition performance on physiological signal benchmarks. The results show significant accuracy gains, particularly in subject-independent settings, highlighting the method&#x27;s effectiveness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [reinforcement learning, training-inference mismatch, vocabulary pruning, gradient estimation, numerical stability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yingru Li, Jiawei Xu, Jiacai Liu, Yuxuan Tong, Ziniu Li, Tianle Cai, Ge Zhang, Qian Liu, Baoxiang Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> (Institutions not explicitly listed in provided content. Affiliation inference requires author list with affiliations or email domains, which are not present in the given text. Therefore, cannot be determined from the provided snippet.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23087" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23087</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proves that the training-inference mismatch in LLM RL has an asymmetric effect, where the bound on log-probability mismatch scales with (1-p), making low-probability &quot;tail&quot; tokens the primary source of instability. 2. Proposes a novel method to stabilize RL training by dynamically pruning the vocabulary to exclude the extreme tail tokens, trading large, biased mismatches for a small, bounded optimization bias. 3. Provides both empirical demonstration of stable training and a theoretical bound on the optimization bias introduced by the proposed vocabulary pruning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83b750895381feb238b14991a4015088fa8d05eb24ab0374082f6c25fb3ddd7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83b750895381feb238b14991a4015088fa8d05eb24ab0374082f6c25fb3ddd7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a fundamental training-inference mismatch in LLM reinforcement learning caused by differing numerical precision between high-throughput inference and stable training systems. To address this, the authors propose dynamically pruning low-probability &quot;tail&quot; tokens from the vocabulary during RL optimization, which stabilizes training by replacing large, biased errors with a small, bounded bias. Both theoretical analysis and empirical results support the effectiveness of this method.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MedSAM-based lung masking for multi-label chest X-ray classification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [MedSAM, lung segmentation, multi-label classification, chest X-ray, spatial prior]</p>
</li>
<li class="">
<p><strong>authors:</strong> Brayden Miao, Zain Rehman, Xin Miao, Siming Liu, Jianjie Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Missouri State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23089" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23089</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a segmentation-guided CXR classification pipeline that integrates a fine-tuned MedSAM model for lung region extraction. 2. Empirically demonstrates that the effect of lung masking is task-dependent and architecture-dependent, revealing a trade-off between abnormality classification and normal case screening. 3. Suggests that lung masking should be treated as a controllable spatial prior tailored to the model backbone and clinical objective, rather than a uniform preprocessing step.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78073436289f27d236dc3f5c9f70b55eb6480c3a7ea02e8c30b8eb1b8e59faa9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78073436289f27d236dc3f5c9f70b55eb6480c3a7ea02e8c30b8eb1b8e59faa9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a method that uses a fine-tuned MedSAM model to extract lung masks from chest X-rays to guide multi-label abnormality classification. The study finds that the impact of masking depends on the task and model architecture, with loose masking improving normal case screening while tight masking aids training efficiency. The conclusion is that lung masking should be a tunable spatial prior aligned with the specific clinical goal and model, not a fixed step.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [Imitation Learning, Reinforcement Learning, KL divergence, Dense Gradient, Sparse Gradient]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yingru Li, Ziniu Li, Jiacai Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in provided content.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23097" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23097</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Derives the exact gradient decomposition of a unified KL+reward objective into analytic Dense and sampled Sparse terms. 2. Provides an efficient logit-level gradient formula for GPU implementation. 3. Establishes mathematical equivalence to KL-regularized RLHF and discusses training curriculum implications.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c49fbea41eedc7a9f58604cc114a9246db61882fc20a851c8ec68a24ff6b343_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c49fbea41eedc7a9f58604cc114a9246db61882fc20a851c8ec68a24ff6b343_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a unified framework for fine-tuning LLMs that integrates Imitation Learning and Reinforcement Learning. It analyzes the gradient of a combined objective to decompose it into a token-level Dense Gradient and a long-horizon Sparse Gradient, enabling efficient implementation. The work clarifies its relationship to existing methods like RLHF and discusses practical training considerations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [reinforcement learning, vision-language model, supervised fine-tuning, generalization paradox, cross-dataset transferability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Armin Berger, Manuela Bergau, Helen Schneider, Saad Ahmad, Tom Anglim Lagones, Gianluca Brugnara, Martha Foltyn-Dumitru, Kai Schlamp, Philipp Vollmuth, Rafet Sifa</p>
</li>
<li class="">
<p><strong>institution:</strong> Fraunhofer IAIS, University of Bonn, Lamarr Institute, Department of Health Queensland, Griffith University, University Hospital Bonn</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23090" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23090</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced ChexReason, a resource-efficient vision-language model for medical imaging trained with an R1-style (SFT+GRPO) method using minimal data and compute. 2. Identified a fundamental tension where RL optimization (GRPO) improves in-distribution benchmark performance but significantly degrades cross-dataset generalization, a pattern also observed in high-resource models. 3. Discovered a generalization paradox where the SFT checkpoint uniquely improves cross-dataset performance, suggesting teacher-guided reasoning captures more institution-agnostic features than RL optimization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c87b0b1a571aa28f5dd9685e96b13ff3420ed36b0e1d569fff8b1394d564751f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c87b0b1a571aa28f5dd9685e96b13ff3420ed36b0e1d569fff8b1394d564751f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper investigates applying reinforcement learning (RL) to vision-language models for medical imaging, finding that while RL improves performance on the training benchmark, it harms the model&#x27;s ability to generalize to new datasets. The authors conclude that for clinical robustness, curated supervised fine-tuning may be more effective than aggressive RL optimization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] How Much Data Is Enough? Uniform Convergence Bounds for Generative &amp; Vision-Language Models under Low-Dimensional Structure</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [statistical learning theory], [uniform convergence, calibration, low-dimensional structure, vision-language models, sample complexity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Paul M. Thompson</p>
</li>
<li class="">
<p><strong>institution:</strong> Stevens Institute of Neuroimaging and Informatics, University of Southern California</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23109" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23109</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides finite-sample uniform convergence bounds for accuracy and calibration of VLM-induced classifiers under Lipschitz stability assumptions. 2. Derives sample complexity bounds that depend on the intrinsic/effective dimension of the embedding space, not the ambient dimension. 3. Offers spectrum-dependent bounds that explicitly link eigenvalue decay in embedding covariance to data requirements, explaining reliable generalization with fewer samples.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dca56fb1537f7d92cc10d66ba9adb9e3272fcc872fe5fdbf475ccf92cc17e24_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dca56fb1537f7d92cc10d66ba9adb9e3272fcc872fe5fdbf475ccf92cc17e24_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies when generative and vision-language models can achieve uniformly accurate and calibrated predictions with practical sample sizes. By assuming model outputs depend smoothly on a low-dimensional semantic representation, it derives finite-sample uniform convergence bounds for VLM-induced classifiers. The main conclusion is that sample complexity depends on intrinsic dimension and eigenvalue decay, providing a framework to assess data sufficiency for reliable biomedical predictions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] It&#x27;s a TRAP! Task-Redirecting Agent Persuasion Benchmark for Web Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [prompt injection], [prompt injection, web agents, social-engineering, benchmark, autonomous agents]</p>
</li>
<li class="">
<p><strong>authors:</strong> Karolina Korgul, Yushi Yang, Arkadiusz Drohomirecki, Piotr Błaszczyk, Will Howard, Lukas Aichberger, Chris Russell, Philip H.S. Torr, Adam Mahdi, Adel Bibi</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Oxford, SoftServe, Johannes Kepler University Linz</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23128" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23128</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Task-Redirecting Agent Persuasion Benchmark (TRAP) for evaluating prompt injection vulnerabilities in web-based LLM agents. 2. Provides a modular social-engineering injection framework for controlled experiments on high-fidelity website clones. 3. Demonstrates systemic vulnerabilities, showing agents are susceptible to injection in 25% of tasks on average, with small interface changes often doubling success rates.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c246d5b23e99374a1d754ec870b203d23f214abac92f8d20d849cc98d00e86ca_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c246d5b23e99374a1d754ec870b203d23f214abac92f8d20d849cc98d00e86ca_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the vulnerability of web-based LLM agents to prompt injection attacks, where hidden adversarial instructions can divert agents from their tasks. It introduces the TRAP benchmark, built on realistic website clones, to evaluate these vulnerabilities. The study finds significant susceptibility across models, revealing systemic, psychologically driven weaknesses in current agents.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning from human feedback (rlhf)], [preference optimization, direct preference optimization, self-reflection, invariance, bradley-terry model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yu Li, Tian Lan, Zhengling Qi</p>
</li>
<li class="">
<p><strong>institution:</strong> George Washington University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23126" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23126</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies two fundamental limitations of DPO: lack of invariance to modeling choices and theoretical suboptimality due to ignoring comparative information in pairwise data. 2. Proposes Intrinsic Self-reflective Preference Optimization (InSPO), a novel family of methods that derives a globally optimal policy conditioned on both context and alternative responses, formalizing self-reflection. 3. Theoretically demonstrates InSPO&#x27;s superiority over DPO/RLHF and its invariance properties, and practically shows it as a plug-and-play enhancement that improves win rates and length-controlled metrics without inference overhead.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4933654befdce9d244a4f36811432e84021ec775f760f24a3cb71dec1951db76_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4933654befdce9d244a4f36811432e84021ec775f760f24a3cb71dec1951db76_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies limitations in Direct Preference Optimization (DPO), such as its sensitivity to modeling choices and failure to use comparative data fully. It proposes InSPO, a method that conditions the policy on both the context and the alternative response to enable intrinsic self-reflection. Experiments show InSPO consistently improves model alignment and robustness as a plug-and-play enhancement to DPO-family algorithms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image synthesis], [diffusion model, disentangled representation, pathological residual, anatomical manifold, seam-aware fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jian Wang, Sixing Rong, Jiarui Xing, Yuling Xu, Weide Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Harvard Medical School, Northeastern University, Yale University, Nanchang University, Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23130" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23130</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified generative framework that reformulates MRI pathology synthesis as a disentangled additive deviation on a stable anatomical manifold. 2. Introduces a Deviation-Space Diffusion Model to learn the conditional distribution of pathological residuals, preserving global structure while modeling local variations. 3. Incorporates a seam-aware fusion strategy and an inference-time stabilization module to suppress boundary artifacts and ensure spatial coherence in synthesized lesions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b87a518aaabc4319ec5eba26d8ed0e23b50b1f611b88f2d8241ebecec605cc8c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b87a518aaabc4319ec5eba26d8ed0e23b50b1f611b88f2d8241ebecec605cc8c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes PathoSyn, a novel framework for synthesizing pathological MRI images by decomposing the task into deterministic anatomical reconstruction and stochastic modeling of pathological deviations using a diffusion model. This approach preserves anatomical integrity while generating realistic lesion heterogeneity. Evaluations show it outperforms existing baselines in perceptual realism and anatomical fidelity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Reservoir Computing inspired Matrix Multiplication-free Language Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [MatMul-free LM, reservoir computing, weight sharing, ternary quantization, MLGRU]</p>
</li>
<li class="">
<p><strong>authors:</strong> Takumi Shiratsuchi, Yuichiro Tanaka, Hakaru Tamukoh</p>
</li>
<li class="">
<p><strong>institution:</strong> Kyushu Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23145" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23145</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel language model architecture that integrates reservoir computing principles into a MatMul-free LM to reduce training costs. 2. Introduces techniques of partially fixing/sharing weights and inserting reservoir layers to obtain dynamic representations without extra training overhead. 3. Combines operations to reduce memory accesses, achieving reductions in parameters, training time, and inference time while maintaining performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b5c1771a82be40c7ba47d813ef33ce372e599bd79d60507444a618ff4e28d2c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b5c1771a82be40c7ba47d813ef33ce372e599bd79d60507444a618ff4e28d2c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high computational cost of large language models by proposing a matrix multiplication-free model enhanced with reservoir computing. The method fixes/shared weights in selected layers and inserts reservoir layers to reduce training overhead and memory accesses. Experiments show the approach reduces parameters by up to 19%, training time by 9.9%, and inference time by 8.0% while maintaining comparable performance to the baseline.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Why We Need a New Framework for Emotional Intelligence in AI</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [affective computing], [emotional intelligence, benchmark evaluation, affective AI, emotion theory, AI assessment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Max Parks, Kheli Atluru, Meera Vinod, Mike Kuniavsky, Jud Brewer, Sean White, Sarah Adler, Wendy Ju</p>
</li>
<li class="">
<p><strong>institution:</strong> Inflection AI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23163" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23163</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A critical review of existing emotional intelligence (EI) theories and their applicability to artificial systems. 2. An analysis of current benchmark frameworks for evaluating EI in AI, identifying their foundational shortcomings. 3. A proposal for new evaluation strategies to better measure relevant aspects of EI in AI systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afa72b09c5f0d3f095e827a81d95825630a6951253349696736df1091d38dd71_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afa72b09c5f0d3f095e827a81d95825630a6951253349696736df1091d38dd71_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper argues that current frameworks for assessing emotional intelligence (EI) in AI are inadequate because they lack a solid theoretical foundation on emotion and fail to distinguish between human-specific and AI-relevant EI components. The authors propose a new framework by first reviewing emotion theories to define EI applicable to AI, then critiquing existing benchmarks, and finally outlining improved evaluation strategies. The main conclusion is that a refined, theoretically-grounded framework is needed to properly evaluate EI capabilities in artificial systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [LLM planning, Monte Carlo Tree Search (MCTS), multi-agent architecture, symbolic reasoning, self-correction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yifan Zhang, Giridhar Ganapavarapu, Srideepika Jayaraman, Bhavna Agrawal, Dhaval Patel, Achille Fokoue</p>
</li>
<li class="">
<p><strong>institution:</strong> IBM T.J. Watson Research Center, Vanderbilt University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23167" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23167</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/IBM/SPIRAL" target="_blank" rel="noopener noreferrer" class="">https://github.com/IBM/SPIRAL</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces SPIRAL, a novel framework that embeds a cognitive architecture of three specialized LLM agents (Planner, Simulator, Critic) into an MCTS loop for planning. 2. Transforms MCTS from a brute-force search into a guided, self-correcting reasoning process by leveraging dense, semantic-aware feedback from the agents. 3. Demonstrates superior performance and token efficiency on benchmark datasets (e.g., DailyLifeAPIs) compared to Chain-of-Thought and other state-of-the-art planning agents.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c24b184565ce8e4c4a35d80f46a857779e111625dc4ea57e56333bef27bea7e6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c24b184565ce8e4c4a35d80f46a857779e111625dc4ea57e56333bef27bea7e6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of LLMs struggling with complex planning tasks due to linear reasoning and lack of self-correction. It proposes SPIRAL, a framework that integrates three specialized LLM agents into a Monte Carlo Tree Search loop to create a guided, reflective, and grounded planning process. The method significantly outperforms existing planning approaches in accuracy and efficiency on benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [adversarial attacks], [jailbreak attacks, large language models, adversarial prompting, equation solving, code completion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhen Liang, Hai Huang, Zhengkui Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang Sci-Tech University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23173" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23173</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/lzzzr123/Equacode" target="_blank" rel="noopener noreferrer" class="">https://github.com/lzzzr123/Equacode</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel multi-strategy jailbreak approach that combines mathematical equation solving and code completion to bypass LLM safety constraints. 2. Demonstrates high attack success rates (e.g., 91.19% on GPT series) with only a single query, outperforming single-strategy attacks. 3. Shows through ablation studies a strong synergistic effect between the equation and code modules, proving the multi-strategy approach is more effective than the sum of its parts.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3023ba644e0cdeddfd98604ca7c5871aceb213706aede21b77e0d35b95cf6d23_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3023ba644e0cdeddfd98604ca7c5871aceb213706aede21b77e0d35b95cf6d23_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces EquaCode, a multi-strategy jailbreak attack that transforms malicious intent into a mathematical problem and forces the LLM to solve it via code, diverting its focus from safety. The method achieves high success rates on various LLMs with a single query, and ablation studies confirm the synergistic benefit of combining equation-solving and code completion strategies.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative ai evaluation], [model belief, token-level probabilities, statistical efficiency, demand estimation, synthetic data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hongshen Sun, Juanjuan Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> MIT Sloan School of Management</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23184" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23184</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces and formalizes the concept of &quot;model belief,&quot; a novel measure derived from an LLM&#x27;s token-level probabilities to capture its belief distribution over choices in a single generation. 2. Proves that model belief is asymptotically equivalent to the mean of model choices but is a more statistically efficient estimator with lower variance and faster convergence, with analogous properties for smooth functions used in downstream applications. 3. Empirically demonstrates that model belief outperforms model choice in explaining and predicting ground-truth choices in practical, limited-run settings (e.g., demand estimation), reducing required computation by roughly a factor of 20.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/997c7eb2e35882ae4411dc7956b1f70a51835fa22d25bcd7ec8b5d6d1cb72413_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/997c7eb2e35882ae4411dc7956b1f70a51835fa22d25bcd7ec8b5d6d1cb72413_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the inefficiency of using single LLM outputs (&quot;model choice&quot;) by proposing &quot;model belief,&quot; a measure based on token-level probabilities that captures the model&#x27;s full belief distribution. The authors prove model belief is a more statistically efficient estimator than model choice and demonstrate its practical superiority in a demand estimation task, where it reduces the computation needed for accurate estimates by about 20 times.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ForCM: Forest Cover Mapping from Multispectral Sentinel-2 Image by Integrating Deep Learning with Object-Based Image Analysis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [semantic segmentation], [Object-Based Image Analysis (OBIA), Deep Learning, Sentinel-2, Forest Cover Mapping, UNet]</p>
</li>
<li class="">
<p><strong>authors:</strong> Maisha Haque, Israt Jahan Ayshi, Sadaf M. Anis, Nahian Tasnim, Mithila Moontaha, Md. Sabbir Ahmed, Muhammad Iqbal Hossain, Mohammad Zavid Parvez, Subrata Chakraborty, Biswajeet Pradhan, Biswajit Banik</p>
</li>
<li class="">
<p><strong>institution:</strong> BRAC University, Charles Sturt University, University of Technology Sydney</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23196" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23196</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes &quot;ForCM&quot;, a novel method that integrates Object-Based Image Analysis (OBIA) with various Deep Learning models for forest cover mapping. 2. Evaluates and compares the performance of multiple DL models (UNet, UNet++, ResUNet, AttentionUNet, ResNet50-Segnet) combined with OBIA against traditional OBIA. 3. Demonstrates the practical application of free tools like QGIS for accurate environmental mapping, achieving improved accuracy (up to 95.64%) over traditional methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/477043abcfb8b4e851144d00c2ae33596765e1b2a27375709fcbcfc01f79e3e5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/477043abcfb8b4e851144d00c2ae33596765e1b2a27375709fcbcfc01f79e3e5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes ForCM, a method for forest cover mapping that combines Object-Based Image Analysis with Deep Learning models like ResUNet and AttentionUNet using Sentinel-2 imagery. The results show that this integration significantly improves mapping accuracy compared to traditional OBIA alone, demonstrating the potential of accessible tools for environmental monitoring.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Not too long do read: Evaluating LLM-generated extreme scientific summaries</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [text summarization], [extreme summarization, TLDR, abstractive summarization, extractive summarization, dataset creation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhuoqi Lyu, Qing Ke</p>
</li>
<li class="">
<p><strong>institution:</strong> City University of Hong Kong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23206" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23206</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/netknowledge/LLM_summarization" target="_blank" rel="noopener noreferrer" class="">https://github.com/netknowledge/LLM_summarization</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces BiomedTLDR, a novel high-quality dataset of researcher-authored scientific TLDRs, curated from author annotations in bibliographies. 2. Evaluates the performance of popular open-weight LLMs in generating scientific TLDRs from paper abstracts. 3. Provides an analysis revealing that LLM-generated summaries tend to be more extractive (closer to the source text&#x27;s lexicon and structure) compared to more abstractive human-written summaries.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e22384bc3a440a2b34601b9d8ed0a9de58fdee4ff92f1d83db278778c4293a7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e22384bc3a440a2b34601b9d8ed0a9de58fdee4ff92f1d83db278778c4293a7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of high-quality datasets for evaluating LLMs in generating scientific extreme summaries (TLDRs) by introducing BiomedTLDR, a dataset of human-authored summaries. It then evaluates open-weight LLMs on this task and finds that, while some can produce human-like summaries, LLMs generally tend to be more extractive and less abstractive than human experts.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Exploring Syn-to-Real Domain Adaptation for Military Target Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [domain adaptation, synthetic-to-real, Unreal Engine, military target detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jongoh Jeong, Youngjin Oh, Gyeongrae Nam, Jeongeun Lee, Kuk-Jin Yoon</p>
</li>
<li class="">
<p><strong>institution:</strong> Korea Advanced Institute of Science and Technology (KAIST), LIG Nex1</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23208" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23208</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed generating a synthetic RGB dataset for military target detection using Unreal Engine to address the lack of real-world data. 2. Conducted and benchmarked synthetic-to-real domain adaptation experiments on a new train-val dataset pair for military targets. 3. Found that domain adaptation methods using minimal supervision (e.g., object class hints) substantially outperform unsupervised or semi-supervised methods in this challenging cross-domain setting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b67711a2133943d8083642ed36e63614aae288f161e8fc258230c5d03bacb3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b67711a2133943d8083642ed36e63614aae288f161e8fc258230c5d03bacb3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of military target detection by generating synthetic RGB data using Unreal Engine to overcome the lack of real datasets and high costs of SAR data. It benchmarks state-of-the-art domain adaptation methods on this synthetic-to-real task and finds that methods using minimal supervision achieve the best performance, highlighting remaining challenges in this area.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [LLM Ensemble, LLM-as-a-Judge, Peer-Review, Unsupervised Selection, Truth Inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhijun Chen, Zeyu Ji, Qianren Mao, Junhang Cheng, Bangjie Qin, Hao Wu, Zhuoran Li, Jingzheng Li, Kai Sun, Zizhe Wang, Yikun Ban, Zhu Sun, Xiangyang Ji, Hailong Sun</p>
</li>
<li class="">
<p><strong>institution:</strong> Beihang University, Zhongguancun Laboratory, Xi&#x27;an Jiaotong University, Hong Kong University of Science and Technology, Tsinghua University, Singapore University of Technology and Design</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23213" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23213</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LLM-PeerReview, a novel, peer-review-inspired, and interpretable framework for unsupervised LLM ensemble selection. 2. Introduces a three-stage process (scoring via LLM-as-a-Judge, reasoning via aggregation, and selection) that leverages multiple LLMs to evaluate each other&#x27;s responses. 3. Demonstrates strong empirical performance, with two variants significantly outperforming a recent advanced baseline (Smoothie-Global) on multiple datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/366f9e4fb3bf94aabbe40f3849a7637d6656821ec3dde88cd37d06effd3ed3f5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/366f9e4fb3bf94aabbe40f3849a7637d6656821ec3dde88cd37d06effd3ed3f5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes LLM-PeerReview, an unsupervised ensemble method that selects the best response from multiple LLM candidates. The method uses a peer-review process where LLMs score each other&#x27;s outputs, then aggregates these scores to make a final selection. The approach is shown to be simple and powerful, outperforming a strong baseline by a significant margin across several datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [ai evaluation], [thermal comfort, cognitive turing test, cross-modal reasoning, causal association, adaptive decision-making]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jingming Li</p>
</li>
<li class="">
<p><strong>institution:</strong> School of Civil Engineering and Architecture, Nanyang Normal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23217" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23217</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes TCEval, the first evaluation framework that uses thermal comfort scenarios to assess AI&#x27;s core cognitive capacities (cross-modal reasoning, causal association, adaptive decision-making). 2. Introduces a methodology using LLM agents with virtual personalities to generate and validate clothing and comfort feedback against real human databases (ASHRAE, Chinese Thermal Comfort Database). 3. Demonstrates the framework&#x27;s ecological validity as a Cognitive Turing Test, revealing that current LLMs have foundational cross-modal reasoning but lack precise causal understanding of nonlinear relationships in thermal comfort.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0080eb693040bd7b2c752a63de09a7937c8abe23c0a11752ee6d9fe7cdd04c7f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0080eb693040bd7b2c752a63de09a7937c8abe23c0a11752ee6d9fe7cdd04c7f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes TCEval, a novel evaluation framework that uses thermal comfort scenarios and LLM agents to assess AI&#x27;s cognitive abilities. The method involves simulating agent decisions and comparing them to human data from established comfort databases. The results show that while LLMs exhibit basic cross-modal reasoning, they lack a precise causal understanding of the complex factors in thermal comfort, validating TCEval as an ecologically valid cognitive test.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Holi-DETR: Holistic Fashion Item Detection Leveraging Contextual Information</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [Detection Transformer (DETR), Contextual Information, Holistic Detection, Fashion Item Detection, Co-occurrence Relationship]</p>
</li>
<li class="">
<p><strong>authors:</strong> Youngchae Kwon, Jinyoung Choi, Injung Kim</p>
</li>
<li class="">
<p><strong>institution:</strong> Handong Global University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23221" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23221</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Holi-DETR, a novel holistic detection framework for fashion items that leverages contextual information to reduce detection ambiguities., 2. Introduces a novel architecture that integrates three distinct types of contextual information (co-occurrence, inter-item spatial arrangements, and item-body keypoint relationships) into DETR-based models., 3. Demonstrates performance improvements over baseline models (vanilla DETR and Co-DETR) in terms of average precision (AP).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dda1eab87d833571a5feac46be0fc3ba879ccabde53c04f72e0d4fcae137cb6e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dda1eab87d833571a5feac46be0fc3ba879ccabde53c04f72e0d4fcae137cb6e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of fashion item detection, which is difficult due to diverse appearances and similar subcategories. The authors propose Holi-DETR, a holistic Detection Transformer that leverages three types of contextual information—co-occurrence, spatial arrangements, and body keypoints—to improve detection accuracy. The method shows improved performance over baseline DETR models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Anomaly Detection by Effectively Leveraging Synthetic Images</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [anomaly detection], [synthetic data, image-to-image translation, image retrieval, two-stage training, MVTec AD]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sungho Kang, Hyunkyu Park, Yeonho Lee, Hanbyul Lee, Mijoo Jeong, YeongHyeon Park, Injae Lee, Juneho Yi</p>
</li>
<li class="">
<p><strong>institution:</strong> Sungkyunkwan University, The University of Texas MD Anderson Cancer Center</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23227" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23227</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel framework that efficiently generates synthetic defect images by leveraging a pre-trained text-guided image-to-image translation model and an image retrieval model for filtering. 2. A two-stage training strategy that pre-trains on a large volume of rule-based synthetic images and then fine-tunes on a smaller set of high-quality generated images. 3. Demonstration of the approach&#x27;s effectiveness in reducing data collection costs while improving anomaly detection performance on the MVTec AD benchmark dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3546b17641c719167618772aa6e4da085e82a3b6d85cd2abc9940897d3e1f92_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3546b17641c719167618772aa6e4da085e82a3b6d85cd2abc9940897d3e1f92_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the trade-off in synthetic data generation for anomaly detection by proposing a framework that uses a pre-trained image-to-image translation model and an image retrieval filter to efficiently create realistic defect images. It also introduces a two-stage training strategy to leverage both cheap, low-quality and expensive, high-quality synthetic data effectively. Experiments on MVTec AD show this method reduces costs and improves detection performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [physics-inspired modeling, edge detection, content-adaptive routing, multi-scale feature fusion, infrared gas leak detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dongsheng Li, Chaobo Chen, Siling Wang, Song Gao</p>
</li>
<li class="">
<p><strong>institution:</strong> (Inferred from author names and arXiv handle; specific institution not provided in the given text. Could be a Chinese research institution or university.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23234" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23234</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a physics-inspired Gas Block module that models gas transport using a diffusion-convection unit with local and large-kernel branches, fused via an edge-gated module to enhance weak plume features. 2. Introduced a novel Adaptive Gradient and Phase Edge Operator (AGPEO) and a Multi-Scale Edge Perception Module (MSEPM) to compute and integrate reliable hierarchical edge priors for boundary reinforcement. 3. Designed a Content-Adaptive Sparse Routing Path Aggregation Network (CASR-PAN) that uses adaptive modulation to selectively propagate informative features across scales based on content and edge cues, improving efficiency and discriminability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee948d9a0589e5467502d1d916e59d51137b1253413c1001ade729584fd4bf55_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee948d9a0589e5467502d1d916e59d51137b1253413c1001ade729584fd4bf55_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes PEG-DRNet, a physics-inspired and edge-guided network for detecting faint infrared gas leaks. The method combines a gas transport model, a novel edge detection operator, and a content-adaptive routing mechanism for multi-scale feature fusion. Experiments show PEG-DRNet achieves superior accuracy and computational efficiency on benchmark datasets compared to existing detectors.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [gpu kernels], [agentic kernel coding, heterogeneous accelerators, retrieval-augmented prompt synthesis, graph-based search, Triton/CuTe DSL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gang Liao, Hongsen Qin, Ying Wang, Alicia Golden, Michael Kuchnik, Yavuz Yetim, Jia Jiunn Ang, Chunli Fu, Yihan He, Samuel Hsia, Zewei Jiang, Dianshi Li, Uladzimir Pashkevich, Varna Puvvada, Feng Shi, Matt Steiner, Ruichao Xiao, Nathan Yan, Xiayu Yu, Zhou Fang, Abdul Zainul-Abedin, Ketan Singh, Hongtao Yu, Wenyuan Chi, Barney Huang, Sean Zhang, Noah Weller, Zach Marine, Wyatt Cook, Carole-Jean Wu, Gaoxiang Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Meta Platforms</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23236" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23236</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes KernelEvolve, an agentic framework that automates kernel generation and optimization for DLRMs across heterogeneous hardware (NVIDIA/AMD GPUs, Meta accelerators) by operating at multiple programming abstractions. 2. Introduces a kernel optimization process modeled as a graph-based search with dynamic adaptation to runtime context via retrieval-augmented prompt synthesis and a persistent hardware knowledge base. 3. Demonstrates the system&#x27;s effectiveness by achieving 100% correctness on benchmark suites and substantial performance speedups (up to 17x) in production, reducing development time from weeks to hours and lowering the programmability barrier for new AI hardware.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/641ba327ba0d01461cd8fabad9a237e7b6667ce170be08aa3e89e6624ada0d38_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/641ba327ba0d01461cd8fabad9a237e7b6667ce170be08aa3e89e6624ada0d38_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents KernelEvolve, an agentic framework that automates the generation and optimization of compute kernels for deep learning recommendation models to address challenges posed by model, kernel, and hardware heterogeneity. The method uses a graph-based search process enhanced with retrieval-augmented prompts and operates across multiple programming abstractions. The system was validated on production models and benchmarks, showing significant performance improvements and reduced development time, effectively mitigating the programmability barrier for new AI accelerators.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [change detection], [vision-language model, remote sensing, semantic change detection, supervised fine-tuning, reinforcement learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xingwei Ma, Shiyang Feng, Bo Zhang, Bin Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Fudan University, Shanghai Artificial Intelligence Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23244" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23244</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ViLaCD-R1, a novel two-stage vision-language framework for semantic change detection in remote sensing, comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). 2. Introduces a training strategy for the VLM using supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks to generate a coarse change mask. 3. Demonstrates that the framework significantly improves semantic change recognition and localization while suppressing non-semantic variations, achieving state-of-the-art performance on multiple benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5871856f6e1854dd58df304f783ce8ea57314887e0296e446d48afb30805307_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5871856f6e1854dd58df304f783ce8ea57314887e0296e446d48afb30805307_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of existing remote sensing change detection methods, such as poor semantic understanding and inaccurate localization, by proposing ViLaCD-R1. This two-stage vision-language framework first uses a fine-tuned VLM to generate a coarse change mask from dual-temporal images, then refines it with a decoder to produce a precise change map. The method shows superior performance in recognizing true semantic changes and suppressing irrelevant variations across several benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [Sparse Autoencoders (SAEs), Low-Rank Adaptation (LoRA), Safety Alignment, Interpretability, Parameter-efficient Fine-tuning (PEFT)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dianyun Wang, Qingsen Ma, Yuhu Shang, Zhifeng Lu, Lechen Ning, Zhenbo Xu, Huijia Wu, Zhaofeng He</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing University of Posts and Telecommunications</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23260" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23260</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel method that uses pre-trained Sparse Autoencoders (SAEs) to construct an explicit, interpretable low-rank subspace for adapter initialization, addressing the black-box nature of traditional LoRA. 2. Provides theoretical analysis proving that SAE-based subspace identification achieves arbitrarily small recovery error under monosemanticity, while direct identification suffers an irreducible error floor due to polysemanticity. 3. Demonstrates state-of-the-art performance on safety alignment, achieving up to 99.6% safety rate while updating only 0.19-0.24% of parameters, and provides interpretable insights into the learned alignment subspace.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e615e6d561cef5b79dc991ed964fd9b6fb069427af26a4b7b42cd33cea4315a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e615e6d561cef5b79dc991ed964fd9b6fb069427af26a4b7b42cd33cea4315a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of interpretability in standard Low-Rank Adaptation (LoRA) methods for fine-tuning large language models. The proposed method leverages Sparse Autoencoders (SAEs) to identify task-relevant features in a disentangled space and uses them to construct an explicit, interpretable low-rank subspace for adapter initialization. The approach achieves superior safety alignment performance and provides transparency into the learned adaptation process.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [domain-specific foundation model, agentic physical ai, variance collapse, physics-based validation, policy distillation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yoonpyo Lee, Kazuma Kobayashi, Sai Puppala, Sajedul Talukder, Seid Koric, Souvik Chakraborty, Syed Bahauddin Alam</p>
</li>
<li class="">
<p><strong>institution:</strong> Hanyang University, University of Illinois Urbana-Champaign, Southern Illinois University, University of Texas at El Paso, National Center for Supercomputing Applications, Indian Institute of Technology Delhi</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23292" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23292</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new paradigm of Agentic Physical AI, where policy optimization is driven by physics-based outcome validation instead of perceptual inference, addressing the structural limitation of general-purpose models in control tasks. 2. Demonstrates that scaling data for a compact (360M parameter) model induces a sharp phase transition and variance collapse (&gt;500x reduction), leading to stable, execution-level behavior for safety-critical control. 3. Shows the model autonomously distills a robust policy (concentrating on a single strategy) and its learned representations transfer across different physics and input modalities without architectural changes, exhibiting early foundation-model properties.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb921102dfde629395ab8293510cb369a00c1199cadd4c88269dc076f8774a1a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb921102dfde629395ab8293510cb369a00c1199cadd4c88269dc076f8774a1a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a fundamental limitation of general-purpose AI models in safety-critical physical control tasks, where they prioritize semantic plausibility over physical correctness. To address this, it introduces Agentic Physical AI, a paradigm using compact language models trained with physics-based validation on synthetic nuclear reactor control data. The key finding is that sufficient data scaling induces a sharp variance collapse, stabilizing the model&#x27;s behavior and enabling it to autonomously distill a reliable control policy that generalizes across tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [medical image classification], [MedGemma, GPT-4, LoRA, zero-shot classification, multimodal LLM]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md. Sazzadul Islam Prottasha, Nabil Walid Rafi</p>
</li>
<li class="">
<p><strong>institution:</strong> Bangladesh University of Professionals</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23304" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23304</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a critical comparison between the open-source MedGemma and proprietary GPT-4 for zero-shot medical disease classification from images. 2. Demonstrated that the LoRA-fine-tuned MedGemma model significantly outperformed the untuned GPT-4 in accuracy and sensitivity for high-stakes clinical tasks. 3. Highlighted the essential role of domain-specific fine-tuning in minimizing hallucinations and enabling complex, evidence-based medical reasoning for clinical implementation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58abac32272c08efbbe249ec33f3b4f4aa3a6f4d477b241718ddbde679cce96a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58abac32272c08efbbe249ec33f3b4f4aa3a6f4d477b241718ddbde679cce96a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study compares the performance of the open-source MedGemma model and the proprietary GPT-4 for zero-shot classification of six diseases from medical images. The MedGemma model, fine-tuned with LoRA, achieved higher mean accuracy and sensitivity than GPT-4. The results show that domain-specific fine-tuning is crucial for reliable clinical applications, positioning MedGemma as a sophisticated tool for medical diagnostics.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [Lyapunov Optimization, Deep Reinforcement Learning, Edge-Cloud Partitioning, Transformer Decomposition, Queue Stability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abolfazl Younesi, Abbas Shabrang Maryan, Elyas Oustad, Zahra Najafabadi Samani, Mohsen Ansari, Thomas Fahringer</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Innsbruck, Sharif University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23310" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23310</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a fine-grained, adaptive partitioning framework (Splitwise) that decomposes transformer layers into attention heads and feed-forward sub-blocks, enabling exponentially more partition choices than layer-wise schemes. 2. Introduces a hierarchical DRL policy guided by Lyapunov optimization to jointly optimize latency, energy, and accuracy while guaranteeing queue stability under stochastic workloads and variable bandwidth. 3. Ensures robustness through partition checkpoints with exponential backoff recovery for communication failures, validated on real edge devices with large models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/121b71aa214f4a7c1671c9df76bf67a9cd64f3cb9e74186606a380ce86f7634f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/121b71aa214f4a7c1671c9df76bf67a9cd64f3cb9e74186606a380ce86f7634f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes Splitwise, a Lyapunov-assisted DRL framework for dynamically partitioning LLM inference between edge and cloud at a fine-grained sub-layer level. It aims to minimize latency and energy while maintaining accuracy under fluctuating network conditions. Experiments show Splitwise significantly reduces latency and energy consumption compared to existing methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [explainable ai (xai)], [inverse kinematics, shapley additive explanations (SHAP), InterpretML, obstacle avoidance, neural network]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sheng-Kai Chen, Yi-Ling Tsai, Chun-Chih Chang, Yan-Chen Chen, Po-Chiang Lin</p>
</li>
<li class="">
<p><strong>institution:</strong> Yuan Ze University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23312" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23312</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an explainability-centered workflow integrating SHapley Additive exPlanations (SHAP) with physics-based obstacle avoidance evaluation for neural inverse kinematics. 2. Introduces and trains two lightweight variants of IKNet (Improved IKNet with residual connections and Focused IKNet with position-orientation decoupling) on a synthetic dataset. 3. Demonstrates through simulation that neural IK architectures with more balanced feature importance attribution tend to maintain wider safety margins without sacrificing accuracy, linking XAI insights to robotic safety.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ab3055f2df7d03d7972b536c04fab2618a41cdd21ee682cf0f1ab9c0c6610f6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ab3055f2df7d03d7972b536c04fab2618a41cdd21ee682cf0f1ab9c0c6610f6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study addresses the lack of transparency in neural network-based inverse kinematics (IK) solvers by proposing an explainable AI workflow. It integrates SHAP analysis with physics-based simulation to evaluate two new IKNet variants on obstacle avoidance tasks. The key finding is that architectures with more evenly distributed feature importance achieve better safety performance, showing how XAI can guide the development of trustworthy robotic manipulation systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On Conformant Planning and Model-Checking of <span class="katex-error" title="ParseError: KaTeX parse error: Double superscript at position 3: ^*^̲*" style="color:#cc0000">^*^*</span> Hyperproperties</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [formal methods], [conformant planning, hyperproperties, model-checking, HyperLTL, ∃∗∀∗]</p>
</li>
<li class="">
<p><strong>authors:</strong> Raven Beutner, Bernd Finkbeiner</p>
</li>
<li class="">
<p><strong>institution:</strong> CISPA Helmholtz Center for Information Security</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23324" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23324</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Establishes a formal connection between conformant planning and model-checking of ∃∗∀∗ hyperproperties, showing they share the same computational core. 2. Provides an efficient, sound, and complete reduction from a hyperproperty model-checking instance to a conformant planning instance. 3. Demonstrates that every conformant planning problem is itself a hyperproperty model-checking task, establishing the converse direction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47343ca7bc4bf16389257261dd21e0c1fa42c0f512178576ff4ba501df841e8b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47343ca7bc4bf16389257261dd21e0c1fa42c0f512178576ff4ba501df841e8b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies and formalizes a deep connection between two seemingly unrelated problems: conformant planning (finding a robust sequential plan under uncertainty) and model-checking of ∃∗∀∗ hyperproperties (verifying system properties that relate multiple execution traces). The authors provide efficient, sound, and complete translations between instances of these two problems, showing they are essentially two sides of the same computational coin. This foundational link aims to enable cross-pollination of solution techniques between the planning and verification communities.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [agent evaluation], [spatial reasoning, long-horizon planning, partial observability, mental simulation, diagnostic benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Huan-ang Gao, Zikang Zhang, Tianwei Luo, Kaisen Yang, Xinzhe Juan, Jiahao Qiu, Tianxing Chen, Bingxiang He, Hao Zhao, Hao Zhou, Shilong Liu, Mengdi Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Princeton University, Shanghai Jiao Tong University &amp; University of Michigan, The University of Hong Kong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23328" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23328</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies three core cognitive challenges (spatial reasoning, long-horizon state tracking, active exploration under partial observation) hindering LLM agents in the physical world. 2. Introduces CubeBench, a novel generative benchmark based on the Rubik&#x27;s Cube with a three-tiered diagnostic framework to isolate and evaluate these capabilities. 3. Provides a diagnostic framework using external solver tools to analyze failure modes and reveals critical limitations of leading LLMs, including a 0.00% pass rate on long-horizon tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/210465a4bf9048c43ec900e17f922e63394d83664c6fe631fec0d54577fd9fb6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/210465a4bf9048c43ec900e17f922e63394d83664c6fe631fec0d54577fd9fb6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces CubeBench, a diagnostic benchmark using a Rubik&#x27;s Cube to evaluate LLM agents&#x27; spatial reasoning and long-horizon planning under partial observation. It employs a three-tiered framework from full symbolic to partial visual states. Experiments show leading LLMs fail completely on long-horizon tasks, highlighting a fundamental gap for physical-world deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scaling laws], [scaling laws, model ensembling, multi-model collaboration, cross-entropy loss, parameter budget]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dakuan Lu, Jiaqi Zhang, Cheng Yuan, Jiawei Shao, Chi Zhang, Xuelong Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Artificial Intelligence (TeleAI), China Telecom</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23340" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23340</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the &quot;Law of Multi-model Collaboration,&quot; a novel scaling law for predicting the performance limits of LLM ensembles based on aggregated parameters. 2. Establishes a method-agnostic theoretical framework using an idealized integration oracle to quantify the intrinsic upper bound of multi-model collaboration. 3. Empirically demonstrates that multi-model systems follow a power-law scaling with better trends and lower loss floors than single models, and that heterogeneous ensembles outperform homogeneous ones.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68212ad5f9cd50ef959cdd80f4b7274178d9a6b124904010fc5b0cf0834b21a1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68212ad5f9cd50ef959cdd80f4b7274178d9a6b124904010fc5b0cf0834b21a1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of a theoretical framework for scaling in multi-model LLM systems. It proposes the &quot;Law of Multi-model Collaboration,&quot; a scaling law based on aggregated parameters, and finds that ensembles scale better and achieve lower loss than single models, with diversity being a key driver of gains.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [memory systems, cognitive neuroscience, LLM-driven agents, memory security, multimodal memory]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiafeng Liang, Hao Li, Chang Li, Jiaqi Zhou, Shixin Jiang, Zekun Wang, Changkai Ji, Zhihao Zhu, Runxuan Liu, Tao Ren, Jinlan Fu, See-Kiong Ng, Xia Liang, Ming Liu, Bing Qin</p>
</li>
<li class="">
<p><strong>institution:</strong> Harbin Institute of Technology, Fudan University, Peking University, National University of Singapore</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23343" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23343</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/AgentMemory/Huaman-Agent-Memory" target="_blank" rel="noopener noreferrer" class="">https://github.com/AgentMemory/Huaman-Agent-Memory</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a systematic synthesis and comparative analysis of memory systems from cognitive neuroscience to LLM-driven autonomous agents. 2. Reviews mainstream benchmarks for evaluating agent memory and explores memory security from attack and defense perspectives. 3. Envisions future research directions, focusing on multimodal memory systems and skill acquisition.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15773eb4c52c63f2641be869baf3af4b7f6bb74f6e36c67247957bfbd039e9b6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15773eb4c52c63f2641be869baf3af4b7f6bb74f6e36c67247957bfbd039e9b6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This survey paper bridges the interdisciplinary gap between cognitive neuroscience and AI by systematically analyzing memory systems for autonomous agents. It compares biological and artificial memory taxonomies, storage, and management, while also reviewing evaluation benchmarks and security issues. The work concludes by outlining future directions, including multimodal memory and skill learning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ECG-RAMBA: Zero-Shot ECG Generalization by Morphology-Rhythm Disentanglement and Long-Range Modeling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [medical signal processing], [ECG classification, morphology-rhythm disentanglement, Mamba, zero-shot generalization, Power Mean pooling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hai Duong Nguyen, Xuan-The Tran</p>
</li>
<li class="">
<p><strong>institution:</strong> HAI-Smartlink Research Lab (Anchi STE Company), Vietnam Maritime University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23347" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23347</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ECG-RAMBA, a framework that explicitly disentangles ECG morphology (via MiniRocket) and rhythm (via HRV descriptors) before fusing them for robust classification. 2. Introduces a numerically stable Power Mean pooling operator (Q=3) for windowed inference to emphasize high-evidence segments. 3. Demonstrates strong zero-shot cross-dataset generalization for ECG classification using a bi-directional Mamba backbone for long-range contextual modeling.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f9b49242e586cadf1889fa40730ea1652c1b4ef5b15cf15697b58832c4dbf6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f9b49242e586cadf1889fa40730ea1652c1b4ef5b15cf15697b58832c4dbf6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of poor generalization of deep learning models for ECG classification across different datasets. It proposes ECG-RAMBA, a method that separates and then fuses morphological and rhythm features, using a Mamba backbone and a novel pooling operator. The results show that this approach achieves robust zero-shot performance on external datasets, outperforming a baseline model.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AGRO-SQL: Agentic Group-Relative Optimization with High-Fidelity Data Synthesis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [text-to-sql], [Reinforcement Learning, Data Synthesis, Policy Optimization, Semantic-Logic Alignment, Group Relative Policy Optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Cehua Yang, Dongyu Xiao, Junming Lin, Yuyang Song, Hanxu Yan, Shawn Guo, Wei Zhang, Jian Yang, Mingjie Tang, Bryan Dai</p>
</li>
<li class="">
<p><strong>institution:</strong> Sichuan University, IQuest Research, Beihang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23366" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23366</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an iterative data factory for synthesizing high-quality, RL-ready Text-to-SQL data with strict semantic-logic verification. 2. Introduces a novel Agentic Reinforcement Learning framework featuring a Diversity-Aware Cold Start stage and Group Relative Policy Optimization (GRPO). 3. Demonstrates state-of-the-art performance on the BIRD and Spider benchmarks through the synergistic combination of data-centric and model-centric approaches.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6740c1fc529b82b509bd38c2a7b5fb405b969bc5c3e11e6e0b7690e7fa791c85_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6740c1fc529b82b509bd38c2a7b5fb405b969bc5c3e11e6e0b7690e7fa791c85_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of data scarcity and limited reasoning in Text-to-SQL systems. It proposes a holistic framework that combines a data-centric approach for synthesizing high-fidelity training data with a model-centric approach using a novel Agentic Reinforcement Learning method called Group Relative Policy Optimization. The method achieves state-of-the-art results on major benchmarks, showing the effectiveness of the synergistic approach.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [post-training quantization, W8A8, W4A8, Ascend NPU, Chain-of-Thought (CoT)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yilun Luo, HuaQing Zheng, Haoqian Meng, Wenyuan Liu, Peng Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tianjin University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23367" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23367</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a unified low-bit inference framework for openPangu-Embedded models, supporting INT8 (W8A8) and W4A8 quantization optimized for the Atlas A2 Ascend NPU. 2. Provides a comprehensive evaluation of quantization across three distinct CoT reasoning modes (slow_think, auto_think, no_think) on code generation benchmarks (HumanEval, MBPP). 3. Demonstrates that INT8 quantization preserves over 90% of FP16 accuracy with a 1.5x prefill speedup, while W4A8 significantly reduces memory consumption, enabling efficient CoT reasoning on edge NPUs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07e9cf3e7e8252aa7eae3fdf2e7647007d4786dd6ade9f5c4e940d1c74c4e2cd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07e9cf3e7e8252aa7eae3fdf2e7647007d4786dd6ade9f5c4e940d1c74c4e2cd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high memory and latency overhead of deploying Chain-of-Thought (CoT) enabled openPangu models on Ascend NPUs by applying post-training quantization (INT8 and W4A8). The proposed framework, optimized for the Atlas A2 hardware, maintains high accuracy for INT8 and reduces memory for W4A8, enabling efficient on-device CoT reasoning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SoulX-LiveTalk Technical Report</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [Self-correcting Bidirectional Distillation, Multi-step Retrospective Self-Correction, hybrid sequence parallelism, Parallel VAE, kernel-level optimizations]</p>
</li>
<li class="">
<p><strong>authors:</strong> Le Shen, Qiao Qian, Tan Yu, Ke Zhou, Tianhang Yu, Yu Zhan, Zhenjie Wang, Ming Tao, Shunshun Yin, Siyuan Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Soul AI Lab, Donghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23379" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23379</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://soul-ailab.github.io/soulx-livetalk/" target="_blank" rel="noopener noreferrer" class="">https://soul-ailab.github.io/soulx-livetalk/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a Self-correcting Bidirectional Distillation strategy that retains bidirectional attention within video chunks to preserve spatiotemporal correlations and enhance visual fidelity. 2. Proposed a Multi-step Retrospective Self-Correction Mechanism to ensure stability during infinite generation by enabling autonomous recovery from accumulated errors. 3. Engineered a full-stack inference acceleration suite with hybrid sequence parallelism, Parallel VAE, and kernel-level optimizations to achieve real-time performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d6b1bb994b3c3273da207d2f19494d99c1ff6bf6663f41b1d85b2f0c69b83bb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d6b1bb994b3c3273da207d2f19494d99c1ff6bf6663f41b1d85b2f0c69b83bb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of deploying large diffusion models for real-time, audio-driven avatar generation by introducing SoulX-LiveTalk, a 14B-parameter framework. It employs a bidirectional distillation strategy and a self-correction mechanism to maintain high visual quality and stability, while a suite of inference optimizations enables sub-second latency and 32 FPS throughput, setting a new standard for interactive digital humans.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [log anomaly detection], [collaborative transformers, multi-head impressed attention, modality adaptation layer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mohammad Nasirzadeh, Jafar Tahmoresnezhad, Parviz Rashidi-Khazaee</p>
</li>
<li class="">
<p><strong>institution:</strong> Urmia University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23380" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23380</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/your-repo/CoLog" target="_blank" rel="noopener noreferrer" class="">https://github.com/your-repo/CoLog</a> (Note: The provided text states &quot;We also provide the implementation of CoLog atthis https URL.&quot; but the specific URL is cut off in the input. Based on the placeholder, the typical format is used. If the exact URL is required, it would be the one following &quot;atthis&quot; in the original text.)</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CoLog, a unified framework for detecting both point and collective anomalies in OS logs by applying multimodal sentiment analysis concepts. 2. Introduces collaborative transformers and multi-head impressed attention to learn interactions between different log data modalities. 3. Incorporates a modality adaptation layer to handle heterogeneity and adapt representations from different log modalities.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of log anomaly detection, where existing methods struggle with the multimodal nature of log data and the interactions between these modalities. It proposes CoLog, a framework that uses collaborative transformers and a modality adaptation layer to learn nuanced patterns across log modalities for comprehensive anomaly detection. Extensive experiments show CoLog achieves state-of-the-art performance, with mean precision, recall, and F1 scores over 99.5% across seven benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [AI Security], [AI supply chain, security taxonomy, distilBERT classifier]</p>
</li>
<li class="">
<p><strong>authors:</strong> Anh Nguyen, Triet Huynh Minh Le, M. Ali Babar</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Adelaide</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23385" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23385</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a pipeline combining keyword matching with a fine-tuned distilBERT classifier to identify 312,868 security discussions from Hugging Face and GitHub. 2. Conducted a thematic analysis to create a fine-grained taxonomy of 32 security issues and 24 solutions across four themes (System/Software, External Tools/Ecosystem, Model, Data). 3. Provided empirical insights revealing that security issues stem from complex dependencies and black-box AI components, with Model and Data challenges often lacking concrete solutions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c00453e76598d08a965d2a15fe6e7b197cf1f19518d88f46a334b638da6327dc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c00453e76598d08a965d2a15fe6e7b197cf1f19518d88f46a334b638da6327dc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates security issues in the AI supply chain by analyzing developer discussions from Hugging Face and GitHub. The authors use a keyword and classifier pipeline to build a large dataset and perform a thematic analysis to create a taxonomy of issues and solutions. They conclude that many security problems arise from dependencies and the black-box nature of AI, with solutions for Model and Data issues being particularly scarce.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Theoretical Foundations of Scaling Law in Familial Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [familial models, scaling law, early exiting, IsoFLOP design, compute-optimal training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Huan Song, Qingfei Zhao, Ting Long, Shuyu Tian, Hongjun An, Jiawei Shao, Chi Zhang, Xuelong Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Artificial Intelligence (TeleAI), China Telecom</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23407" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23407</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Theoretically and empirically extends the neural scaling law to the &quot;familial models&quot; paradigm by introducing granularity (G) as a new fundamental scaling variable alongside model size (N) and tokens (D). 2. Proposes a rigorous IsoFLOP experimental design to decouple architectural impact from computational scale, enabling high-fidelity parameterization of the unified scaling law L(N, D, G). 3. Quantifies that the granularity penalty follows a multiplicative power law with an extremely small exponent (γ≈0.041), validating the &quot;train once, deploy many&quot; paradigm without compromising compute-optimality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc66a2a88c82327d2e67ccabca47fcc7a15e81e139a0ed0135b0f3ea93534985_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc66a2a88c82327d2e67ccabca47fcc7a15e81e139a0ed0135b0f3ea93534985_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of traditional neural scaling laws, which assume a single model, by extending them to familial models that generate multiple sub-models from one backbone. The authors propose a unified scaling law incorporating granularity (G) and validate it using a rigorous IsoFLOP experimental design. The key finding is that the performance penalty for increased granularity is very small, proving that deployment flexibility can be achieved efficiently.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [tool-integrated reasoning, multimodal chain-of-thought, interleaved thinking]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiawei Chen, Xintian Shen, Lihao Zheng, Zhenwei Shao, Hongyuan Zhang, Pengfei Yu, Xudong Rao, Ning Mao, Xiaobo Liu, Lian Wen, Chaoqun Du, Feng Gu, Wei He, Qizhen Li, Shanshan Li, Zide Liu, Jing Luo, Lifu Mu, Xuhao Pan, Chang Ren, Haoyi Sun, Qian Wang, Wei Wang, Hongfu Yang, Jiqing Zhan, Chunpeng Zhou, Zheng Zhou, Hao Ma, Tao Wei, Pan Zhou, Wei Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Li Auto Inc</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23412" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23412</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/TIMMY-CHAN/MindWatcher" target="_blank" rel="noopener noreferrer" class="">https://github.com/TIMMY-CHAN/MindWatcher</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MindWatcher, a TIR agent with interleaved thinking and multimodal CoT reasoning for autonomous tool invocation and coordination. 2. Constructs the MWE-Bench benchmark and releases high-quality datasets and distilled smaller models (2B, 3B, 4B). 3. Designs a more efficient training infrastructure to enhance training speed and hardware utilization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/36f0f229f533ecc1b9565a72c5c77b232eab32880c12545774f94ebd2a19e651_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/36f0f229f533ecc1b9565a72c5c77b232eab32880c12545774f94ebd2a19e651_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces MindWatcher, a multimodal tool-integrated reasoning agent that uses interleaved thinking and chain-of-thought reasoning to autonomously decide when and how to invoke tools. It is equipped with auxiliary tools and a local image database to handle broad-domain problems. Experiments show it matches or exceeds larger models in performance and provides insights like the genetic inheritance phenomenon in agent training.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Directly Constructing Low-Dimensional Solution Subspaces in Deep Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [intrinsic dimension, low-rank approximation, subspace-native distillation, weight matrices, empirical spectral density]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yusuf Kalyoncuoglu</p>
</li>
<li class="">
<p><strong>institution:</strong> RWTH Aachen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23410" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23410</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a constructive method to decouple solution geometry from the ambient search space, bypassing the non-convex optimization bottleneck. 2. Empirically demonstrates significant compression (e.g., factor of 16) of classification heads in models like ResNet-50, ViT, and BERT with minimal performance loss. 3. Introduces &quot;Subspace-Native Distillation&quot; as a novel paradigm to provide a stable geometric coordinate system for student models, enabling &quot;Train Big, Deploy Small&quot;.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d34f960cfbb8a9db281aca58a1b30934c42fc68964647e8066a3754aeb92d38_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d34f960cfbb8a9db281aca58a1b30934c42fc68964647e8066a3754aeb92d38_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the redundancy in large neural networks by proposing a method to directly construct low-dimensional solution subspaces, decoupling the solution geometry from the high-dimensional optimization search space. It shows that classification heads can be heavily compressed without significant performance drops. This leads to a new distillation paradigm that allows student models to learn in a stable, low-dimensional subspace, potentially realizing efficient deployment of compact models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [continual learning], [big world hypothesis, computationally-embedded agent, interactivity, partially observable Markov decision process, model-based reinforcement learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alex Lewandowski, Adtiya A. Ramesh, Edan Meyer, Dale Schuurmans, Marlos C. Machado</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Alberta, Amii, The Swiss AI Lab IDSIA, USI &amp; SUPSI, Canada CIFAR AI Chair, Google DeepMind</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23419" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23419</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a computationally-embedded perspective, representing an agent as an automaton simulated within a universal computer, proving it&#x27;s equivalent to interacting with a POMDP over an infinite state-space. 2. Proposed a new objective called &quot;interactivity&quot; to measure an agent&#x27;s ability to continually adapt its behavior by learning new predictions. 3. Developed a model-based RL algorithm for interactivity-seeking and constructed a synthetic problem to evaluate continual learning, finding deep linear networks outperform nonlinear ones in sustaining interactivity as capacity scales.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1eff9d76987d2bc49a07c8de307183661687471b7fe4f21ca75040ba3e1de25a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1eff9d76987d2bc49a07c8de307183661687471b7fe4f21ca75040ba3e1de25a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a computationally-embedded perspective to formalize the &quot;big world hypothesis&quot; in continual learning, where an agent is modeled as an automaton within the environment. It introduces &quot;interactivity&quot; as a new objective and a corresponding model-based RL algorithm to seek it. The main finding is that, in their synthetic evaluation, deep linear networks sustain higher interactivity as capacity increases, whereas deep nonlinear networks struggle.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [gpu kernels], [kernel generation, multi-agent system, domain-specific languages (DSLs), performance tuning, Triton]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jinye Du, Quan Yuan, Zuyao Zhang, Yanzhi Yi, Jiahui Hu, Wangyi Chen, Yiyang Zhu, Qishui Zheng, Wenxiang Zou, Xiangyu Chang, Zuohe Zheng, Zichun Ye, Chao Liu, Shanni Li, Renwei Zhang, Yiping Deng, Xinwei Hu, Xuefeng Jin, Jie Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Huawei Technologies Co., Ltd., Hunan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23424" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23424</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed AKG kernel agent, a multi-agent framework that automates the generation, migration, and performance tuning of computational kernels for diverse hardware platforms. 2. Designed the system to support multiple Domain-Specific Languages (DSLs) like Triton, TileLang, CPP, and CUDA-C, enabling cross-platform portability and correctness. 3. Demonstrated the system&#x27;s effectiveness through evaluation on KernelBench, achieving an average 1.46x speedup over PyTorch Eager baselines on GPU and NPU backends.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40d5942348375e7b86203ab7a7420bba7105494296f349fdd48174b020a1527e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40d5942348375e7b86203ab7a7420bba7105494296f349fdd48174b020a1527e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes AKG kernel agent, a multi-agent framework that automates the development and optimization of high-performance computational kernels for modern AI workloads across diverse hardware. The system supports multiple DSLs for portability and uses LLMs for code generation and tuning. Evaluation shows it achieves a 1.46x average speedup over baseline implementations, effectively accelerating kernel development.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image classification], [convolutional neural networks, fuzzy logic, road surface classification, intelligent transport systems, data fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mustafa Demetgul, Sanja Lazarova Molnar</p>
</li>
<li class="">
<p><strong>institution:</strong> Karlsruhe Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23436" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23436</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a real-time system for road surface classification by fusing weather-conditional data and road condition data. 2. Compares the performance of multiple deep learning CNNs (AlexNet, LeNet, VGG, ResNet) on both image-based and acceleration-data-as-image classification tasks. 3. Introduces the use of fuzzy logic to classify road surfaces according to environmental factors like weather and time of day, using sensor data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d74437ab4a94c7ac8b2148bba4e1b6dd8d4706d55db8a2ae146a41ace94ef9f9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d74437ab4a94c7ac8b2148bba4e1b6dd8d4706d55db8a2ae146a41ace94ef9f9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a real-time system for road surface condition monitoring. It employs deep learning CNNs to classify road types from images and acceleration data, achieving over 95% accuracy, and suggests using fuzzy logic to incorporate weather and time-of-day factors. The work aims to enhance vehicle safety and autonomous driving systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [DistilHuBERT, 8-bit quantization, cross-corpus validation, Leave-One-Session-Out (LOSO), model compression]</p>
</li>
<li class="">
<p><strong>authors:</strong> Saifelden M. Ismail</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology, Zewail City</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23435" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23435</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a mobile-efficient SER system using a distilled and 8-bit quantized DistilHuBERT model, achieving a 92% parameter reduction and a 23 MB footprint. 2. Demonstrates that cross-corpus training with CREMA-D enhances generalization on IEMOCAP, improving accuracy and reducing variance. 3. Provides an analysis of cross-corpus evaluation on RAVDESS, revealing a &quot;theatricality effect&quot; where predictions cluster by arousal, and establishes a Pareto-optimal trade-off between model size and accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0945944056b62e3ffa17bd0a2e4e36ebfe24da404a4aea1692a6806374437b15_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0945944056b62e3ffa17bd0a2e4e36ebfe24da404a4aea1692a6806374437b15_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of deploying Speech Emotion Recognition (SER) on mobile devices by proposing a system based on the compressed DistilHuBERT model. Through rigorous cross-validation and cross-corpus training, the method achieves a good balance between a small model size (23 MB) and competitive accuracy, enabling practical on-device affect recognition while analyzing generalization challenges across different emotional speech corpora.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [hallucination mitigation, coarse-to-fine conditioning, Wasserstein fusion, generative feedback, training-free decoding]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zongsheng Cao, Yangfan He, Anran Liu, Jun Xie, Feng Chen, Zepeng Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Lenovo (PCIE), University of Minnesota (UMN)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23453" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23453</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/AI-Researcher-Team/CoFi-Dec" target="_blank" rel="noopener noreferrer" class="">https://github.com/AI-Researcher-Team/CoFi-Dec</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CoFi-Dec, a training-free decoding framework that mitigates hallucinations in LVLMs by integrating generative self-feedback with coarse-to-fine visual conditioning. 2. Introduces a Wasserstein-based fusion mechanism to align predictive distributions from multiple visual conditions into a geometrically consistent decoding trajectory. 3. Demonstrates substantial reduction in both entity-level and semantic-level hallucinations across six benchmarks, showing the framework is model-agnostic and requires no additional training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7cab1d37f48692f41be898afb160456b94e821d8b6ea16ba282cb4ee3ac5046_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7cab1d37f48692f41be898afb160456b94e821d8b6ea16ba282cb4ee3ac5046_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of hallucinated content in Large Vision-Language Models (LVLMs). It proposes CoFi-Dec, a training-free decoding framework that uses coarse-to-fine visual conditioning and generative feedback to create multi-level visual hypotheses, which are then unified via a Wasserstein-based fusion mechanism. The method significantly reduces hallucinations across multiple benchmarks and can be applied to various LVLMs without retraining.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [instruction following, hindsight replay, sample-efficient RL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kongcheng Zhang, Qi Yao, Shunyu Liu, Wenjian Zhang, Min Cen, Yang Zhou, Wenkai Fang, Yiru Zhao, Baisheng Lai, Mingli Song</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Cainiao Network, Nanyang Technological University, Dalian University of Technology, University of Science and Technology of China, Alibaba Cloud Computing, Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23457" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23457</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/zhangkc97/HiR" target="_blank" rel="noopener noreferrer" class="">https://github.com/zhangkc97/HiR</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Hindsight instruction Replay (HiR), a novel RL framework that replays failed attempts as successes using a select-then-rewrite strategy to address sparse rewards. 2. Theoretically frames the RL objective as dual-preference learning at both instruction- and response-level, enabling efficient optimization with only binary rewards. 3. Demonstrates sample efficiency and promising results across various instruction following tasks with reduced computational budget.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72e272e5c14b0f640f80b3e8859a1fd3a0a8b8e8608dfacee9528d242698f15_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72e272e5c14b0f640f80b3e8859a1fd3a0a8b8e8608dfacee9528d242698f15_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of sparse rewards in RL for aligning LLMs to follow complex instructions. It proposes HiR, a sample-efficient framework that replays failed responses as successful ones based on partially satisfied constraints, framed as dual-preference learning. Experiments show HiR achieves strong performance on instruction-following tasks while being more computationally efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [motion generation], [flow matching, diffusion transformer (DiT), reinforcement learning from human feedback (RLHF)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuxin Wen, Qing Shuai, Di Kang, Jing Li, Cheng Wen, Yue Qian, Ningxin Jiao, Changhai Chen, Weijie Chen, Yiran Wang, Jinkun Guo, Dongyue An, Han Liu, Yanyu Tong, Chao Zhang, Qing Guo, Juan Chen, Qiao Zhang, Youyi Zhang, Zihao Yao, Cheng Zhang, Hong Duan, Xiaoping Wu, Qi Chen, Fei Cheng, Liang Dong, Peng He, Hao Zhang, Jiaxin Lin, Chao Zhang, Zhongyi Fan, Yifan Li, Zhichao Hu, Yuhong Liu, Linus, Jie Jiang, Xiaolong Li, Linchao Bao</p>
</li>
<li class="">
<p><strong>institution:</strong> Tencent Hunyuan</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23464" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23464</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Tencent-Hunyuan/HY-Motion-1.0" target="_blank" rel="noopener noreferrer" class="">https://github.com/Tencent-Hunyuan/HY-Motion-1.0</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. The first successful scaling of DiT-based flow matching models to billion parameters for motion generation. 2. A comprehensive full-stage training paradigm including large-scale pretraining, fine-tuning, and RLHF. 3. A meticulous data processing pipeline enabling extensive coverage of over 200 motion categories.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3019219aea0683c229d44ce63a0fed59b5ebb795811dc1b1638ae995c9a8156_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3019219aea0683c229d44ce63a0fed59b5ebb795811dc1b1638ae995c9a8156_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces HY-Motion 1.0, a large-scale model for generating 3D human motions from text. It scales up Diffusion Transformer-based flow matching and uses a full-stage training pipeline with pretraining, fine-tuning, and RLHF. The model achieves state-of-the-art performance and broad motion coverage, and is released open-source.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning from human feedback (RLHF)], [reward model, inductive bias, information bottleneck, mutual information, reward hacking]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhuo Li, Pengyu Cheng, Zhechao Yu, Feifei Tong, Anningzhe Gao, Tsung-Hui Chang, Xiang Wan, Erchao Zhao, Xiaoxi Jiang, Guanjun Jiang</p>
</li>
<li class="">
<p><strong>institution:</strong> Alibaba, The Chinese University of Hong Kong, Shenzhen Research Institute of Big Data</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23461" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23461</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Qwen-Applications/DIR" target="_blank" rel="noopener noreferrer" class="">https://github.com/Qwen-Applications/DIR</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DIR, a novel information-theoretic debiasing method for reward models that maximizes mutual information with human preference while minimizing it with biased attributes. 2. Theoretically justifies the method&#x27;s ability to handle complex, non-linear inductive biases, extending beyond simple linear correlation models. 3. Empirically demonstrates DIR&#x27;s effectiveness in mitigating three types of biases (length, sycophancy, format) and shows it enhances RLHF performance and generalization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/522f5bbb5a5776cd8df024fb1b24faf19bb1a1ea6e0408c7951f37bfd1657846_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/522f5bbb5a5776cd8df024fb1b24faf19bb1a1ea6e0408c7951f37bfd1657846_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of inductive biases in reward models (RMs) for RLHF, which can lead to overfitting and reward hacking. It proposes DIR, an information-theoretic debiasing method inspired by the information bottleneck that optimizes mutual information to reduce bias. Experiments show DIR effectively mitigates multiple biases and improves RLHF performance and generalization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [text clustering], [hierarchical clustering, density-based clustering, semantic embeddings, large language models, topic modeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Thomas Haschka, Joseph Bakarji</p>
</li>
<li class="">
<p><strong>institution:</strong> Technische Universität Wien, American University of Beirut</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23471" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23471</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel nested density clustering method to construct hierarchical semantic trees from text embeddings. 2. Demonstrates the method&#x27;s application for data-driven discovery of research areas and subfields without predefined categories. 3. Validates the approach&#x27;s robustness and general applicability across diverse domains using benchmark datasets like 20 Newsgroups and IMDB reviews.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e39c485de109f72521e714f1d7489795a2f6737dcaff2fbddf6af40f9dc170ee_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e39c485de109f72521e714f1d7489795a2f6737dcaff2fbddf6af40f9dc170ee_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of uncovering the global hierarchical semantic structure in text corpora, which remains opaque when using LLM embeddings only for similarity search. It proposes a method that applies nested density clustering on LLM embeddings, gradually relaxing a density criterion to merge clusters into a hierarchical tree. This approach enables the data-driven discovery of semantic relationships and topic hierarchies without predefined categories, as demonstrated on scientific abstracts and benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [software supply chain security], [agentic AI, reinforcement learning, large language model, blockchain security ledger, CI/CD]</p>
</li>
<li class="">
<p><strong>authors:</strong> Toqeer Ali Syed, Mohammad Riyaz Belgaum, Salman Jan, Asadullah Abdullah Khan, Saad Said Alqahtani</p>
</li>
<li class="">
<p><strong>institution:</strong> Islamic University of Madinah, Arab Open University-Bahrain</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23480" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23480</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an autonomous, agentic AI framework for software supply chain security that integrates LLM-based reasoning, RL, and multi-agent coordination for proactive vulnerability identification and mitigation. 2. Implements a system that interfaces with real CI/CD environments (e.g., GitHub Actions, Jenkins) via the Model Context Protocol (MCP) and logs actions to a blockchain for auditability. 3. Demonstrates through experiments that the framework outperforms rule-based and provenance-only baselines in detection accuracy and mitigation latency with acceptable operational overhead.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3f037ac5977c7275a7c48edbcb676154bcff19330c107fe4c9769750efc5350_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3f037ac5977c7275a7c48edbcb676154bcff19330c107fe4c9769750efc5350_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of current software supply chain security frameworks (like SLSA) which focus on provenance but lack active vulnerability mitigation. It proposes an agentic AI system that combines LLMs for semantic analysis and RL for adaptive response, integrated with CI/CD pipelines via MCP and logged on a blockchain. Experiments show it achieves better detection and faster mitigation than baselines, enabling a shift from reactive to proactive defense.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [Parameter-efficient fine-tuning, LoRA, full-rank adaptation, rotational degrees of freedom, hierarchical joint decomposition]</p>
</li>
<li class="">
<p><strong>authors:</strong> Guoan Wan, Tianyu Chen, Fangzheng Feng, Haoyi Zhou, Runhua Xu</p>
</li>
<li class="">
<p><strong>institution:</strong> Beihang University, Huazhong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23485" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23485</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Bane-Elvin/AAAI2026-FRoD" target="_blank" rel="noopener noreferrer" class="">https://github.com/Bane-Elvin/AAAI2026-FRoD</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FRoD, a novel PEFT method that combines hierarchical joint decomposition with rotational degrees of freedom for full-rank updates. 2. Introduces a globally shared basis and sparse, learnable perturbations to enhance expressiveness and efficiency beyond low-rank constraints. 3. Demonstrates that FRoD matches full fine-tuning accuracy on 20 benchmarks while using only 1.72% of trainable parameters and achieves faster convergence.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e211fe6dc2d8e782c731fff29ba32c9fe2a1f958b475d5931d50f6e8fd04fdb8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e211fe6dc2d8e782c731fff29ba32c9fe2a1f958b475d5931d50f6e8fd04fdb8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the slow convergence and limited capacity of low-rank PEFT methods like LoRA. It proposes FRoD, a method that enables full-rank updates via a shared basis and sparse perturbations, achieving faster convergence. The method matches full fine-tuning accuracy on diverse benchmarks while using only a tiny fraction of parameters.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Theory of Mind for Explainable Human-Robot Interaction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [human-robot interaction], [Theory of Mind, Explainable AI, XAI evaluation, human-centered explanation, VXAI framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Marie Bauer, Julia Gachot, Matthias Kerzel, Cornelius Weber, Stefan Wermter</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Hamburg</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23482" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23482</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes to conceptualize Theory of Mind (ToM) in Human-Robot Interaction as a form of Explainable AI (XAI), 2. Identifies a critical gap in ToM-HRI research regarding the fidelity of explanations to the robot&#x27;s actual internal reasoning, 3. Advocates for integrating ToM principles into XAI frameworks to shift focus towards user-centered explanations and enable evaluation using frameworks like VXAI.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16cba35efa080097fd84afa40a6d270891cd44dfcf26d46fde5d29edb9bb1541_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16cba35efa080097fd84afa40a6d270891cd44dfcf26d46fde5d29edb9bb1541_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies that Theory of Mind (ToM) in human-robot interaction and Explainable AI (XAI) share the goal of making AI reasoning understandable. It proposes to treat ToM as a form of XAI and argues for integrating ToM&#x27;s user-centered perspective into XAI frameworks to address the lack of explanation fidelity and user-centered evaluation in current research.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [model selection, capability-cost frontier, constrained optimization, deployment-aware leaderboards, compliance trade-offs]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vassilis Digalakis Jr, Ramayya Krishnan, Gonzalo Martin Fernandez, Agni Orfanoudaki</p>
</li>
<li class="">
<p><strong>institution:</strong> Boston University, Carnegie Mellon University, Universitat Politècnica de Catalunya, Oxford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23487" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23487</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ML Compass, a framework that treats AI model selection as constrained optimization over a capability-cost frontier to bridge the gap between capability leaderboards and deployment decisions. 2. Characterizes optimal model configurations theoretically, showing a three-regime structure in internal measures and deriving comparative statics for budget, regulation, and technology changes. 3. Implements a practical pipeline that extracts internal measures, estimates an empirical frontier, learns task-specific utility, and validates with case studies in conversational and healthcare settings.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c28f58754a80430c57b0351355d200ab9fbfde8dd5fafc1b0d5caf4dd85bbb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c28f58754a80430c57b0351355d200ab9fbfde8dd5fafc1b0d5caf4dd85bbb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the gap between AI model capability rankings and real-world deployment decisions by introducing ML Compass, a framework that formulates model selection as constrained optimization over a capability-cost frontier. It combines theoretical analysis of optimal configurations with an implementation pipeline for recommendation, validated in conversational and healthcare case studies. The framework shows that deployment-aware rankings can differ significantly from capability-only leaderboards, clarifying trade-offs between capability, cost, and compliance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [graph reasoning, information gain, multi-agent, off-graph prediction, path retrieval]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haoyu Pei, Zhongyang Liu, Xiangyi Xiao, Xiaocong Du, Haipeng Zhang, Kunpeng Zhang, Suting Hong</p>
</li>
<li class="">
<p><strong>institution:</strong> ShanghaiTech University, Xi’an Jiaotong-Liverpool University, University of Maryland</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23489" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23489</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://anonymous.4open.science/r/MIRAGE-VC-323F" target="_blank" rel="noopener noreferrer" class="">https://anonymous.4open.science/r/MIRAGE-VC-323F</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed an information-gain-driven path retriever to tackle path explosion in graphs for LLM reasoning. 2. Introduced a multi-agent architecture with learnable gating to fuse heterogeneous evidence streams. 3. Addressed the off-graph prediction challenge in venture capital, demonstrating significant performance gains.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aed24a413df0661b461a6124bbf364c73762a6f25c3c90546bff33f8c4123ebf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aed24a413df0661b461a6124bbf364c73762a6f25c3c90546bff33f8c4123ebf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of predicting venture capital success, an off-graph task requiring reasoning over complex relational evidence. It proposes MIRAGE-VC, a framework that uses information-gain-driven path retrieval and a multi-agent system to distill and reason over investment networks. The method achieves improved prediction performance and offers insights for other off-graph tasks like recommendation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [communication &amp; networking], [URLLC, Link Adaptation, Device Scheduling, Deep Reinforcement Learning, Bayesian Optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wei Gao, Paul Zheng, Peng Wu, Yulin Hu, Anke Schmeink</p>
</li>
<li class="">
<p><strong>institution:</strong> Wuhan University, RWTH Aachen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23493" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23493</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a joint link adaptation and device scheduling design for multi-device URLLC IIoT networks under imperfect CSI, aiming to maximize total transmission rate under strict BLER constraints. 2. Introduces a novel Bayesian Optimization-driven Twin Delayed Deep Deterministic Policy Gradient (BO-TD3) method to adaptively determine device serving order and MCS based on outdated CQI. 3. Develops a BO-based training mechanism to address issues of error sample imbalance and TD3 parameter sensitivity, improving convergence speed and learning reliability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa25685331ccee6042c70838d3438022f95490a2cc609beba627f444cba8cd7c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa25685331ccee6042c70838d3438022f95490a2cc609beba627f444cba8cd7c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of joint link adaptation and device scheduling in URLLC IIoT networks with imperfect channel state information. It proposes a novel deep reinforcement learning method (BO-driven TD3) that adaptively selects the device serving order and modulation schemes, enhanced by Bayesian Optimization for faster and more reliable training. Simulation results show the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [AI alignment], [AI assistance game, AI shutdown, Incomplete preferences, Non-Archimedean utilities]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alessio Benavoli, Alessandro Facchini, Marco Zaffalon</p>
</li>
<li class="">
<p><strong>institution:</strong> Trinity College Dublin, SUPSI (University of Applied Sciences and Arts of Southern Switzerland), Kozminski University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23508" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23508</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formally connects the AI assistance and AI shutdown problems to the need for reasoning under uncertainty. 2. Argues that handling incomplete human preferences is a fundamental requirement for safe AI. 3. Proposes that non-Archimedean utility structures are necessary to correctly model and prioritize safety constraints.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ea603db1434e19dcd83096c95297f9662c571581eb479d66e87432fcf6a9075_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ea603db1434e19dcd83096c95297f9662c571581eb479d66e87432fcf6a9075_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes AI safety through the frameworks of the AI assistance and AI shutdown games. It argues that to address these challenges, AI agents must be designed to reason under uncertainty and handle incomplete and non-Archimedean human preferences. The main conclusion is that these capabilities are essential for ensuring AI systems remain aligned with human values and safe.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [vision-language model, unified model, semantic generation, autoregression, data scaling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Fengjiao Chen, Minhao Jing, Weitao Lu, Yan Feng, Xiaoyu Li, Xuezhi Cao</p>
</li>
<li class="">
<p><strong>institution:</strong> Meituan</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23512" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23512</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that generation enhances understanding in large-scale VLM training only when operating at the semantic level (e.g., autoregressing high-level visual representations), not at the pixel level. 2. Shows that unified generation-understanding models exhibit superior data scaling trends and higher data utilization efficiency compared to understanding-only models. 3. Proposes that autoregression on input embeddings is an effective and modality-independent method for capturing visual details, enabling pixel-level generation from learned semantics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a75dffd20dae849b9b4d37288d6d557fa32f5f2c8bf947c87fc1e79319e9dbe8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a75dffd20dae849b9b4d37288d6d557fa32f5f2c8bf947c87fc1e79319e9dbe8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether visual generation tasks can enhance understanding in large-scale vision-language models. Through large-scale pretraining (&gt;200M samples) with a model called UniHetero, the authors find that semantic-level generation (not pixel-level) improves understanding, reveals better data scaling, and that autoregression on input embeddings effectively captures visual details.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [diffusion models], [multi-subject customization, layout guidance, attention decoupling, training-free, image adapter]</p>
</li>
<li class="">
<p><strong>authors:</strong> Binhe Yu, Zhen Wang, Kexin Li, Yuqian Yuan, Wenqiao Zhang, Long Chen, Juncheng Li, Jun Xiao, Yueting Zhuang</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, HKUST (The Hong Kong University of Science and Technology)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23537" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23537</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes AnyMS, a novel training-free framework for layout-guided multi-subject image customization. 2. Introduces a bottom-up dual-level attention decoupling mechanism (global and local) to balance text alignment, identity preservation, and layout control. 3. Employs pre-trained image adapters to extract subject features without requiring subject-specific training or adapter tuning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e5ac220ed9f71a26f20317e74d4ddc9cd5a957b5751dba85f593ddfeaf39640_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e5ac220ed9f71a26f20317e74d4ddc9cd5a957b5751dba85f593ddfeaf39640_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of generating coherent images containing multiple user-specified subjects while balancing text alignment, subject identity, and layout control. It proposes AnyMS, a training-free framework that uses a bottom-up attention decoupling mechanism and pre-trained adapters to integrate text, subject images, and layout constraints. The method achieves state-of-the-art performance, supporting complex compositions and scaling to many subjects.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [computational pathology], [agentic multimodal model, evidence-seeking inference, reinforcement learning, whole-slide images, vision-language model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shengyi Hua, Jianfeng Wu, Tianle Shen, Kangzhe Hu, Zhongzhen Huang, Shujuan Ni, Zhihong Zhang, Yuan Li, Zhe Wang, Xiaofan Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, Fourth Military Medical University, University of Science and Technology of China, Fudan University, Nanjing Medical University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23545" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23545</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed PathFound, an agentic multimodal model that introduces an evidence-seeking inference paradigm for pathological diagnosis, moving beyond static, single-pass analysis. 2. Integrated pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to enable proactive information acquisition and multi-stage diagnosis refinement. 3. Demonstrated that the evidence-seeking strategy consistently improves diagnostic accuracy across models and that PathFound achieves state-of-the-art performance, showing strong potential for discovering subtle pathological details.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db008475f544af0752cf146b5b6ba57eaf58c0e376cb94807dd3df594fa09037_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db008475f544af0752cf146b5b6ba57eaf58c0e376cb94807dd3df594fa09037_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes PathFound, an agentic multimodal model that mimics clinical workflows by actively seeking evidence for ambiguous pathological diagnoses through multi-turn interactions. It integrates visual foundation models, vision-language models, and reinforcement learning-based reasoning to refine its initial diagnosis. The method achieves state-of-the-art diagnostic accuracy and demonstrates the effectiveness of evidence-seeking workflows in computational pathology.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Act2Goal: From World Model To General Goal-conditioned Policy</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [goal-conditioned policy, world model, multi-scale temporal hashing, hindsight goal relabeling, LoRA]</p>
</li>
<li class="">
<p><strong>authors:</strong> Pengfei Zhou, Liliang Chen, Shengcong Chen, Di Chen, Wenzhi Zhao, Rongjun Jin, Guanghui Ren, Jianlan Luo</p>
</li>
<li class="">
<p><strong>institution:</strong> Agibot Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23541" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23541</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://act2goal.github.io/" target="_blank" rel="noopener noreferrer" class="">https://act2goal.github.io/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control for long-horizon tasks. 2. Introduces Multi-Scale Temporal Hashing (MSTH) to decompose imagined visual trajectories into dense proximal and sparse distal frames for fine-grained control and global consistency. 3. Enables reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c80e64501db97d2a806134a5544d87a826563f38be2334e8bdec4a9d7f9cf78_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c80e64501db97d2a806134a5544d87a826563f38be2334e8bdec4a9d7f9cf78_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of long-horizon robotic manipulation by proposing Act2Goal, a policy that uses a goal-conditioned world model to generate visual plans and a multi-scale temporal control mechanism for robust execution. The method achieves strong zero-shot generalization and allows for rapid online adaptation. Real-robot experiments show it significantly improves success rates on out-of-distribution tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [hallucination detection], [knowledge graphs, self-detection, structured verification, GPT-4o, Gemini-2.5-Flash]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sahil Kale, Antonio Luca Alfeo</p>
</li>
<li class="">
<p><strong>institution:</strong> Knowledge Verse AI, eCampus University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23547" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23547</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/knowledge-verse-ai/kg-hallu-eval" target="_blank" rel="noopener noreferrer" class="">https://github.com/knowledge-verse-ai/kg-hallu-eval</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel hallucination self-detection method that converts LLM responses into knowledge graphs for structured analysis., 2. Introduces a manually curated and enhanced hallucination detection dataset to support more reliable future benchmarking., 3. Demonstrates significant performance improvements (up to 16% accuracy, 20% F1) over standard self-detection and a state-of-the-art baseline (SelfCheckGPT).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a76b4a88e64d73392aa8d986a7f3dab5da424782ba41d701ca8db2d4ab4a12d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a76b4a88e64d73392aa8d986a7f3dab5da424782ba41d701ca8db2d4ab4a12d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of hallucinations in LLMs by proposing a self-detection method that converts model responses into knowledge graphs to better analyze atomic facts and estimate hallucination likelihood. The method, evaluated on GPT-4o and Gemini-2.5-Flash, shows substantial improvements in accuracy and F1-score over existing approaches. The work concludes that structuring facts as knowledge graphs enables more robust hallucination detection, offering a low-cost, model-agnostic path toward safer language models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [ai security], [prompt injection, multi-agent systems, provenance tracking, trust validation, multimodal sanitization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Toqeer Ali Syed, Mishal Ateeq Almutairi, Mahmoud Abdel Moaty</p>
</li>
<li class="">
<p><strong>institution:</strong> Islamic University of Madinah, Arab Open University-Bahrain</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23557" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23557</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Cross-Agent Multimodal Provenance-Aware Defense Framework to secure agentic AI workflows against prompt injection attacks. 2. Introduces a coordinated defense with specialized sanitizer agents (text, visual) and an output validator, managed by a provenance ledger for tracking trust metadata. 3. Demonstrates through experiments that the framework significantly improves multimodal injection detection accuracy and minimizes trust leakage across agents.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05cd3aa22e07307bda78f06b6f87c2195c61c758b4fe44dc5bffbc281d72a8e4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05cd3aa22e07307bda78f06b6f87c2195c61c758b4fe44dc5bffbc281d72a8e4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the security threat of multimodal prompt injection attacks in agentic AI systems like LangChain. It proposes a defense framework that sanitizes inputs and validates outputs using specialized agents coordinated by a provenance ledger. The experimental results show the framework enhances detection accuracy and stabilizes agentic execution pathways.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] VL-RouterBench: A Benchmark for Vision-Language Model Routing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [vision-language model routing, benchmark, cost-accuracy trade-off, model selection, evaluation protocol]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhehao Huang, Baijiong Lin, Jingyuan Zhang, Jingying Wang, Yuhang Liu, Ning Lu, Tao Li, Xiaolin Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23562" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23562</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/K1nght/VL-RouterBench" target="_blank" rel="noopener noreferrer" class="">https://github.com/K1nght/VL-RouterBench</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes VL-RouterBench, the first systematic and reproducible benchmark for evaluating vision-language model (VLM) routing systems. 2. Constructs a large-scale evaluation foundation with quality and cost matrices over 519,180 sample-model pairs from 17 models and 14 datasets. 3. Introduces a comprehensive evaluation protocol that jointly measures accuracy, cost, and throughput, and uses a ranking score based on the harmonic mean for fair comparison across router configurations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/885d9087464eedead5301ad4cd041923ddee6d5773371e117abbd30fc4ae4f09_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/885d9087464eedead5301ad4cd041923ddee6d5773371e117abbd30fc4ae4f09_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces VL-RouterBench, a benchmark to systematically evaluate routing systems for vision-language models. It constructs matrices of quality and cost from extensive inference logs and uses a ranking score to compare routers. The evaluation shows current routers achieve significant gains but still fall short of an ideal Oracle, indicating room for improvement in router design.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal large language models], [chemical reaction understanding, multimodal benchmark, scientific literature, visual perception, cross-modal integration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hanzheng Li, Xi Fang, Yixuan Li, Chaozheng Huang, Junjie Wang, Xi Wang, Hongzhe Bai, Bojun Hao, Shenyu Lin, Huiqi Liang, Linfeng Zhang, Guolin Ke</p>
</li>
<li class="">
<p><strong>institution:</strong> DP Technology, Shanghai Jiao Tong University, Tsinghua University, New York University, Fudan University, Xiamen University, ShanghaiTech University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23565" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23565</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces RxnBench, a multi-tiered benchmark for evaluating MLLMs on chemical reaction understanding from scientific PDFs, featuring two tasks (SF-QA and FD-QA). 2. Provides a comprehensive evaluation revealing a critical capability gap in MLLMs, showing they struggle with deep chemical logic and precise structural recognition despite excelling at text extraction. 3. Highlights the importance of inference-time reasoning and underscores the urgent need for domain-specific visual encoders and stronger reasoning engines for autonomous AI chemists.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e7b73b049eb3232e03516a09f66b0fddafff9357b89527de70340faa28603c6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e7b73b049eb3232e03516a09f66b0fddafff9357b89527de70340faa28603c6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces RxnBench, a multimodal benchmark to evaluate Large Language Models on understanding chemical reactions from scientific literature. The evaluation reveals that while models are good at extracting text, they struggle with chemical logic and structural recognition, showing the need for better domain-specific visual and reasoning components.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [creative text generation], [divergent-convergent thinking, prompting method, creative problem generation, artificial hivemind, constraint satisfaction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Manh Hung Nguyen, Adish Singla</p>
</li>
<li class="">
<p><strong>institution:</strong> MPI-SWS (Max Planck Institute for Software Systems)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23601" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23601</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CreativeDC, a novel two-phase prompting method inspired by human creative thinking to decouple creative exploration from constraint satisfaction in LLMs. 2. Introduces a comprehensive evaluation framework for creative problem generation, measuring diversity, novelty, and utility. 3. Demonstrates that CreativeDC significantly improves the diversity and novelty of generated educational problems compared to baseline methods while maintaining utility.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86ff3954389096abbd357b8cdc09c7b6f3087b3cf9c2edee2cf008bb8924d692_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86ff3954389096abbd357b8cdc09c7b6f3087b3cf9c2edee2cf008bb8924d692_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the &quot;Artificial Hivemind&quot; effect in LLMs, which leads to homogeneous and repetitive outputs, particularly harmful for generating diverse educational problems. It proposes CreativeDC, a prompting method that scaffolds LLM reasoning into divergent (idea exploration) and convergent (constraint satisfaction) phases. The results show that CreativeDC generates problems with significantly higher diversity and novelty than baselines without sacrificing utility.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [transfer learning], [Le Cam Distortion, Deficiency Distance, Directional Simulability, Unsupervised Domain Adaptation, Negative Transfer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Deniz Akdemir</p>
</li>
<li class="">
<p><strong>institution:</strong> None (Institution not specified in provided content)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23617" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23617</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a decision-theoretic framework for robust transfer learning based on Le Cam&#x27;s theory, replacing symmetric invariance with directional simulability. 2. Introduces Le Cam Distortion, quantified by the Deficiency Distance, as a rigorous upper bound for transfer risk. 3. Demonstrates the framework&#x27;s effectiveness across diverse experiments (genomics, vision, RL), showing it prevents source degradation and catastrophic negative transfer where traditional methods fail.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bd736028263c7eaaaaecae78f0df59f633374608f59295b1185c8385eea1e5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bd736028263c7eaaaaecae78f0df59f633374608f59295b1185c8385eea1e5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a flaw in standard Unsupervised Domain Adaptation, which can cause harmful &quot;negative transfer&quot; by forcing invariance between unequally informative domains. It proposes a new framework based on Le Cam&#x27;s theory, using directional simulability and a metric called Le Cam Distortion to enable safe transfer without degrading the source domain. Experiments show this method successfully prevents information loss and catastrophic failure in safety-critical applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Regret-Based Federated Causal Discovery with Unknown Interventions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [causal discovery, unknown interventions, differential privacy, Φ-CPDAG, regret-based]</p>
</li>
<li class="">
<p><strong>authors:</strong> Federico Baldo, Charles K. Assaad</p>
</li>
<li class="">
<p><strong>institution:</strong> Sorbonne Université, INSERM, Institut Pierre Louis d&#x27;Epidémiologie et de Santé Publique</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23626" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23626</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes I-PERI, a novel federated causal discovery algorithm that handles unknown client-level interventions, 2. Introduces the Φ-Markov Equivalence Class (Φ-CPDAG), a tighter equivalence class derived from structural differences across clients, 3. Provides theoretical guarantees on convergence and privacy-preserving properties (differential privacy).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373e2e9b4041d9efb71fbe2d1095901813d7f2551cdfe37d40d79c04d7aa235d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373e2e9b4041d9efb71fbe2d1095901813d7f2551cdfe37d40d79c04d7aa235d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses federated causal discovery where client data are subject to unknown, heterogeneous interventions, a common real-world scenario overlooked by prior work. It proposes the I-PERI algorithm, which first recovers the union CPDAG and then orients additional edges by exploiting intervention-induced structural differences across clients, resulting in a tighter equivalence class called the Φ-CPDAG. Theoretical and empirical results demonstrate the algorithm&#x27;s effectiveness and privacy guarantees.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Physics-Informed Neural Networks for Device and Circuit Modeling: A Case Study of NeuroSPICE</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [physics-informed neural networks, circuit simulation, differential-algebraic equations, surrogate modeling, NeuroSPICE]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chien-Ting Tung, Chenming Hu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California at Berkeley</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23624" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23624</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes NeuroSPICE, a novel PINN-based framework for solving circuit differential-algebraic equations directly, bypassing traditional time-discretized numerical solvers. 2. Demonstrates the framework&#x27;s flexibility for modeling emerging devices and multi-physics systems within a single Python environment, lowering the barrier for rapid prototyping. 3. Highlights the potential of the differentiable PINN model as a surrogate for circuit design optimization and inverse problems, despite not outperforming SPICE in raw training speed or accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7d88b0c7c41bb8bbaf5959a08185913698993a5cc330641c604219b2a1c0622_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7d88b0c7c41bb8bbaf5959a08185913698993a5cc330641c604219b2a1c0622_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces NeuroSPICE, a framework that uses Physics-Informed Neural Networks (PINNs) to simulate electronic circuits by solving their governing differential-algebraic equations through backpropagation. It represents circuit waveforms as continuous, differentiable functions of time, enabling the simulation of novel devices like ferroelectric memories. The main conclusion is that while not faster than SPICE for training, NeuroSPICE offers unique advantages as a flexible, differentiable surrogate model for design optimization and complex multi-physics systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [multi-agent systems, hierarchical agents, bandit optimization, software engineering agents, SWE-bench]</p>
</li>
<li class="">
<p><strong>authors:</strong> Iris Xu, Guangtao Zeng, Zexue He, Charles Jin, Aldo Pareja, Dan Gutfreund, Chuang Gan, Zhang-Wei Hong</p>
</li>
<li class="">
<p><strong>institution:</strong> Massachusetts Institute of Technology, MIT-IBM Watson AI Lab, Stanford University, University of Massachusetts Amherst</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23631" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23631</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/iamxjy/BOAD-SWE-Agent" target="_blank" rel="noopener noreferrer" class="">https://github.com/iamxjy/BOAD-SWE-Agent</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulates the automatic discovery of effective hierarchical multi-agent systems for software engineering as a multi-armed bandit (MAB) problem, enabling efficient exploration under limited budgets. 2. Proposes the BOAD framework, which uses bandit optimization to coordinate specialized sub-agents (e.g., for localization, editing, validation) and attribute credit within a team. 3. Demonstrates that automatically discovered hierarchical agents outperform single-agent and manually designed multi-agent systems on challenging, out-of-distribution SWE benchmarks, including achieving second place on SWE-bench-Live with a 36B model.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67084d40c24e3869f47efa4b52c7ec1479016d613997e911c6730d7e7684254f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67084d40c24e3869f47efa4b52c7ec1479016d613997e911c6730d7e7684254f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the poor generalization of single-agent LLMs on long-horizon, out-of-distribution software engineering tasks by proposing a hierarchical multi-agent system. The core method, BOAD, automatically discovers effective agent hierarchies by formulating the search as a multi-armed bandit optimization problem. The results show that this approach significantly improves performance on SWE benchmarks, surpassing larger models like GPT-4 and Claude.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [educational ai], [generative AI, fine-tuning, randomized controlled trial, Socratic questioning, pedagogical instruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> LearnLM Team Google, Eedi, Albert Wang, Aliya Rysbek, Andrea Huber, Anjali Nambiar, Anna Kenolty, Ben Caulfield, Beth Lilley-Draper, Bibi Groot, Brian Veprek, Chelsea Burdett, Claire Willis, Craig Barton, Digory Smith, George Mu, Harriet Walters, Irina Jurenka, Iris Hulls, James Stalley-Moores, Jonathan Caton, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Liam McCafferty, Lucy Dalton, Markus Kunesch, Pauline Malubay, Rachel Kidson, Rich Wells, Sam Wheeler, Sara Wiltberger, Shakir Mohamed, Simon Woodhead, Vasco Brazão</p>
</li>
<li class="">
<p><strong>institution:</strong> Google, Eedi</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23633" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23633</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a rigorous, in-classroom exploratory RCT to evaluate the safety and efficacy of a generative AI tutor (LearnLM) in a real educational setting. 2. Demonstrated that a pedagogically fine-tuned AI model can reliably draft instructional content, with human tutors approving 76.4% of its messages with minimal or no edits. 3. Showed that AI-supported tutoring led to student performance at least equivalent to human-only tutoring, with a significant 5.5 percentage point improvement in solving novel problems on subsequent topics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b430e7c78534d660126208f226397ed95756e540bade56276301199a0114bc76_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b430e7c78534d660126208f226397ed95756e540bade56276301199a0114bc76_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether generative AI can scale effective one-to-one tutoring. The authors integrated LearnLM, a pedagogically fine-tuned AI model, into a math tutoring platform and conducted a randomized controlled trial where human tutors supervised its outputs. The results show that LearnLM was a reliable tutor, and students using it performed as well as or better than those with human tutors alone, suggesting AI can deliver effective, individualized learning support at scale.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Nested Browser-Use Learning for Agentic Information Seeking</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [information-seeking agents, browser interaction, ReAct-style agents, nested framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Baixuan Li, Jialong Wu, Wenbiao Yin, Kuan Li, Zhongwang Zhang, Huifeng Yin, Zhengwei Tao, Liwen Zhang, Pengjun Xie, Jingren Zhou, Yong Jiang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tongyi Lab, Alibaba Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23647" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23647</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Alibaba-NLP/DeepResearch" target="_blank" rel="noopener noreferrer" class="">https://github.com/Alibaba-NLP/DeepResearch</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a minimal and complete browser-action framework for agents, 2. Introduces a nested structure to decouple interaction control from page exploration, 3. Demonstrates improved performance on deep information-seeking benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab4c5e1fc52fc83cedffe542098b6777a8df396f1f3d30f2a130aebdd36e0dc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab4c5e1fc52fc83cedffe542098b6777a8df396f1f3d30f2a130aebdd36e0dc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the limitation of current information-seeking agents, which rely on simple API calls and cannot perform real browsing. It proposes NestBrowse, a framework that uses a nested structure to enable fine-grained browser control for agents, simplifying reasoning and improving performance on deep search tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [adversarial attacks], [prompt injection, large language models, academic peer review, multilingual, adversarial robustness]</p>
</li>
<li class="">
<p><strong>authors:</strong> Panagiotis Theocharopoulos, Ajinkya Kulkarni, Mathew Magimai.-Doss</p>
</li>
<li class="">
<p><strong>institution:</strong> International School of Athens, Idiap Research Institute</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23684" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23684</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Constructed a dataset of ~500 real ICML papers to empirically evaluate hidden prompt injection attacks in a realistic academic reviewing context. 2. Demonstrated that embedding semantically equivalent adversarial instructions in multiple languages (English, Japanese, Chinese, Arabic) can significantly alter LLM-generated review scores and decisions. 3. Revealed notable cross-lingual differences in attack effectiveness, with Arabic injections having minimal impact compared to others.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6285e0b940378fdc27628286ec6510afd35bf7a004b7ad95ad776e49035c6e1c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6285e0b940378fdc27628286ec6510afd35bf7a004b7ad95ad776e49035c6e1c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the vulnerability of LLM-based academic peer review systems to hidden prompt injection attacks. By injecting adversarial instructions in four languages into a dataset of real papers and having an LLM review them, the authors found that such attacks can substantially change review outcomes for English, Japanese, and Chinese, but not Arabic. The results highlight a critical security risk and language-dependent susceptibility in automated reviewing pipelines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Web World Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [world model, language agent, web framework, structured latent state, deterministic generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jichen Feng, Yifan Zhang, Chenggong Zhang, Yifu Lu, Shilong Liu, Mengdi Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Princeton University, University of California, Los Angeles, University of Pennsylvania</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23676" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23676</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://princeton-ai2-lab.github.io/Web-World-Models/" target="_blank" rel="noopener noreferrer" class="">https://princeton-ai2-lab.github.io/Web-World-Models/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced the Web World Model (WWM), a hybrid architecture that uses ordinary web code to enforce logical consistency and LLMs to generate open-ended content. 2. Built a suite of practical WWM demonstrations across diverse domains (travel, fiction, encyclopedia, games) on a realistic web stack. 3. Identified key design principles for WWMs, such as separating code-defined rules from model-driven imagination and representing latent state as typed web interfaces.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed8017bbc1bd6722a0d7bd0f84c67f735c0f1b24747518e2aa15905d07d1b03c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed8017bbc1bd6722a0d7bd0f84c67f735c0f1b24747518e2aa15905d07d1b03c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Web World Models (WWMs), a framework that combines the reliability of web code for world &quot;physics&quot; with the generative power of LLMs for content and narratives. This hybrid approach aims to provide language agents with controllable, logically consistent, yet open-ended persistent environments. The work demonstrates that standard web stacks can serve as a scalable substrate for building such world models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Complete Anatomy of the Madden-Julian Oscillation Revealed by Artificial Intelligence</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [climate informatics], [similarity-preserving representation, latent space clustering, physics-coherent monitoring]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiao Zhou, Yuze Sun, Jie Wu, Xiaomeng Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, National Climate Centre, China Meteorological Administration</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22144" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22144</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced an &quot;AI-for-theory&quot; paradigm using a deep learning model (PhysAnchor-MJO-AE) to learn a latent representation where distance corresponds to physical-feature similarity for the MJO. 2. Objectively discovered the first complete six-phase anatomical map of the MJO life cycle, isolating two long-hypothesized transitional phases. 3. Constructed a new physics-coherent monitoring framework that decouples location and intensity, drastically reducing spurious propagation and convective misplacement compared to classical methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b09901ad992d27215117f20984faf17d508a192bf9010cc39bd8b74553ff543_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b09901ad992d27215117f20984faf17d508a192bf9010cc39bd8b74553ff543_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of objectively defining the life cycle of the Madden-Julian Oscillation (MJO) by introducing an &quot;AI-for-theory&quot; paradigm. It develops a deep learning model to learn a similarity-preserving latent representation, enabling clustering that reveals a complete six-phase anatomy of the MJO. The derived new monitoring framework significantly outperforms the classical index, demonstrating AI&#x27;s role as a discovery tool for complex systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Neural ocean forecasting from sparse satellite-derived observations: a case-study for SSH dynamics and altimetry data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [spatiotemporal forecasting], [4DVarNet, U-Net, sequence-to-sequence, sea level anomaly, neural forecast]</p>
</li>
<li class="">
<p><strong>authors:</strong> Daria Botvynko, Pierre Haslée, Lucile Gaultier, Bertrand Chapron, Clement de Boyer Montégut, Anass El Aouni, Julien Le Sommer, Ronan Fablet</p>
</li>
<li class="">
<p><strong>institution:</strong> IMT Atlantique, Ifremer, CNRS, Mercator Ocean International</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22152" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22152</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Adapts U-Net and 4DVarNet architectures for short-term forecasting of ocean dynamics from sparse satellite data. 2. Formulates the forecasting task as a sequence-to-sequence mapping using partial SLA snapshots to predict future full-field maps. 3. Demonstrates that the end-to-end neural framework outperforms an operational baseline, especially in high-variability regions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b91a25a42a5dc1b5739ae5689c604765502ed07062eadaa473e043ec5a0d288f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b91a25a42a5dc1b5739ae5689c604765502ed07062eadaa473e043ec5a0d288f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an end-to-end deep learning framework for 7-day forecasting of sea surface dynamics using sparse satellite altimetry data. It adapts U-Net and 4DVarNet models to perform sequence-to-sequence mapping from partial observations to full-field forecasts. The results show the neural model outperforms an operational ocean forecast product, demonstrating the feasibility of neural forecasting for operational oceanography under data-sparse conditions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Super-Resolution Enhancement of Medical Images Based on Diffusion Model: An Optimization Scheme for Low-Resolution Gastric Images</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image super-resolution], [diffusion models, SR3, DDPM, capsule endoscopy, HyperKvasir]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haozhe Jia</p>
</li>
<li class="">
<p><strong>institution:</strong> Boston University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22209" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22209</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Applied the SR3 diffusion model framework to the specific domain of capsule endoscopy image super-resolution, addressing hardware-imposed low-resolution constraints. 2. Demonstrated that the diffusion-based approach outperforms traditional interpolation and GAN-based methods (e.g., ESRGAN) in both quantitative metrics (PSNR, SSIM) and qualitative anatomical fidelity. 3. Showed that architectural enhancements like attention mechanisms further improve performance, achieving a PSNR of 29.3 dB and SSIM of 0.71.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70b95a1328af0d9ae7f16ab6cb15a216b2ad914761eed6496beb2ee934202e23_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70b95a1328af0d9ae7f16ab6cb15a216b2ad914761eed6496beb2ee934202e23_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes using a diffusion model (SR3/DDPM) for super-resolution enhancement of low-resolution capsule endoscopy images. The method learns a probabilistic mapping from low-resolution to high-resolution images and is evaluated on the HyperKvasir dataset. Results show it outperforms traditional and GAN-based methods, better preserving critical anatomical details for clinical diagnosis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Literature Mining System for Nutraceutical Biosynthesis: From AI Framework to Biological Insight</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [literature mining, large language models, prompt engineering, few-shot prompting, domain adaptation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xinyang Sun, Nipon Sarmah, Miao Guo</p>
</li>
<li class="">
<p><strong>institution:</strong> King&#x27;s College London</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22225" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22225</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a domain-adapted AI system using LLMs and advanced prompt engineering to automate the extraction of nutraceutical-producing microbial strains from unstructured text. 2. Created and validated a structured dataset of 35 nutraceutical-strain associations, spanning multiple compound categories. 3. Demonstrated the system&#x27;s performance and provided biological insights, identifying dominant microbial contributors and the framework&#x27;s utility for synthetic biology and precision fermentation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6c767d67875d1e2e25e95c99a9b826daf52082c66775d52728bda6802558969_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6c767d67875d1e2e25e95c99a9b826daf52082c66775d52728bda6802558969_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents an AI-driven literature mining system that uses large language models and prompt engineering to automatically identify microbes that produce nutraceuticals from scientific text. The system, which performed best with the DeepSeek-V3 model and domain-specific prompts, generated a validated dataset and revealed key microbial strains for biosynthesis. This framework enhances the scalability of literature mining and provides actionable insights for strain selection and fermentation strategies.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Field strength-dependent performance variability in deep learning-based analysis of magnetic resonance imaging</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [nnU-Net, MRI field strength, radiomic analysis, UMAP clustering, model generalizability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Muhammad Ibtsaam Qadir, Duane Schonlau, Ulrike Dydak, Fiona R. Kolbinger</p>
</li>
<li class="">
<p><strong>institution:</strong> Purdue University, Indiana University School of Medicine, TUD Dresden University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22176" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22176</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A systematic quantitative evaluation framework to assess the impact of MRI scanner magnetic field strength (1.5T vs. 3.0T) on the performance and generalizability of deep learning segmentation models. 2. Empirical demonstration that training data field strength significantly influences model performance, especially for soft-tissue segmentation tasks, with models trained on 3.0T data often outperforming others. 3. The use of radiomic analysis and UMAP clustering to provide an interpretable, feature-based explanation for the observed performance differences, linking them to field-strength-dependent image characteristics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff8ab0f68e15ce620cdfd03bebfec56b322101c32b9cea5d7a2881045b311bc2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff8ab0f68e15ce620cdfd03bebfec56b322101c32b9cea5d7a2881045b311bc2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study investigates how MRI scanner magnetic field strength affects deep learning-based segmentation models. Using nnU-Net models trained on data from 1.5T, 3.0T, or combined field strengths across three anatomical datasets, the authors found that field strength in training data significantly impacts model performance, particularly for soft tissues. The conclusion is that magnetic field strength should be considered a confounding factor in AI studies for MRI analysis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Space AI: Leveraging Artificial Intelligence for Space to Improve Life on Earth</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [autonomous systems], [autonomous operations, mission planning, in-situ resource utilisation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ziyang Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> IEEE</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22399" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22399</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes and defines &quot;Space AI&quot; as a unified interdisciplinary field at the intersection of AI and space science. 2. Consolidates historical and contemporary progress into a systematic four-context framework (AI on Earth, in Orbit, in Deep Space, for Multi-Planetary Life). 3. Identifies key application areas where AI advances can translate to societal benefits on Earth, such as in sensing, robotics, and trustworthy AI.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6ab175271eb74821199e8998bba499198a030f7f6b329c43a525f851b07aabe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6ab175271eb74821199e8998bba499198a030f7f6b329c43a525f851b07aabe_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces &quot;Space AI&quot; as a new interdisciplinary field and proposes a systematic framework to organize its applications across four mission contexts, from Earth-based planning to multi-planetary life support. It argues that AI is critical for enabling autonomous and resilient space operations under extreme conditions, and that advances in this domain will also yield significant benefits for life on Earth.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [transformer interpretability], [cross-entropy, gradient dynamics, attention mechanism, expectation-maximization, Bayesian inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra</p>
</li>
<li class="">
<p><strong>institution:</strong> Dream Sports, Columbia University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22473" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22473</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Derived an advantage-based routing law and a responsibility-weighted update rule for attention scores and values under cross-entropy training. 2. Showed that the coupled gradient dynamics induce a positive feedback loop that behaves like a two-timescale Expectation-Maximization (EM) procedure. 3. Demonstrated that these gradient dynamics sculpt the low-dimensional manifolds necessary for Bayesian inference, linking optimization to geometry and function.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed145ec9892ca4b4f8b91e5c78948ac6b81399c20bcafdccd4cb429e92da2aed_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed145ec9892ca4b4f8b91e5c78948ac6b81399c20bcafdccd4cb429e92da2aed_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes how cross-entropy training shapes the internal geometry of transformer attention heads. By deriving first-order gradient dynamics, it shows that attention score and value updates form a positive feedback loop analogous to an EM algorithm. The core conclusion is that this gradient flow sculpts the Bayesian manifolds that enable in-context probabilistic reasoning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [medical audio classification], [Audio Spectrogram Transformer, Sharpness-Aware Minimization, ICBHI 2017, class imbalance, loss landscape]</p>
</li>
<li class="">
<p><strong>authors:</strong> Atakan Işık, Selin Vulga Işık, Ahmet Feridun Işık, Mahşuk Taylan</p>
</li>
<li class="">
<p><strong>institution:</strong> Başkent University, Gaziantep University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22564" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22564</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel framework integrating the Audio Spectrogram Transformer (AST) with Sharpness-Aware Minimization (SAM) for robust respiratory sound classification. 2. Implements a weighted sampling strategy to effectively handle the severe class imbalance present in medical datasets like ICBHI 2017. 3. Achieves state-of-the-art performance on the ICBHI 2017 dataset, with a particular focus on improving sensitivity for reliable clinical screening.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0861fcb0d50b762d97367d04dce9ca920f2ffca74cd5112df95b11652972a9b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0861fcb0d50b762d97367d04dce9ca920f2ffca74cd5112df95b11652972a9b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of respiratory sound classification, such as data scarcity and class imbalance, by enhancing the Audio Spectrogram Transformer with Sharpness-Aware Minimization (SAM) to find flatter minima for better generalization. The method also employs weighted sampling and achieves a new state-of-the-art score of 68.10% and a sensitivity of 68.31% on the ICBHI 2017 dataset, demonstrating improved robustness for clinical applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [simulation-based inference], [Bayesian adaptive design, amortized inference, diffusion models, sequential experimental design, policy learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Niels Bracher, Lars Kühmichel, Desi R. Ivanova, Xavier Intes, Paul-Christian Bürkner, Stefan T. Radev</p>
</li>
<li class="">
<p><strong>institution:</strong> Rensselaer Polytechnic Institute, TU Dortmund University, University of Oxford</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22999" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22999</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces JADAI, a framework that jointly amortizes Bayesian adaptive design and inference by training a policy, history, and inference network end-to-end., 2. Proposes a generic loss function that aggregates incremental reductions in posterior error across sequential experiments., 3. Instantiates the inference network with diffusion-based posterior estimators to handle high-dimensional and multimodal posteriors at each experimental step.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a357d49f23f5fdffd9539d05904d86150c3c7775033f4847b56afac3375ad8e6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a357d49f23f5fdffd9539d05904d86150c3c7775033f4847b56afac3375ad8e6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces JADAI, a framework that jointly learns to optimize experimental designs and perform Bayesian inference in a sequential setting. It trains a policy network, a history network, and a diffusion-based inference network end-to-end to minimize posterior error. The method achieves superior or competitive performance on standard adaptive design benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Deep Learning for Art Market Valuation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-modal learning], [multi-modal deep learning, visual embeddings, Grad-CAM, hedonic regression, repeated-sales dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jianping Mei, Michael Moses, Jan Waelty, Yucheng Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> Cheung Kong Graduate School of Business (CKGSB), University of Zurich, Swiss Finance Institute, Art Market Consultancy</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23078" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23078</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces and benchmarks multi-modal deep learning models that fuse tabular data (artist, history) with visual embeddings from artwork images for art market valuation. 2. Demonstrates that visual content provides a distinct and economically significant predictive contribution, especially for fresh-to-market works lacking prior transaction history. 3. Provides interpretability analyses using Grad-CAM and embedding visualizations to show models attend to compositional and stylistic visual cues.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes using multi-modal deep learning to improve art market valuation by incorporating visual content from artwork images alongside traditional tabular data. It finds that while artist identity and history are most predictive overall, visual features provide crucial value for first-time sales where historical data is absent. The results show deep learning offers new insights for valuation, particularly in the most challenging scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Constraint programming model and biased random-key genetic algorithm for the single-machine coupled task scheduling problem with exact delays to minimize the makespan</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [scheduling], [constraint programming, biased random-key genetic algorithm, makespan, exact delays, local search]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vítor A. Barbosa, Rafael A. Melo</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Computing, Universidade Federal da Bahia</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23150" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23150</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A Constraint Programming (CP) model for the single-machine coupled task scheduling problem with exact delays, utilizing well-established global constraints. 2. A novel Biased Random-Key Genetic Algorithm (BRKGA) that incorporates an efficient decoder, periodical restarts, shakes, and a local search algorithm for enhanced exploration. 3. An empirical evaluation demonstrating that the BRKGA provides high-quality solutions quickly, while the CP model with extended resources can find best-known solutions for a majority of benchmark instances.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f80cc4c12e23f8eb80b390efdd4a8b62ea37fb03c01008317951d1079c35c319_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f80cc4c12e23f8eb80b390efdd4a8b62ea37fb03c01008317951d1079c35c319_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the NP-hard single-machine coupled task scheduling problem with exact delays to minimize makespan. It proposes both a Constraint Programming model and a Biased Random-Key Genetic Algorithm (BRKGA) enhanced with local search and shake components. Computational results show the BRKGA finds good solutions quickly, while the CP model with more resources achieves state-of-the-art results on most benchmark instances.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] An Inference-Based Architecture for Intent and Affordance Saturation in Decision-Making</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Kullback-Leibler divergence, decision paralysis, intent selection, affordance selection, hierarchical decision process]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wendyam Eric Lionel Ilboudo, Saori C Tanaka</p>
</li>
<li class="">
<p><strong>institution:</strong> Nara Institute of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23144" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23144</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a computational account of decision paralysis as convergence failure in a hierarchical decision process, separating intent and affordance selection. 2. Formalizes decision commitment as inference under a mixture of reverse-KL (mode-seeking) and forward-KL (mode-covering) objectives. 3. Demonstrates through simulations that forward-KL-biased inference reproduces key features of decision inertia and shutdown, framing autism as an extreme regime of this general decision-making continuum.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae0c1bd96570e940c6fa7d72cbfcec334b1a94d288ce774a384e5c60b2bfe206_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae0c1bd96570e940c6fa7d72cbfcec334b1a94d288ce774a384e5c60b2bfe206_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses decision paralysis by proposing a hierarchical inference-based model that separates intent and affordance selection. Commitment is formalized using a mixture of reverse-KL and forward-KL divergence objectives, where a bias towards forward-KL leads to slow, heavy-tailed response times and distinct failure modes. The model reproduces features of decision inertia and suggests autism represents an extreme case on this decision-making continuum.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] EIR: Enhanced Image Representations for Medical Report Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image captioning], [cross-modal transformer, metadata fusion, domain-specific pre-training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qiang Sun, Zongcheng Ji, Yinlong Xiao, Peng Chang, Jun Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology of China, PAII Inc., Beijing University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23185" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23185</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Enhanced Image Representations (EIR) method for medical report generation. 2. Introduces cross-modal transformers to effectively fuse medical metadata with image features, addressing the information asymmetry problem. 3. Leverages medical domain pre-trained models to encode chest X-ray images, bridging the domain gap between general and medical images.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e359c65576903a65bf75a0600c08943744d6bd91d7b1f2e69d13330e289ce1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e359c65576903a65bf75a0600c08943744d6bd91d7b1f2e69d13330e289ce1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of generating medical reports from chest X-ray images. It proposes the EIR method, which uses cross-modal transformers to fuse metadata with visual features and employs medical domain pre-trained models for better image representation. Experiments on MIMIC and Open-I datasets demonstrate the method&#x27;s effectiveness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [alpha screening, large language models, reinforcement learning, factor investing, economic reasoning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zuoyou Jiang, Li Zhao, Rui Sun, Ruohan Sun, Zhongjian Li, Jing Li, Daxin Jiang, Zuo Bai, Cheng Hua</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, StepFun, FinStep</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23515" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23515</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/FinStep-AI/Alpha-R1" target="_blank" rel="noopener noreferrer" class="">https://github.com/FinStep-AI/Alpha-R1</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. 2. Introduces a method for LLMs to reason over factor logic and real-time news to evaluate alpha relevance under changing market conditions. 3. Demonstrates that the model consistently outperforms benchmarks and shows improved robustness to alpha decay across multiple asset pools.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d921912985e0858276fe1088914641df9c33c30f5de309733b2244c86c21e75e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d921912985e0858276fe1088914641df9c33c30f5de309733b2244c86c21e75e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of alpha decay in non-stationary financial markets by proposing Alpha-R1, a reasoning model trained with reinforcement learning. It uses a large language model to process factor logic and news, selectively activating factors based on contextual economic relevance. Empirical results show it outperforms benchmark strategies and is more robust to signal decay.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PINNs for Electromagnetic Wave Propagation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [Scientific Computing / Computational Physics], [Physics-Informed Neural Networks (PINNs), Maxwell&#x27;s Equations, FDTD, Time Marching, Poynting Regularizer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nilufer K. Bulut</p>
</li>
<li class="">
<p><strong>institution:</strong> Izmir, Turkiye (Inferred from author location; no specific institution mentioned in provided content)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23396" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23396</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a hybrid training strategy combining time marching and causality-aware weighting to address the causality collapse problem in time-dependent PINNs. 2. Proposes a two-stage interface continuity loss to mitigate discontinuities introduced by time marching. 3. Develops a local Poynting-based regularizer to suppress cumulative energy drift and improve energy conservation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c5894c195aba4dd14bb55cc020098ad00e472ec3df24245a9a20ad4ca74b81c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c5894c195aba4dd14bb55cc020098ad00e472ec3df24245a9a20ad4ca74b81c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses accuracy and energy conservation deficiencies in Physics-Informed Neural Networks (PINNs) for electromagnetic wave propagation. It proposes a hybrid training methodology incorporating time marching, causality-aware weighting, and a Poynting-based regularizer. The results show that the enhanced PINNs achieve competitive field accuracy and energy conservation compared to traditional FDTD methods, demonstrating their viability for canonical electromagnetic problems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-30T13:08:39.000Z" itemprop="dateModified">Dec 30, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/csai/20251222-20251228"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251222-20251228 (cs.AI)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/category/csar"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">cs.AR</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-29" class="table-of-contents__link toc-highlight">2025-12-29</a></li><li><a href="#2025-12-30" class="table-of-contents__link toc-highlight">2025-12-30</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/csai/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csai/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>