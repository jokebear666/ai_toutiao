<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_CV/20251222-20251228" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251222-20251228 (cs.CV) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/cscv/20251222-20251228"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251222-20251228 (cs.CV) | AI头条"><meta data-rh="true" name="description" content="2025-12-22"><meta data-rh="true" property="og:description" content="2025-12-22"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/cscv/20251222-20251228"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cscv/20251222-20251228" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cscv/20251222-20251228" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.CV","item":"https://jokebear666.github.io/ai_toutiao/daily/cscv"},{"@type":"ListItem","position":3,"name":"20251222-20251228 (cs.CV)","item":"https://jokebear666.github.io/ai_toutiao/daily/cscv/20251222-20251228"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.bc6dbd00.css">
<script src="/ai_toutiao/assets/js/runtime~main.a6c4f209.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.4d8b05d9.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/daily">Daily</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/paper">Paper</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Collapse sidebar category &#x27;cs.CV&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cs_CV/20251215-20251221"><span title="20251215-20251221 (cs.CV)" class="linkLabel_WmDU">20251215-20251221 (cs.CV)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/cscv/20251222-20251228"><span title="20251222-20251228 (cs.CV)" class="linkLabel_WmDU">20251222-20251228 (cs.CV)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/daily/cscv"><span>cs.CV</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251222-20251228 (cs.CV)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251222-20251228 (cs.CV)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-22">2025-12-22<a href="#2025-12-22" class="hash-link" aria-label="Direct link to 2025-12-22" title="Direct link to 2025-12-22" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251222] AVM: Towards Structure-Preserving Neural Response Modeling in the Visual Cortex Across Stimuli and Individuals</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational neuroscience], [Vision Transformer, modular subnetworks, condition-aware adaptation, structure-preserving framework, neural response modeling]</li>
<li class=""><strong>authors:</strong> Qi Xu, Shuai Gong, Xuming Ran, Haihua Luo, Yangfan Hu</li>
<li class=""><strong>institution:</strong> Dalian University of Technology, National University of Singapore, University of Jyvaskyla, Zhejiang University of Finance and Economics</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16948" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16948</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Adaptive Visual Model (AVM), a structure-preserving framework that uses a frozen Vision Transformer encoder to capture stable visual features and separate modulation paths to adapt to stimulus and individual variations. It demonstrates improved generalization and interpretability in modeling mouse V1 neural responses, outperforming prior models in predictive correlation and explained variance across different experimental conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Lights, Camera, Consistency: A Multistage Pipeline for Character-Stable AI Video Stories</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [visual anchoring, asset-first mechanism, temporal bridge, diffusion models, large language model (LLM), text-to-video (T2V), character consistency, multi-stage pipeline]</li>
<li class=""><strong>authors:</strong> Chayan Jain, Rishant Sharma, Archit Garg, Ishan Bhanuka, Pratik Narang, Dhruv Kumar</li>
<li class=""><strong>institution:</strong> BITS Pilani</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16954" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16954</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a multi-stage pipeline for generating long, character-consistent video stories. It uses an LLM to create a script, a text-to-image model to design consistent character visuals as anchors, and a video generation model to synthesize scenes individually, with a temporal bridge linking them. The method&#x27;s necessity is validated by showing that removing visual anchoring causes a catastrophic drop in character consistency, and cultural biases in current models are also analyzed.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [YOLOv8, Finer-CAM, saliency maps, cross-validation, TLS point cloud projections]</li>
<li class=""><strong>authors:</strong> Adrian Straker, Paul Magdon, Marco Zullich, Maximilian Freudenberg, Christoph Kleinn, Johannes Breidenbach, Stefano Puliti, Nils Nölke</li>
<li class=""><strong>institution:</strong> University of Applied Sciences and Art (HAWK), University of Groningen, University of Göttingen, Norwegian Institute of Bioeconomy Research (NIBIO)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16950" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16950</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a novel method that links Finer-CAM explanations to structural segments in TLS point cloud projections to evaluate which features drive tree species classification using YOLOv8 models. The analysis of saliency maps reveals that models primarily rely on crown features for classification, with stem features being more important for certain species, and that the models&#x27; perception of species similarity aligns with human expert judgment. The results underscore the need for explainable AI to understand model decision processes and build confidence in predictions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Comparison of deep learning models: CNN and VGG-16 in identifying pornographic content</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [convolutional neural network, VGG-16, deep learning, image classification]</li>
<li class=""><strong>authors:</strong> Reza Chandra, Adang Suhendra, Lintang Yuniar Banowosari, Prihandoko</li>
<li class=""><strong>institution:</strong> Gunadarma University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16947" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16947</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f77d0c91314abfde67968e7d31c4f0423abf8cdee43f2eaa3b2d640c1e5984a3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f77d0c91314abfde67968e7d31c4f0423abf8cdee43f2eaa3b2d640c1e5984a3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares two deep learning models, a custom CNN and the VGG-16 architecture, for the task of identifying pornographic image content. The study found that a CNN model with specific hyperparameters (50 epochs, learning rate 0.001) achieved a higher accuracy of 94.87% compared to the VGG-16 model, concluding that the CNN was more effective for fast and accurate detection.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] V-Agent: An Interactive Video Search System Using Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [vision-language model, fine-tuning, retrieval vector, re-ranking, multi-agent system]</li>
<li class=""><strong>authors:</strong> SunYoung Park, Jong-Hyeon Lee, Youngjune Kim, Daegyu Sung, Younghyun Yu, Young-rok Cha, Jeongho Ju</li>
<li class=""><strong>institution:</strong> NC AI, Kakao, Korea Advanced Institute of Science and Technology (KAIST)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16925" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16925</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e827044257e239766415ac9500a06f7ddb67bfc47c517c041d42cc61ac33ad18_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e827044257e239766415ac9500a06f7ddb67bfc47c517c041d42cc61ac33ad18_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> V-Agent is a multi-agent video search system that fine-tunes a vision-language model with a small video preference dataset and enhances it with a retrieval vector to embed video frames and audio transcriptions into a shared multimodal space. It uses three agents—routing, search, and chat—to refine searches and interact with users, achieving state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [discrete video tokenization, transformer-based adaptive compressor, evidence lower bound (ELBO), information-theoretic compression, adaptive tokenization]</li>
<li class=""><strong>authors:</strong> Haotian Ye, Qiyuan He, Jiaqi Han, Puheng Li, Jiaojiao Fan, Zekun Hao, Fitsum Reda, Yogesh Balaji, Huayu Chen, Sheng Liu, Angela Yao, James Zou, Stefano Ermon, Haoxiang Wang, Ming-Yu Liu</li>
<li class=""><strong>institution:</strong> NVIDIA, Stanford University, National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16975" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16975</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da83017a41e69234160f2c416b8a94507a537a66fd8a745a6bafc49c06817395_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da83017a41e69234160f2c416b8a94507a537a66fd8a745a6bafc49c06817395_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces InfoTok, a principled framework for adaptive discrete video tokenization based on information theory, using a novel ELBO-based algorithm and a transformer-based adaptive compressor. It achieves state-of-the-art compression by allocating tokens according to informational richness, saving 20% of tokens without performance loss and outperforming prior heuristic approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [LongShOTBench, LongShOTAgent, agentic tool use, omni-modal reasoning, benchmark, open-ended questions, graded rubric, iterative refinement]</li>
<li class=""><strong>authors:</strong> Mohammed Irfan Kurpath, Jaseel Muhammad Kaithakkodan, Jinxing Zhou, Sahal Shaji Mullappilly, Mohammad Almansoori, Noor Ahsan, Beknur Kalmakhanbet, Sambal Shikhar, Rishabh Lalla, Jean Lahoud, Mariette Awad, Fahad Shahbaz Khan, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal</li>
<li class=""><strong>institution:</strong> Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), American University of Beirut, Linköping University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16978" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16978</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces LongShOTBench, a diagnostic benchmark for long-form video understanding that features open-ended questions and tasks requiring multimodal reasoning and agentic tool use. It also presents LongShOTAgent, an agentic system that analyzes videos through preprocessing, search, and iterative refinement. The results show a significant performance gap, with state-of-the-art models achieving only up to 52.95%, highlighting the difficulty of real-world long-form video understanding.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Endo-SemiS: Towards Robust Semi-Supervised Image Segmentation for Endoscopic Video</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical image segmentation], [semi-supervised learning, cross-supervision, uncertainty-guided pseudo-label, joint pseudo-label supervision, mutual learning, spatiotemporal corrective network]</li>
<li class=""><strong>authors:</strong> Hao Li, Daiwei Lu, Xing Yao, Nicholas Kavoussi, Ipek Oguz</li>
<li class=""><strong>institution:</strong> Vanderbilt University, Vanderbilt University Medical Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16977" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16977</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c2b434a364e1db7fb30807eeda54f751647c7d8a35266a66353a42fbc7d8ed6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c2b434a364e1db7fb30807eeda54f751647c7d8a35266a66353a42fbc7d8ed6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Endo-SemiS, a semi-supervised framework for endoscopic video segmentation that employs cross-supervision, uncertainty-guided pseudo-labeling, joint supervision, and mutual learning between two networks, along with a separate spatiotemporal corrective network. It demonstrates superior performance over state-of-the-art methods on kidney stone and polyp segmentation tasks when labeled data is limited.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] 4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [4D-RGPT, Perceptual 4D Distillation (P4D), R4D-Bench, region-level prompting, 4D Video Question Answering, multimodal LLM]</li>
<li class=""><strong>authors:</strong> Chiao-An Yang, Ryo Hachiuma, Sifei Liu, Subhashree Radhakrishnan, Raymond A. Yeh, Yu-Chiang Frank Wang, Min-Hung Chen</li>
<li class=""><strong>institution:</strong> NVIDIA, Purdue University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17012" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17012</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc967334b04824c1dceb3e282d12da869887b61ee193194bf210064648df8377_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc967334b04824c1dceb3e282d12da869887b61ee193194bf210064648df8377_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces 4D-RGPT, a multimodal LLM enhanced via a Perceptual 4D Distillation (P4D) framework to improve 4D (3D spatial + temporal) understanding from video inputs. It also proposes the R4D-Bench benchmark for evaluating region-level 4D reasoning. The method shows notable performance improvements on both existing and the new benchmark.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] FORMSpoT: A Decade of Tree-Level, Country-Scale Forest Monitoring</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hierarchical transformer model (PVTv2), airborne laser scanning (ALS), spatio-temporal total variation denoising, co-registration, SPOT-6/7 composites]</li>
<li class=""><strong>authors:</strong> Martin Schwartz, Fajwel Fogel, Nikola Besic, Damien Robert, Louis Geist, Jean-Pierre Renaud, Jean-Matthieu Monnet, Clemens Mosig, Cédric Vega, Alexandre d&#x27;Aspremont, Loic Landrieu, Philippe Ciais</li>
<li class=""><strong>institution:</strong> Laboratoire des Sciences du Climat et de l’Environnement (LSCE), École Normale Supérieure – PSL, Université Gustave Eiffel, Université de Lorraine, University of Zurich, École des Ponts, Office National des Forêts, Université Grenoble Alpes, Institute of Earth System Science and Remote Sensing, Leipzig</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17021" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17021</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces FORMSpoT, a method that uses a hierarchical transformer model trained on ALS data to derive high-resolution forest canopy height maps from SPOT satellite time series, combined with a post-processing pipeline for robust change detection. It demonstrates that this approach significantly outperforms existing products in detecting small-scale forest disturbances, enabling tree-level monitoring at a national scale. The results highlight the importance of very-high-resolution satellite data for accurate forest carbon loss quantification and monitoring under climate change.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [infinite homography warping, video diffusion model, data augmentation, camera-controlled video generation]</li>
<li class=""><strong>authors:</strong> Min-Jung Kim, Jeongho Kim, Hoiyeong Jin, Junha Hyung, Jaegul Choo</li>
<li class=""><strong>institution:</strong> KAIST</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17040" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17040</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/984284ddd81cfd062ac9ad33ace38b4ec632b0ff9087f0906272c5b5d34e4175_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/984284ddd81cfd062ac9ad33ace38b4ec632b0ff9087f0906272c5b5d34e4175_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces InfCam, a framework that uses infinite homography warping to encode 3D camera rotations in a 2D latent space of a video diffusion model, avoiding depth estimation errors. It also employs a data augmentation pipeline to create diverse camera trajectories for training. The method demonstrates improved camera-pose accuracy and visual fidelity compared to existing approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Interpretable Similarity of Synthetic Image Utility</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical imaging], [Interpretable Utility Similarity (IUS), generalized neural additive models, synthetic data evaluation, deep learning, clinical decision support]</li>
<li class=""><strong>authors:</strong> Panagiota Gatoula, George Dimas, Dimitris K. Iakovidis</li>
<li class=""><strong>institution:</strong> University of Thessaly</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17080" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17080</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an interpretable measure called Interpretable Utility Similarity (IUS) to assess the similarity between synthetic and real medical images for deep learning-based clinical decision support. IUS uses generalized neural additive models to explain utility based on clinically relevant image features. Experiments show that selecting synthetic images with high IUS can improve classification performance by up to 54.6% across various medical imaging modalities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] DGH: Dynamic Gaussian Hair</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision, graphics], [dynamic Gaussian hair, coarse-to-fine model, strand-guided optimization, differentiable rendering, 3D Gaussian splatting]</li>
<li class=""><strong>authors:</strong> Junying Wang, Yuanlu Xu, Edith Tretschk, Ziyan Wang, Anastasia Ianina, Aljaz Bozic, Ulrich Neumann, Tony Tung</li>
<li class=""><strong>institution:</strong> University of Southern California, Meta Reality Labs Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17094" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17094</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9785766bd69495591006a354948b34ae7d717dcbb9e1e10f4bd690b1e6c2569_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9785766bd69495591006a354948b34ae7d717dcbb9e1e10f4bd690b1e6c2569_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Dynamic Gaussian Hair (DGH), a data-driven framework that learns hair dynamics and appearance using a coarse-to-fine motion model and a strand-guided 3D Gaussian representation. It enables photorealistic novel-view synthesis of hair under head motion without manual physics tuning. DGH provides a scalable, generalizable alternative to traditional simulation-based hair modeling.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Predictive Modeling of Maritime Radar Data Using Transformer Architecture</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [spatiotemporal forecasting], [transformer architecture, predictive modeling, radar frame prediction, spatiotemporal sequence forecasting]</li>
<li class=""><strong>authors:</strong> Bjorna Qesaraku, Jan Steckel</li>
<li class=""><strong>institution:</strong> Not explicitly provided in the given text.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17098" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17098</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/694341ad0b22f753337bbaead36e9cd3e845d080e2995764ba4a45a74113f8d0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/694341ad0b22f753337bbaead36e9cd3e845d080e2995764ba4a45a74113f8d0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This survey paper reviews predictive modeling approaches for maritime radar data, with a specific focus on the application of transformer architectures for spatiotemporal sequence forecasting. It concludes that while transformers have been used for AIS trajectory and sonar frame prediction, their use for maritime radar frame prediction remains an unexplored research gap, identifying a clear direction for future work.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nan Zhou, Huandong Wang, Jiahao Li, Yang Li, Xiao-Ping Zhang, Yong Li, Xinlei Chen</li>
<li class=""><strong>institution:</strong></li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17152" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17152</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4068b512da42fb271d02b97ac83087d5dbffa83b4904b964cddb5a1ffcc6de68_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4068b512da42fb271d02b97ac83087d5dbffa83b4904b964cddb5a1ffcc6de68_w640_q70.webp</a></li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [deep unrolled model, Restormer, learned coil sensitivity map estimator, sampling-aware weighted data consistency, universal conditioning, progressive cascade expansion training]</li>
<li class=""><strong>authors:</strong> Puyang Wang, Pengfei Guo, Keyi Chai, Jinyuan Zhou, Daguang Xu, Shanshan Jiang</li>
<li class=""><strong>institution:</strong> Johns Hopkins University, NVIDIA</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17137" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17137</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13a0f6637b571493e14f364092f2d39a108d211bbddd588a88df25d1db4a8e1c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13a0f6637b571493e14f364092f2d39a108d211bbddd588a88df25d1db4a8e1c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SDUM, a scalable deep unrolled model for universal MRI reconstruction that integrates a Restormer-based reconstructor, learned coil sensitivity estimation, and sampling-aware data consistency. It demonstrates predictable performance scaling with model depth and achieves state-of-the-art results across diverse clinical MRI protocols without task-specific fine-tuning. The work establishes a practical framework for a single, universally applicable reconstruction model in clinical environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Pro-Pose: Unpaired Full-Body Portrait Synthesis via Canonical UV Maps</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [canonical UV maps, SMPL-X poses, novel view synthesis, multi image finetuning, unpaired dataset]</li>
<li class=""><strong>authors:</strong> Sandeep Mishra, Yasamin Jafarian, Andreas Lugmayr, Yingwei Li, Varsha Ramakrishnan, Srivatsan Varadharajan, Alan C. Bovik, Ira Kemelmacher-Shlizerman</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin, Google</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17143" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17143</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ba0fc26561d25ebdc31e5c5b7c54d8ebaf35995e25f7717c3b45565ea557166_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ba0fc26561d25ebdc31e5c5b7c54d8ebaf35995e25f7717c3b45565ea557166_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Pro-Pose, a method for synthesizing professional full-body portraits from a single casual photo by transforming the person into a canonical UV map space and leveraging SMPL-X poses for reposing. This approach uses unpaired datasets and personalizes results through multi-image fine-tuning. It concludes that the method effectively preserves identity while generating high-quality, reposed avatars in varied poses and lighting.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Text-Conditioned Background Generation for Editable Multi-Layer Documents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [latent masking, Automated Readability Optimization (ARO), summarization-and-instruction process, multi-layer composition, WCAG 2.2]</li>
<li class=""><strong>authors:</strong> Taewon Kang, Joseph K J, Chris Tensmeyer, Jihyung Kil, Wanrong Zhu, Ming C. Lin, Vlad I. Morariu</li>
<li class=""><strong>institution:</strong> University of Maryland at College Park, Adobe Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17151" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17151</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3923a8122c7fd309cbab0c07d054d2af0f590d1779870be58f212682ecfafbb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3923a8122c7fd309cbab0c07d054d2af0f590d1779870be58f212682ecfafbb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a training-free framework for generating and editing document backgrounds using a diffusion model, ensuring text readability through latent masking and Automated Readability Optimization (ARO). It maintains multi-page thematic consistency via a summarization-and-instruction process and treats documents as editable multi-layer compositions. The method produces visually coherent, readable, and thematically aligned documents, bridging generative AI with practical design workflows.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Can Synthetic Images Serve as Effective and Efficient Class Prototypes?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multi-modal inference], [CLIP, large language model, diffusion model, zero-shot classification, visual prototypes, prompt generation]</li>
<li class=""><strong>authors:</strong> Dianxing Shi, Dingjie Fu, Yuqiao Liu, Jun Wang</li>
<li class=""><strong>institution:</strong> Beijing Research Institute of Uranium Geology, Huazhong University of Science and Technology, The Hong Kong University of Science and Technology (Guangzhou), Great Bay University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17160" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17160</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6907c7ddddf9ad9be39d226d2f021022e7cd01e6d45d58e12d3f8d9a6af7d1df_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6907c7ddddf9ad9be39d226d2f021022e7cd01e6d45d58e12d3f8d9a6af7d1df_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes LGCLIP, a framework that uses an LLM to generate class-specific prompts for a diffusion model to create synthetic images as visual prototypes, enabling zero-shot classification with only a visual encoder. It eliminates the need for annotated image-text pairs and reduces model complexity. Experiments show LGCLIP is effective and efficient, establishing a new lightweight paradigm for classification.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [CLIP, semantic refinement mechanism, local token-patch alignment, attribute-object binding, compositional image-text matching]</li>
<li class=""><strong>authors:</strong> Qi Zhang, Yuxu Chen, Lei Deng, Lili Shen</li>
<li class=""><strong>institution:</strong> Sichuan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17178" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17178</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea9a6c4b5b64bc9b1ed96212005ee492c3fd2ae615fc413250d7a6ef7c3bc712_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea9a6c4b5b64bc9b1ed96212005ee492c3fd2ae615fc413250d7a6ef7c3bc712_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes ABE-CLIP, a training-free method to enhance attribute-object binding in CLIP models. It uses a semantic refinement mechanism to improve text token embeddings and a local token-patch alignment strategy to compute image-text similarity. Experiments show the method significantly improves compositional matching performance, even surpassing some trained approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] It is not always greener on the other side: Greenery perception across demographics and personalities in multiple cities</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [urban perception analysis], [Green View Index (GVI), street view imagery, pairwise ratings, perception survey]</li>
<li class=""><strong>authors:</strong> Matias Quintana, Fangqi Liu, Jussi Torkko, Youlong Gu, Xiucheng Liang, Yujun Hou, Koichi Ito, Yihan Zhu, Mahmoud Abdelrahman, Tuuli Toivonen, Yi Lu, Filip Biljecki</li>
<li class=""><strong>institution:</strong> Singapore-ETH Centre, City University of Hong Kong, University of Helsinki, National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17186" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17186</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes discrepancies between objective greenery measurements (like Green View Index from street view imagery) and subjective human perceptions collected via surveys across five countries. The main finding is that demographic and personality factors do not significantly influence perception, but the location where people live is a key factor, suggesting cultural and environmental experiences shape how urban greenery is observed.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Globally Optimal Solution to the Generalized Relative Pose Estimation Problem using Affine Correspondences</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [affine correspondences, global optimization, polynomial eigenvalue solver, generalized essential matrix, generalized camera model]</li>
<li class=""><strong>authors:</strong> Zhenbao Yu, Banglei Guan, Shunkun Liang, Zibin Liu, Yang Shang, Qifeng Yu</li>
<li class=""><strong>institution:</strong> National University of Defense Technology, Wuhan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17188" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17188</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2a684e0cd1c8feea699186681976c3ac00a45866e1c3b726129daedbf69d23e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2a684e0cd1c8feea699186681976c3ac00a45866e1c3b726129daedbf69d23e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a globally optimal solver for estimating the generalized relative pose of multi-camera systems using affine correspondences and a known vertical direction. The method decouples rotation and translation, formulates a cost function, and solves it via polynomial eigenvalue techniques. Experimental results on synthetic and real data show the method outperforms state-of-the-art approaches in accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Anatomical Region-Guided Contrastive Decoding: A Plug-and-Play Strategy for Mitigating Hallucinations in Medical VLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [contrastive decoding, anatomical mask, token re-weighting, attention re-weighting, logits re-weighting]</li>
<li class=""><strong>authors:</strong> Xiao Liang, Chenxi Liu, Zhi Ma, Di Wang, Bin Jing, Quan Wang, Yuanyuan Shi</li>
<li class=""><strong>institution:</strong> Xidian University, Capital Medical University, the Ninth Medical Center of the Chinese PLA General Hospital</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17189" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17189</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9ca8b7c91013f45d10742514e49b941b478dcfd8c7f8216c3572f801dc9f1f9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9ca8b7c91013f45d10742514e49b941b478dcfd8c7f8216c3572f801dc9f1f9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Anatomical Region-Guided Contrastive Decoding (ARCD), a plug-and-play method that uses an anatomical mask to guide a three-tiered contrastive decoding process at the token, attention, and logits levels to reduce hallucinations in Medical Vision-Language Models. Experiments across multiple medical imaging datasets show that ARCD effectively improves regional understanding, reduces factually incorrect outputs, and enhances diagnostic accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Fose: Fusion of One-Step Diffusion and End-to-End Network for Pansharpening</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [one-step distillation, lightweight ensemble blocks, four-stage training, pansharpening, diffusion model, end-to-end network]</li>
<li class=""><strong>authors:</strong> Kai Liu, Zeli Lin, Weibo Wang, Linghe Kong, Yulun Zhang</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17202" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17202</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Fose, a lightweight network for pansharpening that fuses a one-step diffusion model and an end-to-end network using a novel four-stage training strategy. It uses one-step distillation to compress a diffusion model&#x27;s inference from 50 steps to 1 and integrates it with an E2E model via lightweight ensemble blocks. The method achieves better performance than state-of-the-art approaches and a 7.42x speedup compared to the baseline diffusion model.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [latent modulation, variational autoencoder, reinforcement learning, supervised fine-tuning, reasoning strategies, controllable exploration]</li>
<li class=""><strong>authors:</strong> Rujiao Long, Yang Li, Xingyao Zhang, Weixun Wang, Tianqianjin Lin, Xi Zhao, Yuchi Xu, Wenbo Su, Junchi Yan, Bo Zheng</li>
<li class=""><strong>institution:</strong> Alibaba Group, Shanghai Jiao Tong University, Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17206" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17206</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bfcb7d02fac2a6f117f80403719551786b6b7722d6b86bf1338ef9e18ddda6b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bfcb7d02fac2a6f117f80403719551786b6b7722d6b86bf1338ef9e18ddda6b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Reasoning Palette, a framework that uses a latent variable from a VAE to modulate a model&#x27;s reasoning trajectory via prepended token prefixes, enabling diverse strategic exploration. It shows that this approach improves exploration efficiency in RL training and leads to consistent performance gains over standard methods on reasoning benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [reinforcement learning, group relative policy optimization, knowledge graph consistency, entity-relation matching, process supervision, hard-example mining]</li>
<li class=""><strong>authors:</strong> Xiao Liang, Yuxuan An, Di Wang, Jiawei Hu, Zhicheng Jiao, Bin Jing, Quan Wang</li>
<li class=""><strong>institution:</strong> Xidian University, Brown University, Capital Medical University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17213" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17213</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c86c0e9aa8fe8870d86c5f63baf1ccd89856e7434ece25e03ba384660733fc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c86c0e9aa8fe8870d86c5f63baf1ccd89856e7434ece25e03ba384660733fc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes CheXPO-v2, an alignment framework that uses a Knowledge Graph Consistency Reward for process supervision to reduce hallucinations in medical VLMs. It parses reasoning into structured triplets to penalize incoherent logic, outperforming prior methods on benchmarks with high data efficiency. The approach achieves state-of-the-art accuracy on MIMIC-CXR-VQA using only 5k samples.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Robust Scene Coordinate Regression via Geometrically-Consistent Global Descriptors</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [scene coordinate regression, global descriptors, covisibility graphs, contrastive loss, batch-mining]</li>
<li class=""><strong>authors:</strong> Son Tung Nguyen, Tobias Fischer, Alejandro Fontan, Michael Milford</li>
<li class=""><strong>institution:</strong> Queensland University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17226" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17226</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a method that learns global descriptors for visual localization by combining geometric structure and visual similarity, using a batch-mining strategy and modified contrastive loss to train without manual labels. This approach corrects errors from unreliable geometric constraints and improves disambiguation in large-scale environments. Experiments show significant gains in localization accuracy while maintaining computational and memory efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] DAVE: A VLM Vision Encoder for Document Understanding and Web Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [vision encoder, self-supervised pretraining, autoregressive pretraining, model merging, ensemble training, SigLIP2]</li>
<li class=""><strong>authors:</strong> Brandon Huang, Hang Hua, Zhuoran Yu, Trevor Darrell, Rogerio Feris, Roei Herzig</li>
<li class=""><strong>institution:</strong> MIT-IBM Watson AI Lab, UC Berkeley, University of Wisconsin–Madison</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17221" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17221</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/344c53941f8829675f0d24c7d720447ce983b23a243af49de527a0a89a83b47b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/344c53941f8829675f0d24c7d720447ce983b23a243af49de527a0a89a83b47b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces DAVE, a vision encoder for VLMs designed for document understanding and web agents. Its training pipeline uses self-supervised pretraining on unlabeled data followed by supervised autoregressive pretraining, enhanced by model-merging and ensemble techniques to improve compatibility and performance. Experiments show DAVE is an effective vision encoder for document and web applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning When to Look: A Disentangled Curriculum for Strategic Perception in Multimodal Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [curriculum learning, supervised fine-tuning, reinforcement learning, perception-grounded chain-of-thought, pivotal perception reward]</li>
<li class=""><strong>authors:</strong> Siqi Yang, Zilve Gao, Haibo Qiu, Fanfan Liu, Peng Shi, Zhixiong Zeng, Qingmin Liao, Lin Ma</li>
<li class=""><strong>institution:</strong> Meituan, Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17227" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17227</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b925dc42674866f1a5d50b8321678a682bbb13a5906e16c5013dcca31b9aea_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b925dc42674866f1a5d50b8321678a682bbb13a5906e16c5013dcca31b9aea_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a two-stage curriculum framework to address visual forgetting in multimodal reasoning. It first builds an abstract reasoning backbone using text-only data and then uses reinforcement learning with a novel reward to teach the model a strategic policy for when to perceive visual information. This approach transforms the model into a more strategic and visually grounded reasoner.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Any-Optical-Model: A Universal Foundation Model for Optical Remote Sensing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [remote sensing foundation model], [spectrum-independent tokenizer, multi-scale adaptive patch embedding, channel-wise self-supervised masking and reconstruction, multi-scale semantic alignment]</li>
<li class=""><strong>authors:</strong> Xuyang Li, Chenyu Li, Danfeng Hong</li>
<li class=""><strong>institution:</strong> Southeast University, Aerospace Information Research Institute, Chinese Academy of Sciences, University of Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17224" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17224</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed61cab861efd49f74d6151efc271fc265c8ca3f3dbd380d3539ee4626e62ea6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed61cab861efd49f74d6151efc271fc265c8ca3f3dbd380d3539ee4626e62ea6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Any-Optical-Model (AOM), a universal foundation model for optical remote sensing designed to handle arbitrary band compositions and spatial resolutions. Its core innovations include a spectrum-independent tokenizer, multi-scale adaptive patch embedding, and a channel-wise self-supervised pretraining strategy. Experiments show AOM achieves state-of-the-art performance in challenging scenarios like band missing and cross-sensor/cross-resolution settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Video Detective: Seek Critical Clues Recurrently to Answer Question from Long Videos</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [question-aware memory mechanism, recurrent processing, token compression, memory tokens, long video question-answering]</li>
<li class=""><strong>authors:</strong> Henghui Du, Chang Zhou, Chunjie Zhang, Xi Chen, Di Hu</li>
<li class=""><strong>institution:</strong> Renmin University of China, Tencent PCG</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17229" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17229</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c8fa371c579b373deada655507be872a0843b939c4fd62131127a51ed2f82f2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c8fa371c579b373deada655507be872a0843b939c4fd62131127a51ed2f82f2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes VideoDetective, a method that uses a question-aware memory mechanism to recurrently process long videos by compressing sub-segments into special memory tokens, enabling efficient question-answering. This approach allows models with limited context length to handle long videos with reduced memory and time. Experimental results show it effectively seeks critical clues from massive video information.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Mitty: Diffusion-based Human-to-Robot Video Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [robotics and computer vision], [diffusion transformer, in-context learning, video generation, human-to-robot translation, bidirectional attention]</li>
<li class=""><strong>authors:</strong> Yiren Song, Cheng Liu, Weijia Mao, Mike Zheng Shou</li>
<li class=""><strong>institution:</strong> National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17253" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17253</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ece085e6946a833f7d094099001b4940be20339b6e48e0def90236df73a83e6d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ece085e6946a833f7d094099001b4940be20339b6e48e0def90236df73a83e6d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Mitty is a Diffusion Transformer model that uses in-context learning to directly convert human demonstration videos into robot-execution videos without intermediate representations. It leverages a pretrained video diffusion model and an automatic synthesis pipeline to address data scarcity. The method achieves state-of-the-art performance and strong generalization in robot learning from human observations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] AnyCXR: Human Anatomy Segmentation of Chest X-ray at Any Acquisition Position using Multi-stage Domain Randomized Synthetic Data with Imperfect Annotations and Conditional Joint Annotation Regularization Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical imaging segmentation], [Multi-stage Domain Randomization (MSDR), Conditional Joint Annotation Regularization (CAR), synthetic data generation, zero-shot generalization, anatomical consistency]</li>
<li class=""><strong>authors:</strong> Dong Zifei, Wu Wenjie, Hao Jinkui, Chen Tianqi, Weng Ziqiao, Zhou Bo</li>
<li class=""><strong>institution:</strong> Northwestern University, Vanderbilt University, Shanxi Medical University, University of Sydney</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17263" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17263</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes AnyCXR, a framework for chest X-ray segmentation that uses synthetic data generated via Multi-stage Domain Randomization and a Conditional Joint Annotation Regularization learning strategy to handle imperfect labels. It demonstrates strong zero-shot generalization to real-world X-rays across different views, enabling accurate multi-organ segmentation and improving downstream clinical tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] WDFFU-Mamba: A Wavelet-guided Dual-attention Feature Fusion Mamba for Breast Tumor Segmentation in Ultrasound Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical image segmentation], [wavelet-guided enhancement, dual-attention feature fusion, U-shaped Mamba architecture, Wavelet-denoised High-Frequency-guided Feature (WHF), Dual Attention Feature Fusion (DAFF)]</li>
<li class=""><strong>authors:</strong> Guoping Cai, Houjin Chen, Yanfeng Li, Jia Sun, Ziwei Chen, Qingzi Geng</li>
<li class=""><strong>institution:</strong> Beijing Jiaotong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17278" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17278</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes WDFFU-Mamba, a novel network for breast ultrasound image segmentation that integrates wavelet-guided enhancement and dual-attention feature fusion within a U-shaped Mamba architecture. It demonstrates superior segmentation accuracy and robustness on public datasets, making it a promising tool for clinical applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Diagnostic Performance of Universal-Learning Ultrasound AI Across Multiple Organs and Tasks: the UUSIC25 Challenge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [deep learning, multi-task learning, domain generalization, segmentation, classification, Dice Similarity Coefficient, Area Under the Curve]</li>
<li class=""><strong>authors:</strong> Zehui Lin, Luyi Han, Xin Wang, Ying Zhou, Yanming Zhang, Tianyu Zhang, Lingyun Bao, Shandong Wu, Dong Xu, Tao Tan, UUSIC25 Challenge Consortium</li>
<li class=""><strong>institution:</strong> Macao Polytechnic University, Netherlands Cancer Institute, Zhejiang Cancer Hospital, The First People’s Hospital of Hangzhou, University of Pittsburgh</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17279" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17279</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates general-purpose deep learning models for multi-organ classification and segmentation in ultrasound imaging through the UUSIC25 challenge. The top model demonstrated high accuracy and efficiency across tasks using a single architecture. However, performance degraded on data from unseen institutions, highlighting the critical need for improved domain generalization before clinical deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Vision-Language Model Guided Image Restoration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [vision-language model, CLIP, diffusion model, cross-attention, LoRA fine-tuning, degradation predictor]</li>
<li class=""><strong>authors:</strong> Cuixin Yang, Rongkang Dong, Kin-Man Lam</li>
<li class=""><strong>institution:</strong> The Hong Kong Polytechnic University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17292" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17292</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abd921f37f90b9dfaf497e986d1ae89307cf4b11dd527bf09bd11e36d239652c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abd921f37f90b9dfaf497e986d1ae89307cf4b11dd527bf09bd11e36d239652c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes the VLMIR framework, which uses a vision-language model (like CLIP) to extract aligned visual and textual features from images and integrates them via cross-attention into a diffusion model for restoration. It concludes that this approach, leveraging linguistic priors for semantic coherence, achieves superior performance in universal and degradation-specific image restoration tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [feature caching, selective computation, constraint-aware scheduling, temporal redundancy, activation schedule]</li>
<li class=""><strong>authors:</strong> Fanpu Cao, Yaofo Chen, Zeng You, Wei Luo, Cen Chen</li>
<li class=""><strong>institution:</strong> South China University of Technology, South China Agricultural University, Pazhou Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17298" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17298</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af7870aa86d8ec0f40bf8ee51c36b05d76a3b23aeb26f3f88d6835a77ed1a314_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af7870aa86d8ec0f40bf8ee51c36b05d76a3b23aeb26f3f88d6835a77ed1a314_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes ProCache, a training-free framework to accelerate Diffusion Transformers (DiTs) by using dynamic feature caching. It introduces a constraint-aware caching pattern search to create non-uniform activation schedules and a selective computation module to mitigate error accumulation. Experiments show ProCache achieves significant speedups with minimal quality loss, outperforming prior caching methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Towards Pixel-Wise Anomaly Location for High-Resolution PCBA \ via Self-Supervised Image Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [self-supervised learning, image reconstruction, anomaly detection, SIR-Gate, ROPS]</li>
<li class=""><strong>authors:</strong> Wuyi Liu, Le Jin, Junxian Yang, Yuanchao Yu, Zishuo Peng, Jinfeng Xu, Xianzhi Li, Jun Zhou</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology, Siemens AG, University of Electronic Science and Technology of China, Peking University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17296" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17296</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d229cd3ccc08641839e524f99e7548770f7ccd524f2ac8dd0882a9248c5c358_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d229cd3ccc08641839e524f99e7548770f7ccd524f2ac8dd0882a9248c5c358_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces HiSIR-Net, a self-supervised image reconstruction framework for pixel-wise anomaly localization on high-resolution PCBA images. It uses two novel modules—SIR-Gate to reduce reconstruction noise and ROPS for coherent patch selection—to achieve accurate defect detection with low false positives. The method demonstrates superior performance on a new dataset (SIPCBA-500) and public benchmarks while maintaining practical inference speed.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] EMAG: Self-Rectifying Diffusion Sampling with Exponential Moving Average Guidance</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [diffusion models], [classifier-free guidance, attention modification, exponential moving average, adaptive layer selection, diffusion transformers]</li>
<li class=""><strong>authors:</strong> Ankit Yadav, Ta Duc Huy, Lingqiao Liu</li>
<li class=""><strong>institution:</strong> Australian Institute for Machine Learning, The University of Adelaide</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17303" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17303</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a085ffcd94c7d87433d4ff44b17cee0378a874440ce80df0ee2bca3a5e2171d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a085ffcd94c7d87433d4ff44b17cee0378a874440ce80df0ee2bca3a5e2171d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Exponential Moving Average Guidance (EMAG), a training-free inference method for diffusion transformers that adaptively modifies attention maps to generate challenging negative samples. This allows the model to correct fine-grained artifacts, improving image quality and human preference scores over standard guidance. The method is shown to be compatible with other advanced guidance techniques for further gains.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [chain-of-thought, multi-turn reasoning, self-reflection, redundancy-penalized policy optimization, supervised fine-tuning, reinforcement learning]</li>
<li class=""><strong>authors:</strong> Wenhao Yang, Yu Xia, Jinlong Huang, Shiyin Lu, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Yuanyu Wan, Lijun Zhang</li>
<li class=""><strong>institution:</strong> Nanjing University, Alibaba Group, Shanghai Jiao Tong University, Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17306" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17306</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/689b25012cac991ff522614c103b42d1e5450440f1eed392ffaf92e6a3ff7c51_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/689b25012cac991ff522614c103b42d1e5450440f1eed392ffaf92e6a3ff7c51_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes DRIM, a model that enhances multi-turn reasoning in vision-language models by integrating a self-reflective chain-of-thought with tool invocation. It uses a three-stage pipeline of data construction, supervised fine-tuning, and redundancy-penalized reinforcement learning to encourage reliable exploration and self-correction. Experiments show DRIM achieves superior performance on visual understanding benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] CodeDance: A Dynamic Tool-integrated MLLM for Executable Visual Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [executable code, tool orchestration, reinforcement learning, visual reasoning, reward shaping]</li>
<li class=""><strong>authors:</strong> Qi Song, Honglin Li, Yingchen Yu, Haoyi Zhou, Lin Yang, Song Bai, Qi She, Zilong Huang, Yunqing Zhao</li>
<li class=""><strong>institution:</strong> Beihang University, Westlake University, ByteDance</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17312" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17312</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51476eaff011f154f477005e3f785b652c69da471d13c3f8a556338b397999d8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51476eaff011f154f477005e3f785b652c69da471d13c3f8a556338b397999d8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces CodeDance, a multimodal large language model that uses executable code to dynamically orchestrate multiple tools for visual reasoning. It employs a reinforcement learning reward to balance tool use, leading to adaptive and efficient reasoning. The method outperforms schema-driven and text-only baselines, and even surpasses advanced closed models like GPT-4o on various benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MatLat: Material Latent Space for PBR Texture Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [latent diffusion, VAE fine-tuning, material latent space, patch-based regularization, correspondence-aware attention]</li>
<li class=""><strong>authors:</strong> Kyeongmin Yeo, Yunhong Min, Jaihoon Kim, Minhyuk Sung</li>
<li class=""><strong>institution:</strong> KAIST</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17302" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17302</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/706f201e005d8752d9c73c8781f7323062cdfffadcfa51a8faf2f6cc621472c2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/706f201e005d8752d9c73c8781f7323062cdfffadcfa51a8faf2f6cc621472c2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes MatLat, a generative framework that fine-tunes a pretrained VAE to create a material latent space for generating high-quality PBR textures, addressing dataset scarcity and distribution shift issues. It introduces a patch-based regularization during VAE fine-tuning to preserve spatial locality between latent codes and image pixels, which is crucial for cross-view consistency. The method demonstrates improved PBR texture fidelity and achieves state-of-the-art performance, with each component being essential.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Auxiliary Descriptive Knowledge for Few-Shot Adaptation of Vision-Language Model</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [vision-language models], [few-shot adaptation, parameter-efficient fine-tuning, auxiliary descriptive knowledge, large language model, non-parametric attention, compositional knowledge, instance-specific knowledge]</li>
<li class=""><strong>authors:</strong> SuBeen Lee, GilHan Park, WonJun Moon, Hyun Seok Seong, Jae-Pil Heo</li>
<li class=""><strong>institution:</strong> Sungkyunkwan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17313" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17313</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2fe4006b163d54a1cba24642885d5db064fb85d1d01ab783d4dee1ada2fbc75_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2fe4006b163d54a1cba24642885d5db064fb85d1d01ab783d4dee1ada2fbc75_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Auxiliary Descriptive Knowledge (ADK), a framework that enhances few-shot adaptation of vision-language models by using an LLM to generate offline descriptive prompts for each class. ADK enriches text representations through averaged compositional knowledge and a lightweight attention mechanism for instance-specific knowledge, improving classification without added inference cost. It consistently boosts existing parameter-efficient fine-tuning methods, achieving state-of-the-art performance across various scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [remote sensing, multimodal evaluation], [RSHR-Bench, adversarial filtering, high-resolution imagery, multimodal large language models, visual question answering, image captioning]</li>
<li class=""><strong>authors:</strong> Yunkai Dang, Meiyi Zhu, Donghao Wang, Yizhuo Zhang, Jiacheng Yang, Qi Fan, Yuekun Yang, Wenbin Li, Feng Miao, Yang Gao</li>
<li class=""><strong>institution:</strong> Nanjing University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17319" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17319</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/283e413d1a1fd46323ee20a12df05789e8ef6dd69af3578d2d3e3e27ce803395_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/283e413d1a1fd46323ee20a12df05789e8ef6dd69af3578d2d3e3e27ce803395_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces RSHR-Bench, a super-high-resolution benchmark for evaluating multimodal large language models (MLLMs) in remote sensing. The benchmark uses adversarial filtering with strong LLMs and human verification to create tasks that reduce reliance on language priors and require genuine visual understanding. The evaluation reveals that existing models, including RS-specific ones, show significant performance gaps when handling ultra-high-resolution remote sensing imagery.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] EMMA: Concept Erasure Benchmark with Comprehensive Semantic Metrics and Diverse Categories</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [concept erasure, text-to-image generation, benchmark evaluation, semantic metrics, robustness testing, bias analysis]</li>
<li class=""><strong>authors:</strong> Lu Wei, Yuta Nakashima, Noa Garcia</li>
<li class=""><strong>institution:</strong> Osaka University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17320" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17320</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/203b001da1851854025aaa0d66f1fc1bc1e32821cd9e1295f8360da31919f60c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/203b001da1851854025aaa0d66f1fc1bc1e32821cd9e1295f8360da31919f60c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces EMMA, a comprehensive benchmark for evaluating concept erasure techniques in text-to-image diffusion models. It tests five key dimensions across 12 metrics, including robustness to indirect prompts and bias. The main conclusion is that existing erasure methods struggle with implicit descriptions and visually similar concepts, and some can amplify gender and ethnicity bias.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Democratizing Pathology Co-Pilots: An Open Pipeline and Dataset for Whole-Slide Vision-Language Modelling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [vision-language model, instruction tuning, visual question answering, whole-slide images, synthetic instruction generation]</li>
<li class=""><strong>authors:</strong> Sander Moonemans, Sebastiaan Ram, Frédérique Meeuwsen, Carlijn Lems, Jeroen van der Laak, Geert Litjens, Francesco Ciompi</li>
<li class=""><strong>institution:</strong> Radboud University Medical Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17326" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17326</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Polysome, a tool for generating synthetic instructions, and uses it to create HISTAI-Instruct, a large instruction-tuning dataset from whole-slide images. It then trains a vision-language model called ANTONI-α on this dataset, which is shown to outperform MedGemma on tasks like tissue identification and differential diagnosis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Rotterdam artery-vein segmentation (RAV) dataset</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical imaging], [color fundus images, artery-vein segmentation, machine learning algorithms, connectivity validation, custom annotation interface]</li>
<li class=""><strong>authors:</strong> Jose Vargas Quiros, Bart Liefers, Karin van Garderen, Jeroen Vermeulen, Eyened Reading Center, Caroline Klaver</li>
<li class=""><strong>institution:</strong> Erasmus University Medical Center, Radboud University Medical Center, Institute of Molecular and Clinical Ophthalmology, University of Basel</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17322" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17322</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the Rotterdam Artery-Vein segmentation dataset, created by sampling and annotating color fundus images from the Rotterdam Study using a custom interface with connectivity validation tools. The dataset includes diverse, high-quality artery-vein segmentation masks and varied image modalities to support the development of robust machine learning models for retinal vascular analysis. The main conclusion is that this heterogeneous dataset enables benchmarking and training of clinically applicable algorithms under real-world variability in image quality and acquisition.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] DESSERT: Diffusion-based Event-driven Single-frame Synthesis via Residual Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [diffusion model, residual training, variational autoencoder, event camera, frame synthesis]</li>
<li class=""><strong>authors:</strong> Jiyun Kong, Jun-Hyuk Kim, Jong-Seok Lee</li>
<li class=""><strong>institution:</strong> Yonsei University, Chung-Ang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17323" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17323</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9efc7d352cf45ecba0837aaa2807f3af645c0b85f2e4138960afaf786d151d9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9efc7d352cf45ecba0837aaa2807f3af645c0b85f2e4138960afaf786d151d9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes DESSERT, a method for event-driven video frame synthesis using a diffusion model trained on inter-frame residuals and conditioned on event data. It employs a two-stage pipeline with an Event-to-Residual Alignment VAE and a diffusion model, enhanced by Diverse-Length Temporal augmentation. The method outperforms existing approaches in producing sharper and more temporally consistent frames.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Multi-level distortion-aware deformable network for omnidirectional image super-resolution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [deformable attention, dilated deformable convolution, low-rank decomposition, multi-level feature fusion]</li>
<li class=""><strong>authors:</strong> Cuixin Yang, Rongkang Dong, Kin-Man Lam, Yuhang Zhang, Guoping Qiu</li>
<li class=""><strong>institution:</strong> The Hong Kong Polytechnic University, Guangzhou University, University of Nottingham</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17343" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17343</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c76dd9c20ef0f6f5224210965ff34e6934829551ddedeec3fc607a5c362844_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c76dd9c20ef0f6f5224210965ff34e6934829551ddedeec3fc607a5c362844_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a Multi-level Distortion-aware Deformable Network (MDDN) for omnidirectional image super-resolution, which uses parallel branches of deformable attention and dilated deformable convolutions to capture geometric distortions. It also employs a low-rank decomposition to reduce computational cost. Experiments show that MDDN outperforms existing state-of-the-art methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SynergyWarpNet: Attention-Guided Cooperative Warping for Neural Portrait Animation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [attention-guided cooperative warping, 3D dense optical flow, cross-attention, 3D keypoints, confidence-guided fusion]</li>
<li class=""><strong>authors:</strong> Shihang Li, Zhiqiang Gong, Minming Ye, Yue Gao, Wen Yao</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Defense Innovation Institute Academy of Military Science, Intelligent Game and Decision Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17331" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17331</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5df9fe07b541e5d7185f0fbd4b4122b096d1ab08ec897c73021adba9ad3835b5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5df9fe07b541e5d7185f0fbd4b4122b096d1ab08ec897c73021adba9ad3835b5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SynergyWarpNet, a three-stage framework for neural portrait animation that combines explicit warping using 3D optical flow with reference-augmented correction via cross-attention and a confidence-guided fusion module. It aims to address the limitations of traditional warping and attention-based methods by integrating geometric alignment with semantic completion. The model achieves state-of-the-art performance on benchmark datasets for high-fidelity talking head synthesis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Beyond Semantic Features: Pixel-level Mapping for Generalized AI-Generated Image Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [pixel-level mapping, high-frequency traces, semantic bias, cross-generator generalization]</li>
<li class=""><strong>authors:</strong> Chenming Zhou, Jiaan Wang, Yu Li, Lei Li, Juan Cao, Sheng Tang</li>
<li class=""><strong>institution:</strong> Institute of Computing Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17350" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17350</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12f826d61dc8f5ba342ef7b8b646a6c2d30576e4692242121c7c5e8cc16f5f7d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12f826d61dc8f5ba342ef7b8b646a6c2d30576e4692242121c7c5e8cc16f5f7d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a pixel-level mapping pre-processing step to disrupt pixel value distributions and break semantic shortcuts, forcing detectors to focus on generalizable high-frequency traces from image generation. This method significantly improves the cross-generator performance of state-of-the-art AI-generated image detectors, verifying that disrupting semantic cues is key to generalization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Towards Deeper Emotional Reflection: Crafting Affective Image Filters with Generative Priors</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [affective image filter, multi-modal transformer, diffusion models, emotional fidelity, content consistency]</li>
<li class=""><strong>authors:</strong> Peixuan Zhang, Shuchen Weng, Jiajun Tang, Si Li, Boxin Shi</li>
<li class=""><strong>institution:</strong> Beijing University of Posts and Telecommunications, Beijing Academy of Artificial Intelligence, Peking University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17376" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17376</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d7baeb5807dd11748bebc6c7cf63706b9492484a047cbe2a12cba4c75d153a1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d7baeb5807dd11748bebc6c7cf63706b9492484a047cbe2a12cba4c75d153a1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes AIF-D, an Affective Image Filter model that extends a multi-modal transformer baseline by leveraging generative priors from pre-trained large-scale diffusion models to reflect emotions from text into images. It demonstrates superior performance in content consistency and emotional fidelity compared to state-of-the-art methods and is more effective at evoking specific emotions according to user studies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [vision-language models], [CulturalToM-VQA, visual question answering, chain-of-thought prompting, compositional chain-of-thought prompting, false belief reasoning, social desirability bias]</li>
<li class=""><strong>authors:</strong> Zabir Al Nazi, G M Shahariar, Abrar Hossain, Wei Peng</li>
<li class=""><strong>institution:</strong> University of California, Riverside, University of Dhaka, Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17394" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17394</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces CulturalToM-VQA, a benchmark dataset built via a VLM-assisted human-in-the-loop pipeline to evaluate cross-cultural Theory of Mind reasoning in Vision-Language Models. It finds that while newer VLMs show strong performance on explicit tasks, they systematically struggle with false belief reasoning, and their results may be inflated by social desirability bias rather than genuine visual understanding.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [visual question answering, vision-language models, fine-tuning, benchmark dataset, CT, MRI]</li>
<li class=""><strong>authors:</strong> Léo Butsanets, Charles Corbière, Julien Khlaut, Pierre Manceron, Corentin Dancette</li>
<li class=""><strong>institution:</strong> Raidium, Université de Paris Cité, Hôpital Européen Georges Pompidou, AP-HP, INSERM</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17396" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17396</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ed7cbd6c6a04ef8f9a23147e56923de1ca94a48cc51c20cfa98200b90baa146_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ed7cbd6c6a04ef8f9a23147e56923de1ca94a48cc51c20cfa98200b90baa146_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces RadImageNet-VQA, a large-scale CT and MRI dataset with expert-curated annotations for radiologic visual question answering, designed to evaluate vision-language models on tasks like abnormality detection and pathology identification. Experiments show that current models struggle with fine-grained pathology identification, especially in open-ended settings, and the dataset avoids linguistic shortcuts as models perform near-random without image inputs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Beyond Occlusion: In Search for Near Real-Time Explainability of CNN-Based Prostate Cancer Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [occlusion, GradCAM++, HiResCAM, Composite-L, CAM, explainable AI, convolutional neural networks]</li>
<li class=""><strong>authors:</strong> Martin Krebs, Jan Obdržálek, Vít Musil, Tomáš Brázdil</li>
<li class=""><strong>institution:</strong> Masaryk University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17416" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17416</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper seeks a faster alternative to the occlusion method for explaining CNN-based prostate cancer classification. By establishing comparison criteria and metrics, the authors evaluate several single-pass explanation methods like GradCAM++ and HiResCAM. They identify a method that reduces explanation time by at least a factor of 10 without compromising output quality, facilitating faster model development and clinical adoption.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] AIFloodSense: A Global Aerial Imagery Dataset for Semantic Segmentation and Understanding of Flooded Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision, disaster management], [semantic segmentation, visual question answering, image classification, deep learning, aerial imagery]</li>
<li class=""><strong>authors:</strong> Georgios Simantiris, Konstantinos Bacharidis, Apostolos Papanikolaou, Petros Giannakakis, Costas Panagiotakis</li>
<li class=""><strong>institution:</strong> Hellenic Mediterranean University, Institute of Computer Science, FORTH</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17432" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17432</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces AIFloodSense, a global aerial imagery dataset for flood detection, supporting tasks like semantic segmentation, image classification, and visual question answering. It establishes baseline benchmarks using state-of-the-art deep learning models. The main conclusion is that the dataset&#x27;s global diversity and multi-task support advance the development of robust, domain-generalized AI tools for climate resilience and disaster assessment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Xiaomi MiMo-VL-Miloco Technical Report</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [supervised fine-tuning, reinforcement learning, Group Relative Policy Optimization, chain-of-thought supervision, token-budget-aware reasoning, quantization]</li>
<li class=""><strong>authors:</strong> Jiaze Li, Jingyang Chen, Yuxun Qu, Jianzhong Ju, Zhenbo Luo, Jian Luan, Shijie Xu, Zhenru Lin, Junyou Zhu, Boshen Xu, Wenhui Tan, Pei Fu</li>
<li class=""><strong>institution:</strong> Xiaomi</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17436" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17436</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5131a57d5cef340803d1dd4e55e39db8e4fc7a8b7925e87579be976c079279c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5131a57d5cef340803d1dd4e55e39db8e4fc7a8b7925e87579be976c079279c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MiMo-VL-Miloco-7B, a vision-language model specialized for smart-home understanding, built via a two-stage training pipeline combining supervised fine-tuning and reinforcement learning. The model achieves leading performance on home-scenario tasks like gesture recognition and also shows gains on general multimodal and language reasoning benchmarks. The authors conclude that targeted home-scenario training enhances activity understanding and can improve text-only reasoning with minimal trade-offs on other tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] LangDriveCTRL: Natural Language Controllable Driving Scene Editing with Multi-modal Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [scene graph, agentic pipeline, video diffusion, 3D scene decomposition, natural language control, trajectory generation]</li>
<li class=""><strong>authors:</strong> Yun He, Francesco Pittaluga, Ziyu Jiang, Matthias Zwicker, Manmohan Chandraker, Zaid Tasneem</li>
<li class=""><strong>institution:</strong> University of Maryland, College Park, NEC Labs America, UC San Diego</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17445" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17445</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0247e2a8c71687baf6433034fd351bc4574e288c71df25dcc17678102ec16d05_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0247e2a8c71687baf6433034fd351bc4574e288c71df25dcc17678102ec16d05_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LangDriveCTRL is a framework that edits real-world driving videos using natural language instructions. It decomposes scenes into 3D graphs and uses an agentic pipeline with specialized modules for object grounding, behavior editing, and review, followed by video diffusion for refinement. The method achieves significantly higher instruction alignment and realism compared to previous state-of-the-art approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [multimodal semantic segmentation, deep neural network, robust training strategies, synchronized sensor data, daytime training for nighttime performance]</li>
<li class=""><strong>authors:</strong> Jon Muhovič, Janez Perš</li>
<li class=""><strong>institution:</strong> University of Ljubljana</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17450" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17450</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6eb14eac17bf3aa4e9ce145e08ce4eae06c5adaaf8ccf31ca3fa25a7f3835fa0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6eb14eac17bf3aa4e9ce145e08ce4eae06c5adaaf8ccf31ca3fa25a7f3835fa0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MULTIAQUA, a multimodal maritime dataset with synchronized RGB, thermal, IR, and LIDAR data, and proposes robust training strategies for multimodal semantic segmentation. The core method enables training a deep neural network using only daytime images to achieve reliable performance in challenging conditions like near-complete darkness. The main conclusion is that this approach simplifies data acquisition and annotation while maintaining robust scene interpretation for unmanned surface vehicles.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] 3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [3D scene reconstruction, generative models, differentiable optimization, compositional framework, camera recovery, spatial optimization]</li>
<li class=""><strong>authors:</strong> Tobias Sautter, Jan-Niklas Dihlmann, Hendrik P.A. Lensch</li>
<li class=""><strong>institution:</strong> University of Tübingen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17459" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17459</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a86f0bc63e56b2eb3e9a7eb63d3a03a4eff7de9fc52eba3ee24958d82a3a3b98_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a86f0bc63e56b2eb3e9a7eb63d3a03a4eff7de9fc52eba3ee24958d82a3a3b98_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> 3D-RE-GEN is a compositional framework that reconstructs a single image into textured 3D objects and a background by integrating models for detection, reconstruction, and placement, using generative models for occluded objects and a novel 4-DoF optimization for layout alignment. It achieves state-of-the-art performance in single-image 3D scene reconstruction, producing coherent, modifiable scenes suitable for VFX and game development.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, digital twin, Vision Transformer, ViT-UNet, privacy-preserving AI, brain tumor segmentation]</li>
<li class=""><strong>authors:</strong> Almustapha A. Wakili, Adamu Hussaini, Abubakar A. Musa, Woosub Jung, Wei Yu</li>
<li class=""><strong>institution:</strong> Towson University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17488" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17488</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67c12280225e186b761a6952a367a2b042a6fcd1886ac481e9bc15f5f81ee64a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67c12280225e186b761a6952a367a2b042a6fcd1886ac481e9bc15f5f81ee64a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes TwinSegNet, a federated learning framework that uses a hybrid ViT-UNet model and personalized digital twins for brain tumor segmentation without sharing raw data. It achieves high accuracy on heterogeneous MRI datasets, demonstrating that privacy can be preserved without sacrificing segmentation performance in multi-institutional settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Validation of Diagnostic Artificial Intelligence Models for Prostate Pathology in a Middle Eastern Cohort</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical diagnostics], [artificial intelligence, digital pathology, prostate cancer, Gleason grading, external validation, Cohen&#x27;s quadratically weighted kappa, compact scanner]</li>
<li class=""><strong>authors:</strong> Peshawa J. Muhammad Ali, Navin Vincent, Saman S. Abdulla, Han N. Mohammed Fadhl, Anders Blilie, Kelvin Szolnoky, Julia Anna Mielcarz, Xiaoyi Ji, Nita Mulliqi, Abdulbasit K. Al-Talabani, Kimmo Kartasalo</li>
<li class=""><strong>institution:</strong> Koya University, Karolinska Institutet</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17499" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17499</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This study externally validated AI models for prostate cancer diagnosis and Gleason grading using a Middle Eastern cohort from Iraq. The AI models demonstrated performance comparable to pathologists and showed high consistency across different digital slide scanners, including low-cost compact models. The findings support the global adoption of AI in pathology, particularly in under-represented regions with limited resources.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] LumiCtrl : Learning Illuminant Prompts for Lighting Control in Personalized Text-to-Image Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [illuminant personalization, physics-based illuminant augmentation, edge-guided prompt disentanglement, masked reconstruction loss, contextual light adaptation]</li>
<li class=""><strong>authors:</strong> Muhammad Atif Butt, Kai Wang, Javier Vazquez-Corral, Joost Van De Weijer</li>
<li class=""><strong>institution:</strong> Computer Vision Center, Universitat Autònoma de Barcelona, City University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17489" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17489</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LumiCtrl is a method for learning illuminant prompts from a single object image to control lighting in personalized text-to-image models. It uses physics-based augmentation, edge-guided prompt disentanglement, and a masked reconstruction loss to achieve contextual light adaptation. The method outperforms existing baselines in illuminant fidelity, aesthetic quality, and scene coherence, as confirmed by a human preference study.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MMLANDMARKS: a Cross-View Instance-Level Benchmark for Geo-Spatial Understanding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multimodal geo-spatial understanding], [cross-view retrieval, geolocalization, CLIP-inspired baseline, multimodal dataset, instance-level benchmark]</li>
<li class=""><strong>authors:</strong> Oskar Kristoffersen, Alba R. Sánchez, Morten R. Hannemose, Anders B. Dahl, Dim P. Papadopoulos</li>
<li class=""><strong>institution:</strong> Technical University of Denmark, Pioneer Center for AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17492" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17492</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80d6684c681e0605e017a84878993e58d1f060cd6906ad893ab11cebf1a5f9d2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80d6684c681e0605e017a84878993e58d1f060cd6906ad893ab11cebf1a5f9d2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MMLANDMARKS, a multimodal dataset with aerial images, ground-view images, text, and GPS coordinates for geo-spatial tasks. Using a simple CLIP-inspired baseline, the authors show competitive performance across tasks like cross-view retrieval and geolocalization, highlighting the need for multimodal datasets for comprehensive geo-spatial understanding.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [4D scene geometry, diffusion-based video generation, occlusion consistency, illumination-aware dataset, mask generation]</li>
<li class=""><strong>authors:</strong> Hoiyeong Jin, Hyojin Jang, Jeongho Kim, Junha Hyung, Kinam Kim, Dongjin Kim, Huijin Choi, Hyeonji Kim, Jaegul Choo</li>
<li class=""><strong>institution:</strong> KAIST AI, SK Telecom</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17504" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17504</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f1824f6e52cce42d09258c21a8f994f87c3330105a845886e13a668bacd45e38_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f1824f6e52cce42d09258c21a8f994f87c3330105a845886e13a668bacd45e38_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents InsertAnywhere, a framework for realistic video object insertion that combines 4D scene geometry reconstruction with a diffusion-based video generation model to ensure geometric and temporal consistency. It introduces a synthetic dataset, ROSE++, for supervised training. The method outperforms existing models in producing visually coherent insertions suitable for production environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [sensor fusion and filtering], [Error-State Extended Kalman Filter, Scaled Unscented Kalman Filter, visual-inertial odometry, quaternion estimation, adaptive covariance, loosely coupled architecture]</li>
<li class=""><strong>authors:</strong> Ufuk Asil, Efendi Nasibov</li>
<li class=""><strong>institution:</strong> Dokuz Eylul University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17505" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17505</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0950fd135a7ca6f717b1ee73ea881e6e27ca5075f9e2e7ca11cb3c42ff286cab_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0950fd135a7ca6f717b1ee73ea881e6e27ca5075f9e2e7ca11cb3c42ff286cab_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid VIO method that combines an Error-State EKF with a targeted Scaled UKF step for orientation refinement, while dynamically adjusting visual measurement noise based on image quality metrics. The approach achieves significant improvements in accuracy over ESKF-based methods and reduces computational cost compared to a full UKF, balancing efficiency and performance in challenging UAV environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multiple instance learning, AutoML, feature extraction, whole-slide images, benchmarking, computational pathology]</li>
<li class=""><strong>authors:</strong> Siemen Brussee, Pieter A. Valkema, Jurre A. J. Weijer, Thom Doeleman, Anne M.R. Schrader, Jesper Kers</li>
<li class=""><strong>institution:</strong> Leiden University Medical Center, Utrecht University Medical Center, Amsterdam University Medical Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17517" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17517</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces PathBench-MIL, an automated machine learning and benchmarking framework designed for Multiple Instance Learning in histopathology. It automates the entire pipeline from preprocessing to model aggregation, enabling standardized and reproducible evaluation of various models and feature extractors on whole-slide image datasets. The main conclusion is that this open-source framework facilitates rapid experimentation and standardization in computational pathology research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [source-free object detection, spatial prior-aware regularization, imbalance-aware noise robust pseudo-labeling, mean-teacher, OV-SAM, domain shift]</li>
<li class=""><strong>authors:</strong> Sairam VCR, Rishabh Lalla, Aveen Dayal, Tejal Kulkarni, Anuj Lalla, Vineeth N Balasubramanian, Muhammad Haris Khan</li>
<li class=""><strong>institution:</strong> IIT Hyderabad, MBZUAI, UC San Diego, IIT Jodhpur, Microsoft Research India</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17514" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17514</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ce0955de2a0c97faf7100d95c356843de584cdb63f24134f9bcbf1fef3e760a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ce0955de2a0c97faf7100d95c356843de584cdb63f24134f9bcbf1fef3e760a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FALCON-SFOD, a framework for source-free object detection that uses foundation model priors (SPAR) to enhance object-focused features and a noise-robust pseudo-labeling method (IRPL) to handle class imbalance. It concludes that this approach strengthens the feature space against domain shift, leading to more reliable pseudo-labels and competitive benchmark performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multimodal evaluation], [visual grounding, benchmark, multimodal large language models, discriminative, spatial, limited, rejection, test-time scaling, data-mixture training]</li>
<li class=""><strong>authors:</strong> Rang Li, Lei Li, Shuhuai Ren, Hao Tian, Shuhao Gu, Shicheng Li, Zihao Yue, Yudong Wang, Wenhan Ma, Zhe Yang, Jingyuan Ma, Zhifang Sui, Fuli Luo</li>
<li class=""><strong>institution:</strong> Peking University, Xiaomi, The University of Hong Kong, Renmin University of China</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17495" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17495</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4020de87a870a946901821a37d57ee80ad4531d2b38356ac7601cb21450e17c4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4020de87a870a946901821a37d57ee80ad4531d2b38356ac7601cb21450e17c4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces GroundingME, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) across four challenging dimensions: discriminative, spatial, limited, and rejection tasks. The evaluation of 25 MLLMs reveals a significant performance gap, with the best model achieving only 45.1% accuracy and most failing on rejection tasks by hallucinating objects. The authors propose test-time scaling and data-mixture training as strategies to partially improve model performance on these complex grounding challenges.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] FLEG: Feed-Forward Language Embedded Gaussian Splatting from Any Views</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [3D reconstruction and language embedding], [feed-forward network, 3D Gaussians, instance-guided contrastive learning, geometry-semantic hierarchical sparsification, 2D-to-3D lifting]</li>
<li class=""><strong>authors:</strong> Qijian Tian, Xin Tan, Jiayu Ying, Xuhong Wang, Yuan Xie, Lizhuang Ma</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, East China Normal University, Shanghai Artificial Intelligence Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17541" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17541</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9847bc8d208db80daeeb6fce65f8b727d7e037fbf74ca700b812950d807f8585_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9847bc8d208db80daeeb6fce65f8b727d7e037fbf74ca700b812950d807f8585_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FLEG is a feed-forward network that reconstructs language-embedded 3D Gaussians from arbitrary uncalibrated multi-view images without requiring 3D annotations. It uses instance-guided contrastive learning and hierarchical sparsification to align 2D semantics with 3D representations efficiently. The method outperforms existing approaches in generating accurate geometry, appearance, and language-aligned semantics from sparse or dense views.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [degradation-aware reasoning, structured reasoning chains, supervised fine-tuning, reward-driven alignment, dynamic reasoning depth scaling]</li>
<li class=""><strong>authors:</strong> Jiaqi Tang, Jianmin Chen, Wei Wei, Xiaogang Xu, Runtao Liu, Xiangyu Wu, Qipeng Xie, Jiafei Wu, Lei Zhang, Qifeng Chen</li>
<li class=""><strong>institution:</strong> Hong Kong University of Science and Technology, Northwestern Polytechnical University, Chinese University of Hong Kong, Nanjing University of Science and Technology, University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17532" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17532</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b624fc0656a14fc68753d26cc52e1c0a78d9633130c6881762bd87e498ed0875_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b624fc0656a14fc68753d26cc52e1c0a78d9633130c6881762bd87e498ed0875_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Robust-R1, a framework that enhances the robustness of Multimodal Large Language Models by explicitly modeling visual degradations through structured reasoning chains. The method integrates supervised fine-tuning, reward-driven alignment, and dynamic reasoning depth scaling, and is supported by a new dataset of realistic degradations. The approach achieves state-of-the-art performance on real-world degradation benchmarks, demonstrating superior anti-degradation capabilities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] G3Splat: Geometrically Consistent Generalizable Gaussian Splatting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [3D Gaussian splatting, geometrically consistent priors, view-synthesis loss, pose-free reconstruction, novel-view synthesis]</li>
<li class=""><strong>authors:</strong> Mehdi Hosseinzadeh, Shin-Fang Chng, Yi Xu, Simon Lucey, Ian Reid, Ravi Garg</li>
<li class=""><strong>institution:</strong> Australian Institute for Machine Learning, Goertek Alpha Labs, MBZUAI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17547" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17547</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04379904f0f6b8d226bffe80f94ab3fe3f745dd41985d72f88a3585212ea8e65_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04379904f0f6b8d226bffe80f94ab3fe3f745dd41985d72f88a3585212ea8e65_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> G3Splat introduces geometric priors to address the ambiguity in learning 3D Gaussian splats from images under self-supervision, enabling geometrically consistent scene reconstruction without requiring camera poses. The method outperforms prior work in geometry recovery, relative pose estimation, and novel-view synthesis, demonstrating strong zero-shot generalization on datasets like ScanNet.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [clothing tailoring, body semantic estimation, body edge prediction, foundational human visual model (FHVM), 3D mesh recovery]</li>
<li class=""><strong>authors:</strong> Yunqi Gao, Leyuan Liu, Yuhan Li, Changxin Gao, Yuanyuan Liu, Jingying Chen</li>
<li class=""><strong>institution:</strong> Central China Normal University, Huazhong University of Science and Technology, China University of Geosciences (Wuhan)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17545" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17545</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3dde00da40b964ce086e0496d0c0c6f668bf5149ef5a44e30539fa3c614601b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3dde00da40b964ce086e0496d0c0c6f668bf5149ef5a44e30539fa3c614601b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ClothHMR, a method for 3D human mesh recovery from a single image that handles diverse clothing via a clothing tailoring module to fit garments to the body silhouette and a mesh recovery module that aligns 3D representations with a foundational human vision model. It demonstrates superior performance over existing methods on benchmark datasets and in-the-wild images, with a practical web application for fashion and shopping.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A unified FLAIR hyperintensity segmentation model for various CNS tumor types and acquisition time points</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Attention U-Net, FLAIR hyperintensity segmentation, Dice score, Raidionics]</li>
<li class=""><strong>authors:</strong> Mathilde Gajda Faanes, David Bouget, Asgeir S. Jakola, Timothy R. Smith, Vasileios K. Kavouridis, Francesco Latini, Margret Jensdottir, Peter Milos, Henrietta Nittby Redebrandt, Rickard L. Sjöberg, Rupavathana Mahesparan, Lars Kjelsberg Pedersen, Ole Solheim, Ingerid Reinertsen</li>
<li class=""><strong>institution:</strong> SINTEF Digital, University of Gothenburg, Harvard Medical School, Uppsala University Hospital, Karolinska University Hospital, Linköping University Hospital, Skåne University Hospital, Umeå University, Haukeland University Hospital, University Hospital of North Norway, Norwegian University of Science and Technology, St. Olavs University Hospital</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17566" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17566</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a unified deep learning model for segmenting FLAIR hyperintensities in brain tumors using an Attention U-Net architecture trained on approximately 5000 MRI scans. The model generalizes well across various tumor types and pre- and post-operative time points, achieving performance comparable to dataset-specific models. It is integrated into the open-source Raidionics software to facilitate clinical deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] RoomEditor++: A Parameter-Sharing Diffusion Architecture for High-Fidelity Furniture Synthesis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [diffusion model, parameter-sharing dual diffusion backbone, U-Net, DiT, inpainting, geometric coherence]</li>
<li class=""><strong>authors:</strong> Qilong Wang, Xiaofan Ming, Zhenyi Lin, Jinwen Li, Dongwei Ren, Wangmeng Zuo, Qinghua Hu</li>
<li class=""><strong>institution:</strong> Tianjin University, Harbin Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17573" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17573</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01defbd3ee3e8db15e4c76b08a2c5a3cbb6ba5c53643caf0e06f01bee07f1f49_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01defbd3ee3e8db15e4c76b08a2c5a3cbb6ba5c53643caf0e06f01bee07f1f49_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes RoomEditor++, a diffusion-based architecture with a parameter-sharing dual diffusion backbone for high-fidelity virtual furniture synthesis. It introduces the RoomBench++ dataset for training and evaluation. Experiments show the method outperforms state-of-the-art approaches in metrics and human preference, demonstrating strong generalization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Medical Imaging AI Competitions Lack Fairness</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical imaging], [benchmarking, fairness, FAIR principles, dataset bias, reproducibility, systematic review]</li>
<li class=""><strong>authors:</strong> Annika Reinke, Evangelia Christodoulou, Sthuthi Sadananda, A. Emre Kavur, Khrystyna Faryna, Daan Schouten, Bennett A. Landman, Carole Sudre, Olivier Colliot, Nick Heller, Sophie Loizillon, Martin Maška, Maëlys Solal, Arya Yazdan-Panah, Vilma Bozgo, Ömer Sümer, Siem de Jong, Sophie Fischer, Michal Kozubek, Tim Rädsch, Nadim Hammoud, Fruzsina Molnár-Gábor, Steven Hicks, Michael A. Riegler, Anindo Saha, Vajira Thambawita, Pal Halvorsen, Amelia Jiménez-Sánchez, Qingyang Yang, Veronika Cheplygina, Sabrina Bottazzi, Alexander Seitel, Spyridon Bakas, Alexandros Karargyris, Kiran Vaidhya Venkadesh, Bram van Ginneken, Lena Maier-Hein</li>
<li class=""><strong>institution:</strong> German Cancer Research Center (DKFZ) Heidelberg</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17581" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17581</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper conducts a large-scale systematic study of 241 biomedical image analysis challenges to assess fairness in terms of dataset representativeness and accessibility. It finds substantial biases in dataset composition and restrictive access conditions, concluding that current benchmarks lack fairness and show a disconnect between leaderboard success and clinical relevance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Gaussian Discriminant Analysis, Z-score distance analysis, cluster-driven deep learning, two-stage framework]</li>
<li class=""><strong>authors:</strong> Tosin Ige, Christopher Kiekintveld, Aritran Piplai, Asif Rahman, Olukunle Kolade, Sasidhar Kunapuli</li>
<li class=""><strong>institution:</strong> The University of Texas at El Paso, University of North Carolina</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17594" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17594</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes MAD-OOD, a two-stage cluster-driven deep learning framework for out-of-distribution malware detection. It uses Gaussian Discriminant Analysis to model class embeddings and Z-score distance analysis to identify anomalies, then integrates these predictions with a neural network for final classification. The method significantly outperforms state-of-the-art approaches on benchmark datasets, achieving high AUC for detecting unseen malware families.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] 3One2: One-step Regression Plus One-step Diffusion for One-hot Modulation in Dual-path Video Snapshot Compressive Imaging</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [one-hot modulation, dual optical path, stochastic differential equation, video inpainting, one-step regression, one-step diffusion]</li>
<li class=""><strong>authors:</strong> Ge Wang, Xing Liu, Xin Yuan</li>
<li class=""><strong>institution:</strong> Zhejiang University, Westlake University, Westlake Institute for Optoelectronics</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17578" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17578</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fc017d71c05a87d24f1951648fbc337f6dcf0174a07c6e9b18d306437014285_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fc017d71c05a87d24f1951648fbc337f6dcf0174a07c6e9b18d306437014285_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a novel algorithm for video snapshot compressive imaging using one-hot modulation, which transforms the reconstruction into a video inpainting problem solved by combining a one-step regression initialization with a one-step diffusion refinement. To address spatial degradation, it introduces a dual optical path hardware design. Experiments show the method effectively reconstructs videos and is the first to integrate diffusion models into video SCI reconstruction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [video anomaly detection], [attention heads, multi-criteria analysis, tuning-free, multimodal large language models (MLLMs), robust head identification]</li>
<li class=""><strong>authors:</strong> Zhaolin Cai, Fan Li, Ziwei Zheng, Haixia Bi, Lijun He</li>
<li class=""><strong>institution:</strong> Xinjiang University, Xi’an Jiaotong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17601" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17601</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0045f2737f70ccb56d547c71a2a55c2ab9160ecc33a3c65b7b564bc9a3329529_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0045f2737f70ccb56d547c71a2a55c2ab9160ecc33a3c65b7b564bc9a3329529_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes HeadHunt-VAD, a tuning-free video anomaly detection method that directly identifies and uses a sparse set of robust, anomaly-sensitive attention heads within a frozen Multimodal Large Language Model (MLLM), bypassing textual generation. It introduces a Robust Head Identification module to select expert heads based on saliency and stability across prompts, followed by a lightweight scorer for detection. The method achieves state-of-the-art performance among tuning-free approaches on major benchmarks, demonstrating the effectiveness of head-level probing in MLLMs for efficient and accurate anomaly detection.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical image registration], [mammography registration, anatomical landmarks, ANTs, VoxelMorph, TransMorph, IDIR, MammoRegNet, benchmark dataset]</li>
<li class=""><strong>authors:</strong> Svetlana Krasnova, Emiliya Starikova, Ilia Naletov, Andrey Krylov, Dmitry Sorokin</li>
<li class=""><strong>institution:</strong> Lomonosov Moscow State University, Third Opinion Platform</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17605" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17605</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1e72bae8c4d2dd504664f6eedc1a325466b12559df8aa6fc5fbe929fb95fcbf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1e72bae8c4d2dd504664f6eedc1a325466b12559df8aa6fc5fbe929fb95fcbf_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MGRegBench, a public benchmark dataset with over 5,000 mammography image pairs and manual annotations for evaluating registration methods. It benchmarks classical, learning-based, and implicit neural representation approaches, finding that deep learning methods like MammoRegNet show strong performance. The dataset and code are released to enable fair comparisons and advance research in mammography registration.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Self-Supervised Weighted Image Guided Quantitative MRI Super-Resolution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [self-supervised learning, physics-informed deep learning, Bayesian maximum a posteriori inference, super-resolution, quantitative MRI relaxometry]</li>
<li class=""><strong>authors:</strong> Alireza Samadifardheris, Dirk H.J. Poot, Florian Wiesinger, Stefan Klein, Juan A. Hernandez-Tamames</li>
<li class=""><strong>institution:</strong> Erasmus MC, GE Healthcare, TU Delft</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17612" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17612</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddc1b71f8c3d47afff52315d3f332374abf579939cd3a9de858b3ceab44b3ff3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddc1b71f8c3d47afff52315d3f332374abf579939cd3a9de858b3ceab44b3ff3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a self-supervised, physics-informed deep learning framework for quantitative MRI super-resolution. It uses routinely acquired high-resolution weighted MRI scans as guidance to enhance low-resolution quantitative maps, eliminating the need for high-resolution ground truth during training. The method enables fast, high-quality quantitative MRI acquisitions, offering a practical pathway for clinical integration.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Semi-Supervised 3D Segmentation for Type-B Aortic Dissection with Slim UNETR</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [semi-supervised learning, 3D segmentation, multi-output CNN, Slim UNETR, data augmentation]</li>
<li class=""><strong>authors:</strong> Denis Mikhailapov, Vladimir Berikov</li>
<li class=""><strong>institution:</strong> Sobolev Institute of Mathematics SB RAS</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17610" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17610</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abd921f37f90b9dfaf497e986d1ae89307cf4b11dd527bf09bd11e36d239652c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abd921f37f90b9dfaf497e986d1ae89307cf4b11dd527bf09bd11e36d239652c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a semi-supervised learning method for multi-output 3D CNNs, specifically using Slim UNETR, to segment aortic structures in Type-B Aortic Dissection. The method leverages data augmentation techniques like rotation and flipping to utilize both labeled and unlabeled data, overcoming the challenge of limited high-quality 3D annotations. It concludes that this approach is a universal and effective strategy for improving segmentation accuracy in medical imaging with complex, multi-class outputs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [autonomous driving perception], [temporal stereo modeling, dynamic confidence gating, sparse query-based detection, multi-view 3D object detection]</li>
<li class=""><strong>authors:</strong> Di Wu, Feng Yang, Wenhui Zhao, Jinwen Yu, Pan Liao, Benlian Xu, Dingwen Zhang</li>
<li class=""><strong>institution:</strong> Northwestern Polytechnical University, Suzhou University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17620" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17620</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87e1ee16ff63d70d8c3aa60f230a9ba8c43c07767bfd05ba0b8a9169945304c1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87e1ee16ff63d70d8c3aa60f230a9ba8c43c07767bfd05ba0b8a9169945304c1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> StereoMV2D integrates temporal stereo modeling into a 2D detection-guided multi-view 3D detector to enhance depth perception by exploiting cross-temporal disparities across adjacent frames. It refines query priors efficiently within 2D regions of interest and uses a dynamic confidence gating mechanism for robust detection under occlusion. The framework achieves superior performance on nuScenes and Argoverse 2 datasets without significant computational overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] PathFLIP: Fine-grained Language-Image Pretraining for Versatile Computational Pathology</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [PathFLIP, fine-grained language-image pretraining, region-level subcaptions, text-conditioned region embeddings, visual-language grounding, large language models (LLMs), whole slide images (WSIs), computational pathology]</li>
<li class=""><strong>authors:</strong> Fengchun Liu, Songhan Jiang, Linghan Cai, Ziyue Wang, Yongbing Zhang</li>
<li class=""><strong>institution:</strong> Harbin Institute of Technology, Shenzhen; National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17621" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17621</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccba362ad9693531c32711d08070cfa491424d893a9b07c00a878fc385f9bdae_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccba362ad9693531c32711d08070cfa491424d893a9b07c00a878fc385f9bdae_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes PathFLIP, a framework for computational pathology that decomposes slide-level captions into region-level subcaptions and generates text-conditioned region embeddings for fine-grained visual-language alignment. It leverages LLMs to follow clinical instructions and adapt to diagnostic contexts. Experiments show it outperforms existing pathological VLMs on multiple benchmarks while using less training data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Region-Constraint In-Context Generation for Instructional Video Editing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [in-context generation, video diffusion, latent regularization, attention regularization, region-constraint, joint denoising]</li>
<li class=""><strong>authors:</strong> Zhongwei Zhang, Fuchen Long, Wei Li, Zhaofan Qiu, Wu Liu, Ting Yao, Tao Mei</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China, HiDream.ai Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17650" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17650</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a42bfbcfb80ccf85890223bbbea68fa4b5ab3ddd21035c9e061032429b289590_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a42bfbcfb80ccf85890223bbbea68fa4b5ab3ddd21035c9e061032429b289590_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents ReCo, a region-constraint in-context generation paradigm for instructional video editing. It introduces latent and attention regularization during joint denoising to precisely modify editing regions and mitigate interference. Experiments show the method&#x27;s superiority across various video editing tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Bitbox: Behavioral Imaging Toolbox for Computational Analysis of Behavior from Videos</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [computational behavioral measurement, video analysis, facial expression, head movement, body action, open-source toolkit, modularity, interpretability]</li>
<li class=""><strong>authors:</strong> Evangelos Sariyanidi, Gokul Nair, Lisa Yankowitz, Casey J. Zampella, Mohan Kashyap Pargi, Aashvi Manakiwala, Maya McNealis, John D. Herrington, Jeffrey Cohn, Robert T. Schultz, Birkan Tunc</li>
<li class=""><strong>institution:</strong> The Children&#x27;s Hospital of Philadelphia, University of Pennsylvania, University of Pittsburgh</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17655" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17655</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Bitbox, an open-source behavioral imaging toolbox that provides a standardized interface for extracting high-level behavioral measurements from video using multiple face, head, and body processors. It is designed to bridge the translational gap by making advanced computational analysis accessible to behavioral and clinical researchers without requiring engineering expertise. The authors conclude that Bitbox will accelerate the integration of computational behavioral measurement into behavioral, clinical, and mental health research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [cognitive steering conduit (CSC), hybrid interaction representations, hybrid guidance strategy, language modeling loss, auxiliary classification loss, open-vocabulary generation]</li>
<li class=""><strong>authors:</strong> Zhaolin Cai, Huiyu Duan, Zitong Xu, Fan Li, Zhi Liu, Jing Liu, Wei Shen, Xiongkuo Min, Guangtao Zhai</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Xi&#x27;an Jiao Tong University, Shandong University, Tianjin University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17640" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17640</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089843bf864a7cfe4c272a400ced858a82e7f97256cba2c56898e8f4e4591dff_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089843bf864a7cfe4c272a400ced858a82e7f97256cba2c56898e8f4e4591dff_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes GRASP-HOI, a framework that reformulates human-object interaction detection as an open-vocabulary generation problem. It uses a lightweight cognitive steering module to inject visual features into a frozen multi-modal LLM for reasoning and employs a hybrid loss for training. This approach achieves state-of-the-art closed-set performance and strong zero-shot generalization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [spatio-temporal feature representation, channel attention, self-attention, recurrent neural networks, video-based gaze estimation]</li>
<li class=""><strong>authors:</strong> Alexandre Personnic, Mihai Bâce</li>
<li class=""><strong>institution:</strong> KU Leuven</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17673" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17673</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes the Spatio-Temporal Gaze Network (ST-Gaze), which combines a CNN backbone with channel and self-attention modules to fuse eye and face features, then models intra- and inter-frame dynamics by treating features as a spatial sequence propagated through time. The method achieves state-of-the-art performance on the EVE dataset, demonstrating that preserving intra-frame spatial context is superior to premature spatial pooling for robust video-based gaze estimation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] An Empirical Study of Sampling Hyperparameters in Diffusion-Based Super-Resolution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [Diffusion Posterior Sampling (DPS), Manifold Constrained Gradient (MCG), conditioning step size, diffusion step count, ablation study]</li>
<li class=""><strong>authors:</strong> Yudhistira Arief Wibowo</li>
<li class=""><strong>institution:</strong> Technical University of Munich, Korea Advanced Institute of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17675</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts an empirical ablation study on diffusion-based super-resolution, focusing on conditioning methods like DPS and MCG. It finds that the conditioning step size is a more critical hyperparameter than the diffusion step count for reconstruction quality. The optimal conditioning step size for best performance in their experiments falls within the range of [2.0, 3.0].</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SAVeD: A First-Person Social Media Video Dataset for ADAS-equipped vehicle Near-Miss and Crash Event Analyses</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [semantic segmentation, monocular depth estimation, Time-to-Collision (TTC), Generalized Extreme Value (GEV) distribution, VideoLLaMA2, InternVL2.5 HiCo R16, domain adaptation]</li>
<li class=""><strong>authors:</strong> Shaoyan Zhai, Mohamed Abdel-Aty, Chenzhu Wang, Rodrigo Vena Garcia</li>
<li class=""><strong>institution:</strong> University of Central Florida</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17724" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17724</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SAVeD, a first-person video dataset from social media for analyzing ADAS vehicle near-misses and crashes. It proposes a framework using semantic segmentation and depth estimation to compute Time-to-Collision and uses extreme value theory to model risk. The dataset&#x27;s annotations are shown to enhance the performance of video-language models through domain adaptation in complex scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] FlexAvatar: Flexible Large Reconstruction Model for Animatable Gaussian Head Avatars with Detailed Deformation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [transformer-based reconstruction, 3D Gaussian Splatting, UV-space position maps, data distribution adjustment, lightweight UNet decoder]</li>
<li class=""><strong>authors:</strong> Cheng Peng, Zhuo Su, Liao Wang, Chen Guo, Zhaohu Li, Chengjiang Long, Zheng Lv, Jingxiang Sun, Chenyangguang Zhang, Yebin Liu</li>
<li class=""><strong>institution:</strong> Tsinghua University, ByteDance</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17717" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17717</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/291c2533d43a0bee45ee61d6f189d197b685c110c8fea100f87b6b779dda6aaf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/291c2533d43a0bee45ee61d6f189d197b685c110c8fea100f87b6b779dda6aaf_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlexAvatar is a flexible large reconstruction model that creates high-fidelity 3D head avatars from single or sparse images without camera poses or expression labels, using a transformer-based approach with structured head query tokens and a lightweight UNet decoder for real-time detailed deformations. It achieves superior 3D consistency and dynamic realism compared to previous methods, offering a practical solution for animatable avatar creation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MambaMIL+: Modeling Long-Term Contextual Patterns for Gigapixel Whole Slide Image</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Mamba, multiple instance learning (MIL), overlapping scanning, selective stripe position encoder (S2PE), contextual token selection (CTS)]</li>
<li class=""><strong>authors:</strong> Qian Zeng, Yihui Wang, Shu Yang, Yingxue Xu, Fengtao Zhou, Jiabo Ma, Dejia Cai, Zhengyu Zhang, Lijuan Qu, Yu Wang, Li Liang, Hao Chen</li>
<li class=""><strong>institution:</strong> Hong Kong University of Science and Technology, Southern Medical University, 900th Hospital of Joint Logistic Support Force, PLA, Zhujiang Hospital, Southern Medical University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17726" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17726</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes MambaMIL+, a multiple instance learning framework that integrates spatial context modeling and long-range dependency for analyzing gigapixel whole-slide images. It introduces overlapping scanning, a selective stripe position encoder, and a contextual token selection mechanism to overcome memory decay and limited context in long sequences. The method achieves state-of-the-art performance across 20 benchmarks for diagnostic classification, molecular prediction, and survival analysis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] AdaptPrompt: Parameter-Efficient Adaptation of VLMs for Generalizable Deepfake Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [deepfake detection], [CLIP, parameter-efficient transfer learning, textual prompts, visual adapters, layer ablation, diffusion models]</li>
<li class=""><strong>authors:</strong> Yichen Jiang, Mohammed Talha Alam, Sohail Ahmed Khan, Duc-Tien Dang-Nguyen, Fakhri Karray</li>
<li class=""><strong>institution:</strong> University of Waterloo, MBZUAI, University of Bergen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17730" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17730</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b69dc4fb6ce4ab4ca6cc4b1419cbe29e322a1c22a4599cafb0bf6800683cf49_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b69dc4fb6ce4ab4ca6cc4b1419cbe29e322a1c22a4599cafb0bf6800683cf49_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes AdaptPrompt, a parameter-efficient framework that adapts the CLIP vision-language model for deepfake detection by jointly learning task-specific textual prompts and visual adapters while freezing the backbone. It also introduces the Diff-Gen dataset and shows that pruning the final transformer block of the vision encoder improves the retention of high-frequency artifacts. The method achieves state-of-the-art generalization across 25 test sets, including unseen generators, and demonstrates strong few-shot performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] LiteGE: Lightweight Geodesic Embedding for Efficient Geodesics Computation and Non-Isometric Shape Correspondence</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [unsigned distance field, PCA, lightweight embedding, geodesic distance, shape correspondence]</li>
<li class=""><strong>authors:</strong> Yohanes Yudhi Adikusuma, Qixing Huang, Ying He</li>
<li class=""><strong>institution:</strong> University of Texas at Austin, Nanyang Technological University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17781" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17781</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LiteGE introduces a lightweight method for computing geodesic distances on 3D shapes by constructing compact shape descriptors using PCA on unsigned distance field samples. This approach eliminates the need for large neural networks, enabling significant reductions in memory usage and inference time while maintaining robustness on sparse point clouds. It also facilitates fast and accurate non-isometric shape correspondence, achieving up to 1000x speedup over state-of-the-art mesh-based methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] UrbanDIFF: A Denoising Diffusion Model for Spatial Gap Filling of Urban Land Surface Temperature Under Dense Cloud Cover</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [denoising diffusion, image inpainting, spatial gap-filling, pixel guided refinement, MODIS Terra LST]</li>
<li class=""><strong>authors:</strong> Arya Chavoshi, Hassan Dashtian, Naveen Sudharsan, Dev Niyogi</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17782" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17782</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces UrbanDIFF, a denoising diffusion model for spatially reconstructing urban land surface temperature imagery obscured by dense clouds, using static urban structure data for conditioning and a pixel-guided refinement step. It demonstrates superior performance over baselines under high cloud coverage, with slower degradation as missing data increases.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Long-Range depth estimation using learning based Hybrid Distortion Model for CCTV cameras</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hybrid distortion model, neural network residual correction, stereo triangulation, camera calibration, long-range depth estimation]</li>
<li class=""><strong>authors:</strong> Ami Pandat, Punna Rajasekhar, G.Aravamuthan, Gopika Vinod, Rohit Shukla</li>
<li class=""><strong>institution:</strong> Homi Bhabha National Institute, Bhabha Atomic Research Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17784" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17784</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid framework for long-range depth estimation by extending conventional camera distortion models with higher-order terms and then enhancing them using a neural network-based residual correction model. This approach improves 3D localization accuracy for distances up to 5 kilometers using CCTV cameras. The method is validated by transforming estimated 3D coordinates to GIS maps, offering a practical calibration solution for long-range photogrammetry.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [vision transformer, neural parametric head models, 3d morphable models, single-image 3d reconstruction, signed distance functions]</li>
<li class=""><strong>authors:</strong> Simon Giebenhain, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Zhe Chen, Matthias Nießner</li>
<li class=""><strong>institution:</strong> Technical University of Munich, Woven by Toyota, Toyota Motor Europe</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17773" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17773</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c26792d83b697ded69eb83a7cee4b4440d0b9637b6c3fbd2061ab2aef67d7bcd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c26792d83b697ded69eb83a7cee4b4440d0b9637b6c3fbd2061ab2aef67d7bcd_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Pix2NPHM, a method that uses a vision transformer to directly regress the parameters of a Neural Parametric Head Model from a single input image. It achieves high-fidelity 3D face reconstruction by training on a mixture of 3D data and 2D videos, and allows for further refinement through inference-time optimization. The authors conclude that their approach yields unprecedented reconstruction quality that generalizes well to in-the-wild data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Animate Any Character in Any World</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [3DGS, conditional autoregressive video generation, pre-trained video generator, natural language control]</li>
<li class=""><strong>authors:</strong> Yitong Wang, Fangyun Wei, Hongyang Zhang, Bo Dai, Yan Lu</li>
<li class=""><strong>institution:</strong> Fudan University, Microsoft Research, University of Waterloo, The University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17796" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17796</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8fb0b1e8242f96e5f0b2988237b2d0603a8f364d90cc833a33b2313d43b1ae4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8fb0b1e8242f96e5f0b2988237b2d0603a8f364d90cc833a33b2313d43b1ae4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces AniX, a system that animates user-provided 3D characters in 3D Gaussian Splatting (3DGS) scenes based on natural language commands. It formulates the task as a conditional autoregressive video generation problem, building upon a pre-trained video generator and a training strategy to enhance motion dynamics. The method enables open-ended character actions while preserving visual fidelity and temporal coherence in the generated video clips.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [3D Gaussian Splatting, multi-teacher pretraining, knowledge distillation, feed-forward encoder, open-vocabulary segmentation, render-and-distill]</li>
<li class=""><strong>authors:</strong> Yue Li, Qi Ma, Runyi Yang, Mengjiao Ma, Bin Ren, Nikola Popovic, Nicu Sebe, Theo Gevers, Luc Van Gool, Danda Pani Paudel, Martin R. Oswald</li>
<li class=""><strong>institution:</strong> University of Amsterdam, ETH Zurich, INSAIT (Sofia University &quot;St. Kliment Ohridski&quot;), Nanjing University of Aeronautics and Astronautics, University of Trento</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17817" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17817</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/93bcfeca01370b73bf751251ff5ff1406aed5442e3fe99bf9680cea6a994f535_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/93bcfeca01370b73bf751251ff5ff1406aed5442e3fe99bf9680cea6a994f535_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Chorus, a multi-teacher pretraining framework that learns a holistic 3D Gaussian Splatting scene encoder by distilling complementary signals from 2D foundation models. The method achieves strong performance on various 3D scene understanding tasks and demonstrates high data efficiency, requiring significantly fewer training scenes than point cloud baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] ReX-MLE: The Autonomous Agent Benchmark for Medical Imaging Challenges</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [autonomous coding agents, benchmark, end-to-end workflows, data preprocessing, model training, medical imaging competitions]</li>
<li class=""><strong>authors:</strong> Roshan Kenia, Xiaoman Zhang, Pranav Rajpurkar</li>
<li class=""><strong>institution:</strong> Harvard Medical School</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17838" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17838</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/55f58c5bf3f50168f0cf79114478d406f849c07ec4093b7557c2b51fc67903fb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/55f58c5bf3f50168f0cf79114478d406f849c07ec4093b7557c2b51fc67903fb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces ReX-MLE, a benchmark for evaluating autonomous coding agents on complex, end-to-end medical imaging challenges derived from real competitions. It finds that current state-of-the-art agents perform poorly, ranking near the 0th percentile compared to human experts, due to domain-knowledge and engineering limitations. The benchmark aims to expose these bottlenecks and guide the development of more capable, domain-aware autonomous AI systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Simulation-Driven Deep Learning Framework for Raman Spectral Denoising Under Fluorescence-Dominant Conditions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [deep learning, noise modeling, cascaded neural network, simulation-driven framework, physics-informed learning]</li>
<li class=""><strong>authors:</strong> Mengkun Chen, Sanidhya D. Tripathi, James W. Tunnell</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17852" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17852</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a simulation-driven deep learning framework that uses a comprehensive noise model to generate realistic Raman spectra for training a cascaded neural network. The network is designed to jointly suppress detector noise and fluorescence background. The results demonstrate that this physics-informed learning approach can improve spectral quality for faster and more accurate tissue analysis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [cross-attention maps, inference-time optimization, compound loss, denoising step, spatial alignment]</li>
<li class=""><strong>authors:</strong> Sarah Rastegar, Violeta Chatalbasheva, Sieger Falkena, Anuj Singh, Yanbo Wang, Tejas Gokhale, Hamid Palangi, Hadi Jamali-Rad</li>
<li class=""><strong>institution:</strong> Delft University of Technology, University of Maryland, Baltimore County, Shell Information Technology International, Google</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17851" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17851</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1a251648d46bb8da396759e99d563b9a2d57eb19fc5ed8f7ece18ccb7bfeeee_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1a251648d46bb8da396759e99d563b9a2d57eb19fc5ed8f7ece18ccb7bfeeee_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> InfSplign is a training-free, inference-time method that improves spatial alignment in text-to-image diffusion models by adjusting the noise at each denoising step using a compound loss based on cross-attention maps. It achieves state-of-the-art performance on spatial reasoning benchmarks, outperforming existing inference-time and fine-tuning baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Convolutional Neural Network (CNN), Attention Mechanism, CBAM, VGG16, Grad-CAM, Layer-wise Relevance Propagation (LRP)]</li>
<li class=""><strong>authors:</strong> Balram Singh, Ram Prakash Sharma, Somnath Dey</li>
<li class=""><strong>institution:</strong> National Institute of Technology Hamirpur, Indian Institute of Technology Indore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17864" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17864</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an interpretable plant leaf disease detection method using a CBAM-enhanced VGG16 CNN model. The model integrates attention modules to improve feature extraction and localization, achieving high accuracy on multiple datasets. The study demonstrates the effectiveness of the approach through performance evaluation and interpretability analysis using attention maps and other explainable AI techniques.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [explainable AI], [Keypoint Counting Classifiers, Vision Transformers, self-explainable models, keypoint matching]</li>
<li class=""><strong>authors:</strong> Kristoffer Wickstrøm, Teresa Dorszewski, Siyan Chen, Michael Kampffmeyer, Elisabeth Wetzer, Robert Jenssen</li>
<li class=""><strong>institution:</strong> UiT The Arctic University of Norway, Technical University of Denmark</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17891" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17891</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3be976f0784a55cb3060e705b880d59783015427100e3ca645b5aa22bfe5f76_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3be976f0784a55cb3060e705b880d59783015427100e3ca645b5aa22bfe5f76_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Keypoint Counting Classifiers (KCCs), a method to convert any pre-trained Vision Transformer (ViT) into a self-explainable model without requiring retraining, by leveraging ViTs&#x27; ability to identify matching keypoints between images. The method creates an interpretable decision process that is directly visualizable. The authors conclude that KCCs improve human-machine communication and represent a step towards more transparent and reliable ViT-based foundation models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Visually Prompted Benchmarks Are Surprisingly Fragile</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [visual prompting, benchmark evaluation, vision-language models, visual marker design, JPEG compression, dataset size, VPBench]</li>
<li class=""><strong>authors:</strong> Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang, Renhao Wang, Trevor Darrell, Alane Suhr, Angjoo Kanazawa</li>
<li class=""><strong>institution:</strong> UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17875" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17875</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1578e85cb159e2bf39916b0de61ec0b774772fc5c07fb3edc1f869eea3ea32e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1578e85cb159e2bf39916b0de61ec0b774772fc5c07fb3edc1f869eea3ea32e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper evaluates vision-language models (VLMs) on visually prompted benchmarks, where questions refer to coordinates marked directly on images. It finds that model performance and leaderboard rankings are surprisingly fragile to minor changes in visual marker design (e.g., color, size) and low-level inference settings like JPEG compression. To address this instability, the authors introduce VPBench, a larger benchmark with multiple visual marker variants.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] RadarGen: Automotive Radar Point Cloud Generation from Cameras</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [diffusion model, bird&#x27;s-eye-view, radar cross section, Doppler, point cloud generation, foundation models]</li>
<li class=""><strong>authors:</strong> Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany</li>
<li class=""><strong>institution:</strong> Technion, MIT, NVIDIA, University of Toronto, Vector Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17897" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17897</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RadarGen is a diffusion model that generates realistic automotive radar point clouds from multi-view camera images by representing radar data in bird&#x27;s-eye-view and conditioning on visual cues. It uses a lightweight recovery step to reconstruct point clouds from the generated maps. Evaluations show it captures real radar statistics and reduces the performance gap for perception models trained on synthetic data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Diffusion Forcing for Multi-Agent Interaction Sequence Modeling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multi-agent motion generation], [diffusion forcing, autoregressive diffusion, transformer, multi-agent interaction, denoising]</li>
<li class=""><strong>authors:</strong> Vongani H. Maluleke, Kie Horiuchi, Lea Wilken, Evonne Ng, Jitendra Malik, Angjoo Kanazawa</li>
<li class=""><strong>institution:</strong> UC Berkeley, Sony Group Corporation, Meta</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17900" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17900</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cd37ce47a0e9e2e02887aa6dab039bf60ed28e99e8eaf07d8c6aaa610b08b8a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cd37ce47a0e9e2e02887aa6dab039bf60ed28e99e8eaf07d8c6aaa610b08b8a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MAGNet, a unified autoregressive diffusion framework for generating multi-agent human motion sequences. It extends Diffusion Forcing to explicitly model inter-agent coupling, enabling coherent coordination for both synchronized and loosely structured social interactions. The method performs on par with specialized dyadic benchmarks and naturally scales to polyadic scenarios with three or more agents.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] InSPECT: Invariant Spectral Features Preservation of Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [invariant spectral features, Fourier coefficients, feature-preserving diffusion, InSPECT, DDPM]</li>
<li class=""><strong>authors:</strong> Baohua Yan, Qingyuan Liu, Jennifer Kava, Xuan Di</li>
<li class=""><strong>institution:</strong> Columbia University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17873" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17873</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e13641f6dbe27376e3ddb6ce03c9f7e6b0f2910fa5365cf52d7c50d8992a2550_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e13641f6dbe27376e3ddb6ce03c9f7e6b0f2910fa5365cf52d7c50d8992a2550_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes InSPECT, a diffusion model that preserves invariant spectral features during the forward and backward processes, preventing the complete destruction of data into white noise. This approach leads to faster convergence, improved generation quality and diversity, and significant reductions in FID and improvements in IS compared to standard DDPM.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Adversarial Robustness of Vision in Open Foundation Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Projected Gradient Descent, adversarial robustness, Visual Question Answering, vision-language models]</li>
<li class=""><strong>authors:</strong> Jonathon Fox, William J Buchanan, Pavlos Papadopoulos</li>
<li class=""><strong>institution:</strong> Edinburgh Napier University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17902" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17902</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates the adversarial robustness of vision-language models LLaVA-1.5-13B and Llama 3.2 Vision-8B-2 by applying untargeted Projected Gradient Descent attacks to their visual inputs and testing on a VQA v2 subset. The main conclusion is that the vision modality is a viable attack vector, and adversarial robustness does not directly correlate with standard benchmark performance, with Llama 3.2 Vision showing a smaller accuracy drop under attack despite a lower baseline.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Colormap-Enhanced Vision Transformers for MRI-Based Multiclass (4-Class) Alzheimer&#x27;s Disease Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Vision Transformers, Pseudo-Color Enhancement, MRI, Multi-Class Classification]</li>
<li class=""><strong>authors:</strong> Faisal Ahmed</li>
<li class=""><strong>institution:</strong> Embry-Riddle Aeronautical University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16964" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16964</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes PseudoColorViT-Alz, a method that enhances MRI scans with pseudo-color transformations and processes them using a Vision Transformer for Alzheimer&#x27;s disease classification. The model achieves state-of-the-art accuracy of 99.79% on the OASIS-1 dataset, demonstrating that combining color augmentation with Vision Transformers significantly improves classification performance over previous methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Dexterous World Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [video diffusion, egocentric hand mesh rendering, hybrid interaction video dataset, scene-action-conditioned generation]</li>
<li class=""><strong>authors:</strong> Byungjun Kim, Taeksoo Kim, Junyoung Lee, Hanbyul Joo</li>
<li class=""><strong>institution:</strong> Seoul National University, RLWRLD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17907" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17907</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d67a89ae639d29f772540022b521f1cb6c865bf4eff8ab082f8c6e9eeeaaa6a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d67a89ae639d29f772540022b521f1cb6c865bf4eff8ab082f8c6e9eeeaaa6a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Dexterous World Models (DWM), a video diffusion framework that generates dynamic, egocentric videos of human-scene interactions by conditioning on static 3D scene renderings and hand motion sequences. It is trained on a hybrid dataset of synthetic and real-world videos. The method produces realistic and physically plausible interactions, representing a step toward interactive digital twins.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [test-time refinement, self-supervised learning, shape from shading, score distillation sampling, monocular depth estimation, diffusion models]</li>
<li class=""><strong>authors:</strong> Ananta R. Bhattarai, Helge Rhodin</li>
<li class=""><strong>institution:</strong> Bielefeld University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17908" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17908</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb5eab47c4353057728b3e9889e13b412f6c11ae6703f713d247c4724fbb909_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb5eab47c4353057728b3e9889e13b412f6c11ae6703f713d247c4724fbb909_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Re-Depth Anything, a test-time self-supervision framework that refines monocular depth predictions by re-lighting the geometry and using a 2D diffusion model&#x27;s priors via Score Distillation Sampling. It updates only specific model embeddings and the decoder to prevent collapse, rather than fine-tuning the entire network. The method shows substantial improvements in depth accuracy and realism over the baseline Depth Anything V2 model.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [latent diffusion models, representation encoders, semantic-pixel reconstruction, variational autoencoder, text-to-image generation, image editing]</li>
<li class=""><strong>authors:</strong> Shilong Zhang, He Zhang, Zhifei Zhang, Chongjian Ge, Shuchen Xue, Shaoteng Liu, Mengwei Ren, Soo Ye Kim, Yuqian Zhou, Qing Liu, Daniil Pakhomov, Kai Zhang, Zhe Lin, Ping Luo</li>
<li class=""><strong>institution:</strong> The University of Hong Kong, Adobe Research, University of Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17909" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17909</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae25315fc54e46dd0abb6b1bc7b9cf8b409962a4f4b702095e19e1f36529da12_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae25315fc54e46dd0abb6b1bc7b9cf8b409962a4f4b702095e19e1f36529da12_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a framework to adapt discriminative representation encoders for generative tasks by introducing a semantic-pixel reconstruction objective, which compresses both semantic and fine-grained details into a compact latent space. The resulting model achieves state-of-the-art image reconstruction and enables unified text-to-image generation and editing, demonstrating that representation encoders can be effectively adapted into robust generative components.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [StyleGAN2-ADA, Denoising Diffusion Probabilistic Models (DDPMs), FID, KID, Inception Score, ViT-B/16, synthetic data augmentation]</li>
<li class=""><strong>authors:</strong> N. A. Adarsh Pritam, Jeba Shiney O, Sanyam Jain</li>
<li class=""><strong>institution:</strong> Alliance University, Østfold University College</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17585" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17585</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/497a14da92ec4e58a3716d9bb6044a8b43d59d0f8ab0aff8c6e70b0d8e5325c9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/497a14da92ec4e58a3716d9bb6044a8b43d59d0f8ab0aff8c6e70b0d8e5325c9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SkinGenBench, a benchmark that evaluates the effects of generative models (StyleGAN2-ADA and DDPMs) and preprocessing pipelines on synthetic dermoscopic image generation for melanoma diagnosis. The main conclusion is that the choice of generative architecture has a stronger impact on image quality and diagnostic utility than preprocessing complexity, with StyleGAN2-ADA outperforming diffusion models in fidelity, and synthetic augmentation significantly boosting downstream classifier performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical imaging], [image registration, radiomics, deep learning, logistic regression, feature selection]</li>
<li class=""><strong>authors:</strong> Rahul Ravi, Ruizhe Li, Tarek Abdelfatah, Stephen Chan, Xin Chen</li>
<li class=""><strong>institution:</strong> University of Nottingham, Nottingham City Hospital</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17759" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17759</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe1fee674daeeb2ff62e605cac35448ddb7b933f1a6948f0aa461b318710117_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe1fee674daeeb2ff62e605cac35448ddb7b933f1a6948f0aa461b318710117_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a framework using aligned longitudinal MRI and clinical data to predict breast cancer treatment response. The method involves tumor segmentation, image registration, and feature extraction, comparing radiomics and deep learning models. The main conclusion is that image registration significantly improves prediction, with radiomics features outperforming deep learning features in predicting pathologic complete response and relapse-free survival.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [3D ConvNeXt, Global Response Normalization, depth scaling, width scaling, context scaling, supervised pretraining, volumetric segmentation]</li>
<li class=""><strong>authors:</strong> Saikat Roy, Yannick Kirchhoff, Constantin Ulrich, Maximillian Rokuss, Tassilo Wald, Fabian Isensee, Klaus Maier-Hein</li>
<li class=""><strong>institution:</strong> German Cancer Research Center (DKFZ), Heidelberg University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17774" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17774</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MedNeXt-v2, a compound-scaled 3D ConvNeXt architecture enhanced with a Global Response Normalization module for large-scale supervised representation learning in medical image segmentation. The authors demonstrate that scaling the backbone network&#x27;s architecture is crucial for effective pretraining, and MedNeXt-v2 achieves state-of-the-art performance when fine-tuned on diverse CT and MR benchmarks. The work establishes that a stronger backbone design yields better downstream segmentation results than simply increasing dataset size.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-23">2025-12-23<a href="#2025-12-23" class="hash-link" aria-label="Direct link to 2025-12-23" title="Direct link to 2025-12-23" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251223] A 96pJ/Frame/Pixel and 61pJ/Event Anti-UAV System with Hybrid Object Tracking Modes</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuncheng Lu, Yucen Shi, Aobo Li, Zehao Li, Junying Li, Bo Wang, Tony Tae-Hyoung Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17939" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17939</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/962df2fe1f424af4979b55e1a459771077ddad5484459248d7cbdb7f3d246823_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/962df2fe1f424af4979b55e1a459771077ddad5484459248d7cbdb7f3d246823_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A 96pJ/Frame/Pixel and 61pJ/Event Anti-UAV System with Hybrid Object Tracking Modes</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Karthik Prabhakar</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17943" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17943</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a630b7d9810838c6059574b3dae2d2f3fcc9c79699030e8640202f2dbcd2d4b0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a630b7d9810838c6059574b3dae2d2f3fcc9c79699030e8640202f2dbcd2d4b0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SuperFlow: Training Flow Matching Models with RL on the Fly</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kaijie Chen, Zhiyang Xu, Ying Shen, Zihao Lin, Yuguang Yao, Lifu Huang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17951" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17951</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/948003a9b36edb891b1eea74cedf1aaab2c086c912b2c476bae0259abbca38e3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/948003a9b36edb891b1eea74cedf1aaab2c086c912b2c476bae0259abbca38e3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SuperFlow: Training Flow Matching Models with RL on the Fly</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Bin Wang, Fadi Dornaika</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17954" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17954</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bf3a9a797cfd9bf487a467cfbf96c2912048bb9780e4ea911d8505de4a3fd09_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bf3a9a797cfd9bf487a467cfbf96c2912048bb9780e4ea911d8505de4a3fd09_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ellie Zhou, Jihoon Chung, Olga Russakovsky</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17953" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17953</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8fc5229281981de8f8ce6b4446b9b416a3bf03a5b7dfe015f7327ed14da6c6c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8fc5229281981de8f8ce6b4446b9b416a3bf03a5b7dfe015f7327ed14da6c6c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Modular Framework for Single-View 3D Reconstruction of Indoor Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuxiao Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17955" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17955</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/984f7066ad7f63ea4f496d0d617a7a83be0b5004c759374d9228f95333891ba4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/984f7066ad7f63ea4f496d0d617a7a83be0b5004c759374d9228f95333891ba4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Modular Framework for Single-View 3D Reconstruction of Indoor Environments</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shubham Kumar Nigam, Parjanya Aditya Shukla, Noel Shallum, Arnab Bhattacharya</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18004" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18004</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2ffa85f9a5ddd1bd5f673a211deae55a7bad9087838680e7b143e6daac52df8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2ffa85f9a5ddd1bd5f673a211deae55a7bad9087838680e7b143e6daac52df8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Omar Faruq Shikdar, Fahad Ahammed, B. M. Shahria Alam, Golam Kibria, Tawhidur Rahman, Nishat Tasnim Niloy</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17987" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17987</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e026b2918baadec02fb27d5fb57e2257968a98d72147f8159070f8c9fbb7d1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e026b2918baadec02fb27d5fb57e2257968a98d72147f8159070f8c9fbb7d1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Robotic VLA Benefits from Joint Learning with Motion Image Diffusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yu Fang, Kanchana Ranasinghe, Le Xue, Honglu Zhou, Juntao Tan, Ran Xu, Shelby Heinecke, Caiming Xiong, Silvio Savarese, Daniel Szafir, Mingyu Ding, Michael S. Ryoo, Juan Carlos Niebles</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18007" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18007</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b59654686bbf5b51e6cb8f755af962b88b4d3f2a65f9c991f5f10b64604828_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b59654686bbf5b51e6cb8f755af962b88b4d3f2a65f9c991f5f10b64604828_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Robotic VLA Benefits from Joint Learning with Motion Image Diffusion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Name That Part: 3D Part Segmentation and Naming</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Soumava Paul, Prakhar Kaushik, Ankit Vaidya, Anand Bhattad, Alan Yuille</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18003" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18003</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7489c4c88b9369f0e55ac4eb9e395d18b5597618ea2266650f451ea4d5f3eec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7489c4c88b9369f0e55ac4eb9e395d18b5597618ea2266650f451ea4d5f3eec_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Name That Part: 3D Part Segmentation and Naming</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18028" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18028</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30065ffa9a2b8c3aa56f4dd49b67477e394ef33e769980a2e7968c85edfa3346_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30065ffa9a2b8c3aa56f4dd49b67477e394ef33e769980a2e7968c85edfa3346_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Fakrul Islam Tushar, Ehsan Samei, Cynthia Rudin, Joseph Y. Lo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18038" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18038</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7823e2e22739636a655f10a5159ed96eedc679b68020943f440020f0658dad87_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7823e2e22739636a655f10a5159ed96eedc679b68020943f440020f0658dad87_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] FOODER: Real-time Facial Authentication and Expression Recognition</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sabri Mustafa Kahya, Muhammet Sami Yavuz, Boran Hamdi Sivrikaya, Eckehard Steinbach</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18057" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18057</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17048a3dede1c165a0f52aab61de33bef5559518cf2996ad61858f9a9d680457_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17048a3dede1c165a0f52aab61de33bef5559518cf2996ad61858f9a9d680457_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FOODER: Real-time Facial Authentication and Expression Recognition</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] YolovN-CBi: A Lightweight and Efficient Architecture for Real-Time Detection of Small UAVs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ami Pandat, Punna Rajasekhar, Gopika Vinod, Rohit Shukla</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18046" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18046</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74e5d6d01e4d7b6683268a6dfd04d4c061677c1530a7ce95eaaf2ce9e55a0600_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74e5d6d01e4d7b6683268a6dfd04d4c061677c1530a7ce95eaaf2ce9e55a0600_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> YolovN-CBi: A Lightweight and Efficient Architecture for Real-Time Detection of Small UAVs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] FPBench: A Comprehensive Benchmark of Multimodal Large Language Models for Fingerprint Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ekta Balkrishna Gavas, Sudipta Banerjee, Chinmay Hegde, Nasir Memon</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18073" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18073</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6335692d3a738711a3704dedcaa3eaa60dbe8b762e225fb2857789d12a428e0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6335692d3a738711a3704dedcaa3eaa60dbe8b762e225fb2857789d12a428e0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FPBench: A Comprehensive Benchmark of Multimodal Large Language Models for Fingerprint Analysis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shreshth Rajan, Raymond Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18082" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18082</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6dd5ca9b561b8ad179709036acde1f313672cfa0c00a271b9235f8ab0e640d0a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6dd5ca9b561b8ad179709036acde1f313672cfa0c00a271b9235f8ab0e640d0a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Changxu Duan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18115" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18115</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13ec88807faf2814dad94d8489b36894b8aa8c290f9026ca2108a6c17ac22ca6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13ec88807faf2814dad94d8489b36894b8aa8c290f9026ca2108a6c17ac22ca6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SERA-H: Beyond Native Sentinel Spatial Limits for High-Resolution Canopy Height Mapping</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Thomas Boudras, Martin Schwartz, Rasmus Fensholt, Martin Brandt, Ibrahim Fayad, Jean-Pierre Wigneron, Gabriel Belouze, Fajwel Fogel, Philippe Ciais</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18128" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18128</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40cedf819eb6eb6cd85f9bb6838fd612711be404b26144f5bf2363883a2eac28_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40cedf819eb6eb6cd85f9bb6838fd612711be404b26144f5bf2363883a2eac28_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SERA-H: Beyond Native Sentinel Spatial Limits for High-Resolution Canopy Height Mapping</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] EndoStreamDepth: Temporally Consistent Monocular Depth Estimation for Endoscopic Video Streams</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hao Li, Daiwei Lu, Jiacheng Wang, Robert J. Webster III, Ipek Oguz</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18159" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18159</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c6bfdccbbe4e580860cba1ef6a6436d8fc986b7e46d121db741672f2f3a233_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c6bfdccbbe4e580860cba1ef6a6436d8fc986b7e46d121db741672f2f3a233_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EndoStreamDepth: Temporally Consistent Monocular Depth Estimation for Endoscopic Video Streams</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Midhat Urooj, Ayan Banerjee, Sandeep Gupta</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18177" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18177</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7ef165e056ae631599bd024eb4341c57c1705258598a82662ae1166302f2947_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7ef165e056ae631599bd024eb4341c57c1705258598a82662ae1166302f2947_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Local Patches Meet Global Context: Scalable 3D Diffusion Priors for Computed Tomography Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Taewon Yang, Jason Hu, Jeffrey A. Fessler, Liyue Shen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18161" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18161</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6f2d7690da449f60ab28b9f1c8644a2829a779454fd531b2852837f43afe4bb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6f2d7690da449f60ab28b9f1c8644a2829a779454fd531b2852837f43afe4bb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Local Patches Meet Global Context: Scalable 3D Diffusion Priors for Computed Tomography Reconstruction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Atlas is Your Perfect Context: One-Shot Customization for Generalizable Foundational Medical Image Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ziyu Zhang, Yi Yu, Simeng Zhu, Ahmed Aly, Yunhe Gao, Ning Gu, Yuan Xue</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18176" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18176</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/011e5fe0a450b1348324ad1d5a24fbfca2367916d6e12d05481fe9aeccee423d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/011e5fe0a450b1348324ad1d5a24fbfca2367916d6e12d05481fe9aeccee423d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Atlas is Your Perfect Context: One-Shot Customization for Generalizable Foundational Medical Image Segmentation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Is There a Better Source Distribution than Gaussian? Exploring Source Distributions for Image Flow Matching</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Junho Lee, Kwanseok Kim, Joonseok Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18184" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18184</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4568908c3f989abda31d63ece857e924f12b9a87e165bc3d982eeccfbbad2aa2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4568908c3f989abda31d63ece857e924f12b9a87e165bc3d982eeccfbbad2aa2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Is There a Better Source Distribution than Gaussian? Exploring Source Distributions for Image Flow Matching</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ALIGN: Advanced Query Initialization with LiDAR-Image Guidance for Occlusion-Robust 3D Object Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Janghyun Baek, Mincheol Chang, Seokha Moon, Seung Joon Lee, Jinkyu Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18187" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18187</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1281826184eb88510283760e8d18262afdfcf1143f3ba9a7c22b684fb4f4489_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1281826184eb88510283760e8d18262afdfcf1143f3ba9a7c22b684fb4f4489_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ALIGN: Advanced Query Initialization with LiDAR-Image Guidance for Occlusion-Robust 3D Object Detection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MACE-Dance: Motion-Appearance Cascaded Experts for Music-Driven Dance Video Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kaixing Yang, Jiashu Zhu, Xulong Tang, Ziqiao Peng, Xiangyue Zhang, Puwei Wang, Jiahong Wu, Xiangxiang Chu, Hongyan Liu, Jun He</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18181" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18181</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f91052deb821e034255a2a5fe3873c583d9fcbf1ff48249b8457b8bc7b44e261_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f91052deb821e034255a2a5fe3873c583d9fcbf1ff48249b8457b8bc7b44e261_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MACE-Dance: Motion-Appearance Cascaded Experts for Music-Driven Dance Video Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Multi-Part Object Representations via Graph Structures and Co-Part Discovery</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Alex Foo, Wynne Hsu, Mong Li Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18192" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18192</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/691a99ce976b35cbbbe1a4ae36131ee76e0a4332b25068ac660d1ddb59eb7132_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/691a99ce976b35cbbbe1a4ae36131ee76e0a4332b25068ac660d1ddb59eb7132_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multi-Part Object Representations via Graph Structures and Co-Part Discovery</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Unsupervised Anomaly Detection with an Enhanced Teacher for Student-Teacher Feature Pyramid Matching</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mohammad Zolfaghari, Hedieh Sajedi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18219" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18219</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49c3ce25aa0d46d7576b119913f2eb1d74bf6b2d970566f7b3c4a55bb19063d9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49c3ce25aa0d46d7576b119913f2eb1d74bf6b2d970566f7b3c4a55bb19063d9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Unsupervised Anomaly Detection with an Enhanced Teacher for Student-Teacher Feature Pyramid Matching</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Stable and Efficient Single-Rollout RL for Multimodal Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rui Liu, Dian Yu, Lei Ke, Haolin Liu, Yujun Zhou, Zhenwen Liang, Haitao Mi, Pratap Tokekar, Dong Yu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18215" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18215</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81e83852cf5de78a73c53dc039a80d4328420c326e71217ed656de7348457003_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81e83852cf5de78a73c53dc039a80d4328420c326e71217ed656de7348457003_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Stable and Efficient Single-Rollout RL for Multimodal Reasoning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Investigating Spatial Attention Bias in Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Aryan Chaudhary, Sanchit Goyal, Pratik Narang, Dhruv Kumar</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18231" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18231</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be5cd89ea4d004fbb18410473c51ae236387bc088e028d1ddd84dbce2d703bfc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be5cd89ea4d004fbb18410473c51ae236387bc088e028d1ddd84dbce2d703bfc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Investigating Spatial Attention Bias in Vision-Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Multifaceted Exploration of Spatial Openness in Rental Housing: A Big Data Analysis in Tokyo&#x27;s 23 Wards</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Takuya OKi, Yuan Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18226" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18226</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa20e5bd064a3600a6a9399f88b0f303ee4cc42c7edd5c8236408c1ec3052eb8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa20e5bd064a3600a6a9399f88b0f303ee4cc42c7edd5c8236408c1ec3052eb8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multifaceted Exploration of Spatial Openness in Rental Housing: A Big Data Analysis in Tokyo&#x27;s 23 Wards</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SG-RIFE: Semantic-Guided Real-Time Intermediate Flow Estimation with Diffusion-Competitive Perceptual Quality</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pan Ben Wong, Chengli Wu, Hanyue Lu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18241" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18241</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/973299de069f19e1fee6c5ddbbadac21151ac4addddd917990024e02cf59b042_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/973299de069f19e1fee6c5ddbbadac21151ac4addddd917990024e02cf59b042_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SG-RIFE: Semantic-Guided Real-Time Intermediate Flow Estimation with Diffusion-Competitive Perceptual Quality</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shahram Najam Syed, Yitian Hu, Yuchao Yao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18237" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18237</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b04b692d1acf5e8524839243e2fe0778331620858ff93eb02166948c9684c4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b04b692d1acf5e8524839243e2fe0778331620858ff93eb02166948c9684c4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Spectral Discrepancy and Cross-modal Semantic Consistency Learning for Object Detection in Hyperspectral Image</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiao He, Chang Tang, Xinwang Liu, Wei Zhang, Zhimin Gao, Chuankun Li, Shaohua Qiu, Jiangfeng Xu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18245" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18245</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028cfc83b63f51bf798c2014f1dc8e1f4c786134910334880eb4c0cf340a5eb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028cfc83b63f51bf798c2014f1dc8e1f4c786134910334880eb4c0cf340a5eb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Spectral Discrepancy and Cross-modal Semantic Consistency Learning for Object Detection in Hyperspectral Image</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Who Can See Through You? Adversarial Shielding Against VLM-Based Attribute Inference Attacks</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yucheng Fan, Jiawei Chen, Yu Tian, Zhaoxia Yin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18264" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18264</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6181175b477e5c50e93e9a1a3a4690fb1673471073933b770754e208c7b0e776_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6181175b477e5c50e93e9a1a3a4690fb1673471073933b770754e208c7b0e776_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Who Can See Through You? Adversarial Shielding Against VLM-Based Attribute Inference Attacks</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Towards Ancient Plant Seed Classification: A Benchmark Dataset and Baseline Model</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rui Xing, Runmin Cong, Yingying Wu, Can Wang, Zhongming Tang, Fen Wang, Hao Wu, Sam Kwong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18247" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18247</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d341345aee67211dc7e3cea021a11051fd66231eb649e4da442fda6828319f0d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d341345aee67211dc7e3cea021a11051fd66231eb649e4da442fda6828319f0d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards Ancient Plant Seed Classification: A Benchmark Dataset and Baseline Model</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Building UI/UX Dataset for Dark Pattern Detection and YOLOv12x-based Real-Time Object Recognition Detection System</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Se-Young Jang, Su-Yeon Yoon, Jae-Woong Jung, Dong-Hun Lee, Seong-Hun Choi, Soo-Kyung Jun, Yu-Bin Kim, Young-Seon Ju, Kyounggon Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18269" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18269</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/367924eac7293dd60fe2ea42c0f8cc5f30a08d5958f71c4e9e1a77afcf26f521_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/367924eac7293dd60fe2ea42c0f8cc5f30a08d5958f71c4e9e1a77afcf26f521_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Building UI/UX Dataset for Dark Pattern Detection and YOLOv12x-based Real-Time Object Recognition Detection System</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Loom: Diffusion-Transformer for Interleaved Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mingcheng Ye, Jiaming Liu, Yiren Song</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18254" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18254</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2702848bca515645a4d065b0a7036ee7ddebe3d80478578da0afc6637e8987e1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2702848bca515645a4d065b0a7036ee7ddebe3d80478578da0afc6637e8987e1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Loom: Diffusion-Transformer for Interleaved Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] UniMPR: A Unified Framework for Multimodal Place Recognition with Arbitrary Sensor Configurations</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhangshuo Qi, Jingyi Xu, Luqi Cheng, Shichen Wen, Yiming Ma, Guangming Xiong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18279" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18279</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d97551a04523bb912a1f4f7a4089cc0954eacc215acb16ece3401441f9ae898e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d97551a04523bb912a1f4f7a4089cc0954eacc215acb16ece3401441f9ae898e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> UniMPR: A Unified Framework for Multimodal Place Recognition with Arbitrary Sensor Configurations</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Pyramidal Adaptive Cross-Gating for Multimodal Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zidong Gu, Shoufu Tian</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18291" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18291</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/686814403fba53ef7884ea37a0cf31a6148bda927776fcfca1291870ba0502e3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/686814403fba53ef7884ea37a0cf31a6148bda927776fcfca1291870ba0502e3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Pyramidal Adaptive Cross-Gating for Multimodal Detection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Eren Caglar, Amirkia Rafiei Oskooei, Mehmet Kutanoglu, Mustafa Keles, Mehmet S. Aktas</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18318" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18318</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff046f84477e998c712a0f584e02cdfbe6c1bef89652e720bfe7cbb5bb7764ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff046f84477e998c712a0f584e02cdfbe6c1bef89652e720bfe7cbb5bb7764ac_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MatE: Material Extraction from Single-Image via Geometric Prior</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zeyu Zhang, Wei Zhai, Jian Yang, Yang Cao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18312" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18312</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb74dfaa1e58347f80eafe1b8d6c9ccb8621df2345727b43170380c7a9ec7111_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb74dfaa1e58347f80eafe1b8d6c9ccb8621df2345727b43170380c7a9ec7111_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MatE: Material Extraction from Single-Image via Geometric Prior</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MatSpray: Fusing 2D Material World Knowledge on 3D Geometry</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Philipp Langsteiner, Jan-Niklas Dihlmann, Hendrik P.A. Lensch</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18314" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18314</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bc201c384a37d43b2bd10a9944a6df93bc871b921e058d53b3a80eb95aa3784_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bc201c384a37d43b2bd10a9944a6df93bc871b921e058d53b3a80eb95aa3784_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MatSpray: Fusing 2D Material World Knowledge on 3D Geometry</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A two-stream network with global-local feature fusion for bone age assessment</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Qiong Lou, Han Yang, Fang Lu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18331" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18331</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d26c3d7fded2a10e84ebad25bc6bb1810428c48d125c9ad4b3a401fe14ca66d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d26c3d7fded2a10e84ebad25bc6bb1810428c48d125c9ad4b3a401fe14ca66d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A two-stream network with global-local feature fusion for bone age assessment</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MCVI-SANet: A lightweight semi-supervised model for LAI and SPAD estimation of winter wheat under vegetation index saturation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhiheng Zhang, Jiajun Yang, Hong Sun, Dong Wang, Honghua Jiang, Yaru Chen, Tangyuan Ning</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18344" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18344</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f19d6aa9f347693da897453773133b3a6f0b66ee06e5f6bdf421bb24507e5f56_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f19d6aa9f347693da897453773133b3a6f0b66ee06e5f6bdf421bb24507e5f56_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MCVI-SANet: A lightweight semi-supervised model for LAI and SPAD estimation of winter wheat under vegetation index saturation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Enhancing 3D Semantic Scene Completion with a Refinement Module</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dunxing Zhang, Jiachen Lu, Han Yang, Lei Bao, Bo Song</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18363" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18363</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87b47e037d36c9a89056d6636a00bc4c1b54b6f312aac970b439024a3aa71b85_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87b47e037d36c9a89056d6636a00bc4c1b54b6f312aac970b439024a3aa71b85_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Enhancing 3D Semantic Scene Completion with a Refinement Module</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] RecurGS: Interactive Scene Modeling via Discrete-State Recurrent Gaussian Fusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wenhao Hu, Haonan Zhou, Zesheng Li, Liu Liu, Jiacheng Dong, Zhizhong Su, Gaoang Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18386" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18386</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e36e78691b7bf84af271eac170b5599e6d8dffe940baf4afb324df4c1c26545_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e36e78691b7bf84af271eac170b5599e6d8dffe940baf4afb324df4c1c26545_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RecurGS: Interactive Scene Modeling via Discrete-State Recurrent Gaussian Fusion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Automated Mosaic Tesserae Segmentation via Deep Learning Techniques</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Charilaos Kapelonis, Marios Antonakakis, Konstantinos Politof, Aristomenis Antoniadis, Michalis Zervakis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18406" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18406</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6893130bacdfd82f19e38a48fe6c1a981e8a6e139face8bc5ca4379fb98ab0c8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6893130bacdfd82f19e38a48fe6c1a981e8a6e139face8bc5ca4379fb98ab0c8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Automated Mosaic Tesserae Segmentation via Deep Learning Techniques</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Fei Song, Yi Li, Jiangmeng Li, Rui Wang, Changwen Zheng, Fanjiang Xu, Hui Xiong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18411" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18411</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c480d382f1a840aaa7b8f0ed1cc380b9ece2b0a04c01a7bfeaf7551356ef5a0c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c480d382f1a840aaa7b8f0ed1cc380b9ece2b0a04c01a7bfeaf7551356ef5a0c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Badr Moufad, Navid Bagheri Shouraki, Alain Oliviero Durmus, Thomas Hirtz, Eric Moulines, Jimmy Olsson, Yazid Janati</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18365" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18365</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb10f5246f2791c1a5c5d83727bb85643a1c89da44e041633e646e724bfa1d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb10f5246f2791c1a5c5d83727bb85643a1c89da44e041633e646e724bfa1d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Through the PRISm: Importance-Aware Scene Graphs for Image Retrieval</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dimitrios Georgoulopoulos, Nikolaos Chaidos, Angeliki Dimitriou, Giorgos Stamou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18407" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18407</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8250115733311080fc732003a6de9ee573531db16fb388825dac28ed45b0904f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8250115733311080fc732003a6de9ee573531db16fb388825dac28ed45b0904f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Through the PRISm: Importance-Aware Scene Graphs for Image Retrieval</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] E-RGB-D: Real-Time Event-Based Perception with Structured Light</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Seyed Ehsan Marjani Bajestani, Giovanni Beltrame</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18429" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18429</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adf7afb7ba214958716e0fa581dd6ddbddce2e0c6851dde58c99a8a067def71d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adf7afb7ba214958716e0fa581dd6ddbddce2e0c6851dde58c99a8a067def71d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> E-RGB-D: Real-Time Event-Based Perception with Structured Light</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Object-Centric Framework for Video Moment Retrieval</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zongyao Li, Yongkang Wong, Satoshi Yamazaki, Jianquan Liu, Mohan Kankanhalli</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18448" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18448</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aeaf50384c57c0a0975048f565d619000d31089ae116153752950a50d948d161_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aeaf50384c57c0a0975048f565d619000d31089ae116153752950a50d948d161_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Object-Centric Framework for Video Moment Retrieval</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Agent-Based Output Drift Detection for Breast Cancer Response Prediction in a Multisite Clinical Decision Support System</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xavier Rafael-Palou, Jose Munuera, Ana Jimenez-Pastor, Richard Osuala, Karim Lekadir, Oliver Diaz</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18450" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18450</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a54a0b377f2d61cdb058b9dd088fe0948517999354f7c502389a3f1149e8a3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a54a0b377f2d61cdb058b9dd088fe0948517999354f7c502389a3f1149e8a3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Agent-Based Output Drift Detection for Breast Cancer Response Prediction in a Multisite Clinical Decision Support System</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MeniMV: A Multi-view Benchmark for Meniscus Injury Severity Grading</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shurui Xu, Siqi Yang, Jiapin Ren, Zhong Cao, Hongwei Yang, Mengzhen Fan, Yuyu Sun, Shuyan Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18437" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18437</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82297f34fc5f7dfab35d4ab984e7ba607ad5ac2c849bffbec01d38cbdcf42e3d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82297f34fc5f7dfab35d4ab984e7ba607ad5ac2c849bffbec01d38cbdcf42e3d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MeniMV: A Multi-view Benchmark for Meniscus Injury Severity Grading</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jayant Lohia</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18453" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18453</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e4686d2a1c44f6a0f1d346c2e5401709f3d974dfd9062781f00cc8eb5d71952_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e4686d2a1c44f6a0f1d346c2e5401709f3d974dfd9062781f00cc8eb5d71952_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Plasticine: A Traceable Diffusion Model for Medical Image Translation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tianyang Zhanng, Xinxing Cheng, Jun Cheng, Shaoming Zheng, He Zhao, Huazhu Fu, Alejandro F Frangi, Jiang Liu, Jinming Duan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18455" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18455</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2148612b9f3ccd68ab3abdcc94c84b8f1b0d8d4ed68811c6de5496dd1c2352d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2148612b9f3ccd68ab3abdcc94c84b8f1b0d8d4ed68811c6de5496dd1c2352d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Plasticine: A Traceable Diffusion Model for Medical Image Translation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Santwana Sagnika, Manav Malhotra, Ishtaj Kaur Deol, Soumyajit Roy, Swarnav Kumar</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18500" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18500</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3d3024bb6cf729a0d0827d13bba185e065be8eb34bd4dffa40d58782bc4e5ab_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3d3024bb6cf729a0d0827d13bba185e065be8eb34bd4dffa40d58782bc4e5ab_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] STORM: Search-Guided Generative World Models for Robotic Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wenjun Lin, Jensen Zhang, Kaitong Cai, Keze Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18477" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18477</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79949cb474434bd024983169a211bb0a029e6412a974a6dd6f4ee7a51cf05349_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79949cb474434bd024983169a211bb0a029e6412a974a6dd6f4ee7a51cf05349_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> STORM: Search-Guided Generative World Models for Robotic Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Adaptive-VoCo: Complexity-Aware Visual Token Compression for Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiaoyang Guo, Keze Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18496" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18496</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff175aad1ff6f6b7f47faa5f2c01c3d0f7b27451e06bf33f2c763508f1ff6f0e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff175aad1ff6f6b7f47faa5f2c01c3d0f7b27451e06bf33f2c763508f1ff6f0e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Adaptive-VoCo: Complexity-Aware Visual Token Compression for Vision-Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] NASTaR: NovaSAR Automated Ship Target Recognition Dataset</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Benyamin Hosseiny, Kamirul Kamirul, Odysseas Pappas, Alin Achim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18503" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18503</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4ac29e833bc5eaed0a9881e81dd877d55f600f6bd8722c27c1466b58afc1378_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4ac29e833bc5eaed0a9881e81dd877d55f600f6bd8722c27c1466b58afc1378_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NASTaR: NovaSAR Automated Ship Target Recognition Dataset</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] GTMA: Dynamic Representation Optimization for OOD Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jensen Zhang, Ningyuan Liu, Keze Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18504" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18504</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec81480aabd317eb3778c36f956b818e8aed383ab7f9a13fe2c867243fcec025_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec81480aabd317eb3778c36f956b818e8aed383ab7f9a13fe2c867243fcec025_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GTMA: Dynamic Representation Optimization for OOD Vision-Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] WoundNet-Ensemble: A Novel IoMT System Integrating Self-Supervised Deep Learning and Multi-Model Fusion for Automated, High-Accuracy Wound Classification and Healing Progression Monitoring</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Moses Kiprono</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18528" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18528</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c30b62937b70850aaf6278e7cee47b3850562dc10f4c505a5d923c08c6e1e20c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c30b62937b70850aaf6278e7cee47b3850562dc10f4c505a5d923c08c6e1e20c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> WoundNet-Ensemble: A Novel IoMT System Integrating Self-Supervised Deep Learning and Multi-Model Fusion for Automated, High-Accuracy Wound Classification and Healing Progression Monitoring</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rahul Yumlembam, Biju Issac, Nauman Aslam, Eaby Kollonoor Babu, Josh Collyer, Fraser Kennedy</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18527" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18527</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1520c61491b8f395b60f64432e37eff57c8608eba74cd9029c6109d32db7554_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1520c61491b8f395b60f64432e37eff57c8608eba74cd9029c6109d32db7554_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Enhancing Medical Large Vision-Language Models via Alignment Distillation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Aofei Chang, Ting Wang, Fenglong Ma</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18554" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18554</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65cfdaf6c50eb13e1535902993b0e58d2ccb2d1d8a89304254b7eb116f2e3bec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65cfdaf6c50eb13e1535902993b0e58d2ccb2d1d8a89304254b7eb116f2e3bec_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Enhancing Medical Large Vision-Language Models via Alignment Distillation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Hierarchical Bayesian Framework for Multisource Domain Adaptation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Alexander M. Glandon, Khan M. Iftekharuddin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18553" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18553</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8faca3b573bb5e3938ff1e6d161ec550a7c61bbaaa60d1a247d4dd139982ad93_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8faca3b573bb5e3938ff1e6d161ec550a7c61bbaaa60d1a247d4dd139982ad93_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Hierarchical Bayesian Framework for Multisource Domain Adaptation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sumaiya Ali, Areej Alhothali, Ohoud Alzamzami, Sameera Albasri, Ahmed Abduljabbar, Muhammad Alwazzan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18573" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18573</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/387c39b67caa5e84daf0327dfb4c7a948d6d72c292a3404e6c7e8a2bcd6db7df_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/387c39b67caa5e84daf0327dfb4c7a948d6d72c292a3404e6c7e8a2bcd6db7df_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] OpenView: Empowering MLLMs with Out-of-view VQA</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Qixiang Chen, Cheng Zhang, Chi-Wing Fu, Jingwen Ye, Jianfei Cai</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18563" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18563</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd24136fa7bb319394718494b72886c4e4dd61a8ff3664079b5014b3c8e9d20_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd24136fa7bb319394718494b72886c4e4dd61a8ff3664079b5014b3c8e9d20_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> OpenView: Empowering MLLMs with Out-of-view VQA</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Weijie Zhou, Xuangtang Xiong, Ye Tian, Lijun Yue, Xinyu Wu, Wei Li, Chaoyang Zhao, Honghui Dong, Ming Tang, Jinqiao Wang, Zhengyou Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18571" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18571</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e48075d8fabfbd12f97c2605f729d021ebb805999e01bbf511f08a872cf4cbae_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e48075d8fabfbd12f97c2605f729d021ebb805999e01bbf511f08a872cf4cbae_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Commercial Vehicle Braking Optimization: A Robust SIFT-Trajectory Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhe Li, Kun Cheng, Hanyue Mo, Jintao Lu, Ziwen Kuang, Jianwen Ye, Lixu Xu, Xinya Meng, Jiahui Zhao, Shengda Ji, Shuyuan Liu, Mengyu Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18597" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18597</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e804718e966934e64cc22d02e796b1123866e2dd8d037801ef21cbcbee4c0537_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e804718e966934e64cc22d02e796b1123866e2dd8d037801ef21cbcbee4c0537_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Commercial Vehicle Braking Optimization: A Robust SIFT-Trajectory Approach</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jianglin Lu, Yuanwei Wu, Ziyi Zhao, Hongcheng Wang, Felix Jimenez, Abrar Majeedi, Yun Fu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18599" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18599</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/196c51c23a0210e0d5f61fa9aa109b4c1b71673440779c2559e8762b7020daf5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/196c51c23a0210e0d5f61fa9aa109b4c1b71673440779c2559e8762b7020daf5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Saeideh Yousefzadeh, Hamidreza Pourreza</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18613" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18613</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15155edf4c23c5fa3645a1383be98a1728e9025130895c36fb1d8c7a536e2335_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15155edf4c23c5fa3645a1383be98a1728e9025130895c36fb1d8c7a536e2335_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] PTTA: A Pure Text-to-Animation Framework for High-Quality Creation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ruiqi Chen, Kaitong Cai, Yijia Fan, Keze Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18614" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18614</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95e6d5eb29d8f200e9b4d751e2105c378ddc6c8efc5742b330b0ff8118931fb1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95e6d5eb29d8f200e9b4d751e2105c378ddc6c8efc5742b330b0ff8118931fb1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PTTA: A Pure Text-to-Animation Framework for High-Quality Creation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Adversarial Robustness in Zero-Shot Learning<!-- -->:An<!-- --> Empirical Study on Class and Concept-Level Vulnerabilities</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhiyuan Peng, Zihan Ye, Shreyank N Gowda, Yuping Yan, Haotian Xu, Ling Shao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18651" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18651</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/723c55bcd003bd92c1a8106676da980d908b4363b91caeab957599d3002e301d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/723c55bcd003bd92c1a8106676da980d908b4363b91caeab957599d3002e301d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Adversarial Robustness in Zero-Shot Learning<!-- -->:An<!-- --> Empirical Study on Class and Concept-Level Vulnerabilities</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pengxiang Ouyang, Qing Ma, Zheng Wang, Cong Bai</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18660" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18660</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06fe8e37dc2d1d1d6a5d48298128004f86b1be4d6ecbeca32f5c72786b1a535c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06fe8e37dc2d1d1d6a5d48298128004f86b1be4d6ecbeca32f5c72786b1a535c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SplatBright: Generalizable Low-Light Scene Reconstruction from Sparse Views via Physically-Guided Gaussian Enhancement</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yue Wen, Liang Song, Hesheng Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18655" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18655</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e36abe6a038f84b030a1de9a4b07a7b5e542151e56041f8a4107ffcc75a8c4fa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e36abe6a038f84b030a1de9a4b07a7b5e542151e56041f8a4107ffcc75a8c4fa_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SplatBright: Generalizable Low-Light Scene Reconstruction from Sparse Views via Physically-Guided Gaussian Enhancement</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Offline Reinforcement Learning for End-to-End Autonomous Driving</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chihiro Noguchi, Takaki Yamamoto</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18662" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18662</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48070c89f8318c57738214d175fd04c9e38d7e43e2838b599453ae0e31d8c27f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48070c89f8318c57738214d175fd04c9e38d7e43e2838b599453ae0e31d8c27f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Offline Reinforcement Learning for End-to-End Autonomous Driving</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Geometric-Photometric Event-based 3D Gaussian Ray Tracing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kai Kohyama, Yoshimitsu Aoki, Guillermo Gallego, Shintaro Shiba</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18640" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18640</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85632c631e489e2f789f1b5319b05b69e8e883a12a7b626de475c98e6066ef45_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85632c631e489e2f789f1b5319b05b69e8e883a12a7b626de475c98e6066ef45_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Geometric-Photometric Event-based 3D Gaussian Ray Tracing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SmartSight: Mitigating Hallucination in Video-LLMs Without Compromising Video Understanding via Temporal Attention Collapse</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yiming Sun, Mi Zhang, Feifei Li, Geng Hong, Min Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18671" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18671</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aba3b21e95cf3e99fe497ad62634d3e70efc41858e08b88889ea760e9fc41f24_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aba3b21e95cf3e99fe497ad62634d3e70efc41858e08b88889ea760e9fc41f24_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SmartSight: Mitigating Hallucination in Video-LLMs Without Compromising Video Understanding via Temporal Attention Collapse</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Uni-Neur2Img: Unified Neural Signal-Guided Image Generation, Editing, and Stylization via Diffusion Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiyue Bai, Ronghao Yu, Jia Xiu, Pengfei Zhou, Jie Xia, Peng Ji</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18635" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18635</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c6acb4dae57f2c9f4c60c94122091feee1beca126d52be260f7006e1f25433b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c6acb4dae57f2c9f4c60c94122091feee1beca126d52be260f7006e1f25433b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Uni-Neur2Img: Unified Neural Signal-Guided Image Generation, Editing, and Stylization via Diffusion Transformers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] brat: Aligned Multi-View Embeddings for Brain MRI Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Maxime Kayser, Maksim Gridnev, Wanting Wang, Max Bain, Aneesh Rangnekar, Avijit Chatterjee, Aleksandr Petrov, Harini Veeraraghavan, Nathaniel C. Swinburne</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18679" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18679</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e2d9cbeed3e332a02f5cc9adf5abc0ec370a32b059de24cd550cc930eb84a82_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e2d9cbeed3e332a02f5cc9adf5abc0ec370a32b059de24cd550cc930eb84a82_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> brat: Aligned Multi-View Embeddings for Brain MRI Analysis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] AsyncDiff: Asynchronous Timestep Conditioning for Enhanced Text-to-Image Diffusion Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Longhuan Xu, Feng Yin, Cunjian Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18675</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b77064b6897ecbdf7c4ad30fb422607730ad3fdb70df7e888f14b1c35318262c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b77064b6897ecbdf7c4ad30fb422607730ad3fdb70df7e888f14b1c35318262c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AsyncDiff: Asynchronous Timestep Conditioning for Enhanced Text-to-Image Diffusion Inference</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Study of Finetuning Video Transformers for Multi-view Geometry Tasks</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Huimin Wu, Kwang-Ting Cheng, Stephen Lin, Zhirong Wu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18684" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18684</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbb0d58e96608a91e501c2ec636326573d0b8bbd34db61dfabc4e1544edbaaef_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbb0d58e96608a91e501c2ec636326573d0b8bbd34db61dfabc4e1544edbaaef_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Study of Finetuning Video Transformers for Multi-view Geometry Tasks</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] EcoSplat: Efficiency-controllable Feed-forward 3D Gaussian Splatting from Multi-view Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jongmin Park, Minh-Quan Viet Bui, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18692" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18692</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbc186e81fb4bc0dba691524b3e057afe1a2f63cefe632fd49cd141b1ddd38ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbc186e81fb4bc0dba691524b3e057afe1a2f63cefe632fd49cd141b1ddd38ac_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EcoSplat: Efficiency-controllable Feed-forward 3D Gaussian Splatting from Multi-view Images</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Rectification Reimagined: A Unified Mamba Model for Image Correction and Rectangling with Prompts</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Linwei Qiu, Gongzhe Li, Xiaozhe Zhang, Qinlin Sun, Fengying Xie</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18718" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18718</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1fb34acfd0681b1e7d5e9dd5a9b2ead7a096ce7f062456abd5c1d666d63c9f6d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1fb34acfd0681b1e7d5e9dd5a9b2ead7a096ce7f062456abd5c1d666d63c9f6d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Rectification Reimagined: A Unified Mamba Model for Image Correction and Rectangling with Prompts</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Breast Cancer Recurrence Risk Prediction Based on Multiple Instance Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jinqiu Chen, Huyan Xu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18734" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18734</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/703d5b9807b107a0861def2e3d8e792e6ba0cafc487dd572e6b8472d4694e38c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/703d5b9807b107a0861def2e3d8e792e6ba0cafc487dd572e6b8472d4694e38c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Breast Cancer Recurrence Risk Prediction Based on Multiple Instance Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>M</mi><mn>3</mn></msup><mo>−</mo><mi>V</mi><mi>e</mi><mi>r</mi><mi>s</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">M^3-Verse</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mord mathnormal">erse</span></span></span></span>: A &quot;Spot the Difference&quot; Challenge for Large Multimodal Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kewei Wei, Bocheng Hu, Jie Cao, Xiaohan Chen, Zhengxi Lu, Wubing Xia, Weili Xu, Jiaao Wu, Junchen He, Mingyu Jia, Ciyun Zhao, Ye Sun, Yizhi Li, Zhonghan Zhao, Jian Zhang, Gaoang Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18735" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18735</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52a20f9bfaa32af67c363579cc4ad37ddbcaa7484f53c2ad3e6f6287dffcb22d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52a20f9bfaa32af67c363579cc4ad37ddbcaa7484f53c2ad3e6f6287dffcb22d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>M</mi><mn>3</mn></msup><mo>−</mo><mi>V</mi><mi>e</mi><mi>r</mi><mi>s</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">M^3-Verse</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mord mathnormal">erse</span></span></span></span>: A &quot;Spot the Difference&quot; Challenge for Large Multimodal Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] AMLID: An Adaptive Multispectral Landmine Identification Dataset for Drone-Based Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> James E. Gallagher, Edward J. Oughton</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18738" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18738</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd46cd4988eda7be0f3fc725e20d0cd6d84d2d855b2fe6ca9ada6da8ce6533a1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd46cd4988eda7be0f3fc725e20d0cd6d84d2d855b2fe6ca9ada6da8ce6533a1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AMLID: An Adaptive Multispectral Landmine Identification Dataset for Drone-Based Detection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tianrui Zhu, Shiyi Zhang, Zhirui Sun, Jingqi Tian, Yansong Tang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18741" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18741</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17ec1d3f6ff19284c38fae7fffb7c89e33c210bb632c6e29bab17704791956af_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17ec1d3f6ff19284c38fae7fffb7c89e33c210bb632c6e29bab17704791956af_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Context-Aware Network Based on Multi-scale Spatio-temporal Attention for Action Recognition in Videos</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiaoyang Li, Wenzhu Yang, Kanglin Wang, Tiebiao Wang, Qingsong Fei</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18750" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18750</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d25c33c0793bb9ac5e532b11ef64839818a8ed3877ba9c30ed6683982c402d3b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d25c33c0793bb9ac5e532b11ef64839818a8ed3877ba9c30ed6683982c402d3b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Context-Aware Network Based on Multi-scale Spatio-temporal Attention for Action Recognition in Videos</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] IPCV: Information-Preserving Compression for MLLM Visual Encoders</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuan Chen, Zichen Wen, Yuzhou Wu, Xuyang Liu, Shuang Chen, Junpeng Ma, Weijia Li, Conghui He, Linfeng Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18747" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18747</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23f93076434c2a627bcdaf65dfd58999db9105798eceb360e343a3f4018cc020_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23f93076434c2a627bcdaf65dfd58999db9105798eceb360e343a3f4018cc020_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> IPCV: Information-Preserving Compression for MLLM Visual Encoders</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kaican Li, Lewei Yao, Jiannan Wu, Tiezheng Yu, Jierun Chen, Haoli Bai, Lu Hou, Lanqing Hong, Wei Zhang, Nevin L. Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18745" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18745</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87556fdc09b55b65c0d472d205a384cc42453256afeb9e7a28db98178fe1d145_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87556fdc09b55b65c0d472d205a384cc42453256afeb9e7a28db98178fe1d145_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] In-Context Audio Control of Video Diffusion Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wenze Liu, Weicai Ye, Minghong Cai, Quande Liu, Xintao Wang, Xiangyu Yue</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18772" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18772</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2f6222ebfd1fddbc353b3e719e28fb0ea12ffea379c74f8f8539de1c07ea2e3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2f6222ebfd1fddbc353b3e719e28fb0ea12ffea379c74f8f8539de1c07ea2e3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> In-Context Audio Control of Video Diffusion Transformers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MaskFocus: Focusing Policy Optimization on Critical Steps for Masked Image Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Guohui Zhang, Hu Yu, Xiaoxiao Ma, Yaning Pan, Hang Xu, Feng Zhao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18766" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18766</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e30d5e631331f72f5d3daa88862f5f4de5bcba185997003c4eb4c2df4ba695e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e30d5e631331f72f5d3daa88862f5f4de5bcba185997003c4eb4c2df4ba695e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MaskFocus: Focusing Policy Optimization on Critical Steps for Masked Image Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Fanis Mathioulakis, Gorjan Radevski, Tinne Tuytelaars</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18784" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18784</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0254e0e9393c18241de33ec503d4cc8d75622e94c6a0d08ec4ee16da3e3b6b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0254e0e9393c18241de33ec503d4cc8d75622e94c6a0d08ec4ee16da3e3b6b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Guangtao Lyu, Chenghao Xu, Qi Liu, Jiexi Yan, Muli Yang, Fen Fang, Cheng Deng</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18804" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18804</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dab2ab51309551324dcfdba087c2e5ec1e6dfdae10633047423e7233f30fdf84_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dab2ab51309551324dcfdba087c2e5ec1e6dfdae10633047423e7233f30fdf84_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ziyuan Tao, Chuanzhi Xu, Sandaru Jayawardana, Wei Bao, Kanchana Thilakarathna, Teng Joon Lim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18809" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18809</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/396672768b4960b9fefe0f861b938a8b78842c8181043ded9d12c7e8f28dbdfe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/396672768b4960b9fefe0f861b938a8b78842c8181043ded9d12c7e8f28dbdfe_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Revealing Perception and Generation Dynamics in LVLMs: Mitigating Hallucinations via Validated Dominance Correction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Guangtao Lyu, Xinyi Cheng, Chenghao Xu, Qi Liu, Muli Yang, Fen Fang, Huilin Chen, Jiexi Yan, Xu Yang, Cheng Deng</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18813" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18813</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ecc3b83a3bbbf2f3d70f79eb80fb5574e88c49c36b11bed230f795915fb03_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ecc3b83a3bbbf2f3d70f79eb80fb5574e88c49c36b11bed230f795915fb03_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Revealing Perception and Generation Dynamics in LVLMs: Mitigating Hallucinations via Validated Dominance Correction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuxiao Yang, Hualian Sheng, Sijia Cai, Jing Lin, Jiahao Wang, Bing Deng, Junzhe Lu, Haoqian Wang, Jieping Ye</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18814" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18814</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a2ba0db72977cbda98db1501df497120d084239fa62014678ae702cb513369e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a2ba0db72977cbda98db1501df497120d084239fa62014678ae702cb513369e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Brain-Gen: Towards Interpreting Neural Signals for Stimulus Reconstruction Using Transformers and Latent Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hasib Aslam, Muhammad Talal Faiz, Muhammad Imran Malik</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18843" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18843</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c10b7d7d79ed94bbb6547ab2dcefe6d392d199280eab76c815bf57db3927efb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c10b7d7d79ed94bbb6547ab2dcefe6d392d199280eab76c815bf57db3927efb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Brain-Gen: Towards Interpreting Neural Signals for Stimulus Reconstruction Using Transformers and Latent Diffusion Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] VizDefender: Unmasking Visualization Tampering through Proactive Localization and Intent Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sicheng Song, Yanjie Zhang, Zixin Chen, Huamin Qu, Changbo Wang, Chenhui Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18853" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18853</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6df051754b8f594a7ef88a46e4855d2eb43c4c662b1afbca9997caf2b4fbad53_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6df051754b8f594a7ef88a46e4855d2eb43c4c662b1afbca9997caf2b4fbad53_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VizDefender: Unmasking Visualization Tampering through Proactive Localization and Intent Inference</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Application of deep learning approaches for medieval historical documents transcription</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Maksym Voloshchuk, Bohdana Zarembovska, Mykola Kozlenko</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18865" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18865</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bc697efc3bfd30e66b187f56c365b6a8add07756fb768bc77ba4586f1ab7d205_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bc697efc3bfd30e66b187f56c365b6a8add07756fb768bc77ba4586f1ab7d205_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Application of deep learning approaches for medieval historical documents transcription</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Cross-modal Counterfactual Explanations: Uncovering Decision Factors and Dataset Biases in Subjective Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Alina Elena Baia, Andrea Cavallaro</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18864" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18864</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54def5a8025f36fedd1db00274e2f13348c15e8cd933c948a5286d52b31223ae_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54def5a8025f36fedd1db00274e2f13348c15e8cd933c948a5286d52b31223ae_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Cross-modal Counterfactual Explanations: Uncovering Decision Factors and Dataset Biases in Subjective Classification</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Localising Shortcut Learning in Pixel Space via Ordinal Scoring Correlations for Attribution Representations (OSCAR)</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Akshit Achara, Peter Triantafillou, Esther Puyol-Antón, Alexander Hammers, Andrew P. King</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18888" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18888</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c85ecde5b204bf7cf4f7ce66885bc81ef00aee9a5651c7332b2bd485f6e54e2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c85ecde5b204bf7cf4f7ce66885bc81ef00aee9a5651c7332b2bd485f6e54e2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Localising Shortcut Learning in Pixel Space via Ordinal Scoring Correlations for Attribution Representations (OSCAR)</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kaidi Liang, Ke Li, Xianbiao Hu, Ruwen Qin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18878" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18878</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40cc111afbcdc76e8f9c40d867f3e3d92fefb4f06215bb877943f16f5fc7f761_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40cc111afbcdc76e8f9c40d867f3e3d92fefb4f06215bb877943f16f5fc7f761_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Thinking Beyond Labels: Vocabulary-Free Fine-Grained Recognition using Reasoning-Augmented LMMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dmitry Demidov, Zaigham Zaheer, Zongyan Han, Omkar Thawakar, Rao Anwer</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18897" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18897</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8993f53f872c636102637faace6bbf220931bba2b5c07be198751d05e23fa52_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8993f53f872c636102637faace6bbf220931bba2b5c07be198751d05e23fa52_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Thinking Beyond Labels: Vocabulary-Free Fine-Grained Recognition using Reasoning-Augmented LMMs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Delta-LLaVA: Base-then-Specialize Alignment for Token-Efficient Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mohamad Zamini, Diksha Shukla</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18910" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18910</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82dc1b903fe39645cfdac67e793b8aaf7d0b36f43bd3088781f3ec68c702dafb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82dc1b903fe39645cfdac67e793b8aaf7d0b36f43bd3088781f3ec68c702dafb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Delta-LLaVA: Base-then-Specialize Alignment for Token-Efficient Vision-Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Point What You Mean: Visually Grounded Instruction Policy</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hang Yu, Juntu Zhao, Yufeng Liu, Kaiyu Li, Cheng Ma, Di Zhang, Yingdong Hu, Guang Chen, Junyuan Xie, Junliang Guo, Junqiao Zhao, Yang Gao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18933" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18933</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0cc4da48f9bff61ef73842c7f9922cf58cd10179d0b3d6cb1241fbc052909cc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0cc4da48f9bff61ef73842c7f9922cf58cd10179d0b3d6cb1241fbc052909cc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Point What You Mean: Visually Grounded Instruction Policy</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Raina Panda, Daniel Fein, Arpita Singhal, Mark Fiore, Maneesh Agrawala, Matyas Bohacek</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18930" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18930</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17d9e52f35a5e302d613cba6423f95b6e9bb58c2b559fbd5a209c0516f8e2326_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17d9e52f35a5e302d613cba6423f95b6e9bb58c2b559fbd5a209c0516f8e2326_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Symmetrization of 3D Generative Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nicolas Caytuiro, Ivan Sipiran</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18953" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18953</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67504b808f29cea86fc99619a21f55d7a71b7b925241cf4a90f7273b07bddf83_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67504b808f29cea86fc99619a21f55d7a71b7b925241cf4a90f7273b07bddf83_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Symmetrization of 3D Generative Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] DVI: Disentangling Semantic and Visual Identity for Training-Free Personalized Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Guandong Li, Yijun Ding</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18964" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18964</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5f5e482fec6d1911329e83df08fb77c38f618acd901b6be5084bffe87124bfb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5f5e482fec6d1911329e83df08fb77c38f618acd901b6be5084bffe87124bfb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DVI: Disentangling Semantic and Visual Identity for Training-Free Personalized Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Total Curvature Regularization and its_Minimization for Surface and Image Smoothing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tianle Lu, Ke Chen, Yuping Duan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18968" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18968</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e49d05c1cc8ec54ae283a3da000454b18542cf8f4a1408bd3980e43be6a00e3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e49d05c1cc8ec54ae283a3da000454b18542cf8f4a1408bd3980e43be6a00e3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Total Curvature Regularization and its_Minimization for Surface and Image Smoothing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Cheng-Hong Chang, Pei-Hsuan Tsai</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18969" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18969</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05062b9ac64115654a255df578e1f3f61c6740d8e9db0b67ceca3387185661df_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05062b9ac64115654a255df578e1f3f61c6740d8e9db0b67ceca3387185661df_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zaidao Han, Risa Higashita, Jiang Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18954" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18954</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a277490a1036cca55f864049064f469e9f72d6c27becedf02e6e0afb0212e89_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a277490a1036cca55f864049064f469e9f72d6c27becedf02e6e0afb0212e89_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Towards AI-Guided Open-World Ecological Taxonomic Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Cheng Yaw Low, Heejoon Koo, Jaewoo Park, Kaleb Mesfin Asfaw, Meeyoung Cha</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18994" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18994</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/780199ccb79605e70d0938bc92678f9a79a328a23356a7c9a76644dcf9ab4dfe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/780199ccb79605e70d0938bc92678f9a79a328a23356a7c9a76644dcf9ab4dfe_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards AI-Guided Open-World Ecological Taxonomic Classification</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ryosuke Korekata, Quanting Xie, Yonatan Bisk, Komei Sugiura</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18987" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18987</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/456f5beecc1c2c45f598578f8491d702dc76e9f500bae44e01454f783dc10b05_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/456f5beecc1c2c45f598578f8491d702dc76e9f500bae44e01454f783dc10b05_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Gyeongrok Oh, Youngdong Jang, Jonghyun Choi, Suk-Ju Kang, Guang Lin, Sangpil Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18991" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18991</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca47fc4083c13826225296ae5380ec0a994e6b8b0a936b8582bb3acb510605f6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca47fc4083c13826225296ae5380ec0a994e6b8b0a936b8582bb3acb510605f6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Steering Vision-Language Pre-trained Models for Incremental Face Presentation Attack Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Haoze Li, Jie Zhang, Guoying Zhao, Stephen Lin, Shiguang Shan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19022" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19022</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/462a93cd76a387b34292a443a8359c9699b91dba8c90d7da0799846e9545ee6a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/462a93cd76a387b34292a443a8359c9699b91dba8c90d7da0799846e9545ee6a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Steering Vision-Language Pre-trained Models for Incremental Face Presentation Attack Detection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sihao Lin, Zerui Li, Xunyi Zhao, Gengze Zhou, Liuyi Wang, Rong Wei, Rui Tang, Juncheng Li, Hanqing Wang, Jiangmiao Pang, Anton van den Hengel, Jiajun Liu, Qi Wu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19021" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19021</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73e25aea53ce484be801e13db8703f7ac91dcd519e03aa8f1f11869324466841_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73e25aea53ce484be801e13db8703f7ac91dcd519e03aa8f1f11869324466841_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Automatic Neuronal Activity Segmentation in Fast Four Dimensional Spatio-Temporal Fluorescence Imaging using Bayesian Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ran Li, Pan Xiao, Kaushik Dutta, Youdong Guo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19032" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19032</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/caef29a5648e5f1039a8eef029aa3d268223d6542939af5d70efe56c4350e067_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/caef29a5648e5f1039a8eef029aa3d268223d6542939af5d70efe56c4350e067_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Automatic Neuronal Activity Segmentation in Fast Four Dimensional Spatio-Temporal Fluorescence Imaging using Bayesian Approach</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zelin Zhao, Xinyu Gong, Bangya Liu, Ziyang Song, Jun Zhang, Suhui Wu, Yongxin Chen, Hao Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19020" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19020</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7bd5a2abb18589060985877478ba075c83351c0fe7a3b61f7db28dccf9b3d5b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7bd5a2abb18589060985877478ba075c83351c0fe7a3b61f7db28dccf9b3d5b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Connor Kilrain, David Carlyn, Julia Chae, Sara Beery, Wei-Lun Chao, Jianyang Gu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19026" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19026</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9212089c44919f3c00bd82b13c0b39b407491c5d9de5c2c72ed790b2f0a0a2f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9212089c44919f3c00bd82b13c0b39b407491c5d9de5c2c72ed790b2f0a0a2f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Distinguishing Visually Similar Actions: Prompt-Guided Semantic Prototype Modulation for Few-Shot Action Recognition</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiaoyang Li, Mingming Lu, Ruiqi Wang, Hao Li, Zewei Le</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19036" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19036</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3cd50ba346b66d56f1f55a399d0c82cbfbd81d4c166ba855757c74c8346c7c2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3cd50ba346b66d56f1f55a399d0c82cbfbd81d4c166ba855757c74c8346c7c2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Distinguishing Visually Similar Actions: Prompt-Guided Semantic Prototype Modulation for Few-Shot Action Recognition</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] WaTeRFlow: Watermark Temporal Robustness via Flow Consistency</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Utae Jeong, Sumin In, Hyunju Ryu, Jaewan Choi, Feng Yang, Jongheon Jeong, Seungryong Kim, Sangpil Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19048" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19048</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f8776ce3ae3dfd5e014702d94d69af26362743bfccc9cf4f4e1fc818a95a539_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f8776ce3ae3dfd5e014702d94d69af26362743bfccc9cf4f4e1fc818a95a539_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> WaTeRFlow: Watermark Temporal Robustness via Flow Consistency</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] 6DAttack: Backdoor Attacks in the 6DoF Pose Estimation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jihui Guo, Zongmin Zhang, Zhen Sun, Yuhao Yang, Jinlin Wu, Fu Zhang, Xinlei He</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19058" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19058</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d7cd1a6d20a70ace7bef5d2b8b091492883b5588753a885d31218a86b7cb53e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d7cd1a6d20a70ace7bef5d2b8b091492883b5588753a885d31218a86b7cb53e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> 6DAttack: Backdoor Attacks in the 6DoF Pose Estimation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ruiqi Ma, Yu Yan, Chunhong Zhang, Minghao Yin, XinChao Liu, Zhihong Jin, Zheng Hu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19070" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19070</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d56f2f61fcc600eaa02837139337fdefe7e31f2b7f624524c972734f069b6c0f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d56f2f61fcc600eaa02837139337fdefe7e31f2b7f624524c972734f069b6c0f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Khanh Nguyen, Dasith de Silva Edirimuni, Ghulam Mubashar Hassan, Ajmal Mian</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19088" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19088</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d400fa28859b94a82c7a1ca0fe5b0ee2133535554ef479d696c7d0032c8503d3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d400fa28859b94a82c7a1ca0fe5b0ee2133535554ef479d696c7d0032c8503d3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Decoupled Generative Modeling for Human-Object Interaction Synthesis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hwanhee Jung, Seunggwan Lee, Jeongyoon Yoon, SeungHyeon Kim, Giljoo Nam, Qixing Huang, Sangpil Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19049" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19049</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efa5fbcac724949235f9de6d44ddfad055396c38cff93f03bbf6611e7504dfe4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efa5fbcac724949235f9de6d44ddfad055396c38cff93f03bbf6611e7504dfe4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Decoupled Generative Modeling for Human-Object Interaction Synthesis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ariel Lubonja, Pedro R. A. S. Bassi, Wenxuan Li, Hualin Qiao, Randal Burns, Alan L. Yuille, Zongwei Zhou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19091" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19091</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8b75e4c6b7e493a0c6d3b57468d0ae3edcb9efaeaaf9076c00744a6a04c7c6a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8b75e4c6b7e493a0c6d3b57468d0ae3edcb9efaeaaf9076c00744a6a04c7c6a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Mamba-Based Modality Disentanglement Network for Multi-Contrast MRI Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Weiyi Lyu, Xinming Fang, Jun Wang, Jun Shi, Guixu Zhang, Juncheng Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19095" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19095</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/660b7ce425295dabbd643561507450b5b391330b19646c8c466147c1770bdf63_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/660b7ce425295dabbd643561507450b5b391330b19646c8c466147c1770bdf63_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Mamba-Based Modality Disentanglement Network for Multi-Contrast MRI Reconstruction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Trifocal Tensor and Relative Pose Estimation with Known Vertical Direction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tao Li, Zhenbao Yu, Banglei Guan, Jianli Han, Weimin Lv, Friedrich Fraundorfer</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19110" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19110</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59e59cb599a47e950626af160c385ffc182ff74568bbebdcb198397b16efd9d1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59e59cb599a47e950626af160c385ffc182ff74568bbebdcb198397b16efd9d1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Trifocal Tensor and Relative Pose Estimation with Known Vertical Direction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hengyi Feng, Zeang Sheng, Meiyi Qiang, Wentao Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19115" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19115</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1ccbf3745011850a9bf45220f261e89ca40bd9c98025e6d92083fb946fbf5c6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1ccbf3745011850a9bf45220f261e89ca40bd9c98025e6d92083fb946fbf5c6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] GaussianImage++: Boosted Image Representation and Compression with 2D Gaussian Splatting</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tiantian Li, Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Jun Zhang, Yan Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19108" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19108</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a01ced772b8d2640ac7da9b4aa353acacbc0a7a10ccf022639e513769de09272_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a01ced772b8d2640ac7da9b4aa353acacbc0a7a10ccf022639e513769de09272_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GaussianImage++: Boosted Image Representation and Compression with 2D Gaussian Splatting</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pengxuan Yang, Ben Lu, Zhongpu Xia, Chao Han, Yinfeng Gao, Teng Zhang, Kun Zhan, XianPeng Lang, Yupeng Zheng, Qichao Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19133" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19133</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95928df29dd1d6db8d9cc4946cd472f8722d19286d4bfa47dd919093172f5948_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95928df29dd1d6db8d9cc4946cd472f8722d19286d4bfa47dd919093172f5948_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ruikai Li, Xinrun Li, Mengwei Xie, Hao Shan, Shoumeng Qiu, Xinyuan Chang, Yizhe Fan, Feng Xiong, Han Jiang, Yilong Ren, Haiyang Yu, Mu Xu, Yang Long, Varun Ojha, Zhiyong Cui</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19150" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19150</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aaaf65665b46ca412dac28df472cffd8faf9ff9cf5ae8945c3d13bbd791e3c88_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aaaf65665b46ca412dac28df472cffd8faf9ff9cf5ae8945c3d13bbd791e3c88_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wendong Bu, Kaihang Pan, Yuze Lin, Jiacheng Li, Kai Shen, Wenqiao Zhang, Juncheng Li, Jun Xiao, Siliang Tang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19159" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19159</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b0a157ededdb6863038e7005281e71ab57fdcaf96eba9b30702558285b1ed2e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b0a157ededdb6863038e7005281e71ab57fdcaf96eba9b30702558285b1ed2e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CycleChart: A Unified Consistency-Based Learning Framework for Bidirectional Chart Understanding and Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dazhen Deng, Sen Yang, Yuchen He, Yuan Tian, Yingcai Wu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19173" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19173</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4924c981ea9ee58543e98b565a1e8fca0e8ce65bad7464a4041f4c5ffc756918_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4924c981ea9ee58543e98b565a1e8fca0e8ce65bad7464a4041f4c5ffc756918_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CycleChart: A Unified Consistency-Based Learning Framework for Bidirectional Chart Understanding and Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Marios Thoma, Zenonas Theodosiou, Harris Partaourides, Vassilis Vassiliades, Loizos Michael, Andreas Lanitis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19190" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19190</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a96c05d7294228300f6794f5bbda416f7d11e2b7c58157c93485f6f6ba933ade_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a96c05d7294228300f6794f5bbda416f7d11e2b7c58157c93485f6f6ba933ade_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] InvCoSS: Inversion-driven Continual Self-supervised Learning in Medical Multi-modal Image Pre-training</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zihao Luo, Shaohao Rui, Zhenyu Tang, Guotai Wang, Xiaosong Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19213" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19213</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c90f4c95929734535ee76b711f8c5c23d3fb093f6e99a2ba0fea1407c036c313_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c90f4c95929734535ee76b711f8c5c23d3fb093f6e99a2ba0fea1407c036c313_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> InvCoSS: Inversion-driven Continual Self-supervised Learning in Medical Multi-modal Image Pre-training</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] HippMetric: A skeletal-representation-based framework for cross-sectional and longitudinal hippocampal substructural morphometry</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Na Gao, Chenfei Ye, Yanwu Yang, Anqi Li, Zhengbo He, Li Liang, Zhiyuan Liu, Xingyu Hao, Ting Ma, Tengfei Guo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19214" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19214</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e03fac6363ee3c142ec7100157cae8cca39cfd14894e066b219b9ace6425131_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e03fac6363ee3c142ec7100157cae8cca39cfd14894e066b219b9ace6425131_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HippMetric: A skeletal-representation-based framework for cross-sectional and longitudinal hippocampal substructural morphometry</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Towards Minimal Fine-Tuning of VLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tiange Luo, Lajanugen Logeswaran, Jaekyeom Kim, Justin Johnson, Honglak Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19219" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19219</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/420c22fc5b580697b05c90c8fbf0115c2cea8a82f971a19f125c0456c3405309_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/420c22fc5b580697b05c90c8fbf0115c2cea8a82f971a19f125c0456c3405309_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards Minimal Fine-Tuning of VLMs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] From Pixels to Predicates Structuring urban perception with scene graphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yunlong Liu, Shuyang Li, Pengyuan Liu, Yu Zhang, Rudi Stouffs</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19221" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19221</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/937a8aa60cbd8f9c8016e6a36a9941d4f6c5a8af94206f4d30a8f70eac213a2a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/937a8aa60cbd8f9c8016e6a36a9941d4f6c5a8af94206f4d30a8f70eac213a2a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> From Pixels to Predicates Structuring urban perception with scene graphs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Meng Chu, Senqiao Yang, Haoxuan Che, Suiyun Zhang, Xichen Zhang, Shaozuo Yu, Haokun Gui, Zhefan Rao, Dandan Tu, Rui Liu, Jiaya Jia</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19243" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19243</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/189a642ba98e396da7852dfdc680536e07570eee6c947c570c81130b8f827924_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/189a642ba98e396da7852dfdc680536e07570eee6c947c570c81130b8f827924_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Carla Crivoi, Radu Tudor Ionescu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19253" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19253</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d70a17b56ecda8937a9bd488550e13c9d9833f836ea7a0e16e024c8b5e950c5b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d70a17b56ecda8937a9bd488550e13c9d9833f836ea7a0e16e024c8b5e950c5b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] 3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xinyang Song, Libin Wang, Weining Wang, Zhiwei Li, Jianxin Sun, Dandan Zheng, Jingdong Chen, Qi Li, Zhenan Sun</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19271" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19271</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be02858b75b1adbf6d43c4ad543ed82d07789eb9f6bc5adee21b6c3bc2806e5d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be02858b75b1adbf6d43c4ad543ed82d07789eb9f6bc5adee21b6c3bc2806e5d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> 3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kyungwon Cho, Hanbyul Joo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19283" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19283</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eeffc3f64bbeee78f48fdd58e03a511d7a87a0d92465f1e6a49a7e082ffac166_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eeffc3f64bbeee78f48fdd58e03a511d7a87a0d92465f1e6a49a7e082ffac166_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ivan DeAndres-Tame, Chengwei Ye, Ruben Tolosana, Ruben Vera-Rodriguez, Shiqi Yu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19275" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19275</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d12748fd351c66664603a2df32d39e3ed4e526013cc4fc7f328dc001dea6e6a2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d12748fd351c66664603a2df32d39e3ed4e526013cc4fc7f328dc001dea6e6a2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] RMLer: Synthesizing Novel Objects across Diverse Categories via Reinforcement Mixing Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jun Li, Zikun Chen, Haibo Chen, Shuo Chen, Jian Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19300" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19300</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4417f7bedbf213cebde42bcbbe520cb32e68e60eec8444be79b07344e3b47020_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4417f7bedbf213cebde42bcbbe520cb32e68e60eec8444be79b07344e3b47020_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RMLer: Synthesizing Novel Objects across Diverse Categories via Reinforcement Mixing Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xu Zhang, Junyao Ge, Yang Zheng, Kaitai Guo, Jimin Liang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19302" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19302</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05bd6cba707b6fa3a38c669135a7bae69d4a297f430743568b2ec5258f28c554_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05bd6cba707b6fa3a38c669135a7bae69d4a297f430743568b2ec5258f28c554_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MAGIC: Achieving Superior Model Merging via Magnitude Calibration</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yayuan Li, Jian Zhang, Jintao Guo, Zihan Cheng, Lei Qi, Yinghuan Shi, Yang Gao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19320" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19320</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7204250ada52cfa8e70ee24634b9a58aab0085ac9d5854e5c672a585fb92a0a6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7204250ada52cfa8e70ee24634b9a58aab0085ac9d5854e5c672a585fb92a0a6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MAGIC: Achieving Superior Model Merging via Magnitude Calibration</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Marica Muffoletto, Uxio Hermida, Charlène Mauger, Avan Suinesiaputra, Yiyang Xu, Richard Burns, Lisa Pankewitz, Andrew D McCulloch, Steffen E Petersen, Daniel Rueckert, Alistair A Young</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19316" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19316</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2169c1336bc2f9517102921ae45f8684aaa37f9e051cbaf98dee4fa4fd0d1da4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2169c1336bc2f9517102921ae45f8684aaa37f9e051cbaf98dee4fa4fd0d1da4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hui Li, Jiayue Lyu, Fu-Yun Wang, Kaihui Cheng, Siyu Zhu, Jingdong Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19311" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19311</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/333652b938a5ecac737d2dfcea2bae93a2f072e15ac1c354ce9b2da1931714cc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/333652b938a5ecac737d2dfcea2bae93a2f072e15ac1c354ce9b2da1931714cc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Extended OpenTT Games Dataset: A table tennis dataset for fine-grained shot type and point outcome</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Moamal Fadhil Abdul, Jonas Bruun Hubrechts, Thomas Martini Jørgensen, Emil Hovad</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19327" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19327</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7996df4a8c51ad78b9cf6e38d68e28803f1e0e8374ec48e5258b82ca6726f085_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7996df4a8c51ad78b9cf6e38d68e28803f1e0e8374ec48e5258b82ca6726f085_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Extended OpenTT Games Dataset: A table tennis dataset for fine-grained shot type and point outcome</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] DeltaMIL: Gated Memory Integration for Efficient and Discriminative Whole Slide Image Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yueting Zhu, Yuehao Song, Shuai Zhang, Wenyu Liu, Xinggang Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19331" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19331</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09dc308a5637ec12a3c4efc8fea104a4feed83c0622643ceb02afbe80f82e58c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09dc308a5637ec12a3c4efc8fea104a4feed83c0622643ceb02afbe80f82e58c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DeltaMIL: Gated Memory Integration for Efficient and Discriminative Whole Slide Image Analysis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] GANeXt: A Fully ConvNeXt-Enhanced Generative Adversarial Network for MRI- and CBCT-to-CT Synthesis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Siyuan Mei, Yan Xia, Fuxin Fan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19336" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19336</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da8f1615b6e017ea5a7845dd78d938dfd6175022b2518294c8d72a1f24b2befa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da8f1615b6e017ea5a7845dd78d938dfd6175022b2518294c8d72a1f24b2befa_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GANeXt: A Fully ConvNeXt-Enhanced Generative Adversarial Network for MRI- and CBCT-to-CT Synthesis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhenyang Huang, Xiao Yu, Yi Zhang, Decheng Wang, Hang Ruan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19354" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19354</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe8395ab00c36c5c60a6a3cce591a67d23872c3d730ee745491ee87f86bf9993_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe8395ab00c36c5c60a6a3cce591a67d23872c3d730ee745491ee87f86bf9993_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Efficient Spike-driven Transformer for High-performance Drone-View Geo-Localization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhongwei Chen, Hai-Jun Rong, Zhao-Xu Yang, Guoqi Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19365" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19365</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e5753a79bc7c6daf06150580604d523fe5ee37173b2bd7e625292862bfda957a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e5753a79bc7c6daf06150580604d523fe5ee37173b2bd7e625292862bfda957a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Efficient Spike-driven Transformer for High-performance Drone-View Geo-Localization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hongwei Fan, Hang Dai, Jiyao Zhang, Jinzhou Li, Qiyang Yan, Yujie Zhao, Mingju Gao, Jinghang Wu, Hao Tang, Hao Dong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19390" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19390</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf1e9d4003951dba668a306d606416b006b75689695b9667afa58a3ba76ea89_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf1e9d4003951dba668a306d606416b006b75689695b9667afa58a3ba76ea89_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yueyao Chen, Kai-Ni Wang, Dario Tayupo, Arnaud Huaulm&#x27;e, Krystel Nyangoh Timoh, Pierre Jannin, Qi Dou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19387" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19387</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d09473d7af5f8db0e2099dcd5a18592d023dddc412cada060f4f05596921ff9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d09473d7af5f8db0e2099dcd5a18592d023dddc412cada060f4f05596921ff9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yujie Zhao, Hongwei Fan, Di Chen, Shengcong Chen, Liliang Chen, Xiaoqi Li, Guanghui Ren, Hao Dong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19402" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19402</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3585a78a8a01def680be454d4e0abbbbcf24961bd331f0f18d30d4fa2409128_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3585a78a8a01def680be454d4e0abbbbcf24961bd331f0f18d30d4fa2409128_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Non-Contrast CT Esophageal Varices Grading through Clinical Prior-Enhanced Multi-Organ Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiaoming Zhang, Chunli Li, Jiacheng Hao, Yuan Gao, Danyang Tu, Jianyi Qiao, Xiaoli Yin, Le Lu, Ling Zhang, Ke Yan, Yang Hou, Yu Shi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19415" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19415</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d823776fb42e2083e867607e00670f7745cc6a59e773b0a990924b705cbb63ce_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d823776fb42e2083e867607e00670f7745cc6a59e773b0a990924b705cbb63ce_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Non-Contrast CT Esophageal Varices Grading through Clinical Prior-Enhanced Multi-Organ Analysis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Fei Ge, Ying Huang, Jie Liu, Guixuan Zhang, Zhi Zeng, Shuwu Zhang, Hu Guan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19438" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19438</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c5f1cc5f1508ff7f3dac04d0e138fe9c30202bb132ef0c4725622de8caf6c5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c5f1cc5f1508ff7f3dac04d0e138fe9c30202bb132ef0c4725622de8caf6c5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yi Xin, Siqi Luo, Qi Qin, Haoxing Chen, Kaiwen Zhu, Zhiwei Zhang, Yangfan He, Rongchao Zhang, Jinbin Bai, Shuo Cao, Bin Fu, Junjun He, Yihao Liu, Yuewen Cao, Xiaohong Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19433" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19433</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e45156e486a1bc9ed274f1c77ca361208ca741b5f093796d4e95c9d983ba1b84_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e45156e486a1bc9ed274f1c77ca361208ca741b5f093796d4e95c9d983ba1b84_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Evelyn Zhang, Fufu Yu, Aoqi Wu, Zichen Wen, Ke Yan, Shouhong Ding, Biqing Qi, Linfeng Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19443" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19443</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b276a091b11e8296a53fd51f1160b3e57aa375c543150f9b00e1f03505b59fa1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b276a091b11e8296a53fd51f1160b3e57aa375c543150f9b00e1f03505b59fa1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Sign Language Recognition using Parallel Bidirectional Reservoir Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nitin Kumar Singh, Arie Rachmad Syulistyo, Yuichiro Tanaka, Hakaru Tamukoh</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19451" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19451</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/554347db8f25765f90ed0de383600422b9f8c928bbd223207e7df0e299098ce4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/554347db8f25765f90ed0de383600422b9f8c928bbd223207e7df0e299098ce4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Sign Language Recognition using Parallel Bidirectional Reservoir Computing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Emotion-Director: Bridging Affective Shortcut in Emotion-Oriented Image Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Guoli Jia, Junyao Hu, Xinwei Long, Kai Tian, Kaiyan Zhang, KaiKai Zhao, Ning Ding, Bowen Zhou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19479" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19479</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc7b54a9b2b78f7914a0108337ac26045e762e0d69fe4d901c3f358805a1715b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc7b54a9b2b78f7914a0108337ac26045e762e0d69fe4d901c3f358805a1715b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Emotion-Director: Bridging Affective Shortcut in Emotion-Oriented Image Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Dynamic Stream Network for Combinatorial Explosion Problem in Deformable Medical Image Registration</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shaochen Bi, Yuting He, Weiming Wang, Hao Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19486" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19486</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb129fee202e72128dcf5cd341766c1e138b9f80593cb33ba80612c8b99c355a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb129fee202e72128dcf5cd341766c1e138b9f80593cb33ba80612c8b99c355a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dynamic Stream Network for Combinatorial Explosion Problem in Deformable Medical Image Registration</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ziyang Song, Zelin Zang, Zuyao Chen, Xusheng Liang, Dong Yi, Jinlin Wu, Hongbin Liu, Jiebo Luo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19512" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19512</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d14f0ebe1771704c1788aacd2e3db88e7c3b990ef8113a061936422f9bb95889_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d14f0ebe1771704c1788aacd2e3db88e7c3b990ef8113a061936422f9bb95889_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Georgios Voulgaris</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19504" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19504</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b44fc34cd61f2154ab40ee342bff48a158e28f41d4da554eeb8a5175ecd28b9d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b44fc34cd61f2154ab40ee342bff48a158e28f41d4da554eeb8a5175ecd28b9d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Convolutional Neural Deferred Shader for Physics Based Rendering</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhuo He, Yingdong Ru, Qianying Liu, Paul Henderson, Nicolas Pugeault</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19522" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19522</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6cedf1c27957e3514f227025357a6a77d6e8cc183d8b8443ff380a0198a2b27e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6cedf1c27957e3514f227025357a6a77d6e8cc183d8b8443ff380a0198a2b27e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Convolutional Neural Deferred Shader for Physics Based Rendering</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SlicerOrbitSurgerySim: An Open-Source Platform for Virtual Registration and Quantitative Comparison of Preformed Orbital Plates</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chi Zhang, Braedon Gunn, Andrew M. Read-Fuller</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19534" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19534</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2dda742639cf4aa80038c7f5a4810b7a39a34acaa57f71b1646ebafce80e27e8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2dda742639cf4aa80038c7f5a4810b7a39a34acaa57f71b1646ebafce80e27e8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SlicerOrbitSurgerySim: An Open-Source Platform for Virtual Registration and Quantitative Comparison of Preformed Orbital Plates</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Multi-Modal Soccer Scene Analysis with Masked Pre-Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Marc Peral, Guillem Capellera, Luis Ferraz, Antonio Rubio, Antonio Agudo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19528" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19528</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca38cc1fee364a3f5a5912d1d267f89ff10d2261c0295e00eeca8f554a12e3d5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca38cc1fee364a3f5a5912d1d267f89ff10d2261c0295e00eeca8f554a12e3d5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multi-Modal Soccer Scene Analysis with Masked Pre-Training</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Moritz Böhle, Amélie Royer, Juliette Marrie, Edouard Grave, Patrick Pérez</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19535" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19535</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b20ae57f683974a6959785befd45010cb87dde73d9ccc345f16e11af116951e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b20ae57f683974a6959785befd45010cb87dde73d9ccc345f16e11af116951e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] StoryMem: Multi-shot Long Video Storytelling with Memory</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kaiwen Zhang, Liming Jiang, Angtian Wang, Jacob Zhiyuan Fang, Tiancheng Zhi, Qing Yan, Hao Kang, Xin Lu, Xingang Pan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19539" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19539</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a7a12b30bf051906255b90290fece33407e3691deaf21452976c48e1d52f546_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a7a12b30bf051906255b90290fece33407e3691deaf21452976c48e1d52f546_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> StoryMem: Multi-shot Long Video Storytelling with Memory</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ziqiao Peng, Yi Chen, Yifeng Ma, Guozhen Zhang, Zhiyao Sun, Zixiang Zhou, Youliang Zhang, Zhengguang Zhou, Zhaoxin Fan, Hongyan Liu, Yuan Zhou, Qinglin Lu, Jun He</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19546" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19546</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a48fa101dd9fce1c329f7df974385f6ec3d96f2ce0111f84aa53a212233bd01_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a48fa101dd9fce1c329f7df974385f6ec3d96f2ce0111f84aa53a212233bd01_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] BabyFlow: 3D modeling of realistic and expressive infant faces</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Antonia Alomar, Mireia Masias, Marius George Linguraru, Federico M. Sukno, Gemma Piella</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19560" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19560</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4bdcd1bc9dd6e1141b9b283f24968fbb4a40dc22257a719c5e16fbac178220f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4bdcd1bc9dd6e1141b9b283f24968fbb4a40dc22257a719c5e16fbac178220f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> BabyFlow: 3D modeling of realistic and expressive infant faces</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] No Data? No Problem: Robust Vision-Tabular Learning with Missing Values</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Marta Hasny, Laura Daza, Keno Bressem, Maxime Di Folco, Julia Schnabel</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19602" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19602</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/750e85c1bc605ca0702dd33fe3c21b8f7a30131f1bd4c08b119b6cd868aa6e98_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/750e85c1bc605ca0702dd33fe3c21b8f7a30131f1bd4c08b119b6cd868aa6e98_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> No Data? No Problem: Robust Vision-Tabular Learning with Missing Values</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Eric Zimmermann, Harley Wiltzer, Justin Szeto, David Alvarez-Melis, Lester Mackey</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19605" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19605</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8876f353bd3b6bc215381979aff16556177f0d7ca7e5b9cf3aa366d3fbd3c21d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8876f353bd3b6bc215381979aff16556177f0d7ca7e5b9cf3aa366d3fbd3c21d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jiaqi Peng, Wenzhe Cai, Yuqiang Yang, Tai Wang, Yuan Shen, Jiangmiao Pang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19629" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19629</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c789e71ef34ed4c917c96dfb4b01a72b2ebbbaf1b31aedd33c19890927a051c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c789e71ef34ed4c917c96dfb4b01a72b2ebbbaf1b31aedd33c19890927a051c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MapTrace: Scalable Data Generation for Route Tracing on Maps</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Artemis Panagopoulou, Aveek Purohit, Achin Kulshrestha, Soroosh Yazdani, Mohit Goyal</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19609" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19609</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7cc350a173ade821ef309bec6e6155158ae8332042895a8d6f45396c8b70c06_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7cc350a173ade821ef309bec6e6155158ae8332042895a8d6f45396c8b70c06_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MapTrace: Scalable Data Generation for Route Tracing on Maps</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Generative diffusion models for agricultural AI: plant image generation, indoor-to-outdoor translation, and expert preference alignment</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Da Tan, Michael Beck, Christopher P. Bidinosti, Robert H. Gulden, Christopher J. Henry</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19632" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19632</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2f66f3db49949bf517b4441efbd4d2391e38803e0ee9434c98db47da1306caf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2f66f3db49949bf517b4441efbd4d2391e38803e0ee9434c98db47da1306caf_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generative diffusion models for agricultural AI: plant image generation, indoor-to-outdoor translation, and expert preference alignment</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Argha Kamal Samanta, Harshika Goyal, Vasudha Joshi, Tushar Mungle, Pabitra Mitra</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19663" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19663</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cd268a98c5baac8b164ba733c255159614de6b969e8c6dd3f943fa74d98e5a1b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cd268a98c5baac8b164ba733c255159614de6b969e8c6dd3f943fa74d98e5a1b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] 4D Gaussian Splatting as a Learned Dynamical System</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Arnold Caleb Asiimwe, Carl Vondrick</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19648" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19648</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a72a5f6b1ca2d0116e20e704886e46a58e929e4b87b6d69b18361c69528e49d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a72a5f6b1ca2d0116e20e704886e46a58e929e4b87b6d69b18361c69528e49d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> 4D Gaussian Splatting as a Learned Dynamical System</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hanyang Kong, Xingyi Yang, Xiaoxu Zheng, Xinchao Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19678" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19678</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4f113f5d19fa4ca45dd649c8c074bdcd96d439b1640d1340c9f10070d3b5a62_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4f113f5d19fa4ca45dd649c8c074bdcd96d439b1640d1340c9f10070d3b5a62_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Over++: Generative Video Compositing for Layer Interaction Effects</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Luchao Qi, Jiaye Wu, Jun Myeong Choi, Cary Phillips, Roni Sengupta, Dan B Goldman</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19661" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19661</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e4b3add32e80abbfca0b3aa657bd70035f1f8f7db466fff302f7edb85698647_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e4b3add32e80abbfca0b3aa657bd70035f1f8f7db466fff302f7edb85698647_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Over++: Generative Video Compositing for Layer Interaction Effects</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mojtaba Safari, Shansong Wang, Vanessa L Wildman, Mingzhe Hu, Zach Eidex, Chih-Wei Chang, Erik H Middlebrooks, Richard L.J Qiu, Pretesh Patel, Ashesh B. Jania, Hui Mao, Zhen Tian, Xiaofeng Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19676" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19676</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/342e0f596d7de60763df762d3410c257d5603f80f4a044d27f87795d68b9c1dd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/342e0f596d7de60763df762d3410c257d5603f80f4a044d27f87795d68b9c1dd_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zixuan Ye, Quande Liu, Cong Wei, Yuanxing Zhang, Xintao Wang, Pengfei Wan, Kun Gai, Wenhan Luo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19686" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19686</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/843c501d9e520248d2e1a28597703cd3b54542449f629d50d194d54e77fa641e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/843c501d9e520248d2e1a28597703cd3b54542449f629d50d194d54e77fa641e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] VA-<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">π</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">π</span></span></span></span>: Variational Policy Alignment for Pixel-Aware Autoregressive Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xinyao Liao, Qiyuan He, Kai Xu, Xiaoye Qu, Yicong Li, Wei Wei, Angela Yao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19680" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19680</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce1823fb4399aafb9a993608f6bc9e7070ee1def4476bd9868d19ab0d1633442_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce1823fb4399aafb9a993608f6bc9e7070ee1def4476bd9868d19ab0d1633442_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VA-<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">π</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">π</span></span></span></span>: Variational Policy Alignment for Pixel-Aware Autoregressive Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Weichen Fan, Haiwen Diao, Quan Wang, Dahua Lin, Ziwei Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19693" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19693</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/546a4d6b5b3d2bcc5adcfd25b27a121f46760fd0f9e85b7b4c89467641629c03_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/546a4d6b5b3d2bcc5adcfd25b27a121f46760fd0f9e85b7b4c89467641629c03_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Zero-shot Reconstruction of In-Scene Object Manipulation from Video</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dixuan Lin, Tianyou Wang, Zhuoyang Pan, Yufu Wang, Lingjie Liu, Kostas Daniilidis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19684" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19684</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/967b97ff200c76a7e13dbcc9b4b2be1132807200b91f7312ad2eb4984f48bb88_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/967b97ff200c76a7e13dbcc9b4b2be1132807200b91f7312ad2eb4984f48bb88_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Zero-shot Reconstruction of In-Scene Object Manipulation from Video</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A curated UK rain radar data set for training and benchmarking nowcasting models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Viv Atureta, Rifki Priansyah Jasin, Stefan Siegert</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17924" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17924</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71fd337b26c3bfd8670295386b9f2b4df184043caa1e2c1b8a39442f6cdb1c15_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71fd337b26c3bfd8670295386b9f2b4df184043caa1e2c1b8a39442f6cdb1c15_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A curated UK rain radar data set for training and benchmarking nowcasting models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Apoorv Vyas, Heng-Jui Chang, Cheng-Fu Yang, Po-Yao Huang, Luya Gao, Julius Richter, Sanyuan Chen, Matt Le, Piotr Dollár, Christoph Feichtenhofer, Ann Lee, Wei-Ning Hsu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19687" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19687</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de3d411e351c730f5a4f6bcbb2b3a9ec59b568b77417127cf865aadf04270b18_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de3d411e351c730f5a4f6bcbb2b3a9ec59b568b77417127cf865aadf04270b18_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Disentangled representations via score-based variational autoencoders</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Benjamin S. H. Lyo, Eero P. Simoncelli, Cristina Savin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17127" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17127</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c4fff8035bd7179107df39681970450e10d3f516b687330e2caedbac602c68b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c4fff8035bd7179107df39681970450e10d3f516b687330e2caedbac602c68b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Disentangled representations via score-based variational autoencoders</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mingrui Wu, Zhaozhi Wang, Fangjinhua Wang, Jiaolong Yang, Marc Pollefeys, Tong Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19683" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19683</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7452b3f045d2d058f3a16f8690c6b4f168b374381f31024557431e4ca6b30cf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7452b3f045d2d058f3a16f8690c6b4f168b374381f31024557431e4ca6b30cf_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pablo Ruiz-Ponce, Sergio Escalera, José García-Rodríguez, Jiankang Deng, Rolandos Alexandros Potamias</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19692" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19692</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c358cce0c6057dff9e9db7532908f886d71d12899237ce5cdd1ce4b783108cd6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c358cce0c6057dff9e9db7532908f886d71d12899237ce5cdd1ce4b783108cd6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CytoDINO: Risk-Aware and Biologically-Informed Adaptation of DINOv3 for Bone Marrow Cytomorphology</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Aziz Muminov, Anne Pham</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17930" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17930</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7391864933852f8b3d970d31b51ddd7af845e9cf2ef6c035d939e6f6cca13967_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7391864933852f8b3d970d31b51ddd7af845e9cf2ef6c035d939e6f6cca13967_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CytoDINO: Risk-Aware and Biologically-Informed Adaptation of DINOv3 for Bone Marrow Cytomorphology</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Standardized Evaluation of Automatic Methods for Perivascular Spaces Segmentation in MRI -- MICCAI 2024 Challenge Results</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yilei Wu, Yichi Zhang, Zijian Dong, Fang Ji, An Sen Tan, Gifford Tan, Sizhao Tang, Huijuan Chen, Zijiao Chen, Eric Kwun Kei Ng, Jose Bernal, Hang Min, Ying Xia, Ines Vati, Liz Cooper, Xiaoyu Hu, Yuchen Pei, Yutao Ma, Victor Nozais, Ami Tsuchida, Pierre-Yves Hervé, Philippe Boutinaud, Marc Joliot, Junghwa Kang, Wooseung Kim, Dayeon Bak, Rachika E. Hamadache, Valeriia Abramova, Xavier Lladó, Yuntao Zhu, Zhenyu Gong, Xin Chen, John McFadden, Pek Lan Khong, Roberto Duarte Coello, Hongwei Bran Li, Woon Puay Koh, Christopher Chen, Joanna M. Wardlaw, Maria del C. Valdés Hernández, Juan Helen Zhou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18197" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18197</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/010f615acca3968f6c9a01a9f1e16da3691c03b0a86b7fb021c3877735493e22_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/010f615acca3968f6c9a01a9f1e16da3691c03b0a86b7fb021c3877735493e22_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Standardized Evaluation of Automatic Methods for Perivascular Spaces Segmentation in MRI -- MICCAI 2024 Challenge Results</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SLIM: Semantic-based Low-bitrate Image compression for Machines by leveraging diffusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hyeonjin Lee, Jun-Hyuk Kim, Jong-Seok Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18200" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18200</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9c567930d6d4ee43e33b9cf92ff8599d4af08021295eca7c47d016de7557640_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9c567930d6d4ee43e33b9cf92ff8599d4af08021295eca7c47d016de7557640_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SLIM: Semantic-based Low-bitrate Image compression for Machines by leveraging diffusion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Selective Phase-Aware Training of nnU-Net for Robust Breast Cancer Segmentation in Multi-Center DCE-MRI</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Beyza Zayim, Aissiou Ikram, Boukhiar Naima</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19225" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19225</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ddf6be7e15b5d6602d6e9aa40624abefc0e83cbee9e90302c334bd468ba1ea9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ddf6be7e15b5d6602d6e9aa40624abefc0e83cbee9e90302c334bd468ba1ea9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Selective Phase-Aware Training of nnU-Net for Robust Breast Cancer Segmentation in Multi-Center DCE-MRI</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Deep Learning for Primordial <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span>-mode Extraction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Eric Guzman, Joel Meyers</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19577" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19577</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9bbecd5a2eae70e7ea81960a62c36c37f9e7805f682cebee98cb7ab81bd67249_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9bbecd5a2eae70e7ea81960a62c36c37f9e7805f682cebee98cb7ab81bd67249_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Deep Learning for Primordial <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span>-mode Extraction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Rethinking Coupled Tensor Analysis for Hyperspectral Super-Resolution: Recoverable Modeling Under Endmember Variability</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Meng Ding, Xiao Fu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19489" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19489</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c4dd05c6080abf0291c4f46eccdfa18752df5e2fee1aad694dbdd4d50e61cd3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c4dd05c6080abf0291c4f46eccdfa18752df5e2fee1aad694dbdd4d50e61cd3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Rethinking Coupled Tensor Analysis for Hyperspectral Super-Resolution: Recoverable Modeling Under Endmember Variability</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ziqian Huang, Boxiao Yu, Siqi Li, Savas Ozdemir, Sangjin Bae, Jae Sung Lee, Guobao Wang, Kuang Gong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19584" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19584</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/35afd74d755844e3f7dd7c04116e75ba37dbf5e148c533c0e847939d30f5c6fc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/35afd74d755844e3f7dd7c04116e75ba37dbf5e148c533c0e847939d30f5c6fc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Niclas Griesshaber, Jochen Streb</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19675</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d49754318d6e9803642e66f7555bb2551be305239532d52cb9f0b2b55048ad6b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d49754318d6e9803642e66f7555bb2551be305239532d52cb9f0b2b55048ad6b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-24">2025-12-24<a href="#2025-12-24" class="hash-link" aria-label="Direct link to 2025-12-24" title="Direct link to 2025-12-24" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251224] PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Md Nahid Hasan Shuvo, Moinul Hossain</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19711" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19711</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d516fcc6c3de6b0e65c078e5e3f3dda23bfd77fbb9c5f4abfe2c509c2cb6dfe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d516fcc6c3de6b0e65c078e5e3f3dda23bfd77fbb9c5f4abfe2c509c2cb6dfe_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiangzhong Luo, Weichen Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19731" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19731</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c10d82314834148af959284b60a6d6f0fa9e36928106648626f8c43d4f07b79_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c10d82314834148af959284b60a6d6f0fa9e36928106648626f8c43d4f07b79_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Learning to Refocus with Video Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> SaiKiran Tedla, Zhoutong Zhang, Xuaner Zhang, Shumian Xin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19823" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19823</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d67b80f0c15466284688038780928e07f307c7269e0df0ded0424d3e770fcd6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d67b80f0c15466284688038780928e07f307c7269e0df0ded0424d3e770fcd6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning to Refocus with Video Diffusion Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] RANSAC Scoring Functions: Analysis and Reality Check</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> A. Shekhovtsov</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19850" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19850</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd50e25656a023b3c2995d13c741fc721d03038e35f7503eb116497b3cb8637d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd50e25656a023b3c2995d13c741fc721d03038e35f7503eb116497b3cb8637d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RANSAC Scoring Functions: Analysis and Reality Check</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Generating the Past, Present and Future from a Motion-Blurred Image</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> SaiKiran Tedla, Kelly Zhu, Trevor Canham, Felix Taubner, Michael S. Brown, Kiriakos N. Kutulakos, David B. Lindell</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19817" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19817</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d34df9bd29ac26646aab435f8fb3761d7ba6f54b4c9919286b04b5436dba1d0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d34df9bd29ac26646aab435f8fb3761d7ba6f54b4c9919286b04b5436dba1d0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generating the Past, Present and Future from a Motion-Blurred Image</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jong Wook Kim, Wonseok Roh, Ha Dam Baek, Pilhyeon Lee, Jonghyun Choi, Sangpil Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19871" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19871</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a4679f5f60c2af6c77a6712d0c27272bc4719706cb56235956b7caf4c1bac0f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a4679f5f60c2af6c77a6712d0c27272bc4719706cb56235956b7caf4c1bac0f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Unified Brain Surface and Volume Registration</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> S. Mazdak Abulnaga, Andrew Hoopes, Malte Hoffmann, Robin Magnet, Maks Ovsjanikov, Lilla Zöllei, John Guttag, Bruce Fischl, Adrian Dalca</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19928" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19928</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52472b7c53844ab7af247ff699ee1c659d2b3ff27e2e21ee8f187677824a5d8d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52472b7c53844ab7af247ff699ee1c659d2b3ff27e2e21ee8f187677824a5d8d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Unified Brain Surface and Volume Registration</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Houston H. Zhang, Tao Zhang, Baoze Lin, Yuanqi Xue, Yincheng Zhu, Huan Liu, Li Gu, Linfeng Ye, Ziqiang Wang, Xinxin Zuo, Yang Wang, Yuanhao Yu, Zhixiang Chi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19918" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19918</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afcd12646a75911cc37486f5af7f55491eaf6f54718182f47a36695579c55660_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afcd12646a75911cc37486f5af7f55491eaf6f54718182f47a36695579c55660_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Vehicle-centric Perception via Multimodal Structured Pre-training</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wentao Wu, Xiao Wang, Chenglong Li, Jin Tang, Bin Luo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19934" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19934</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25393280cf5e09ecc25c375a88de26bef6b6904fd90a6775cf7591ed8a615fe1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25393280cf5e09ecc25c375a88de26bef6b6904fd90a6775cf7591ed8a615fe1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Vehicle-centric Perception via Multimodal Structured Pre-training</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Block-Recurrent Dynamics in Vision Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mozes Jacobs, Thomas Fel, Richard Hakim, Alessandra Brondetta, Demba Ba, T. Andy Keller</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19941" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19941</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e10f9b4ab6d210902b2c5092ebfe1fcc82dd7a3d2032bfc97fbfadd16867c2a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e10f9b4ab6d210902b2c5092ebfe1fcc82dd7a3d2032bfc97fbfadd16867c2a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Block-Recurrent Dynamics in Vision Transformers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] SE360: Semantic Edit in 360<span class="katex-error" title="ParseError: KaTeX parse error: Expected group after &#x27;^&#x27; at position 1: ^̲" style="color:#cc0000">^</span> Panoramas via Hierarchical Data Construction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Haoyi Zhong, Fang-Lue Zhang, Andrew Chalmers, Taehyun Rhee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19943" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19943</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5acefee064893ec36480803c8a614ffcd70f5e22f389b691a22dff4da224abf7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5acefee064893ec36480803c8a614ffcd70f5e22f389b691a22dff4da224abf7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SE360: Semantic Edit in 360<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mo>∘</mo></msup></mrow><annotation encoding="application/x-tex">^\circ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6741em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6741em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∘</span></span></span></span></span></span></span></span></span></span></span> Panoramas via Hierarchical Data Construction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] HistoWAS: A Pathomics Framework for Large-Scale Feature-Wide Association Studies of Tissue Topology and Patient Outcomes</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuechen Yang, Junlin Guo, Yanfan Zhu, Jialin Yue, Junchao Zhu, Yu Wang, Shilin Zhao, Haichun Yang, Xingyi Guo, Jovan Tanevski, Laura Barisoni, Avi Z. Rosenberg, Yuankai Huo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19954" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19954</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebf1e5c1a2595dc81a69957123729d9aa0fa024a71bdfda164ce0ae6bf95d110_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebf1e5c1a2595dc81a69957123729d9aa0fa024a71bdfda164ce0ae6bf95d110_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HistoWAS: A Pathomics Framework for Large-Scale Feature-Wide Association Studies of Tissue Topology and Patient Outcomes</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] How Much 3D Do Video Foundation Models Encode?</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zixuan Huang, Xiang Li, Zhaoyang Lv, James M. Rehg</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19949" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19949</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b54fab192e555a908a0f7daf8d7992c85cd3241844d6a17c19d685c99c93dc5e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b54fab192e555a908a0f7daf8d7992c85cd3241844d6a17c19d685c99c93dc5e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> How Much 3D Do Video Foundation Models Encode?</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Peng Gao, Ke Li, Di Wang, Yongshan Zhu, Yiming Zhang, Xuemei Luo, Yifeng Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19990" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19990</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1691a202602f3bcf2d0e97787c25b8551ee2510f815f2c351ebacb3b3a37953_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1691a202602f3bcf2d0e97787c25b8551ee2510f815f2c351ebacb3b3a37953_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] WSD-MIL: Window Scale Decay Multiple Instance Learning for Whole Slide Image Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Le Feng, Li Xiao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19982" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19982</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1fecc5e0134d3112ae90b5c8ac61ae68bcd52b0697a6c32184a7fc10d72e0640_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1fecc5e0134d3112ae90b5c8ac61ae68bcd52b0697a6c32184a7fc10d72e0640_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> WSD-MIL: Window Scale Decay Multiple Instance Learning for Whole Slide Image Classification</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tamim Ahasan Rijon, Yeasin Arafath</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19989" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19989</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7415dab1086d45b2bb6f7a49ab33be753b3da047de23310de552ead99dc2448_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7415dab1086d45b2bb6f7a49ab33be753b3da047de23310de552ead99dc2448_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhenhao Li, Shaohan Yi, Zheng Liu, Leonartinus Gao, Minh Ngoc Le, Ambrose Ling, Zhuoran Wang, Md Amirul Islam, Zhixiang Chi, Yuanhao Yu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20000" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20000</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f2d07a108f93995c65cf13d740ca889c8e7dd494a7ef3c9222caba5e6ccf3c3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f2d07a108f93995c65cf13d740ca889c8e7dd494a7ef3c9222caba5e6ccf3c3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Blessing Agyei Kyem, Joshua Kofi Asamoah, Anthony Dontoh, Andrews Danyo, Eugene Denteh, Armstrong Aboah</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20011" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20011</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff4db8075e196efc52322813cedc23193239565c930612aec052f3acbb95eab5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff4db8075e196efc52322813cedc23193239565c930612aec052f3acbb95eab5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zepeng Xin, Kaiyu Li, Luodi Chen, Wanchen Li, Yuchen Xiao, Hui Qiao, Weizhan Zhang, Deyu Meng, Xiangyong Cao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20013" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20013</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24d12d8c96876ce1d14987d2507601d9688f72e9f86e06abfd01bc397241cbd6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24d12d8c96876ce1d14987d2507601d9688f72e9f86e06abfd01bc397241cbd6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chang Sun, Dongliang Xie, Bo Qin, Hong Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20032" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20032</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/800e8f7c8c22b74ebd9c804082b6c7f5a5c4998365cdbe511167991351910a98_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/800e8f7c8c22b74ebd9c804082b6c7f5a5c4998365cdbe511167991351910a98_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Anthony Dontoh, Stephanie Ivey, Armstrong Aboah</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20025" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20025</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8e8d7557e6f332097ab10b8c45895c9a3f2b19aa186bcd6c3071ac6e02ee8e7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8e8d7557e6f332097ab10b8c45895c9a3f2b19aa186bcd6c3071ac6e02ee8e7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">H^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Lin Li, Jiahui Li, Jiaming Lei, Jun Xiao, Feifei Shao, Long Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20029" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20029</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1aa2e8e4d0ae946b7e977e841084f034f6f83372776d08e5a2bf43a04e81003_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1aa2e8e4d0ae946b7e977e841084f034f6f83372776d08e5a2bf43a04e81003_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>{</mtext><mi>H</mi><msup><mo stretchy="false">}</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\text\{H\}^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">{</span></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ziwei Qin, Xuhui Song, Deqing Huang, Na Qin, Jun Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20026" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20026</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7dc5f8098baa15ab99d6ec49af6ea9b561b6b787a33ac1ec6e365ad910668048_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7dc5f8098baa15ab99d6ec49af6ea9b561b6b787a33ac1ec6e365ad910668048_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nguyen Lam Phu Quy, Pham Phu Hoa, Tran Chi Nguyen, Dao Sy Duy Minh, Nguyen Hoang Minh Ngoc, Huynh Trung Kiet</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20042" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20042</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9fb136c3c44d734ccd03ee7608dfc92d3612b466bfffc82e1d2efdfd79e5f161_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9fb136c3c44d734ccd03ee7608dfc92d3612b466bfffc82e1d2efdfd79e5f161_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] FlashLips: 100-FPS Mask-Free Latent Lip-Sync using Reconstruction Instead of Diffusion or GANs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Andreas Zinonos, Michał Stypułkowski, Antoni Bigata, Stavros Petridis, Maja Pantic, Nikita Drobyshev</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20033" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20033</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24426a84ed65236012992da21d9b902f793ee45db54516610ca6203c50494625_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24426a84ed65236012992da21d9b902f793ee45db54516610ca6203c50494625_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlashLips: 100-FPS Mask-Free Latent Lip-Sync using Reconstruction Instead of Diffusion or GANs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Progressive Learned Image Compression for Machine Perception</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jungwoo Kim, Jun-Hyuk Kim, Jong-Seok Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20070" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20070</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24993bfda343219d771658c8c9767048aec44f5b6784dcfb07bb075ba4304dd1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24993bfda343219d771658c8c9767048aec44f5b6784dcfb07bb075ba4304dd1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Progressive Learned Image Compression for Machine Perception</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hao Li, Fabian Deuser, Wenping Yin, Steffen Knoblauch, Wufan Zhao, Filip Biljecki, Yong Xue, Wei Huang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20056" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20056</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c37110ad3319d1434c17c04ae94a4dcfa94a278faf51360e359e1a063ce32e7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c37110ad3319d1434c17c04ae94a4dcfa94a278faf51360e359e1a063ce32e7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Haiyun Wei, Fan Lu, Yunwei Zhu, Zehan Zheng, Weiyi Xue, Lin Shao, Xudong Zhang, Ya Wu, Rong Fu, Guang Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20105" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20105</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a2dee7bb51f34affab3884be56b1bfb46b76c6a66e40400ed09830c5aabd0b0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a2dee7bb51f34affab3884be56b1bfb46b76c6a66e40400ed09830c5aabd0b0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Subrata Kumer Paula, Dewan Nafiul Islam Noora, Rakhi Rani Paula, Md. Ekramul Hamidb, Fahmid Al Faridc, Hezerul Abdul Karimd, Md. Maruf Al Hossain Princee, Abu Saleh Musa Miahb</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20104" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20104</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e8b67a33af7843f8ee6ec84eb87891b7d12b57e4af5fa1f77bce5e76a6b92b0d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e8b67a33af7843f8ee6ec84eb87891b7d12b57e4af5fa1f77bce5e76a6b92b0d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jinyoung Choi, Youngchae Kwon, Injung Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20088" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20088</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1946d7c3329c37db07556d963544aad7dbfeda194c515e4debdc6e3754d8920_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1946d7c3329c37db07556d963544aad7dbfeda194c515e4debdc6e3754d8920_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Alireza Moayedikia, Sattar Dorafshan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20113" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20113</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00b50254f658c253763f9df9bd57707873bdb0982863020f26c263961aa323d8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00b50254f658c253763f9df9bd57707873bdb0982863020f26c263961aa323d8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Niraj Prakash Kini, Shiau-Rung Tsai, Guan-Hsun Lin, Wen-Hsiao Peng, Ching-Wen Ma, Jenq-Neng Hwang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20128" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20128</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b40cd83392a8ac698c9e282aef2bd70809bcd4957965aa0c36506eb68c77ec30_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b40cd83392a8ac698c9e282aef2bd70809bcd4957965aa0c36506eb68c77ec30_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mohammad Helal Uddin, Liam Seymour, Sabur Baidya</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20120" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20120</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f965376a2586790dd27548936a060ee2437b3c5085b58331952dfddc1265b29f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f965376a2586790dd27548936a060ee2437b3c5085b58331952dfddc1265b29f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Cyrus Vachha, Yixiao Kang, Zach Dive, Ashwat Chidambaram, Anik Gupta, Eunice Jun, Bjoern Hartmann</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20129" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20129</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8400f1033a93635b0a5b706c568585f557f04cf8533a2b77ce1c55f08e14bef6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8400f1033a93635b0a5b706c568585f557f04cf8533a2b77ce1c55f08e14bef6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jingqi Tian, Yiheng Du, Haoji Zhang, Yuji Wang, Isaac Ning Lee, Xulong Bai, Tianrui Zhu, Jingxuan Niu, Yansong Tang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20117" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20117</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a290a5343ed508cf106c79abdb63608b4b4b69f2e0d42fc218565b45b8eb64f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a290a5343ed508cf106c79abdb63608b4b4b69f2e0d42fc218565b45b8eb64f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Thanh-Tung Le, Tuan Pham, Tung Nguyen, Deying Kong, Xiaohui Xie, Stephan Mandt</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20107" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20107</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/668ef8c168df89e0d5190bbd1ac0a2d6de0affc034e4a50d8e050242fd138dd8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/668ef8c168df89e0d5190bbd1ac0a2d6de0affc034e4a50d8e050242fd138dd8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Retrieval-augmented Prompt Learning for Pre-trained Foundation Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiang Chen, Yixin Ou, Quan Feng, Lei Li, Piji Li, Haibo Ye, Sheng-Jun Huang, Shuofei Qiao, Shumin Deng, Huajun Chen, Ningyu Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20145" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20145</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028ab8e7171d7f497cbdc48e136d419e8642bf876111786382aee29b15d0992_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028ab8e7171d7f497cbdc48e136d419e8642bf876111786382aee29b15d0992_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Retrieval-augmented Prompt Learning for Pre-trained Foundation Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Robert van de Ven, Trim Bresilla, Bram Nelissen, Ard Nieuwenhuizen, Eldert J. van Henten, Gert Kootstra</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20148" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20148</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0476ec59995b5f3259a9b2ab1456d67ddbbb750902c5acd3d684600b97c47598_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0476ec59995b5f3259a9b2ab1456d67ddbbb750902c5acd3d684600b97c47598_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] CoDi -- an exemplar-conditioned diffusion model for low-shot counting</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Grega Šuštar, Jer Pelhan, Alan Lukežič, Matej Kristan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20153" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20153</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19ee3eb6d09129b7fef5f1398fe3a279f203f7ff1a53a416574b63094eeefdf5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19ee3eb6d09129b7fef5f1398fe3a279f203f7ff1a53a416574b63094eeefdf5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CoDi -- an exemplar-conditioned diffusion model for low-shot counting</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sofian Chaybouti, Sanath Narayan, Yasser Dahou, Phúc H. Lê Khac, Ankit Singh, Ngoc Dung Huynh, Wamiq Reyaz Para, Hilde Kuehne, Hakim Hacid</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20157" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20157</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61612d4352aae9f7b2e5272f0843ecd4da94205a817c26dac3655aa0b24e73b2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61612d4352aae9f7b2e5272f0843ecd4da94205a817c26dac3655aa0b24e73b2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hao Guo, Xugong Qin, Jun Jie Ou Yang, Peng Zhang, Gangyan Zeng, Yubo Li, Hailun Lin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20174" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20174</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a74295652803d99c4b0631ef38e9684d12032ddd9797f217033f356497ff647_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a74295652803d99c4b0631ef38e9684d12032ddd9797f217033f356497ff647_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Generative Latent Coding for Ultra-Low Bitrate Image Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhaoyang Jia, Jiahao Li, Bin Li, Houqiang Li, Yan Lu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20194" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20194</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e95ca206803e3acc79be8a04fea7c616d1d6d5dc840a97528ce68341f589c143_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e95ca206803e3acc79be8a04fea7c616d1d6d5dc840a97528ce68341f589c143_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generative Latent Coding for Ultra-Low Bitrate Image Compression</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiangxuan Ren, Zhongdao Wang, Pin Tang, Guoqing Wang, Jilai Zheng, Chao Ma</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20217" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20217</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8799680440a14b4dded747c223ca374b82c834b527822c9fff101d2247c1d65d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8799680440a14b4dded747c223ca374b82c834b527822c9fff101d2247c1d65d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] JDPNet: A Network Based on Joint Degradation Processing for Underwater Image Enhancement</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tao Ye, Hongbin Ren, Chongbing Zhang, Haoran Chen, Xiaosong Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20213" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20213</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c290a68b3d97be1a375bfedfbc89e4ad87f03c4cfd59b51b99adeb151186d5ed_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c290a68b3d97be1a375bfedfbc89e4ad87f03c4cfd59b51b99adeb151186d5ed_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> JDPNet: A Network Based on Joint Degradation Processing for Underwater Image Enhancement</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] How I Met Your Bias: Investigating Bias Amplification in Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nathan Roos, Ekaterina Iakovleva, Ani Gjergji, Vito Paolo Pastore, Enzo Tartaglione</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20233" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20233</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3c43be0c18a78b4304c027f45372c35a663531d4f9536a15c2b4ca0dbb155b2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3c43be0c18a78b4304c027f45372c35a663531d4f9536a15c2b4ca0dbb155b2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> How I Met Your Bias: Investigating Bias Amplification in Diffusion Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xuanyu Hu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20249" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20249</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1c6a732f9a0082b695f01f79b2bcc47f909daa0f9d50a6af4d86a633213b328_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1c6a732f9a0082b695f01f79b2bcc47f909daa0f9d50a6af4d86a633213b328_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] IndicDLP: A Foundational Dataset for Multi-Lingual and Multi-Domain Document Layout Parsing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Oikantik Nath, Sahithi Kukkala, Mitesh Khapra, Ravi Kiran Sarvadevabhatla</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20236" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20236</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/027b9426177f33cb782197baba41e405dda8dcde86fe03cd16387612d6f69294_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/027b9426177f33cb782197baba41e405dda8dcde86fe03cd16387612d6f69294_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> IndicDLP: A Foundational Dataset for Multi-Lingual and Multi-Domain Document Layout Parsing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jinghao Shi, Jianing Song</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20255" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20255</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2b3920e5370f3c041a4e84894da06ffa2a4f371a30e7ead753e72b679159943_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2b3920e5370f3c041a4e84894da06ffa2a4f371a30e7ead753e72b679159943_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Daniele Cardullo, Simone Teglia, Irene Amerini</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20257" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20257</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77044e7e1eb0cbde388f554efc56df1d6faee9e6b4ec8be9ca9c2664befbf208_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77044e7e1eb0cbde388f554efc56df1d6faee9e6b4ec8be9ca9c2664befbf208_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Degradation-Aware Metric Prompting for Hyperspectral Image Restoration</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Binfeng Wang, Di Wang, Haonan Guo, Ying Fu, Jing Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20251" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20251</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4f8150acede0f77c8c88a8f6399592fa53d83830252a9609aa77972be050f28_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4f8150acede0f77c8c88a8f6399592fa53d83830252a9609aa77972be050f28_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Degradation-Aware Metric Prompting for Hyperspectral Image Restoration</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>D</mi><msup><mo stretchy="false">}</mo><mo stretchy="false">{</mo></msup><mn>3</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{D\}^\{3\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mopen mtight">{</span></span></span></span></span></span></span></span><span class="mord">3</span><span class="mclose">}</span></span></span></span>{ETOR}: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>D</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{D\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose">}</span></span></span></span>ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>D</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{D\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose">}</span></span></span></span>ebiasing for Weakly-Supervised Camouflaged Object <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>D</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{D\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose">}</span></span></span></span>etection with Scribble Annotations</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jiawei Ge, Jiuxin Cao, Xinyi Li, Xuelin Zhu, Chang Liu, Bo Liu, Chen Feng, Ioannis Patras</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20260" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20260</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee0606d3fe6e6ae30bbf9417baa7948dab878cfcc6b70e36031424c107aafb81_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee0606d3fe6e6ae30bbf9417baa7948dab878cfcc6b70e36031424c107aafb81_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>D</mi><msup><mo stretchy="false">}</mo><mo stretchy="false">{</mo></msup><mn>3</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{D\}^\{3\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mopen mtight">{</span></span></span></span></span></span></span></span><span class="mord">3</span><span class="mclose">}</span></span></span></span>{ETOR}: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>D</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{D\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose">}</span></span></span></span>ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>D</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{D\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose">}</span></span></span></span>ebiasing for Weakly-Supervised Camouflaged Object <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>D</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{D\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose">}</span></span></span></span>etection with Scribble Annotations</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] UbiQVision: Quantifying Uncertainty in XAI for Image Recognition</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Akshat Dubey, Aleksandar Anžel, Bahar İlgen, Georges Hattab</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20288" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20288</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6106b87e48429449f0b11c2765bdedb47797d7822e4b38ad3a9fe7f94b92dacb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6106b87e48429449f0b11c2765bdedb47797d7822e4b38ad3a9fe7f94b92dacb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> UbiQVision: Quantifying Uncertainty in XAI for Image Recognition</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ji-Hoon Kim, Junseok Ahn, Doyeop Kwak, Joon Son Chung, Shinji Watanabe</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20296" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20296</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/217e6d95bfc0e38be8f5a7356d3e869c3b3a5b9d4daf05b6435c3c756df247c6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/217e6d95bfc0e38be8f5a7356d3e869c3b3a5b9d4daf05b6435c3c756df247c6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhongyu Xia, Wenhao Chen, Yongtao Wang, Ming-Hsuan Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20299" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20299</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0961ae49fbc925ad5eacb6602aeec6cfdfabae07b252a044cd181bdb5a47746_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0961ae49fbc925ad5eacb6602aeec6cfdfabae07b252a044cd181bdb5a47746_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Qingdong He, Xueqin Chen, Yanjie Pan, Peng Tang, Pengcheng Xu, Zhenye Gan, Chengjie Wang, Xiaobin Hu, Jiangning Zhang, Yabiao Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20340" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20340</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce852f1d570d91487912f5d0abf0a93c49ad00eeecfdd840d9dbb9025ff8fd3a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce852f1d570d91487912f5d0abf0a93c49ad00eeecfdd840d9dbb9025ff8fd3a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> V. Kovalev, A. Kuvshinov, A. Buzovkin, D. Pokidov, D. Timonin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20362" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20362</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9180090a8d8d701e63ea635b11cc6a39fed9f252167568980aefb8786a66ef43_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9180090a8d8d701e63ea635b11cc6a39fed9f252167568980aefb8786a66ef43_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Field-Space Attention for Structure-Preserving Earth System Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Maximilian Witte, Johannes Meuer, Étienne Plésiat, Christopher Kadow</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20350" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20350</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8623c14b77fd771294e1b936a58143cff6a715df1b4a5026fe2d59708383e6e2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8623c14b77fd771294e1b936a58143cff6a715df1b4a5026fe2d59708383e6e2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Field-Space Attention for Structure-Preserving Earth System Transformers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Linking Faces and Voices Across Languages: Insights from the FAME 2026 Challenge</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Marta Moscati, Ahmed Abdullah, Muhammad Saad Saeed, Shah Nawaz, Rohan Kumar Das, Muhammad Zaigham Zaheer, Junaid Mir, Muhammad Haroon Yousaf, Khalid Mahmood Malik, Markus Schedl</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20376" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20376</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/483debde320d1a401b91478496a4b53ca3f74d52718353845af8598a16bf6167_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/483debde320d1a401b91478496a4b53ca3f74d52718353845af8598a16bf6167_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Linking Faces and Voices Across Languages: Insights from the FAME 2026 Challenge</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Linfei Li, Lin Zhang, Zhong Wang, Ying Shen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20377" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20377</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06dc77546a31f168b83f87c8efbfe002590b29ab252385b482db477a74d615ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06dc77546a31f168b83f87c8efbfe002590b29ab252385b482db477a74d615ac_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> YuChe Hsu, AnJui Wang, TsaiChing Ni, YuanFu Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20387" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20387</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d9aa5293bf4ca8e839b122277f1484ffb43b6211d54741d7eb7f58312f5509_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d9aa5293bf4ca8e839b122277f1484ffb43b6211d54741d7eb7f58312f5509_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Chain-of-Anomaly Thoughts with Large Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pedro Domingos, João Pereira, Vasco Lopes, João Neves, David Semedo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20417" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20417</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e26e3aba92227ff2a24c227033ffa15e8e191cf437a0a97c819748f4dfb7c94_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e26e3aba92227ff2a24c227033ffa15e8e191cf437a0a97c819748f4dfb7c94_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Chain-of-Anomaly Thoughts with Large Vision-Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Simplifying Multi-Task Architectures Through Task-Specific Normalization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mihai Suteu, Ovidiu Serban</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20420" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20420</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d24e6af533166dd44855079b97d7233c65878aa61e02267e65f4e306467d04b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d24e6af533166dd44855079b97d7233c65878aa61e02267e65f4e306467d04b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Simplifying Multi-Task Architectures Through Task-Specific Normalization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Junho Yoon, Jaemo Jung, Hyunju Kim, Dongman Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20409" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20409</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d4c944d1aa99e38d69c83c388503646f62271c8bb649aafcafb57ad0ba7c7d0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d4c944d1aa99e38d69c83c388503646f62271c8bb649aafcafb57ad0ba7c7d0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Abdullah Al Shafi, Abdul Muntakim, Pintu Chandra Shill, Rowzatul Zannat, Abdullah Al-Amin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20431" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20431</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/905e502b77110f2fdb7bcfd863c906242998a2915d794e39c3da2377a2743392_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/905e502b77110f2fdb7bcfd863c906242998a2915d794e39c3da2377a2743392_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] High Dimensional Data Decomposition for Anomaly Detection of Textured Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ji Song, Xing Wang, Jianguo Wu, Xiaowei Yue</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20432" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20432</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1df48b800de53f9fff1712283209aa73147125a9725200492a8bcc4cd35e7182_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1df48b800de53f9fff1712283209aa73147125a9725200492a8bcc4cd35e7182_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> High Dimensional Data Decomposition for Anomaly Detection of Textured Images</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Anh Dao, Manh Tran, Yufei Zhang, Xiaoming Liu, Zijun Cui</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20451" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20451</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dab892d1698a54ca5bea9ec4d990ec556b33cc4b339ac925ef5d18dba7b5dbae_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dab892d1698a54ca5bea9ec4d990ec556b33cc4b339ac925ef5d18dba7b5dbae_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> James E. Gallagher, Edward J. Oughton, Jana Kosecka</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20487" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20487</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09ac139242f7c2d01b6739f6aa8ae3ee3439e687ec6c7b420e17bccf33e9de40_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09ac139242f7c2d01b6739f6aa8ae3ee3439e687ec6c7b420e17bccf33e9de40_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] SirenPose: Dynamic Scene Reconstruction via Geometric Supervision</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kaitong Cai, Jensen Zhang, Jing Yang, Keze Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20531" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20531</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbf03f3fbe032434f49cece08cea9e779ea7ea57fa26c8f00eeed2e7c9080766_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbf03f3fbe032434f49cece08cea9e779ea7ea57fa26c8f00eeed2e7c9080766_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SirenPose: Dynamic Scene Reconstruction via Geometric Supervision</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shengchao Zhou, Yuxin Chen, Yuying Ge, Wei Huang, Jiehong Lin, Ying Shan, Xiaojuan Qi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20557" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20557</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e481c21d437a92edaec37fae6af73e02495508b9597f95d16d784b230f3165c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e481c21d437a92edaec37fae6af73e02495508b9597f95d16d784b230f3165c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Gorjan Radevski</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20501" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20501</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e4fc93af34bdcb98669d1a3c900d03900cb8c1359b89368e444c1c70848cdd4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e4fc93af34bdcb98669d1a3c900d03900cb8c1359b89368e444c1c70848cdd4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yiming Zhao, Yuanpeng Gao, Yuxuan Luo, Jiwei Duan, Shisong Lin, Longfei Xiong, Zhouhui Lian</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20479" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20479</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4093d97a2621866450d2ab0e47bbdaa622a6fe81e981843cdd087e11e4301902_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4093d97a2621866450d2ab0e47bbdaa622a6fe81e981843cdd087e11e4301902_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Long Nguyen, Micha Fauth, Bernhard Jaeger, Daniel Dauner, Maximilian Igl, Andreas Geiger, Kashyap Chitta</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20563" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20563</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f62c50da026de5eec286e01930af394650fb8b9c309ff48cd8f733ee9ca220b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f62c50da026de5eec286e01930af394650fb8b9c309ff48cd8f733ee9ca220b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kaitong Cai, Jusheng Zhang, Jing Yang, Yijia Fan, Pengtao Xie, Jian Wang, Keze Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20561" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20561</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72707d221b008970aee7befd580ee702951e338a32b995af62d9c893fe16e7e1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72707d221b008970aee7befd580ee702951e338a32b995af62d9c893fe16e7e1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Anna Šárová Mikeštíková, Médéric Fourmy, Martin Cífka, Josef Sivic, Vladimir Petrik</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20538" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20538</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f6450c33e31646a32cf9129dad4fbbcc84ead81c36e23e134b1b0f9930b9db8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f6450c33e31646a32cf9129dad4fbbcc84ead81c36e23e134b1b0f9930b9db8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mingwei Tang, Jiahao Nie, Guang Yang, Ziqing Cui, Jie Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20556" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20556</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7eb3ac652519f5f42206d482e71edb24ef458336500aad066aa021c53b988cca_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7eb3ac652519f5f42206d482e71edb24ef458336500aad066aa021c53b988cca_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dhruv Anand, Ehsan Shareghi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20595" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20595</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb95f031dbfb3f7bfc348a6d3e77d5c3ae7e2408f06dd57529b77764de0ce0b7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb95f031dbfb3f7bfc348a6d3e77d5c3ae7e2408f06dd57529b77764de0ce0b7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] FedPOD: the deployable units of training for federated learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Daewoon Kim, Si Young Yie, Jae Sung Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20610" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20610</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e8e1f095bc0447c82283e16e3fb09acd3ae3773107b9096b3a06a1659a978e0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e8e1f095bc0447c82283e16e3fb09acd3ae3773107b9096b3a06a1659a978e0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FedPOD: the deployable units of training for federated learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LongVideoAgent: Multi-Agent Reasoning with Long Videos</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20618" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20618</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e38fba460d45fac945020bc323269f04211978c3e2e544d86072d5b062f40958_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e38fba460d45fac945020bc323269f04211978c3e2e544d86072d5b062f40958_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LongVideoAgent: Multi-Agent Reasoning with Long Videos</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Active Intelligence in Video Avatars via Closed-loop World Modeling</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xuanhua He, Tianyu Yang, Ke Cao, Ruiqi Wu, Cheng Meng, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Qifeng Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20615" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20615</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/507ac9f976cb24dc0dc72fb35cca194ceb4a25829fd305c6f45493a8a9531c76_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/507ac9f976cb24dc0dc72fb35cca194ceb4a25829fd305c6f45493a8a9531c76_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Active Intelligence in Video Avatars via Closed-loop World Modeling</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Repurposing Video Diffusion Transformers for Robust Point Tracking</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Soowon Son, Honggyu An, Chaehyun Kim, Hyunah Ko, Jisu Nam, Dahyun Chung, Siyoon Jin, Jung Yi, Jaewon Min, Junhwa Hur, Seungryong Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20606" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20606</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415d9c5c4a502667ccd87f6d4f24a9276a6e9f5e7f729b850e01ea33b4b6681a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415d9c5c4a502667ccd87f6d4f24a9276a6e9f5e7f729b850e01ea33b4b6681a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Repurposing Video Diffusion Transformers for Robust Point Tracking</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] SpatialTree: How Spatial Abilities Branch Out in MLLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuxi Xiao, Longfei Li, Shen Yan, Xinhang Liu, Sida Peng, Yunchao Wei, Xiaowei Zhou, Bingyi Kang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20617" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20617</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c18a98475f6303ed360b86b5c0ec7ec6b4ab088ad0e70f925606628c827b46d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c18a98475f6303ed360b86b5c0ec7ec6b4ab088ad0e70f925606628c827b46d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SpatialTree: How Spatial Abilities Branch Out in MLLMs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] SemanticGen: Video Generation in Semantic Space</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jianhong Bai, Xiaoshi Wu, Xintao Wang, Fu Xiao, Yuanxing Zhang, Qinghe Wang, Xiaoyu Shi, Menghan Xia, Zuozhu Liu, Haoji Hu, Pengfei Wan, Kun Gai</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20619" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20619</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77e0d53ce289ecacefa061dc5d124f7b98c80f5b9417f83e06a4ccaafb10e8a8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77e0d53ce289ecacefa061dc5d124f7b98c80f5b9417f83e06a4ccaafb10e8a8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SemanticGen: Video Generation in Semantic Space</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] SAM Audio: Segment Anything in Audio</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Bowen Shi, Andros Tjandra, John Hoffman, Helin Wang, Yi-Chiao Wu, Luya Gao, Julius Richter, Matt Le, Apoorv Vyas, Sanyuan Chen, Christoph Feichtenhofer, Piotr Dollár, Wei-Ning Hsu, Ann Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18099" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18099</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4568ea657a5ec8dc7d7944a03205d941e2bc9d054f9586938d2caa09e0b272e6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4568ea657a5ec8dc7d7944a03205d941e2bc9d054f9586938d2caa09e0b272e6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SAM Audio: Segment Anything in Audio</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Dual-Encoder Transformer-Based Multimodal Learning for Ischemic Stroke Lesion Segmentation Using Diffusion MRI</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Muhammad Usman, Azka Rehman, Muhammad Mutti Ur Rehman, Abd Ur Rehman, Muhammad Umar Farooq</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20436" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20436</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efdfcdd8483dd2617be0701188cb1e86708fc680b1516caff77f5075ed2baf49_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efdfcdd8483dd2617be0701188cb1e86708fc680b1516caff77f5075ed2baf49_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dual-Encoder Transformer-Based Multimodal Learning for Ischemic Stroke Lesion Segmentation Using Diffusion MRI</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Snapshot 3D image projection using a diffractive decoder</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Cagatay Isil, Alexander Chen, Yuhang Li, F. Onuralp Ardic, Shiqi Chen, Che-Yung Shen, Aydogan Ozcan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20464" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20464</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f05e75cf0d45d4f4a6b73b6c248e120e8372e756e8b025e6f78b8ce6098b183_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f05e75cf0d45d4f4a6b73b6c248e120e8372e756e8b025e6f78b8ce6098b183_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Snapshot 3D image projection using a diffractive decoder</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yujia Fu, Zhiyu Dong, Tianwen Qian, Chenye Zheng, Danian Ji, Linhai Zhuo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20374" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20374</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b125441b1639c4b3867045b3843d2e46f162ef46f6b24b77dee1c01e02ade5a8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b125441b1639c4b3867045b3843d2e46f162ef46f6b24b77dee1c01e02ade5a8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-24T07:41:26.000Z" itemprop="dateModified">Dec 24, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/cs_CV/20251215-20251221"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251215-20251221 (cs.CV)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/category/cscy"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">cs.CY</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-22" class="table-of-contents__link toc-highlight">2025-12-22</a></li><li><a href="#2025-12-23" class="table-of-contents__link toc-highlight">2025-12-23</a></li><li><a href="#2025-12-24" class="table-of-contents__link toc-highlight">2025-12-24</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>