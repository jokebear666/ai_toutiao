<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_CV/20251222-20251228" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251222-20251228 (cs.CV) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/cscv/20251222-20251228"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251222-20251228 (cs.CV) | AI头条"><meta data-rh="true" name="description" content="2025-12-22"><meta data-rh="true" property="og:description" content="2025-12-22"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/cscv/20251222-20251228"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cscv/20251222-20251228" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cscv/20251222-20251228" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.CV","item":"https://jokebear666.github.io/ai_toutiao/daily/cscv"},{"@type":"ListItem","position":3,"name":"20251222-20251228 (cs.CV)","item":"https://jokebear666.github.io/ai_toutiao/daily/cscv/20251222-20251228"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.647bd1ff.css">
<script src="/ai_toutiao/assets/js/runtime~main.2d8f4289.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.2f1f9916.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/arxiv-daily"><span title="Arxiv每日论文" class="linkLabel_WmDU">Arxiv每日论文</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Collapse sidebar category &#x27;cs.CV&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cs_CV/20251215-20251221"><span title="20251215-20251221 (cs.CV)" class="linkLabel_WmDU">20251215-20251221 (cs.CV)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/cscv/20251222-20251228"><span title="20251222-20251228 (cs.CV)" class="linkLabel_WmDU">20251222-20251228 (cs.CV)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cscv/20251229-20260104"><span title="20251229-20260104 (cs.CV)" class="linkLabel_WmDU">20251229-20260104 (cs.CV)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csoh"><span title="cs.OH" class="categoryLinkLabel_W154">cs.OH</span></a><button aria-label="Expand sidebar category &#x27;cs.OH&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/daily/cscv"><span>cs.CV</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251222-20251228 (cs.CV)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251222-20251228 (cs.CV)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-22">2025-12-22<a href="#2025-12-22" class="hash-link" aria-label="Direct link to 2025-12-22" title="Direct link to 2025-12-22" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251222] AVM: Towards Structure-Preserving Neural Response Modeling in the Visual Cortex Across Stimuli and Individuals</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational neuroscience], [Vision Transformer, modular subnetworks, condition-aware adaptation, structure-preserving framework, neural response modeling]</li>
<li class=""><strong>authors:</strong> Qi Xu, Shuai Gong, Xuming Ran, Haihua Luo, Yangfan Hu</li>
<li class=""><strong>institution:</strong> Dalian University of Technology, National University of Singapore, University of Jyvaskyla, Zhejiang University of Finance and Economics</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16948" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16948</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Adaptive Visual Model (AVM), a structure-preserving framework that uses a frozen Vision Transformer encoder to capture stable visual features and separate modulation paths to adapt to stimulus and individual variations. It demonstrates improved generalization and interpretability in modeling mouse V1 neural responses, outperforming prior models in predictive correlation and explained variance across different experimental conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Lights, Camera, Consistency: A Multistage Pipeline for Character-Stable AI Video Stories</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [visual anchoring, asset-first mechanism, temporal bridge, diffusion models, large language model (LLM), text-to-video (T2V), character consistency, multi-stage pipeline]</li>
<li class=""><strong>authors:</strong> Chayan Jain, Rishant Sharma, Archit Garg, Ishan Bhanuka, Pratik Narang, Dhruv Kumar</li>
<li class=""><strong>institution:</strong> BITS Pilani</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16954" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16954</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a multi-stage pipeline for generating long, character-consistent video stories. It uses an LLM to create a script, a text-to-image model to design consistent character visuals as anchors, and a video generation model to synthesize scenes individually, with a temporal bridge linking them. The method&#x27;s necessity is validated by showing that removing visual anchoring causes a catastrophic drop in character consistency, and cultural biases in current models are also analyzed.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [YOLOv8, Finer-CAM, saliency maps, cross-validation, TLS point cloud projections]</li>
<li class=""><strong>authors:</strong> Adrian Straker, Paul Magdon, Marco Zullich, Maximilian Freudenberg, Christoph Kleinn, Johannes Breidenbach, Stefano Puliti, Nils Nölke</li>
<li class=""><strong>institution:</strong> University of Applied Sciences and Art (HAWK), University of Groningen, University of Göttingen, Norwegian Institute of Bioeconomy Research (NIBIO)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16950" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16950</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a novel method that links Finer-CAM explanations to structural segments in TLS point cloud projections to evaluate which features drive tree species classification using YOLOv8 models. The analysis of saliency maps reveals that models primarily rely on crown features for classification, with stem features being more important for certain species, and that the models&#x27; perception of species similarity aligns with human expert judgment. The results underscore the need for explainable AI to understand model decision processes and build confidence in predictions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Comparison of deep learning models: CNN and VGG-16 in identifying pornographic content</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [convolutional neural network, VGG-16, deep learning, image classification]</li>
<li class=""><strong>authors:</strong> Reza Chandra, Adang Suhendra, Lintang Yuniar Banowosari, Prihandoko</li>
<li class=""><strong>institution:</strong> Gunadarma University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16947" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16947</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f77d0c91314abfde67968e7d31c4f0423abf8cdee43f2eaa3b2d640c1e5984a3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f77d0c91314abfde67968e7d31c4f0423abf8cdee43f2eaa3b2d640c1e5984a3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares two deep learning models, a custom CNN and the VGG-16 architecture, for the task of identifying pornographic image content. The study found that a CNN model with specific hyperparameters (50 epochs, learning rate 0.001) achieved a higher accuracy of 94.87% compared to the VGG-16 model, concluding that the CNN was more effective for fast and accurate detection.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] V-Agent: An Interactive Video Search System Using Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [vision-language model, fine-tuning, retrieval vector, re-ranking, multi-agent system]</li>
<li class=""><strong>authors:</strong> SunYoung Park, Jong-Hyeon Lee, Youngjune Kim, Daegyu Sung, Younghyun Yu, Young-rok Cha, Jeongho Ju</li>
<li class=""><strong>institution:</strong> NC AI, Kakao, Korea Advanced Institute of Science and Technology (KAIST)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16925" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16925</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e827044257e239766415ac9500a06f7ddb67bfc47c517c041d42cc61ac33ad18_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e827044257e239766415ac9500a06f7ddb67bfc47c517c041d42cc61ac33ad18_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> V-Agent is a multi-agent video search system that fine-tunes a vision-language model with a small video preference dataset and enhances it with a retrieval vector to embed video frames and audio transcriptions into a shared multimodal space. It uses three agents—routing, search, and chat—to refine searches and interact with users, achieving state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [discrete video tokenization, transformer-based adaptive compressor, evidence lower bound (ELBO), information-theoretic compression, adaptive tokenization]</li>
<li class=""><strong>authors:</strong> Haotian Ye, Qiyuan He, Jiaqi Han, Puheng Li, Jiaojiao Fan, Zekun Hao, Fitsum Reda, Yogesh Balaji, Huayu Chen, Sheng Liu, Angela Yao, James Zou, Stefano Ermon, Haoxiang Wang, Ming-Yu Liu</li>
<li class=""><strong>institution:</strong> NVIDIA, Stanford University, National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16975" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16975</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da83017a41e69234160f2c416b8a94507a537a66fd8a745a6bafc49c06817395_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da83017a41e69234160f2c416b8a94507a537a66fd8a745a6bafc49c06817395_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces InfoTok, a principled framework for adaptive discrete video tokenization based on information theory, using a novel ELBO-based algorithm and a transformer-based adaptive compressor. It achieves state-of-the-art compression by allocating tokens according to informational richness, saving 20% of tokens without performance loss and outperforming prior heuristic approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [LongShOTBench, LongShOTAgent, agentic tool use, omni-modal reasoning, benchmark, open-ended questions, graded rubric, iterative refinement]</li>
<li class=""><strong>authors:</strong> Mohammed Irfan Kurpath, Jaseel Muhammad Kaithakkodan, Jinxing Zhou, Sahal Shaji Mullappilly, Mohammad Almansoori, Noor Ahsan, Beknur Kalmakhanbet, Sambal Shikhar, Rishabh Lalla, Jean Lahoud, Mariette Awad, Fahad Shahbaz Khan, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal</li>
<li class=""><strong>institution:</strong> Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), American University of Beirut, Linköping University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16978" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16978</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces LongShOTBench, a diagnostic benchmark for long-form video understanding that features open-ended questions and tasks requiring multimodal reasoning and agentic tool use. It also presents LongShOTAgent, an agentic system that analyzes videos through preprocessing, search, and iterative refinement. The results show a significant performance gap, with state-of-the-art models achieving only up to 52.95%, highlighting the difficulty of real-world long-form video understanding.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Endo-SemiS: Towards Robust Semi-Supervised Image Segmentation for Endoscopic Video</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical image segmentation], [semi-supervised learning, cross-supervision, uncertainty-guided pseudo-label, joint pseudo-label supervision, mutual learning, spatiotemporal corrective network]</li>
<li class=""><strong>authors:</strong> Hao Li, Daiwei Lu, Xing Yao, Nicholas Kavoussi, Ipek Oguz</li>
<li class=""><strong>institution:</strong> Vanderbilt University, Vanderbilt University Medical Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16977" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16977</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c2b434a364e1db7fb30807eeda54f751647c7d8a35266a66353a42fbc7d8ed6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c2b434a364e1db7fb30807eeda54f751647c7d8a35266a66353a42fbc7d8ed6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Endo-SemiS, a semi-supervised framework for endoscopic video segmentation that employs cross-supervision, uncertainty-guided pseudo-labeling, joint supervision, and mutual learning between two networks, along with a separate spatiotemporal corrective network. It demonstrates superior performance over state-of-the-art methods on kidney stone and polyp segmentation tasks when labeled data is limited.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] 4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [4D-RGPT, Perceptual 4D Distillation (P4D), R4D-Bench, region-level prompting, 4D Video Question Answering, multimodal LLM]</li>
<li class=""><strong>authors:</strong> Chiao-An Yang, Ryo Hachiuma, Sifei Liu, Subhashree Radhakrishnan, Raymond A. Yeh, Yu-Chiang Frank Wang, Min-Hung Chen</li>
<li class=""><strong>institution:</strong> NVIDIA, Purdue University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17012" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17012</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc967334b04824c1dceb3e282d12da869887b61ee193194bf210064648df8377_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc967334b04824c1dceb3e282d12da869887b61ee193194bf210064648df8377_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces 4D-RGPT, a multimodal LLM enhanced via a Perceptual 4D Distillation (P4D) framework to improve 4D (3D spatial + temporal) understanding from video inputs. It also proposes the R4D-Bench benchmark for evaluating region-level 4D reasoning. The method shows notable performance improvements on both existing and the new benchmark.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] FORMSpoT: A Decade of Tree-Level, Country-Scale Forest Monitoring</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hierarchical transformer model (PVTv2), airborne laser scanning (ALS), spatio-temporal total variation denoising, co-registration, SPOT-6/7 composites]</li>
<li class=""><strong>authors:</strong> Martin Schwartz, Fajwel Fogel, Nikola Besic, Damien Robert, Louis Geist, Jean-Pierre Renaud, Jean-Matthieu Monnet, Clemens Mosig, Cédric Vega, Alexandre d&#x27;Aspremont, Loic Landrieu, Philippe Ciais</li>
<li class=""><strong>institution:</strong> Laboratoire des Sciences du Climat et de l’Environnement (LSCE), École Normale Supérieure – PSL, Université Gustave Eiffel, Université de Lorraine, University of Zurich, École des Ponts, Office National des Forêts, Université Grenoble Alpes, Institute of Earth System Science and Remote Sensing, Leipzig</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17021" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17021</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces FORMSpoT, a method that uses a hierarchical transformer model trained on ALS data to derive high-resolution forest canopy height maps from SPOT satellite time series, combined with a post-processing pipeline for robust change detection. It demonstrates that this approach significantly outperforms existing products in detecting small-scale forest disturbances, enabling tree-level monitoring at a national scale. The results highlight the importance of very-high-resolution satellite data for accurate forest carbon loss quantification and monitoring under climate change.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [infinite homography warping, video diffusion model, data augmentation, camera-controlled video generation]</li>
<li class=""><strong>authors:</strong> Min-Jung Kim, Jeongho Kim, Hoiyeong Jin, Junha Hyung, Jaegul Choo</li>
<li class=""><strong>institution:</strong> KAIST</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17040" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17040</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/984284ddd81cfd062ac9ad33ace38b4ec632b0ff9087f0906272c5b5d34e4175_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/984284ddd81cfd062ac9ad33ace38b4ec632b0ff9087f0906272c5b5d34e4175_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces InfCam, a framework that uses infinite homography warping to encode 3D camera rotations in a 2D latent space of a video diffusion model, avoiding depth estimation errors. It also employs a data augmentation pipeline to create diverse camera trajectories for training. The method demonstrates improved camera-pose accuracy and visual fidelity compared to existing approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Interpretable Similarity of Synthetic Image Utility</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical imaging], [Interpretable Utility Similarity (IUS), generalized neural additive models, synthetic data evaluation, deep learning, clinical decision support]</li>
<li class=""><strong>authors:</strong> Panagiota Gatoula, George Dimas, Dimitris K. Iakovidis</li>
<li class=""><strong>institution:</strong> University of Thessaly</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17080" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17080</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an interpretable measure called Interpretable Utility Similarity (IUS) to assess the similarity between synthetic and real medical images for deep learning-based clinical decision support. IUS uses generalized neural additive models to explain utility based on clinically relevant image features. Experiments show that selecting synthetic images with high IUS can improve classification performance by up to 54.6% across various medical imaging modalities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] DGH: Dynamic Gaussian Hair</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision, graphics], [dynamic Gaussian hair, coarse-to-fine model, strand-guided optimization, differentiable rendering, 3D Gaussian splatting]</li>
<li class=""><strong>authors:</strong> Junying Wang, Yuanlu Xu, Edith Tretschk, Ziyan Wang, Anastasia Ianina, Aljaz Bozic, Ulrich Neumann, Tony Tung</li>
<li class=""><strong>institution:</strong> University of Southern California, Meta Reality Labs Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17094" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17094</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9785766bd69495591006a354948b34ae7d717dcbb9e1e10f4bd690b1e6c2569_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9785766bd69495591006a354948b34ae7d717dcbb9e1e10f4bd690b1e6c2569_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Dynamic Gaussian Hair (DGH), a data-driven framework that learns hair dynamics and appearance using a coarse-to-fine motion model and a strand-guided 3D Gaussian representation. It enables photorealistic novel-view synthesis of hair under head motion without manual physics tuning. DGH provides a scalable, generalizable alternative to traditional simulation-based hair modeling.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Predictive Modeling of Maritime Radar Data Using Transformer Architecture</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [spatiotemporal forecasting], [transformer architecture, predictive modeling, radar frame prediction, spatiotemporal sequence forecasting]</li>
<li class=""><strong>authors:</strong> Bjorna Qesaraku, Jan Steckel</li>
<li class=""><strong>institution:</strong> Not explicitly provided in the given text.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17098" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17098</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/694341ad0b22f753337bbaead36e9cd3e845d080e2995764ba4a45a74113f8d0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/694341ad0b22f753337bbaead36e9cd3e845d080e2995764ba4a45a74113f8d0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This survey paper reviews predictive modeling approaches for maritime radar data, with a specific focus on the application of transformer architectures for spatiotemporal sequence forecasting. It concludes that while transformers have been used for AIS trajectory and sonar frame prediction, their use for maritime radar frame prediction remains an unexplored research gap, identifying a clear direction for future work.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nan Zhou, Huandong Wang, Jiahao Li, Yang Li, Xiao-Ping Zhang, Yong Li, Xinlei Chen</li>
<li class=""><strong>institution:</strong></li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17152" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17152</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4068b512da42fb271d02b97ac83087d5dbffa83b4904b964cddb5a1ffcc6de68_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4068b512da42fb271d02b97ac83087d5dbffa83b4904b964cddb5a1ffcc6de68_w640_q70.webp</a></li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [deep unrolled model, Restormer, learned coil sensitivity map estimator, sampling-aware weighted data consistency, universal conditioning, progressive cascade expansion training]</li>
<li class=""><strong>authors:</strong> Puyang Wang, Pengfei Guo, Keyi Chai, Jinyuan Zhou, Daguang Xu, Shanshan Jiang</li>
<li class=""><strong>institution:</strong> Johns Hopkins University, NVIDIA</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17137" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17137</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13a0f6637b571493e14f364092f2d39a108d211bbddd588a88df25d1db4a8e1c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13a0f6637b571493e14f364092f2d39a108d211bbddd588a88df25d1db4a8e1c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SDUM, a scalable deep unrolled model for universal MRI reconstruction that integrates a Restormer-based reconstructor, learned coil sensitivity estimation, and sampling-aware data consistency. It demonstrates predictable performance scaling with model depth and achieves state-of-the-art results across diverse clinical MRI protocols without task-specific fine-tuning. The work establishes a practical framework for a single, universally applicable reconstruction model in clinical environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Pro-Pose: Unpaired Full-Body Portrait Synthesis via Canonical UV Maps</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [canonical UV maps, SMPL-X poses, novel view synthesis, multi image finetuning, unpaired dataset]</li>
<li class=""><strong>authors:</strong> Sandeep Mishra, Yasamin Jafarian, Andreas Lugmayr, Yingwei Li, Varsha Ramakrishnan, Srivatsan Varadharajan, Alan C. Bovik, Ira Kemelmacher-Shlizerman</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin, Google</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17143" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17143</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ba0fc26561d25ebdc31e5c5b7c54d8ebaf35995e25f7717c3b45565ea557166_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ba0fc26561d25ebdc31e5c5b7c54d8ebaf35995e25f7717c3b45565ea557166_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Pro-Pose, a method for synthesizing professional full-body portraits from a single casual photo by transforming the person into a canonical UV map space and leveraging SMPL-X poses for reposing. This approach uses unpaired datasets and personalizes results through multi-image fine-tuning. It concludes that the method effectively preserves identity while generating high-quality, reposed avatars in varied poses and lighting.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Text-Conditioned Background Generation for Editable Multi-Layer Documents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [latent masking, Automated Readability Optimization (ARO), summarization-and-instruction process, multi-layer composition, WCAG 2.2]</li>
<li class=""><strong>authors:</strong> Taewon Kang, Joseph K J, Chris Tensmeyer, Jihyung Kil, Wanrong Zhu, Ming C. Lin, Vlad I. Morariu</li>
<li class=""><strong>institution:</strong> University of Maryland at College Park, Adobe Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17151" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17151</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3923a8122c7fd309cbab0c07d054d2af0f590d1779870be58f212682ecfafbb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3923a8122c7fd309cbab0c07d054d2af0f590d1779870be58f212682ecfafbb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a training-free framework for generating and editing document backgrounds using a diffusion model, ensuring text readability through latent masking and Automated Readability Optimization (ARO). It maintains multi-page thematic consistency via a summarization-and-instruction process and treats documents as editable multi-layer compositions. The method produces visually coherent, readable, and thematically aligned documents, bridging generative AI with practical design workflows.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Can Synthetic Images Serve as Effective and Efficient Class Prototypes?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multi-modal inference], [CLIP, large language model, diffusion model, zero-shot classification, visual prototypes, prompt generation]</li>
<li class=""><strong>authors:</strong> Dianxing Shi, Dingjie Fu, Yuqiao Liu, Jun Wang</li>
<li class=""><strong>institution:</strong> Beijing Research Institute of Uranium Geology, Huazhong University of Science and Technology, The Hong Kong University of Science and Technology (Guangzhou), Great Bay University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17160" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17160</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6907c7ddddf9ad9be39d226d2f021022e7cd01e6d45d58e12d3f8d9a6af7d1df_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6907c7ddddf9ad9be39d226d2f021022e7cd01e6d45d58e12d3f8d9a6af7d1df_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes LGCLIP, a framework that uses an LLM to generate class-specific prompts for a diffusion model to create synthetic images as visual prototypes, enabling zero-shot classification with only a visual encoder. It eliminates the need for annotated image-text pairs and reduces model complexity. Experiments show LGCLIP is effective and efficient, establishing a new lightweight paradigm for classification.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [CLIP, semantic refinement mechanism, local token-patch alignment, attribute-object binding, compositional image-text matching]</li>
<li class=""><strong>authors:</strong> Qi Zhang, Yuxu Chen, Lei Deng, Lili Shen</li>
<li class=""><strong>institution:</strong> Sichuan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17178" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17178</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea9a6c4b5b64bc9b1ed96212005ee492c3fd2ae615fc413250d7a6ef7c3bc712_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea9a6c4b5b64bc9b1ed96212005ee492c3fd2ae615fc413250d7a6ef7c3bc712_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes ABE-CLIP, a training-free method to enhance attribute-object binding in CLIP models. It uses a semantic refinement mechanism to improve text token embeddings and a local token-patch alignment strategy to compute image-text similarity. Experiments show the method significantly improves compositional matching performance, even surpassing some trained approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] It is not always greener on the other side: Greenery perception across demographics and personalities in multiple cities</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [urban perception analysis], [Green View Index (GVI), street view imagery, pairwise ratings, perception survey]</li>
<li class=""><strong>authors:</strong> Matias Quintana, Fangqi Liu, Jussi Torkko, Youlong Gu, Xiucheng Liang, Yujun Hou, Koichi Ito, Yihan Zhu, Mahmoud Abdelrahman, Tuuli Toivonen, Yi Lu, Filip Biljecki</li>
<li class=""><strong>institution:</strong> Singapore-ETH Centre, City University of Hong Kong, University of Helsinki, National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17186" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17186</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes discrepancies between objective greenery measurements (like Green View Index from street view imagery) and subjective human perceptions collected via surveys across five countries. The main finding is that demographic and personality factors do not significantly influence perception, but the location where people live is a key factor, suggesting cultural and environmental experiences shape how urban greenery is observed.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Globally Optimal Solution to the Generalized Relative Pose Estimation Problem using Affine Correspondences</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [affine correspondences, global optimization, polynomial eigenvalue solver, generalized essential matrix, generalized camera model]</li>
<li class=""><strong>authors:</strong> Zhenbao Yu, Banglei Guan, Shunkun Liang, Zibin Liu, Yang Shang, Qifeng Yu</li>
<li class=""><strong>institution:</strong> National University of Defense Technology, Wuhan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17188" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17188</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2a684e0cd1c8feea699186681976c3ac00a45866e1c3b726129daedbf69d23e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2a684e0cd1c8feea699186681976c3ac00a45866e1c3b726129daedbf69d23e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a globally optimal solver for estimating the generalized relative pose of multi-camera systems using affine correspondences and a known vertical direction. The method decouples rotation and translation, formulates a cost function, and solves it via polynomial eigenvalue techniques. Experimental results on synthetic and real data show the method outperforms state-of-the-art approaches in accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Anatomical Region-Guided Contrastive Decoding: A Plug-and-Play Strategy for Mitigating Hallucinations in Medical VLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [contrastive decoding, anatomical mask, token re-weighting, attention re-weighting, logits re-weighting]</li>
<li class=""><strong>authors:</strong> Xiao Liang, Chenxi Liu, Zhi Ma, Di Wang, Bin Jing, Quan Wang, Yuanyuan Shi</li>
<li class=""><strong>institution:</strong> Xidian University, Capital Medical University, the Ninth Medical Center of the Chinese PLA General Hospital</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17189" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17189</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9ca8b7c91013f45d10742514e49b941b478dcfd8c7f8216c3572f801dc9f1f9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9ca8b7c91013f45d10742514e49b941b478dcfd8c7f8216c3572f801dc9f1f9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Anatomical Region-Guided Contrastive Decoding (ARCD), a plug-and-play method that uses an anatomical mask to guide a three-tiered contrastive decoding process at the token, attention, and logits levels to reduce hallucinations in Medical Vision-Language Models. Experiments across multiple medical imaging datasets show that ARCD effectively improves regional understanding, reduces factually incorrect outputs, and enhances diagnostic accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Fose: Fusion of One-Step Diffusion and End-to-End Network for Pansharpening</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [one-step distillation, lightweight ensemble blocks, four-stage training, pansharpening, diffusion model, end-to-end network]</li>
<li class=""><strong>authors:</strong> Kai Liu, Zeli Lin, Weibo Wang, Linghe Kong, Yulun Zhang</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17202" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17202</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Fose, a lightweight network for pansharpening that fuses a one-step diffusion model and an end-to-end network using a novel four-stage training strategy. It uses one-step distillation to compress a diffusion model&#x27;s inference from 50 steps to 1 and integrates it with an E2E model via lightweight ensemble blocks. The method achieves better performance than state-of-the-art approaches and a 7.42x speedup compared to the baseline diffusion model.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [latent modulation, variational autoencoder, reinforcement learning, supervised fine-tuning, reasoning strategies, controllable exploration]</li>
<li class=""><strong>authors:</strong> Rujiao Long, Yang Li, Xingyao Zhang, Weixun Wang, Tianqianjin Lin, Xi Zhao, Yuchi Xu, Wenbo Su, Junchi Yan, Bo Zheng</li>
<li class=""><strong>institution:</strong> Alibaba Group, Shanghai Jiao Tong University, Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17206" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17206</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bfcb7d02fac2a6f117f80403719551786b6b7722d6b86bf1338ef9e18ddda6b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bfcb7d02fac2a6f117f80403719551786b6b7722d6b86bf1338ef9e18ddda6b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Reasoning Palette, a framework that uses a latent variable from a VAE to modulate a model&#x27;s reasoning trajectory via prepended token prefixes, enabling diverse strategic exploration. It shows that this approach improves exploration efficiency in RL training and leads to consistent performance gains over standard methods on reasoning benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [reinforcement learning, group relative policy optimization, knowledge graph consistency, entity-relation matching, process supervision, hard-example mining]</li>
<li class=""><strong>authors:</strong> Xiao Liang, Yuxuan An, Di Wang, Jiawei Hu, Zhicheng Jiao, Bin Jing, Quan Wang</li>
<li class=""><strong>institution:</strong> Xidian University, Brown University, Capital Medical University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17213" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17213</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c86c0e9aa8fe8870d86c5f63baf1ccd89856e7434ece25e03ba384660733fc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c86c0e9aa8fe8870d86c5f63baf1ccd89856e7434ece25e03ba384660733fc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes CheXPO-v2, an alignment framework that uses a Knowledge Graph Consistency Reward for process supervision to reduce hallucinations in medical VLMs. It parses reasoning into structured triplets to penalize incoherent logic, outperforming prior methods on benchmarks with high data efficiency. The approach achieves state-of-the-art accuracy on MIMIC-CXR-VQA using only 5k samples.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Robust Scene Coordinate Regression via Geometrically-Consistent Global Descriptors</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [scene coordinate regression, global descriptors, covisibility graphs, contrastive loss, batch-mining]</li>
<li class=""><strong>authors:</strong> Son Tung Nguyen, Tobias Fischer, Alejandro Fontan, Michael Milford</li>
<li class=""><strong>institution:</strong> Queensland University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17226" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17226</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a method that learns global descriptors for visual localization by combining geometric structure and visual similarity, using a batch-mining strategy and modified contrastive loss to train without manual labels. This approach corrects errors from unreliable geometric constraints and improves disambiguation in large-scale environments. Experiments show significant gains in localization accuracy while maintaining computational and memory efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] DAVE: A VLM Vision Encoder for Document Understanding and Web Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [vision encoder, self-supervised pretraining, autoregressive pretraining, model merging, ensemble training, SigLIP2]</li>
<li class=""><strong>authors:</strong> Brandon Huang, Hang Hua, Zhuoran Yu, Trevor Darrell, Rogerio Feris, Roei Herzig</li>
<li class=""><strong>institution:</strong> MIT-IBM Watson AI Lab, UC Berkeley, University of Wisconsin–Madison</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17221" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17221</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/344c53941f8829675f0d24c7d720447ce983b23a243af49de527a0a89a83b47b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/344c53941f8829675f0d24c7d720447ce983b23a243af49de527a0a89a83b47b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces DAVE, a vision encoder for VLMs designed for document understanding and web agents. Its training pipeline uses self-supervised pretraining on unlabeled data followed by supervised autoregressive pretraining, enhanced by model-merging and ensemble techniques to improve compatibility and performance. Experiments show DAVE is an effective vision encoder for document and web applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning When to Look: A Disentangled Curriculum for Strategic Perception in Multimodal Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [curriculum learning, supervised fine-tuning, reinforcement learning, perception-grounded chain-of-thought, pivotal perception reward]</li>
<li class=""><strong>authors:</strong> Siqi Yang, Zilve Gao, Haibo Qiu, Fanfan Liu, Peng Shi, Zhixiong Zeng, Qingmin Liao, Lin Ma</li>
<li class=""><strong>institution:</strong> Meituan, Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17227" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17227</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b925dc42674866f1a5d50b8321678a682bbb13a5906e16c5013dcca31b9aea_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b925dc42674866f1a5d50b8321678a682bbb13a5906e16c5013dcca31b9aea_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a two-stage curriculum framework to address visual forgetting in multimodal reasoning. It first builds an abstract reasoning backbone using text-only data and then uses reinforcement learning with a novel reward to teach the model a strategic policy for when to perceive visual information. This approach transforms the model into a more strategic and visually grounded reasoner.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Any-Optical-Model: A Universal Foundation Model for Optical Remote Sensing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [remote sensing foundation model], [spectrum-independent tokenizer, multi-scale adaptive patch embedding, channel-wise self-supervised masking and reconstruction, multi-scale semantic alignment]</li>
<li class=""><strong>authors:</strong> Xuyang Li, Chenyu Li, Danfeng Hong</li>
<li class=""><strong>institution:</strong> Southeast University, Aerospace Information Research Institute, Chinese Academy of Sciences, University of Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17224" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17224</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed61cab861efd49f74d6151efc271fc265c8ca3f3dbd380d3539ee4626e62ea6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed61cab861efd49f74d6151efc271fc265c8ca3f3dbd380d3539ee4626e62ea6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Any-Optical-Model (AOM), a universal foundation model for optical remote sensing designed to handle arbitrary band compositions and spatial resolutions. Its core innovations include a spectrum-independent tokenizer, multi-scale adaptive patch embedding, and a channel-wise self-supervised pretraining strategy. Experiments show AOM achieves state-of-the-art performance in challenging scenarios like band missing and cross-sensor/cross-resolution settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Video Detective: Seek Critical Clues Recurrently to Answer Question from Long Videos</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [question-aware memory mechanism, recurrent processing, token compression, memory tokens, long video question-answering]</li>
<li class=""><strong>authors:</strong> Henghui Du, Chang Zhou, Chunjie Zhang, Xi Chen, Di Hu</li>
<li class=""><strong>institution:</strong> Renmin University of China, Tencent PCG</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17229" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17229</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c8fa371c579b373deada655507be872a0843b939c4fd62131127a51ed2f82f2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c8fa371c579b373deada655507be872a0843b939c4fd62131127a51ed2f82f2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes VideoDetective, a method that uses a question-aware memory mechanism to recurrently process long videos by compressing sub-segments into special memory tokens, enabling efficient question-answering. This approach allows models with limited context length to handle long videos with reduced memory and time. Experimental results show it effectively seeks critical clues from massive video information.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Mitty: Diffusion-based Human-to-Robot Video Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [robotics and computer vision], [diffusion transformer, in-context learning, video generation, human-to-robot translation, bidirectional attention]</li>
<li class=""><strong>authors:</strong> Yiren Song, Cheng Liu, Weijia Mao, Mike Zheng Shou</li>
<li class=""><strong>institution:</strong> National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17253" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17253</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ece085e6946a833f7d094099001b4940be20339b6e48e0def90236df73a83e6d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ece085e6946a833f7d094099001b4940be20339b6e48e0def90236df73a83e6d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Mitty is a Diffusion Transformer model that uses in-context learning to directly convert human demonstration videos into robot-execution videos without intermediate representations. It leverages a pretrained video diffusion model and an automatic synthesis pipeline to address data scarcity. The method achieves state-of-the-art performance and strong generalization in robot learning from human observations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] AnyCXR: Human Anatomy Segmentation of Chest X-ray at Any Acquisition Position using Multi-stage Domain Randomized Synthetic Data with Imperfect Annotations and Conditional Joint Annotation Regularization Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical imaging segmentation], [Multi-stage Domain Randomization (MSDR), Conditional Joint Annotation Regularization (CAR), synthetic data generation, zero-shot generalization, anatomical consistency]</li>
<li class=""><strong>authors:</strong> Dong Zifei, Wu Wenjie, Hao Jinkui, Chen Tianqi, Weng Ziqiao, Zhou Bo</li>
<li class=""><strong>institution:</strong> Northwestern University, Vanderbilt University, Shanxi Medical University, University of Sydney</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17263" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17263</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes AnyCXR, a framework for chest X-ray segmentation that uses synthetic data generated via Multi-stage Domain Randomization and a Conditional Joint Annotation Regularization learning strategy to handle imperfect labels. It demonstrates strong zero-shot generalization to real-world X-rays across different views, enabling accurate multi-organ segmentation and improving downstream clinical tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] WDFFU-Mamba: A Wavelet-guided Dual-attention Feature Fusion Mamba for Breast Tumor Segmentation in Ultrasound Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical image segmentation], [wavelet-guided enhancement, dual-attention feature fusion, U-shaped Mamba architecture, Wavelet-denoised High-Frequency-guided Feature (WHF), Dual Attention Feature Fusion (DAFF)]</li>
<li class=""><strong>authors:</strong> Guoping Cai, Houjin Chen, Yanfeng Li, Jia Sun, Ziwei Chen, Qingzi Geng</li>
<li class=""><strong>institution:</strong> Beijing Jiaotong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17278" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17278</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes WDFFU-Mamba, a novel network for breast ultrasound image segmentation that integrates wavelet-guided enhancement and dual-attention feature fusion within a U-shaped Mamba architecture. It demonstrates superior segmentation accuracy and robustness on public datasets, making it a promising tool for clinical applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Diagnostic Performance of Universal-Learning Ultrasound AI Across Multiple Organs and Tasks: the UUSIC25 Challenge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [deep learning, multi-task learning, domain generalization, segmentation, classification, Dice Similarity Coefficient, Area Under the Curve]</li>
<li class=""><strong>authors:</strong> Zehui Lin, Luyi Han, Xin Wang, Ying Zhou, Yanming Zhang, Tianyu Zhang, Lingyun Bao, Shandong Wu, Dong Xu, Tao Tan, UUSIC25 Challenge Consortium</li>
<li class=""><strong>institution:</strong> Macao Polytechnic University, Netherlands Cancer Institute, Zhejiang Cancer Hospital, The First People’s Hospital of Hangzhou, University of Pittsburgh</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17279" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17279</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates general-purpose deep learning models for multi-organ classification and segmentation in ultrasound imaging through the UUSIC25 challenge. The top model demonstrated high accuracy and efficiency across tasks using a single architecture. However, performance degraded on data from unseen institutions, highlighting the critical need for improved domain generalization before clinical deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Vision-Language Model Guided Image Restoration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [vision-language model, CLIP, diffusion model, cross-attention, LoRA fine-tuning, degradation predictor]</li>
<li class=""><strong>authors:</strong> Cuixin Yang, Rongkang Dong, Kin-Man Lam</li>
<li class=""><strong>institution:</strong> The Hong Kong Polytechnic University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17292" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17292</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abd921f37f90b9dfaf497e986d1ae89307cf4b11dd527bf09bd11e36d239652c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abd921f37f90b9dfaf497e986d1ae89307cf4b11dd527bf09bd11e36d239652c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes the VLMIR framework, which uses a vision-language model (like CLIP) to extract aligned visual and textual features from images and integrates them via cross-attention into a diffusion model for restoration. It concludes that this approach, leveraging linguistic priors for semantic coherence, achieves superior performance in universal and degradation-specific image restoration tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [feature caching, selective computation, constraint-aware scheduling, temporal redundancy, activation schedule]</li>
<li class=""><strong>authors:</strong> Fanpu Cao, Yaofo Chen, Zeng You, Wei Luo, Cen Chen</li>
<li class=""><strong>institution:</strong> South China University of Technology, South China Agricultural University, Pazhou Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17298" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17298</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af7870aa86d8ec0f40bf8ee51c36b05d76a3b23aeb26f3f88d6835a77ed1a314_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af7870aa86d8ec0f40bf8ee51c36b05d76a3b23aeb26f3f88d6835a77ed1a314_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes ProCache, a training-free framework to accelerate Diffusion Transformers (DiTs) by using dynamic feature caching. It introduces a constraint-aware caching pattern search to create non-uniform activation schedules and a selective computation module to mitigate error accumulation. Experiments show ProCache achieves significant speedups with minimal quality loss, outperforming prior caching methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Towards Pixel-Wise Anomaly Location for High-Resolution PCBA \ via Self-Supervised Image Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [self-supervised learning, image reconstruction, anomaly detection, SIR-Gate, ROPS]</li>
<li class=""><strong>authors:</strong> Wuyi Liu, Le Jin, Junxian Yang, Yuanchao Yu, Zishuo Peng, Jinfeng Xu, Xianzhi Li, Jun Zhou</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology, Siemens AG, University of Electronic Science and Technology of China, Peking University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17296" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17296</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d229cd3ccc08641839e524f99e7548770f7ccd524f2ac8dd0882a9248c5c358_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d229cd3ccc08641839e524f99e7548770f7ccd524f2ac8dd0882a9248c5c358_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces HiSIR-Net, a self-supervised image reconstruction framework for pixel-wise anomaly localization on high-resolution PCBA images. It uses two novel modules—SIR-Gate to reduce reconstruction noise and ROPS for coherent patch selection—to achieve accurate defect detection with low false positives. The method demonstrates superior performance on a new dataset (SIPCBA-500) and public benchmarks while maintaining practical inference speed.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] EMAG: Self-Rectifying Diffusion Sampling with Exponential Moving Average Guidance</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [diffusion models], [classifier-free guidance, attention modification, exponential moving average, adaptive layer selection, diffusion transformers]</li>
<li class=""><strong>authors:</strong> Ankit Yadav, Ta Duc Huy, Lingqiao Liu</li>
<li class=""><strong>institution:</strong> Australian Institute for Machine Learning, The University of Adelaide</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17303" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17303</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a085ffcd94c7d87433d4ff44b17cee0378a874440ce80df0ee2bca3a5e2171d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a085ffcd94c7d87433d4ff44b17cee0378a874440ce80df0ee2bca3a5e2171d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Exponential Moving Average Guidance (EMAG), a training-free inference method for diffusion transformers that adaptively modifies attention maps to generate challenging negative samples. This allows the model to correct fine-grained artifacts, improving image quality and human preference scores over standard guidance. The method is shown to be compatible with other advanced guidance techniques for further gains.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [chain-of-thought, multi-turn reasoning, self-reflection, redundancy-penalized policy optimization, supervised fine-tuning, reinforcement learning]</li>
<li class=""><strong>authors:</strong> Wenhao Yang, Yu Xia, Jinlong Huang, Shiyin Lu, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Yuanyu Wan, Lijun Zhang</li>
<li class=""><strong>institution:</strong> Nanjing University, Alibaba Group, Shanghai Jiao Tong University, Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17306" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17306</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/689b25012cac991ff522614c103b42d1e5450440f1eed392ffaf92e6a3ff7c51_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/689b25012cac991ff522614c103b42d1e5450440f1eed392ffaf92e6a3ff7c51_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes DRIM, a model that enhances multi-turn reasoning in vision-language models by integrating a self-reflective chain-of-thought with tool invocation. It uses a three-stage pipeline of data construction, supervised fine-tuning, and redundancy-penalized reinforcement learning to encourage reliable exploration and self-correction. Experiments show DRIM achieves superior performance on visual understanding benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] CodeDance: A Dynamic Tool-integrated MLLM for Executable Visual Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [executable code, tool orchestration, reinforcement learning, visual reasoning, reward shaping]</li>
<li class=""><strong>authors:</strong> Qi Song, Honglin Li, Yingchen Yu, Haoyi Zhou, Lin Yang, Song Bai, Qi She, Zilong Huang, Yunqing Zhao</li>
<li class=""><strong>institution:</strong> Beihang University, Westlake University, ByteDance</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17312" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17312</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51476eaff011f154f477005e3f785b652c69da471d13c3f8a556338b397999d8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51476eaff011f154f477005e3f785b652c69da471d13c3f8a556338b397999d8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces CodeDance, a multimodal large language model that uses executable code to dynamically orchestrate multiple tools for visual reasoning. It employs a reinforcement learning reward to balance tool use, leading to adaptive and efficient reasoning. The method outperforms schema-driven and text-only baselines, and even surpasses advanced closed models like GPT-4o on various benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MatLat: Material Latent Space for PBR Texture Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [latent diffusion, VAE fine-tuning, material latent space, patch-based regularization, correspondence-aware attention]</li>
<li class=""><strong>authors:</strong> Kyeongmin Yeo, Yunhong Min, Jaihoon Kim, Minhyuk Sung</li>
<li class=""><strong>institution:</strong> KAIST</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17302" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17302</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/706f201e005d8752d9c73c8781f7323062cdfffadcfa51a8faf2f6cc621472c2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/706f201e005d8752d9c73c8781f7323062cdfffadcfa51a8faf2f6cc621472c2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes MatLat, a generative framework that fine-tunes a pretrained VAE to create a material latent space for generating high-quality PBR textures, addressing dataset scarcity and distribution shift issues. It introduces a patch-based regularization during VAE fine-tuning to preserve spatial locality between latent codes and image pixels, which is crucial for cross-view consistency. The method demonstrates improved PBR texture fidelity and achieves state-of-the-art performance, with each component being essential.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Auxiliary Descriptive Knowledge for Few-Shot Adaptation of Vision-Language Model</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [vision-language models], [few-shot adaptation, parameter-efficient fine-tuning, auxiliary descriptive knowledge, large language model, non-parametric attention, compositional knowledge, instance-specific knowledge]</li>
<li class=""><strong>authors:</strong> SuBeen Lee, GilHan Park, WonJun Moon, Hyun Seok Seong, Jae-Pil Heo</li>
<li class=""><strong>institution:</strong> Sungkyunkwan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17313" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17313</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2fe4006b163d54a1cba24642885d5db064fb85d1d01ab783d4dee1ada2fbc75_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2fe4006b163d54a1cba24642885d5db064fb85d1d01ab783d4dee1ada2fbc75_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Auxiliary Descriptive Knowledge (ADK), a framework that enhances few-shot adaptation of vision-language models by using an LLM to generate offline descriptive prompts for each class. ADK enriches text representations through averaged compositional knowledge and a lightweight attention mechanism for instance-specific knowledge, improving classification without added inference cost. It consistently boosts existing parameter-efficient fine-tuning methods, achieving state-of-the-art performance across various scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [remote sensing, multimodal evaluation], [RSHR-Bench, adversarial filtering, high-resolution imagery, multimodal large language models, visual question answering, image captioning]</li>
<li class=""><strong>authors:</strong> Yunkai Dang, Meiyi Zhu, Donghao Wang, Yizhuo Zhang, Jiacheng Yang, Qi Fan, Yuekun Yang, Wenbin Li, Feng Miao, Yang Gao</li>
<li class=""><strong>institution:</strong> Nanjing University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17319" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17319</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/283e413d1a1fd46323ee20a12df05789e8ef6dd69af3578d2d3e3e27ce803395_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/283e413d1a1fd46323ee20a12df05789e8ef6dd69af3578d2d3e3e27ce803395_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces RSHR-Bench, a super-high-resolution benchmark for evaluating multimodal large language models (MLLMs) in remote sensing. The benchmark uses adversarial filtering with strong LLMs and human verification to create tasks that reduce reliance on language priors and require genuine visual understanding. The evaluation reveals that existing models, including RS-specific ones, show significant performance gaps when handling ultra-high-resolution remote sensing imagery.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] EMMA: Concept Erasure Benchmark with Comprehensive Semantic Metrics and Diverse Categories</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [concept erasure, text-to-image generation, benchmark evaluation, semantic metrics, robustness testing, bias analysis]</li>
<li class=""><strong>authors:</strong> Lu Wei, Yuta Nakashima, Noa Garcia</li>
<li class=""><strong>institution:</strong> Osaka University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17320" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17320</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/203b001da1851854025aaa0d66f1fc1bc1e32821cd9e1295f8360da31919f60c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/203b001da1851854025aaa0d66f1fc1bc1e32821cd9e1295f8360da31919f60c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces EMMA, a comprehensive benchmark for evaluating concept erasure techniques in text-to-image diffusion models. It tests five key dimensions across 12 metrics, including robustness to indirect prompts and bias. The main conclusion is that existing erasure methods struggle with implicit descriptions and visually similar concepts, and some can amplify gender and ethnicity bias.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Democratizing Pathology Co-Pilots: An Open Pipeline and Dataset for Whole-Slide Vision-Language Modelling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [vision-language model, instruction tuning, visual question answering, whole-slide images, synthetic instruction generation]</li>
<li class=""><strong>authors:</strong> Sander Moonemans, Sebastiaan Ram, Frédérique Meeuwsen, Carlijn Lems, Jeroen van der Laak, Geert Litjens, Francesco Ciompi</li>
<li class=""><strong>institution:</strong> Radboud University Medical Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17326" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17326</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Polysome, a tool for generating synthetic instructions, and uses it to create HISTAI-Instruct, a large instruction-tuning dataset from whole-slide images. It then trains a vision-language model called ANTONI-α on this dataset, which is shown to outperform MedGemma on tasks like tissue identification and differential diagnosis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Rotterdam artery-vein segmentation (RAV) dataset</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical imaging], [color fundus images, artery-vein segmentation, machine learning algorithms, connectivity validation, custom annotation interface]</li>
<li class=""><strong>authors:</strong> Jose Vargas Quiros, Bart Liefers, Karin van Garderen, Jeroen Vermeulen, Eyened Reading Center, Caroline Klaver</li>
<li class=""><strong>institution:</strong> Erasmus University Medical Center, Radboud University Medical Center, Institute of Molecular and Clinical Ophthalmology, University of Basel</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17322" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17322</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the Rotterdam Artery-Vein segmentation dataset, created by sampling and annotating color fundus images from the Rotterdam Study using a custom interface with connectivity validation tools. The dataset includes diverse, high-quality artery-vein segmentation masks and varied image modalities to support the development of robust machine learning models for retinal vascular analysis. The main conclusion is that this heterogeneous dataset enables benchmarking and training of clinically applicable algorithms under real-world variability in image quality and acquisition.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] DESSERT: Diffusion-based Event-driven Single-frame Synthesis via Residual Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [diffusion model, residual training, variational autoencoder, event camera, frame synthesis]</li>
<li class=""><strong>authors:</strong> Jiyun Kong, Jun-Hyuk Kim, Jong-Seok Lee</li>
<li class=""><strong>institution:</strong> Yonsei University, Chung-Ang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17323" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17323</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9efc7d352cf45ecba0837aaa2807f3af645c0b85f2e4138960afaf786d151d9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9efc7d352cf45ecba0837aaa2807f3af645c0b85f2e4138960afaf786d151d9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes DESSERT, a method for event-driven video frame synthesis using a diffusion model trained on inter-frame residuals and conditioned on event data. It employs a two-stage pipeline with an Event-to-Residual Alignment VAE and a diffusion model, enhanced by Diverse-Length Temporal augmentation. The method outperforms existing approaches in producing sharper and more temporally consistent frames.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Multi-level distortion-aware deformable network for omnidirectional image super-resolution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [deformable attention, dilated deformable convolution, low-rank decomposition, multi-level feature fusion]</li>
<li class=""><strong>authors:</strong> Cuixin Yang, Rongkang Dong, Kin-Man Lam, Yuhang Zhang, Guoping Qiu</li>
<li class=""><strong>institution:</strong> The Hong Kong Polytechnic University, Guangzhou University, University of Nottingham</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17343" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17343</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c76dd9c20ef0f6f5224210965ff34e6934829551ddedeec3fc607a5c362844_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c76dd9c20ef0f6f5224210965ff34e6934829551ddedeec3fc607a5c362844_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a Multi-level Distortion-aware Deformable Network (MDDN) for omnidirectional image super-resolution, which uses parallel branches of deformable attention and dilated deformable convolutions to capture geometric distortions. It also employs a low-rank decomposition to reduce computational cost. Experiments show that MDDN outperforms existing state-of-the-art methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SynergyWarpNet: Attention-Guided Cooperative Warping for Neural Portrait Animation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [attention-guided cooperative warping, 3D dense optical flow, cross-attention, 3D keypoints, confidence-guided fusion]</li>
<li class=""><strong>authors:</strong> Shihang Li, Zhiqiang Gong, Minming Ye, Yue Gao, Wen Yao</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Defense Innovation Institute Academy of Military Science, Intelligent Game and Decision Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17331" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17331</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5df9fe07b541e5d7185f0fbd4b4122b096d1ab08ec897c73021adba9ad3835b5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5df9fe07b541e5d7185f0fbd4b4122b096d1ab08ec897c73021adba9ad3835b5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SynergyWarpNet, a three-stage framework for neural portrait animation that combines explicit warping using 3D optical flow with reference-augmented correction via cross-attention and a confidence-guided fusion module. It aims to address the limitations of traditional warping and attention-based methods by integrating geometric alignment with semantic completion. The model achieves state-of-the-art performance on benchmark datasets for high-fidelity talking head synthesis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Beyond Semantic Features: Pixel-level Mapping for Generalized AI-Generated Image Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [pixel-level mapping, high-frequency traces, semantic bias, cross-generator generalization]</li>
<li class=""><strong>authors:</strong> Chenming Zhou, Jiaan Wang, Yu Li, Lei Li, Juan Cao, Sheng Tang</li>
<li class=""><strong>institution:</strong> Institute of Computing Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17350" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17350</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12f826d61dc8f5ba342ef7b8b646a6c2d30576e4692242121c7c5e8cc16f5f7d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12f826d61dc8f5ba342ef7b8b646a6c2d30576e4692242121c7c5e8cc16f5f7d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a pixel-level mapping pre-processing step to disrupt pixel value distributions and break semantic shortcuts, forcing detectors to focus on generalizable high-frequency traces from image generation. This method significantly improves the cross-generator performance of state-of-the-art AI-generated image detectors, verifying that disrupting semantic cues is key to generalization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Towards Deeper Emotional Reflection: Crafting Affective Image Filters with Generative Priors</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [affective image filter, multi-modal transformer, diffusion models, emotional fidelity, content consistency]</li>
<li class=""><strong>authors:</strong> Peixuan Zhang, Shuchen Weng, Jiajun Tang, Si Li, Boxin Shi</li>
<li class=""><strong>institution:</strong> Beijing University of Posts and Telecommunications, Beijing Academy of Artificial Intelligence, Peking University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17376" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17376</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d7baeb5807dd11748bebc6c7cf63706b9492484a047cbe2a12cba4c75d153a1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d7baeb5807dd11748bebc6c7cf63706b9492484a047cbe2a12cba4c75d153a1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes AIF-D, an Affective Image Filter model that extends a multi-modal transformer baseline by leveraging generative priors from pre-trained large-scale diffusion models to reflect emotions from text into images. It demonstrates superior performance in content consistency and emotional fidelity compared to state-of-the-art methods and is more effective at evoking specific emotions according to user studies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [vision-language models], [CulturalToM-VQA, visual question answering, chain-of-thought prompting, compositional chain-of-thought prompting, false belief reasoning, social desirability bias]</li>
<li class=""><strong>authors:</strong> Zabir Al Nazi, G M Shahariar, Abrar Hossain, Wei Peng</li>
<li class=""><strong>institution:</strong> University of California, Riverside, University of Dhaka, Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17394" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17394</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces CulturalToM-VQA, a benchmark dataset built via a VLM-assisted human-in-the-loop pipeline to evaluate cross-cultural Theory of Mind reasoning in Vision-Language Models. It finds that while newer VLMs show strong performance on explicit tasks, they systematically struggle with false belief reasoning, and their results may be inflated by social desirability bias rather than genuine visual understanding.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [visual question answering, vision-language models, fine-tuning, benchmark dataset, CT, MRI]</li>
<li class=""><strong>authors:</strong> Léo Butsanets, Charles Corbière, Julien Khlaut, Pierre Manceron, Corentin Dancette</li>
<li class=""><strong>institution:</strong> Raidium, Université de Paris Cité, Hôpital Européen Georges Pompidou, AP-HP, INSERM</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17396" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17396</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ed7cbd6c6a04ef8f9a23147e56923de1ca94a48cc51c20cfa98200b90baa146_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ed7cbd6c6a04ef8f9a23147e56923de1ca94a48cc51c20cfa98200b90baa146_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces RadImageNet-VQA, a large-scale CT and MRI dataset with expert-curated annotations for radiologic visual question answering, designed to evaluate vision-language models on tasks like abnormality detection and pathology identification. Experiments show that current models struggle with fine-grained pathology identification, especially in open-ended settings, and the dataset avoids linguistic shortcuts as models perform near-random without image inputs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Beyond Occlusion: In Search for Near Real-Time Explainability of CNN-Based Prostate Cancer Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [occlusion, GradCAM++, HiResCAM, Composite-L, CAM, explainable AI, convolutional neural networks]</li>
<li class=""><strong>authors:</strong> Martin Krebs, Jan Obdržálek, Vít Musil, Tomáš Brázdil</li>
<li class=""><strong>institution:</strong> Masaryk University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17416" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17416</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper seeks a faster alternative to the occlusion method for explaining CNN-based prostate cancer classification. By establishing comparison criteria and metrics, the authors evaluate several single-pass explanation methods like GradCAM++ and HiResCAM. They identify a method that reduces explanation time by at least a factor of 10 without compromising output quality, facilitating faster model development and clinical adoption.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] AIFloodSense: A Global Aerial Imagery Dataset for Semantic Segmentation and Understanding of Flooded Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision, disaster management], [semantic segmentation, visual question answering, image classification, deep learning, aerial imagery]</li>
<li class=""><strong>authors:</strong> Georgios Simantiris, Konstantinos Bacharidis, Apostolos Papanikolaou, Petros Giannakakis, Costas Panagiotakis</li>
<li class=""><strong>institution:</strong> Hellenic Mediterranean University, Institute of Computer Science, FORTH</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17432" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17432</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces AIFloodSense, a global aerial imagery dataset for flood detection, supporting tasks like semantic segmentation, image classification, and visual question answering. It establishes baseline benchmarks using state-of-the-art deep learning models. The main conclusion is that the dataset&#x27;s global diversity and multi-task support advance the development of robust, domain-generalized AI tools for climate resilience and disaster assessment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Xiaomi MiMo-VL-Miloco Technical Report</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [supervised fine-tuning, reinforcement learning, Group Relative Policy Optimization, chain-of-thought supervision, token-budget-aware reasoning, quantization]</li>
<li class=""><strong>authors:</strong> Jiaze Li, Jingyang Chen, Yuxun Qu, Jianzhong Ju, Zhenbo Luo, Jian Luan, Shijie Xu, Zhenru Lin, Junyou Zhu, Boshen Xu, Wenhui Tan, Pei Fu</li>
<li class=""><strong>institution:</strong> Xiaomi</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17436" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17436</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5131a57d5cef340803d1dd4e55e39db8e4fc7a8b7925e87579be976c079279c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5131a57d5cef340803d1dd4e55e39db8e4fc7a8b7925e87579be976c079279c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MiMo-VL-Miloco-7B, a vision-language model specialized for smart-home understanding, built via a two-stage training pipeline combining supervised fine-tuning and reinforcement learning. The model achieves leading performance on home-scenario tasks like gesture recognition and also shows gains on general multimodal and language reasoning benchmarks. The authors conclude that targeted home-scenario training enhances activity understanding and can improve text-only reasoning with minimal trade-offs on other tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] LangDriveCTRL: Natural Language Controllable Driving Scene Editing with Multi-modal Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [scene graph, agentic pipeline, video diffusion, 3D scene decomposition, natural language control, trajectory generation]</li>
<li class=""><strong>authors:</strong> Yun He, Francesco Pittaluga, Ziyu Jiang, Matthias Zwicker, Manmohan Chandraker, Zaid Tasneem</li>
<li class=""><strong>institution:</strong> University of Maryland, College Park, NEC Labs America, UC San Diego</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17445" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17445</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0247e2a8c71687baf6433034fd351bc4574e288c71df25dcc17678102ec16d05_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0247e2a8c71687baf6433034fd351bc4574e288c71df25dcc17678102ec16d05_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LangDriveCTRL is a framework that edits real-world driving videos using natural language instructions. It decomposes scenes into 3D graphs and uses an agentic pipeline with specialized modules for object grounding, behavior editing, and review, followed by video diffusion for refinement. The method achieves significantly higher instruction alignment and realism compared to previous state-of-the-art approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [multimodal semantic segmentation, deep neural network, robust training strategies, synchronized sensor data, daytime training for nighttime performance]</li>
<li class=""><strong>authors:</strong> Jon Muhovič, Janez Perš</li>
<li class=""><strong>institution:</strong> University of Ljubljana</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17450" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17450</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6eb14eac17bf3aa4e9ce145e08ce4eae06c5adaaf8ccf31ca3fa25a7f3835fa0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6eb14eac17bf3aa4e9ce145e08ce4eae06c5adaaf8ccf31ca3fa25a7f3835fa0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MULTIAQUA, a multimodal maritime dataset with synchronized RGB, thermal, IR, and LIDAR data, and proposes robust training strategies for multimodal semantic segmentation. The core method enables training a deep neural network using only daytime images to achieve reliable performance in challenging conditions like near-complete darkness. The main conclusion is that this approach simplifies data acquisition and annotation while maintaining robust scene interpretation for unmanned surface vehicles.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] 3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [3D scene reconstruction, generative models, differentiable optimization, compositional framework, camera recovery, spatial optimization]</li>
<li class=""><strong>authors:</strong> Tobias Sautter, Jan-Niklas Dihlmann, Hendrik P.A. Lensch</li>
<li class=""><strong>institution:</strong> University of Tübingen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17459" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17459</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a86f0bc63e56b2eb3e9a7eb63d3a03a4eff7de9fc52eba3ee24958d82a3a3b98_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a86f0bc63e56b2eb3e9a7eb63d3a03a4eff7de9fc52eba3ee24958d82a3a3b98_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> 3D-RE-GEN is a compositional framework that reconstructs a single image into textured 3D objects and a background by integrating models for detection, reconstruction, and placement, using generative models for occluded objects and a novel 4-DoF optimization for layout alignment. It achieves state-of-the-art performance in single-image 3D scene reconstruction, producing coherent, modifiable scenes suitable for VFX and game development.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, digital twin, Vision Transformer, ViT-UNet, privacy-preserving AI, brain tumor segmentation]</li>
<li class=""><strong>authors:</strong> Almustapha A. Wakili, Adamu Hussaini, Abubakar A. Musa, Woosub Jung, Wei Yu</li>
<li class=""><strong>institution:</strong> Towson University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17488" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17488</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67c12280225e186b761a6952a367a2b042a6fcd1886ac481e9bc15f5f81ee64a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67c12280225e186b761a6952a367a2b042a6fcd1886ac481e9bc15f5f81ee64a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes TwinSegNet, a federated learning framework that uses a hybrid ViT-UNet model and personalized digital twins for brain tumor segmentation without sharing raw data. It achieves high accuracy on heterogeneous MRI datasets, demonstrating that privacy can be preserved without sacrificing segmentation performance in multi-institutional settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Validation of Diagnostic Artificial Intelligence Models for Prostate Pathology in a Middle Eastern Cohort</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical diagnostics], [artificial intelligence, digital pathology, prostate cancer, Gleason grading, external validation, Cohen&#x27;s quadratically weighted kappa, compact scanner]</li>
<li class=""><strong>authors:</strong> Peshawa J. Muhammad Ali, Navin Vincent, Saman S. Abdulla, Han N. Mohammed Fadhl, Anders Blilie, Kelvin Szolnoky, Julia Anna Mielcarz, Xiaoyi Ji, Nita Mulliqi, Abdulbasit K. Al-Talabani, Kimmo Kartasalo</li>
<li class=""><strong>institution:</strong> Koya University, Karolinska Institutet</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17499" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17499</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This study externally validated AI models for prostate cancer diagnosis and Gleason grading using a Middle Eastern cohort from Iraq. The AI models demonstrated performance comparable to pathologists and showed high consistency across different digital slide scanners, including low-cost compact models. The findings support the global adoption of AI in pathology, particularly in under-represented regions with limited resources.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] LumiCtrl : Learning Illuminant Prompts for Lighting Control in Personalized Text-to-Image Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [illuminant personalization, physics-based illuminant augmentation, edge-guided prompt disentanglement, masked reconstruction loss, contextual light adaptation]</li>
<li class=""><strong>authors:</strong> Muhammad Atif Butt, Kai Wang, Javier Vazquez-Corral, Joost Van De Weijer</li>
<li class=""><strong>institution:</strong> Computer Vision Center, Universitat Autònoma de Barcelona, City University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17489" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17489</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LumiCtrl is a method for learning illuminant prompts from a single object image to control lighting in personalized text-to-image models. It uses physics-based augmentation, edge-guided prompt disentanglement, and a masked reconstruction loss to achieve contextual light adaptation. The method outperforms existing baselines in illuminant fidelity, aesthetic quality, and scene coherence, as confirmed by a human preference study.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MMLANDMARKS: a Cross-View Instance-Level Benchmark for Geo-Spatial Understanding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multimodal geo-spatial understanding], [cross-view retrieval, geolocalization, CLIP-inspired baseline, multimodal dataset, instance-level benchmark]</li>
<li class=""><strong>authors:</strong> Oskar Kristoffersen, Alba R. Sánchez, Morten R. Hannemose, Anders B. Dahl, Dim P. Papadopoulos</li>
<li class=""><strong>institution:</strong> Technical University of Denmark, Pioneer Center for AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17492" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17492</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80d6684c681e0605e017a84878993e58d1f060cd6906ad893ab11cebf1a5f9d2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80d6684c681e0605e017a84878993e58d1f060cd6906ad893ab11cebf1a5f9d2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MMLANDMARKS, a multimodal dataset with aerial images, ground-view images, text, and GPS coordinates for geo-spatial tasks. Using a simple CLIP-inspired baseline, the authors show competitive performance across tasks like cross-view retrieval and geolocalization, highlighting the need for multimodal datasets for comprehensive geo-spatial understanding.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [4D scene geometry, diffusion-based video generation, occlusion consistency, illumination-aware dataset, mask generation]</li>
<li class=""><strong>authors:</strong> Hoiyeong Jin, Hyojin Jang, Jeongho Kim, Junha Hyung, Kinam Kim, Dongjin Kim, Huijin Choi, Hyeonji Kim, Jaegul Choo</li>
<li class=""><strong>institution:</strong> KAIST AI, SK Telecom</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17504" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17504</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f1824f6e52cce42d09258c21a8f994f87c3330105a845886e13a668bacd45e38_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f1824f6e52cce42d09258c21a8f994f87c3330105a845886e13a668bacd45e38_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents InsertAnywhere, a framework for realistic video object insertion that combines 4D scene geometry reconstruction with a diffusion-based video generation model to ensure geometric and temporal consistency. It introduces a synthetic dataset, ROSE++, for supervised training. The method outperforms existing models in producing visually coherent insertions suitable for production environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [sensor fusion and filtering], [Error-State Extended Kalman Filter, Scaled Unscented Kalman Filter, visual-inertial odometry, quaternion estimation, adaptive covariance, loosely coupled architecture]</li>
<li class=""><strong>authors:</strong> Ufuk Asil, Efendi Nasibov</li>
<li class=""><strong>institution:</strong> Dokuz Eylul University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17505" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17505</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0950fd135a7ca6f717b1ee73ea881e6e27ca5075f9e2e7ca11cb3c42ff286cab_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0950fd135a7ca6f717b1ee73ea881e6e27ca5075f9e2e7ca11cb3c42ff286cab_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid VIO method that combines an Error-State EKF with a targeted Scaled UKF step for orientation refinement, while dynamically adjusting visual measurement noise based on image quality metrics. The approach achieves significant improvements in accuracy over ESKF-based methods and reduces computational cost compared to a full UKF, balancing efficiency and performance in challenging UAV environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multiple instance learning, AutoML, feature extraction, whole-slide images, benchmarking, computational pathology]</li>
<li class=""><strong>authors:</strong> Siemen Brussee, Pieter A. Valkema, Jurre A. J. Weijer, Thom Doeleman, Anne M.R. Schrader, Jesper Kers</li>
<li class=""><strong>institution:</strong> Leiden University Medical Center, Utrecht University Medical Center, Amsterdam University Medical Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17517" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17517</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces PathBench-MIL, an automated machine learning and benchmarking framework designed for Multiple Instance Learning in histopathology. It automates the entire pipeline from preprocessing to model aggregation, enabling standardized and reproducible evaluation of various models and feature extractors on whole-slide image datasets. The main conclusion is that this open-source framework facilitates rapid experimentation and standardization in computational pathology research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [source-free object detection, spatial prior-aware regularization, imbalance-aware noise robust pseudo-labeling, mean-teacher, OV-SAM, domain shift]</li>
<li class=""><strong>authors:</strong> Sairam VCR, Rishabh Lalla, Aveen Dayal, Tejal Kulkarni, Anuj Lalla, Vineeth N Balasubramanian, Muhammad Haris Khan</li>
<li class=""><strong>institution:</strong> IIT Hyderabad, MBZUAI, UC San Diego, IIT Jodhpur, Microsoft Research India</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17514" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17514</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ce0955de2a0c97faf7100d95c356843de584cdb63f24134f9bcbf1fef3e760a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ce0955de2a0c97faf7100d95c356843de584cdb63f24134f9bcbf1fef3e760a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FALCON-SFOD, a framework for source-free object detection that uses foundation model priors (SPAR) to enhance object-focused features and a noise-robust pseudo-labeling method (IRPL) to handle class imbalance. It concludes that this approach strengthens the feature space against domain shift, leading to more reliable pseudo-labels and competitive benchmark performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multimodal evaluation], [visual grounding, benchmark, multimodal large language models, discriminative, spatial, limited, rejection, test-time scaling, data-mixture training]</li>
<li class=""><strong>authors:</strong> Rang Li, Lei Li, Shuhuai Ren, Hao Tian, Shuhao Gu, Shicheng Li, Zihao Yue, Yudong Wang, Wenhan Ma, Zhe Yang, Jingyuan Ma, Zhifang Sui, Fuli Luo</li>
<li class=""><strong>institution:</strong> Peking University, Xiaomi, The University of Hong Kong, Renmin University of China</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17495" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17495</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4020de87a870a946901821a37d57ee80ad4531d2b38356ac7601cb21450e17c4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4020de87a870a946901821a37d57ee80ad4531d2b38356ac7601cb21450e17c4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces GroundingME, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) across four challenging dimensions: discriminative, spatial, limited, and rejection tasks. The evaluation of 25 MLLMs reveals a significant performance gap, with the best model achieving only 45.1% accuracy and most failing on rejection tasks by hallucinating objects. The authors propose test-time scaling and data-mixture training as strategies to partially improve model performance on these complex grounding challenges.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] FLEG: Feed-Forward Language Embedded Gaussian Splatting from Any Views</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [3D reconstruction and language embedding], [feed-forward network, 3D Gaussians, instance-guided contrastive learning, geometry-semantic hierarchical sparsification, 2D-to-3D lifting]</li>
<li class=""><strong>authors:</strong> Qijian Tian, Xin Tan, Jiayu Ying, Xuhong Wang, Yuan Xie, Lizhuang Ma</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, East China Normal University, Shanghai Artificial Intelligence Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17541" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17541</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9847bc8d208db80daeeb6fce65f8b727d7e037fbf74ca700b812950d807f8585_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9847bc8d208db80daeeb6fce65f8b727d7e037fbf74ca700b812950d807f8585_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FLEG is a feed-forward network that reconstructs language-embedded 3D Gaussians from arbitrary uncalibrated multi-view images without requiring 3D annotations. It uses instance-guided contrastive learning and hierarchical sparsification to align 2D semantics with 3D representations efficiently. The method outperforms existing approaches in generating accurate geometry, appearance, and language-aligned semantics from sparse or dense views.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [degradation-aware reasoning, structured reasoning chains, supervised fine-tuning, reward-driven alignment, dynamic reasoning depth scaling]</li>
<li class=""><strong>authors:</strong> Jiaqi Tang, Jianmin Chen, Wei Wei, Xiaogang Xu, Runtao Liu, Xiangyu Wu, Qipeng Xie, Jiafei Wu, Lei Zhang, Qifeng Chen</li>
<li class=""><strong>institution:</strong> Hong Kong University of Science and Technology, Northwestern Polytechnical University, Chinese University of Hong Kong, Nanjing University of Science and Technology, University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17532" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17532</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b624fc0656a14fc68753d26cc52e1c0a78d9633130c6881762bd87e498ed0875_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b624fc0656a14fc68753d26cc52e1c0a78d9633130c6881762bd87e498ed0875_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Robust-R1, a framework that enhances the robustness of Multimodal Large Language Models by explicitly modeling visual degradations through structured reasoning chains. The method integrates supervised fine-tuning, reward-driven alignment, and dynamic reasoning depth scaling, and is supported by a new dataset of realistic degradations. The approach achieves state-of-the-art performance on real-world degradation benchmarks, demonstrating superior anti-degradation capabilities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] G3Splat: Geometrically Consistent Generalizable Gaussian Splatting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [3D Gaussian splatting, geometrically consistent priors, view-synthesis loss, pose-free reconstruction, novel-view synthesis]</li>
<li class=""><strong>authors:</strong> Mehdi Hosseinzadeh, Shin-Fang Chng, Yi Xu, Simon Lucey, Ian Reid, Ravi Garg</li>
<li class=""><strong>institution:</strong> Australian Institute for Machine Learning, Goertek Alpha Labs, MBZUAI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17547" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17547</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04379904f0f6b8d226bffe80f94ab3fe3f745dd41985d72f88a3585212ea8e65_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04379904f0f6b8d226bffe80f94ab3fe3f745dd41985d72f88a3585212ea8e65_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> G3Splat introduces geometric priors to address the ambiguity in learning 3D Gaussian splats from images under self-supervision, enabling geometrically consistent scene reconstruction without requiring camera poses. The method outperforms prior work in geometry recovery, relative pose estimation, and novel-view synthesis, demonstrating strong zero-shot generalization on datasets like ScanNet.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [clothing tailoring, body semantic estimation, body edge prediction, foundational human visual model (FHVM), 3D mesh recovery]</li>
<li class=""><strong>authors:</strong> Yunqi Gao, Leyuan Liu, Yuhan Li, Changxin Gao, Yuanyuan Liu, Jingying Chen</li>
<li class=""><strong>institution:</strong> Central China Normal University, Huazhong University of Science and Technology, China University of Geosciences (Wuhan)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17545" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17545</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3dde00da40b964ce086e0496d0c0c6f668bf5149ef5a44e30539fa3c614601b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3dde00da40b964ce086e0496d0c0c6f668bf5149ef5a44e30539fa3c614601b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ClothHMR, a method for 3D human mesh recovery from a single image that handles diverse clothing via a clothing tailoring module to fit garments to the body silhouette and a mesh recovery module that aligns 3D representations with a foundational human vision model. It demonstrates superior performance over existing methods on benchmark datasets and in-the-wild images, with a practical web application for fashion and shopping.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A unified FLAIR hyperintensity segmentation model for various CNS tumor types and acquisition time points</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Attention U-Net, FLAIR hyperintensity segmentation, Dice score, Raidionics]</li>
<li class=""><strong>authors:</strong> Mathilde Gajda Faanes, David Bouget, Asgeir S. Jakola, Timothy R. Smith, Vasileios K. Kavouridis, Francesco Latini, Margret Jensdottir, Peter Milos, Henrietta Nittby Redebrandt, Rickard L. Sjöberg, Rupavathana Mahesparan, Lars Kjelsberg Pedersen, Ole Solheim, Ingerid Reinertsen</li>
<li class=""><strong>institution:</strong> SINTEF Digital, University of Gothenburg, Harvard Medical School, Uppsala University Hospital, Karolinska University Hospital, Linköping University Hospital, Skåne University Hospital, Umeå University, Haukeland University Hospital, University Hospital of North Norway, Norwegian University of Science and Technology, St. Olavs University Hospital</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17566" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17566</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a unified deep learning model for segmenting FLAIR hyperintensities in brain tumors using an Attention U-Net architecture trained on approximately 5000 MRI scans. The model generalizes well across various tumor types and pre- and post-operative time points, achieving performance comparable to dataset-specific models. It is integrated into the open-source Raidionics software to facilitate clinical deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] RoomEditor++: A Parameter-Sharing Diffusion Architecture for High-Fidelity Furniture Synthesis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [diffusion model, parameter-sharing dual diffusion backbone, U-Net, DiT, inpainting, geometric coherence]</li>
<li class=""><strong>authors:</strong> Qilong Wang, Xiaofan Ming, Zhenyi Lin, Jinwen Li, Dongwei Ren, Wangmeng Zuo, Qinghua Hu</li>
<li class=""><strong>institution:</strong> Tianjin University, Harbin Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17573" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17573</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01defbd3ee3e8db15e4c76b08a2c5a3cbb6ba5c53643caf0e06f01bee07f1f49_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01defbd3ee3e8db15e4c76b08a2c5a3cbb6ba5c53643caf0e06f01bee07f1f49_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes RoomEditor++, a diffusion-based architecture with a parameter-sharing dual diffusion backbone for high-fidelity virtual furniture synthesis. It introduces the RoomBench++ dataset for training and evaluation. Experiments show the method outperforms state-of-the-art approaches in metrics and human preference, demonstrating strong generalization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Medical Imaging AI Competitions Lack Fairness</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical imaging], [benchmarking, fairness, FAIR principles, dataset bias, reproducibility, systematic review]</li>
<li class=""><strong>authors:</strong> Annika Reinke, Evangelia Christodoulou, Sthuthi Sadananda, A. Emre Kavur, Khrystyna Faryna, Daan Schouten, Bennett A. Landman, Carole Sudre, Olivier Colliot, Nick Heller, Sophie Loizillon, Martin Maška, Maëlys Solal, Arya Yazdan-Panah, Vilma Bozgo, Ömer Sümer, Siem de Jong, Sophie Fischer, Michal Kozubek, Tim Rädsch, Nadim Hammoud, Fruzsina Molnár-Gábor, Steven Hicks, Michael A. Riegler, Anindo Saha, Vajira Thambawita, Pal Halvorsen, Amelia Jiménez-Sánchez, Qingyang Yang, Veronika Cheplygina, Sabrina Bottazzi, Alexander Seitel, Spyridon Bakas, Alexandros Karargyris, Kiran Vaidhya Venkadesh, Bram van Ginneken, Lena Maier-Hein</li>
<li class=""><strong>institution:</strong> German Cancer Research Center (DKFZ) Heidelberg</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17581" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17581</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper conducts a large-scale systematic study of 241 biomedical image analysis challenges to assess fairness in terms of dataset representativeness and accessibility. It finds substantial biases in dataset composition and restrictive access conditions, concluding that current benchmarks lack fairness and show a disconnect between leaderboard success and clinical relevance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Gaussian Discriminant Analysis, Z-score distance analysis, cluster-driven deep learning, two-stage framework]</li>
<li class=""><strong>authors:</strong> Tosin Ige, Christopher Kiekintveld, Aritran Piplai, Asif Rahman, Olukunle Kolade, Sasidhar Kunapuli</li>
<li class=""><strong>institution:</strong> The University of Texas at El Paso, University of North Carolina</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17594" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17594</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes MAD-OOD, a two-stage cluster-driven deep learning framework for out-of-distribution malware detection. It uses Gaussian Discriminant Analysis to model class embeddings and Z-score distance analysis to identify anomalies, then integrates these predictions with a neural network for final classification. The method significantly outperforms state-of-the-art approaches on benchmark datasets, achieving high AUC for detecting unseen malware families.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] 3One2: One-step Regression Plus One-step Diffusion for One-hot Modulation in Dual-path Video Snapshot Compressive Imaging</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [one-hot modulation, dual optical path, stochastic differential equation, video inpainting, one-step regression, one-step diffusion]</li>
<li class=""><strong>authors:</strong> Ge Wang, Xing Liu, Xin Yuan</li>
<li class=""><strong>institution:</strong> Zhejiang University, Westlake University, Westlake Institute for Optoelectronics</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17578" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17578</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fc017d71c05a87d24f1951648fbc337f6dcf0174a07c6e9b18d306437014285_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fc017d71c05a87d24f1951648fbc337f6dcf0174a07c6e9b18d306437014285_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a novel algorithm for video snapshot compressive imaging using one-hot modulation, which transforms the reconstruction into a video inpainting problem solved by combining a one-step regression initialization with a one-step diffusion refinement. To address spatial degradation, it introduces a dual optical path hardware design. Experiments show the method effectively reconstructs videos and is the first to integrate diffusion models into video SCI reconstruction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [video anomaly detection], [attention heads, multi-criteria analysis, tuning-free, multimodal large language models (MLLMs), robust head identification]</li>
<li class=""><strong>authors:</strong> Zhaolin Cai, Fan Li, Ziwei Zheng, Haixia Bi, Lijun He</li>
<li class=""><strong>institution:</strong> Xinjiang University, Xi’an Jiaotong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17601" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17601</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0045f2737f70ccb56d547c71a2a55c2ab9160ecc33a3c65b7b564bc9a3329529_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0045f2737f70ccb56d547c71a2a55c2ab9160ecc33a3c65b7b564bc9a3329529_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes HeadHunt-VAD, a tuning-free video anomaly detection method that directly identifies and uses a sparse set of robust, anomaly-sensitive attention heads within a frozen Multimodal Large Language Model (MLLM), bypassing textual generation. It introduces a Robust Head Identification module to select expert heads based on saliency and stability across prompts, followed by a lightweight scorer for detection. The method achieves state-of-the-art performance among tuning-free approaches on major benchmarks, demonstrating the effectiveness of head-level probing in MLLMs for efficient and accurate anomaly detection.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical image registration], [mammography registration, anatomical landmarks, ANTs, VoxelMorph, TransMorph, IDIR, MammoRegNet, benchmark dataset]</li>
<li class=""><strong>authors:</strong> Svetlana Krasnova, Emiliya Starikova, Ilia Naletov, Andrey Krylov, Dmitry Sorokin</li>
<li class=""><strong>institution:</strong> Lomonosov Moscow State University, Third Opinion Platform</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17605" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17605</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1e72bae8c4d2dd504664f6eedc1a325466b12559df8aa6fc5fbe929fb95fcbf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1e72bae8c4d2dd504664f6eedc1a325466b12559df8aa6fc5fbe929fb95fcbf_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MGRegBench, a public benchmark dataset with over 5,000 mammography image pairs and manual annotations for evaluating registration methods. It benchmarks classical, learning-based, and implicit neural representation approaches, finding that deep learning methods like MammoRegNet show strong performance. The dataset and code are released to enable fair comparisons and advance research in mammography registration.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Self-Supervised Weighted Image Guided Quantitative MRI Super-Resolution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [self-supervised learning, physics-informed deep learning, Bayesian maximum a posteriori inference, super-resolution, quantitative MRI relaxometry]</li>
<li class=""><strong>authors:</strong> Alireza Samadifardheris, Dirk H.J. Poot, Florian Wiesinger, Stefan Klein, Juan A. Hernandez-Tamames</li>
<li class=""><strong>institution:</strong> Erasmus MC, GE Healthcare, TU Delft</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17612" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17612</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddc1b71f8c3d47afff52315d3f332374abf579939cd3a9de858b3ceab44b3ff3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddc1b71f8c3d47afff52315d3f332374abf579939cd3a9de858b3ceab44b3ff3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a self-supervised, physics-informed deep learning framework for quantitative MRI super-resolution. It uses routinely acquired high-resolution weighted MRI scans as guidance to enhance low-resolution quantitative maps, eliminating the need for high-resolution ground truth during training. The method enables fast, high-quality quantitative MRI acquisitions, offering a practical pathway for clinical integration.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Semi-Supervised 3D Segmentation for Type-B Aortic Dissection with Slim UNETR</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [semi-supervised learning, 3D segmentation, multi-output CNN, Slim UNETR, data augmentation]</li>
<li class=""><strong>authors:</strong> Denis Mikhailapov, Vladimir Berikov</li>
<li class=""><strong>institution:</strong> Sobolev Institute of Mathematics SB RAS</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17610" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17610</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abd921f37f90b9dfaf497e986d1ae89307cf4b11dd527bf09bd11e36d239652c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abd921f37f90b9dfaf497e986d1ae89307cf4b11dd527bf09bd11e36d239652c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a semi-supervised learning method for multi-output 3D CNNs, specifically using Slim UNETR, to segment aortic structures in Type-B Aortic Dissection. The method leverages data augmentation techniques like rotation and flipping to utilize both labeled and unlabeled data, overcoming the challenge of limited high-quality 3D annotations. It concludes that this approach is a universal and effective strategy for improving segmentation accuracy in medical imaging with complex, multi-class outputs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [autonomous driving perception], [temporal stereo modeling, dynamic confidence gating, sparse query-based detection, multi-view 3D object detection]</li>
<li class=""><strong>authors:</strong> Di Wu, Feng Yang, Wenhui Zhao, Jinwen Yu, Pan Liao, Benlian Xu, Dingwen Zhang</li>
<li class=""><strong>institution:</strong> Northwestern Polytechnical University, Suzhou University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17620" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17620</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87e1ee16ff63d70d8c3aa60f230a9ba8c43c07767bfd05ba0b8a9169945304c1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87e1ee16ff63d70d8c3aa60f230a9ba8c43c07767bfd05ba0b8a9169945304c1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> StereoMV2D integrates temporal stereo modeling into a 2D detection-guided multi-view 3D detector to enhance depth perception by exploiting cross-temporal disparities across adjacent frames. It refines query priors efficiently within 2D regions of interest and uses a dynamic confidence gating mechanism for robust detection under occlusion. The framework achieves superior performance on nuScenes and Argoverse 2 datasets without significant computational overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] PathFLIP: Fine-grained Language-Image Pretraining for Versatile Computational Pathology</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [PathFLIP, fine-grained language-image pretraining, region-level subcaptions, text-conditioned region embeddings, visual-language grounding, large language models (LLMs), whole slide images (WSIs), computational pathology]</li>
<li class=""><strong>authors:</strong> Fengchun Liu, Songhan Jiang, Linghan Cai, Ziyue Wang, Yongbing Zhang</li>
<li class=""><strong>institution:</strong> Harbin Institute of Technology, Shenzhen; National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17621" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17621</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccba362ad9693531c32711d08070cfa491424d893a9b07c00a878fc385f9bdae_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccba362ad9693531c32711d08070cfa491424d893a9b07c00a878fc385f9bdae_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes PathFLIP, a framework for computational pathology that decomposes slide-level captions into region-level subcaptions and generates text-conditioned region embeddings for fine-grained visual-language alignment. It leverages LLMs to follow clinical instructions and adapt to diagnostic contexts. Experiments show it outperforms existing pathological VLMs on multiple benchmarks while using less training data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Region-Constraint In-Context Generation for Instructional Video Editing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [in-context generation, video diffusion, latent regularization, attention regularization, region-constraint, joint denoising]</li>
<li class=""><strong>authors:</strong> Zhongwei Zhang, Fuchen Long, Wei Li, Zhaofan Qiu, Wu Liu, Ting Yao, Tao Mei</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China, HiDream.ai Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17650" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17650</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a42bfbcfb80ccf85890223bbbea68fa4b5ab3ddd21035c9e061032429b289590_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a42bfbcfb80ccf85890223bbbea68fa4b5ab3ddd21035c9e061032429b289590_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents ReCo, a region-constraint in-context generation paradigm for instructional video editing. It introduces latent and attention regularization during joint denoising to precisely modify editing regions and mitigate interference. Experiments show the method&#x27;s superiority across various video editing tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Bitbox: Behavioral Imaging Toolbox for Computational Analysis of Behavior from Videos</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [computational behavioral measurement, video analysis, facial expression, head movement, body action, open-source toolkit, modularity, interpretability]</li>
<li class=""><strong>authors:</strong> Evangelos Sariyanidi, Gokul Nair, Lisa Yankowitz, Casey J. Zampella, Mohan Kashyap Pargi, Aashvi Manakiwala, Maya McNealis, John D. Herrington, Jeffrey Cohn, Robert T. Schultz, Birkan Tunc</li>
<li class=""><strong>institution:</strong> The Children&#x27;s Hospital of Philadelphia, University of Pennsylvania, University of Pittsburgh</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17655" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17655</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Bitbox, an open-source behavioral imaging toolbox that provides a standardized interface for extracting high-level behavioral measurements from video using multiple face, head, and body processors. It is designed to bridge the translational gap by making advanced computational analysis accessible to behavioral and clinical researchers without requiring engineering expertise. The authors conclude that Bitbox will accelerate the integration of computational behavioral measurement into behavioral, clinical, and mental health research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [cognitive steering conduit (CSC), hybrid interaction representations, hybrid guidance strategy, language modeling loss, auxiliary classification loss, open-vocabulary generation]</li>
<li class=""><strong>authors:</strong> Zhaolin Cai, Huiyu Duan, Zitong Xu, Fan Li, Zhi Liu, Jing Liu, Wei Shen, Xiongkuo Min, Guangtao Zhai</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Xi&#x27;an Jiao Tong University, Shandong University, Tianjin University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17640" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17640</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089843bf864a7cfe4c272a400ced858a82e7f97256cba2c56898e8f4e4591dff_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089843bf864a7cfe4c272a400ced858a82e7f97256cba2c56898e8f4e4591dff_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes GRASP-HOI, a framework that reformulates human-object interaction detection as an open-vocabulary generation problem. It uses a lightweight cognitive steering module to inject visual features into a frozen multi-modal LLM for reasoning and employs a hybrid loss for training. This approach achieves state-of-the-art closed-set performance and strong zero-shot generalization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [spatio-temporal feature representation, channel attention, self-attention, recurrent neural networks, video-based gaze estimation]</li>
<li class=""><strong>authors:</strong> Alexandre Personnic, Mihai Bâce</li>
<li class=""><strong>institution:</strong> KU Leuven</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17673" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17673</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes the Spatio-Temporal Gaze Network (ST-Gaze), which combines a CNN backbone with channel and self-attention modules to fuse eye and face features, then models intra- and inter-frame dynamics by treating features as a spatial sequence propagated through time. The method achieves state-of-the-art performance on the EVE dataset, demonstrating that preserving intra-frame spatial context is superior to premature spatial pooling for robust video-based gaze estimation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] An Empirical Study of Sampling Hyperparameters in Diffusion-Based Super-Resolution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [Diffusion Posterior Sampling (DPS), Manifold Constrained Gradient (MCG), conditioning step size, diffusion step count, ablation study]</li>
<li class=""><strong>authors:</strong> Yudhistira Arief Wibowo</li>
<li class=""><strong>institution:</strong> Technical University of Munich, Korea Advanced Institute of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17675</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts an empirical ablation study on diffusion-based super-resolution, focusing on conditioning methods like DPS and MCG. It finds that the conditioning step size is a more critical hyperparameter than the diffusion step count for reconstruction quality. The optimal conditioning step size for best performance in their experiments falls within the range of [2.0, 3.0].</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SAVeD: A First-Person Social Media Video Dataset for ADAS-equipped vehicle Near-Miss and Crash Event Analyses</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [semantic segmentation, monocular depth estimation, Time-to-Collision (TTC), Generalized Extreme Value (GEV) distribution, VideoLLaMA2, InternVL2.5 HiCo R16, domain adaptation]</li>
<li class=""><strong>authors:</strong> Shaoyan Zhai, Mohamed Abdel-Aty, Chenzhu Wang, Rodrigo Vena Garcia</li>
<li class=""><strong>institution:</strong> University of Central Florida</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17724" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17724</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SAVeD, a first-person video dataset from social media for analyzing ADAS vehicle near-misses and crashes. It proposes a framework using semantic segmentation and depth estimation to compute Time-to-Collision and uses extreme value theory to model risk. The dataset&#x27;s annotations are shown to enhance the performance of video-language models through domain adaptation in complex scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] FlexAvatar: Flexible Large Reconstruction Model for Animatable Gaussian Head Avatars with Detailed Deformation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [transformer-based reconstruction, 3D Gaussian Splatting, UV-space position maps, data distribution adjustment, lightweight UNet decoder]</li>
<li class=""><strong>authors:</strong> Cheng Peng, Zhuo Su, Liao Wang, Chen Guo, Zhaohu Li, Chengjiang Long, Zheng Lv, Jingxiang Sun, Chenyangguang Zhang, Yebin Liu</li>
<li class=""><strong>institution:</strong> Tsinghua University, ByteDance</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17717" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17717</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/291c2533d43a0bee45ee61d6f189d197b685c110c8fea100f87b6b779dda6aaf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/291c2533d43a0bee45ee61d6f189d197b685c110c8fea100f87b6b779dda6aaf_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlexAvatar is a flexible large reconstruction model that creates high-fidelity 3D head avatars from single or sparse images without camera poses or expression labels, using a transformer-based approach with structured head query tokens and a lightweight UNet decoder for real-time detailed deformations. It achieves superior 3D consistency and dynamic realism compared to previous methods, offering a practical solution for animatable avatar creation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MambaMIL+: Modeling Long-Term Contextual Patterns for Gigapixel Whole Slide Image</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Mamba, multiple instance learning (MIL), overlapping scanning, selective stripe position encoder (S2PE), contextual token selection (CTS)]</li>
<li class=""><strong>authors:</strong> Qian Zeng, Yihui Wang, Shu Yang, Yingxue Xu, Fengtao Zhou, Jiabo Ma, Dejia Cai, Zhengyu Zhang, Lijuan Qu, Yu Wang, Li Liang, Hao Chen</li>
<li class=""><strong>institution:</strong> Hong Kong University of Science and Technology, Southern Medical University, 900th Hospital of Joint Logistic Support Force, PLA, Zhujiang Hospital, Southern Medical University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17726" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17726</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes MambaMIL+, a multiple instance learning framework that integrates spatial context modeling and long-range dependency for analyzing gigapixel whole-slide images. It introduces overlapping scanning, a selective stripe position encoder, and a contextual token selection mechanism to overcome memory decay and limited context in long sequences. The method achieves state-of-the-art performance across 20 benchmarks for diagnostic classification, molecular prediction, and survival analysis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] AdaptPrompt: Parameter-Efficient Adaptation of VLMs for Generalizable Deepfake Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [deepfake detection], [CLIP, parameter-efficient transfer learning, textual prompts, visual adapters, layer ablation, diffusion models]</li>
<li class=""><strong>authors:</strong> Yichen Jiang, Mohammed Talha Alam, Sohail Ahmed Khan, Duc-Tien Dang-Nguyen, Fakhri Karray</li>
<li class=""><strong>institution:</strong> University of Waterloo, MBZUAI, University of Bergen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17730" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17730</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b69dc4fb6ce4ab4ca6cc4b1419cbe29e322a1c22a4599cafb0bf6800683cf49_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b69dc4fb6ce4ab4ca6cc4b1419cbe29e322a1c22a4599cafb0bf6800683cf49_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes AdaptPrompt, a parameter-efficient framework that adapts the CLIP vision-language model for deepfake detection by jointly learning task-specific textual prompts and visual adapters while freezing the backbone. It also introduces the Diff-Gen dataset and shows that pruning the final transformer block of the vision encoder improves the retention of high-frequency artifacts. The method achieves state-of-the-art generalization across 25 test sets, including unseen generators, and demonstrates strong few-shot performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] LiteGE: Lightweight Geodesic Embedding for Efficient Geodesics Computation and Non-Isometric Shape Correspondence</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [unsigned distance field, PCA, lightweight embedding, geodesic distance, shape correspondence]</li>
<li class=""><strong>authors:</strong> Yohanes Yudhi Adikusuma, Qixing Huang, Ying He</li>
<li class=""><strong>institution:</strong> University of Texas at Austin, Nanyang Technological University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17781" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17781</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LiteGE introduces a lightweight method for computing geodesic distances on 3D shapes by constructing compact shape descriptors using PCA on unsigned distance field samples. This approach eliminates the need for large neural networks, enabling significant reductions in memory usage and inference time while maintaining robustness on sparse point clouds. It also facilitates fast and accurate non-isometric shape correspondence, achieving up to 1000x speedup over state-of-the-art mesh-based methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] UrbanDIFF: A Denoising Diffusion Model for Spatial Gap Filling of Urban Land Surface Temperature Under Dense Cloud Cover</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [denoising diffusion, image inpainting, spatial gap-filling, pixel guided refinement, MODIS Terra LST]</li>
<li class=""><strong>authors:</strong> Arya Chavoshi, Hassan Dashtian, Naveen Sudharsan, Dev Niyogi</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17782" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17782</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces UrbanDIFF, a denoising diffusion model for spatially reconstructing urban land surface temperature imagery obscured by dense clouds, using static urban structure data for conditioning and a pixel-guided refinement step. It demonstrates superior performance over baselines under high cloud coverage, with slower degradation as missing data increases.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Long-Range depth estimation using learning based Hybrid Distortion Model for CCTV cameras</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hybrid distortion model, neural network residual correction, stereo triangulation, camera calibration, long-range depth estimation]</li>
<li class=""><strong>authors:</strong> Ami Pandat, Punna Rajasekhar, G.Aravamuthan, Gopika Vinod, Rohit Shukla</li>
<li class=""><strong>institution:</strong> Homi Bhabha National Institute, Bhabha Atomic Research Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17784" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17784</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid framework for long-range depth estimation by extending conventional camera distortion models with higher-order terms and then enhancing them using a neural network-based residual correction model. This approach improves 3D localization accuracy for distances up to 5 kilometers using CCTV cameras. The method is validated by transforming estimated 3D coordinates to GIS maps, offering a practical calibration solution for long-range photogrammetry.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [vision transformer, neural parametric head models, 3d morphable models, single-image 3d reconstruction, signed distance functions]</li>
<li class=""><strong>authors:</strong> Simon Giebenhain, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Zhe Chen, Matthias Nießner</li>
<li class=""><strong>institution:</strong> Technical University of Munich, Woven by Toyota, Toyota Motor Europe</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17773" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17773</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c26792d83b697ded69eb83a7cee4b4440d0b9637b6c3fbd2061ab2aef67d7bcd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c26792d83b697ded69eb83a7cee4b4440d0b9637b6c3fbd2061ab2aef67d7bcd_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Pix2NPHM, a method that uses a vision transformer to directly regress the parameters of a Neural Parametric Head Model from a single input image. It achieves high-fidelity 3D face reconstruction by training on a mixture of 3D data and 2D videos, and allows for further refinement through inference-time optimization. The authors conclude that their approach yields unprecedented reconstruction quality that generalizes well to in-the-wild data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Animate Any Character in Any World</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [3DGS, conditional autoregressive video generation, pre-trained video generator, natural language control]</li>
<li class=""><strong>authors:</strong> Yitong Wang, Fangyun Wei, Hongyang Zhang, Bo Dai, Yan Lu</li>
<li class=""><strong>institution:</strong> Fudan University, Microsoft Research, University of Waterloo, The University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17796" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17796</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8fb0b1e8242f96e5f0b2988237b2d0603a8f364d90cc833a33b2313d43b1ae4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8fb0b1e8242f96e5f0b2988237b2d0603a8f364d90cc833a33b2313d43b1ae4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces AniX, a system that animates user-provided 3D characters in 3D Gaussian Splatting (3DGS) scenes based on natural language commands. It formulates the task as a conditional autoregressive video generation problem, building upon a pre-trained video generator and a training strategy to enhance motion dynamics. The method enables open-ended character actions while preserving visual fidelity and temporal coherence in the generated video clips.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [3D Gaussian Splatting, multi-teacher pretraining, knowledge distillation, feed-forward encoder, open-vocabulary segmentation, render-and-distill]</li>
<li class=""><strong>authors:</strong> Yue Li, Qi Ma, Runyi Yang, Mengjiao Ma, Bin Ren, Nikola Popovic, Nicu Sebe, Theo Gevers, Luc Van Gool, Danda Pani Paudel, Martin R. Oswald</li>
<li class=""><strong>institution:</strong> University of Amsterdam, ETH Zurich, INSAIT (Sofia University &quot;St. Kliment Ohridski&quot;), Nanjing University of Aeronautics and Astronautics, University of Trento</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17817" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17817</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/93bcfeca01370b73bf751251ff5ff1406aed5442e3fe99bf9680cea6a994f535_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/93bcfeca01370b73bf751251ff5ff1406aed5442e3fe99bf9680cea6a994f535_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Chorus, a multi-teacher pretraining framework that learns a holistic 3D Gaussian Splatting scene encoder by distilling complementary signals from 2D foundation models. The method achieves strong performance on various 3D scene understanding tasks and demonstrates high data efficiency, requiring significantly fewer training scenes than point cloud baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] ReX-MLE: The Autonomous Agent Benchmark for Medical Imaging Challenges</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [autonomous coding agents, benchmark, end-to-end workflows, data preprocessing, model training, medical imaging competitions]</li>
<li class=""><strong>authors:</strong> Roshan Kenia, Xiaoman Zhang, Pranav Rajpurkar</li>
<li class=""><strong>institution:</strong> Harvard Medical School</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17838" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17838</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/55f58c5bf3f50168f0cf79114478d406f849c07ec4093b7557c2b51fc67903fb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/55f58c5bf3f50168f0cf79114478d406f849c07ec4093b7557c2b51fc67903fb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces ReX-MLE, a benchmark for evaluating autonomous coding agents on complex, end-to-end medical imaging challenges derived from real competitions. It finds that current state-of-the-art agents perform poorly, ranking near the 0th percentile compared to human experts, due to domain-knowledge and engineering limitations. The benchmark aims to expose these bottlenecks and guide the development of more capable, domain-aware autonomous AI systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Simulation-Driven Deep Learning Framework for Raman Spectral Denoising Under Fluorescence-Dominant Conditions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [deep learning, noise modeling, cascaded neural network, simulation-driven framework, physics-informed learning]</li>
<li class=""><strong>authors:</strong> Mengkun Chen, Sanidhya D. Tripathi, James W. Tunnell</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17852" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17852</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a simulation-driven deep learning framework that uses a comprehensive noise model to generate realistic Raman spectra for training a cascaded neural network. The network is designed to jointly suppress detector noise and fluorescence background. The results demonstrate that this physics-informed learning approach can improve spectral quality for faster and more accurate tissue analysis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [cross-attention maps, inference-time optimization, compound loss, denoising step, spatial alignment]</li>
<li class=""><strong>authors:</strong> Sarah Rastegar, Violeta Chatalbasheva, Sieger Falkena, Anuj Singh, Yanbo Wang, Tejas Gokhale, Hamid Palangi, Hadi Jamali-Rad</li>
<li class=""><strong>institution:</strong> Delft University of Technology, University of Maryland, Baltimore County, Shell Information Technology International, Google</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17851" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17851</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1a251648d46bb8da396759e99d563b9a2d57eb19fc5ed8f7ece18ccb7bfeeee_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1a251648d46bb8da396759e99d563b9a2d57eb19fc5ed8f7ece18ccb7bfeeee_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> InfSplign is a training-free, inference-time method that improves spatial alignment in text-to-image diffusion models by adjusting the noise at each denoising step using a compound loss based on cross-attention maps. It achieves state-of-the-art performance on spatial reasoning benchmarks, outperforming existing inference-time and fine-tuning baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Convolutional Neural Network (CNN), Attention Mechanism, CBAM, VGG16, Grad-CAM, Layer-wise Relevance Propagation (LRP)]</li>
<li class=""><strong>authors:</strong> Balram Singh, Ram Prakash Sharma, Somnath Dey</li>
<li class=""><strong>institution:</strong> National Institute of Technology Hamirpur, Indian Institute of Technology Indore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17864" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17864</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an interpretable plant leaf disease detection method using a CBAM-enhanced VGG16 CNN model. The model integrates attention modules to improve feature extraction and localization, achieving high accuracy on multiple datasets. The study demonstrates the effectiveness of the approach through performance evaluation and interpretability analysis using attention maps and other explainable AI techniques.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [explainable AI], [Keypoint Counting Classifiers, Vision Transformers, self-explainable models, keypoint matching]</li>
<li class=""><strong>authors:</strong> Kristoffer Wickstrøm, Teresa Dorszewski, Siyan Chen, Michael Kampffmeyer, Elisabeth Wetzer, Robert Jenssen</li>
<li class=""><strong>institution:</strong> UiT The Arctic University of Norway, Technical University of Denmark</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17891" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17891</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3be976f0784a55cb3060e705b880d59783015427100e3ca645b5aa22bfe5f76_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3be976f0784a55cb3060e705b880d59783015427100e3ca645b5aa22bfe5f76_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Keypoint Counting Classifiers (KCCs), a method to convert any pre-trained Vision Transformer (ViT) into a self-explainable model without requiring retraining, by leveraging ViTs&#x27; ability to identify matching keypoints between images. The method creates an interpretable decision process that is directly visualizable. The authors conclude that KCCs improve human-machine communication and represent a step towards more transparent and reliable ViT-based foundation models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Visually Prompted Benchmarks Are Surprisingly Fragile</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [visual prompting, benchmark evaluation, vision-language models, visual marker design, JPEG compression, dataset size, VPBench]</li>
<li class=""><strong>authors:</strong> Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang, Renhao Wang, Trevor Darrell, Alane Suhr, Angjoo Kanazawa</li>
<li class=""><strong>institution:</strong> UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17875" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17875</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1578e85cb159e2bf39916b0de61ec0b774772fc5c07fb3edc1f869eea3ea32e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1578e85cb159e2bf39916b0de61ec0b774772fc5c07fb3edc1f869eea3ea32e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper evaluates vision-language models (VLMs) on visually prompted benchmarks, where questions refer to coordinates marked directly on images. It finds that model performance and leaderboard rankings are surprisingly fragile to minor changes in visual marker design (e.g., color, size) and low-level inference settings like JPEG compression. To address this instability, the authors introduce VPBench, a larger benchmark with multiple visual marker variants.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] RadarGen: Automotive Radar Point Cloud Generation from Cameras</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [diffusion model, bird&#x27;s-eye-view, radar cross section, Doppler, point cloud generation, foundation models]</li>
<li class=""><strong>authors:</strong> Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany</li>
<li class=""><strong>institution:</strong> Technion, MIT, NVIDIA, University of Toronto, Vector Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17897" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17897</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RadarGen is a diffusion model that generates realistic automotive radar point clouds from multi-view camera images by representing radar data in bird&#x27;s-eye-view and conditioning on visual cues. It uses a lightweight recovery step to reconstruct point clouds from the generated maps. Evaluations show it captures real radar statistics and reduces the performance gap for perception models trained on synthetic data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Diffusion Forcing for Multi-Agent Interaction Sequence Modeling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multi-agent motion generation], [diffusion forcing, autoregressive diffusion, transformer, multi-agent interaction, denoising]</li>
<li class=""><strong>authors:</strong> Vongani H. Maluleke, Kie Horiuchi, Lea Wilken, Evonne Ng, Jitendra Malik, Angjoo Kanazawa</li>
<li class=""><strong>institution:</strong> UC Berkeley, Sony Group Corporation, Meta</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17900" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17900</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cd37ce47a0e9e2e02887aa6dab039bf60ed28e99e8eaf07d8c6aaa610b08b8a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cd37ce47a0e9e2e02887aa6dab039bf60ed28e99e8eaf07d8c6aaa610b08b8a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MAGNet, a unified autoregressive diffusion framework for generating multi-agent human motion sequences. It extends Diffusion Forcing to explicitly model inter-agent coupling, enabling coherent coordination for both synchronized and loosely structured social interactions. The method performs on par with specialized dyadic benchmarks and naturally scales to polyadic scenarios with three or more agents.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] InSPECT: Invariant Spectral Features Preservation of Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [invariant spectral features, Fourier coefficients, feature-preserving diffusion, InSPECT, DDPM]</li>
<li class=""><strong>authors:</strong> Baohua Yan, Qingyuan Liu, Jennifer Kava, Xuan Di</li>
<li class=""><strong>institution:</strong> Columbia University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17873" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17873</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e13641f6dbe27376e3ddb6ce03c9f7e6b0f2910fa5365cf52d7c50d8992a2550_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e13641f6dbe27376e3ddb6ce03c9f7e6b0f2910fa5365cf52d7c50d8992a2550_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes InSPECT, a diffusion model that preserves invariant spectral features during the forward and backward processes, preventing the complete destruction of data into white noise. This approach leads to faster convergence, improved generation quality and diversity, and significant reductions in FID and improvements in IS compared to standard DDPM.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Adversarial Robustness of Vision in Open Foundation Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Projected Gradient Descent, adversarial robustness, Visual Question Answering, vision-language models]</li>
<li class=""><strong>authors:</strong> Jonathon Fox, William J Buchanan, Pavlos Papadopoulos</li>
<li class=""><strong>institution:</strong> Edinburgh Napier University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17902" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17902</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates the adversarial robustness of vision-language models LLaVA-1.5-13B and Llama 3.2 Vision-8B-2 by applying untargeted Projected Gradient Descent attacks to their visual inputs and testing on a VQA v2 subset. The main conclusion is that the vision modality is a viable attack vector, and adversarial robustness does not directly correlate with standard benchmark performance, with Llama 3.2 Vision showing a smaller accuracy drop under attack despite a lower baseline.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Colormap-Enhanced Vision Transformers for MRI-Based Multiclass (4-Class) Alzheimer&#x27;s Disease Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Vision Transformers, Pseudo-Color Enhancement, MRI, Multi-Class Classification]</li>
<li class=""><strong>authors:</strong> Faisal Ahmed</li>
<li class=""><strong>institution:</strong> Embry-Riddle Aeronautical University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16964" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16964</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes PseudoColorViT-Alz, a method that enhances MRI scans with pseudo-color transformations and processes them using a Vision Transformer for Alzheimer&#x27;s disease classification. The model achieves state-of-the-art accuracy of 99.79% on the OASIS-1 dataset, demonstrating that combining color augmentation with Vision Transformers significantly improves classification performance over previous methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Dexterous World Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [video diffusion, egocentric hand mesh rendering, hybrid interaction video dataset, scene-action-conditioned generation]</li>
<li class=""><strong>authors:</strong> Byungjun Kim, Taeksoo Kim, Junyoung Lee, Hanbyul Joo</li>
<li class=""><strong>institution:</strong> Seoul National University, RLWRLD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17907" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17907</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d67a89ae639d29f772540022b521f1cb6c865bf4eff8ab082f8c6e9eeeaaa6a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d67a89ae639d29f772540022b521f1cb6c865bf4eff8ab082f8c6e9eeeaaa6a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Dexterous World Models (DWM), a video diffusion framework that generates dynamic, egocentric videos of human-scene interactions by conditioning on static 3D scene renderings and hand motion sequences. It is trained on a hybrid dataset of synthetic and real-world videos. The method produces realistic and physically plausible interactions, representing a step toward interactive digital twins.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [test-time refinement, self-supervised learning, shape from shading, score distillation sampling, monocular depth estimation, diffusion models]</li>
<li class=""><strong>authors:</strong> Ananta R. Bhattarai, Helge Rhodin</li>
<li class=""><strong>institution:</strong> Bielefeld University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17908" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17908</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb5eab47c4353057728b3e9889e13b412f6c11ae6703f713d247c4724fbb909_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb5eab47c4353057728b3e9889e13b412f6c11ae6703f713d247c4724fbb909_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Re-Depth Anything, a test-time self-supervision framework that refines monocular depth predictions by re-lighting the geometry and using a 2D diffusion model&#x27;s priors via Score Distillation Sampling. It updates only specific model embeddings and the decoder to prevent collapse, rather than fine-tuning the entire network. The method shows substantial improvements in depth accuracy and realism over the baseline Depth Anything V2 model.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [latent diffusion models, representation encoders, semantic-pixel reconstruction, variational autoencoder, text-to-image generation, image editing]</li>
<li class=""><strong>authors:</strong> Shilong Zhang, He Zhang, Zhifei Zhang, Chongjian Ge, Shuchen Xue, Shaoteng Liu, Mengwei Ren, Soo Ye Kim, Yuqian Zhou, Qing Liu, Daniil Pakhomov, Kai Zhang, Zhe Lin, Ping Luo</li>
<li class=""><strong>institution:</strong> The University of Hong Kong, Adobe Research, University of Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17909" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17909</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae25315fc54e46dd0abb6b1bc7b9cf8b409962a4f4b702095e19e1f36529da12_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae25315fc54e46dd0abb6b1bc7b9cf8b409962a4f4b702095e19e1f36529da12_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a framework to adapt discriminative representation encoders for generative tasks by introducing a semantic-pixel reconstruction objective, which compresses both semantic and fine-grained details into a compact latent space. The resulting model achieves state-of-the-art image reconstruction and enables unified text-to-image generation and editing, demonstrating that representation encoders can be effectively adapted into robust generative components.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [StyleGAN2-ADA, Denoising Diffusion Probabilistic Models (DDPMs), FID, KID, Inception Score, ViT-B/16, synthetic data augmentation]</li>
<li class=""><strong>authors:</strong> N. A. Adarsh Pritam, Jeba Shiney O, Sanyam Jain</li>
<li class=""><strong>institution:</strong> Alliance University, Østfold University College</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17585" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17585</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/497a14da92ec4e58a3716d9bb6044a8b43d59d0f8ab0aff8c6e70b0d8e5325c9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/497a14da92ec4e58a3716d9bb6044a8b43d59d0f8ab0aff8c6e70b0d8e5325c9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SkinGenBench, a benchmark that evaluates the effects of generative models (StyleGAN2-ADA and DDPMs) and preprocessing pipelines on synthetic dermoscopic image generation for melanoma diagnosis. The main conclusion is that the choice of generative architecture has a stronger impact on image quality and diagnostic utility than preprocessing complexity, with StyleGAN2-ADA outperforming diffusion models in fidelity, and synthetic augmentation significantly boosting downstream classifier performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical imaging], [image registration, radiomics, deep learning, logistic regression, feature selection]</li>
<li class=""><strong>authors:</strong> Rahul Ravi, Ruizhe Li, Tarek Abdelfatah, Stephen Chan, Xin Chen</li>
<li class=""><strong>institution:</strong> University of Nottingham, Nottingham City Hospital</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17759" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17759</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe1fee674daeeb2ff62e605cac35448ddb7b933f1a6948f0aa461b318710117_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe1fee674daeeb2ff62e605cac35448ddb7b933f1a6948f0aa461b318710117_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a framework using aligned longitudinal MRI and clinical data to predict breast cancer treatment response. The method involves tumor segmentation, image registration, and feature extraction, comparing radiomics and deep learning models. The main conclusion is that image registration significantly improves prediction, with radiomics features outperforming deep learning features in predicting pathologic complete response and relapse-free survival.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [3D ConvNeXt, Global Response Normalization, depth scaling, width scaling, context scaling, supervised pretraining, volumetric segmentation]</li>
<li class=""><strong>authors:</strong> Saikat Roy, Yannick Kirchhoff, Constantin Ulrich, Maximillian Rokuss, Tassilo Wald, Fabian Isensee, Klaus Maier-Hein</li>
<li class=""><strong>institution:</strong> German Cancer Research Center (DKFZ), Heidelberg University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17774" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17774</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MedNeXt-v2, a compound-scaled 3D ConvNeXt architecture enhanced with a Global Response Normalization module for large-scale supervised representation learning in medical image segmentation. The authors demonstrate that scaling the backbone network&#x27;s architecture is crucial for effective pretraining, and MedNeXt-v2 achieves state-of-the-art performance when fine-tuned on diverse CT and MR benchmarks. The work establishes that a stronger backbone design yields better downstream segmentation results than simply increasing dataset size.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-23">2025-12-23<a href="#2025-12-23" class="hash-link" aria-label="Direct link to 2025-12-23" title="Direct link to 2025-12-23" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251223] A 96pJ/Frame/Pixel and 61pJ/Event Anti-UAV System with Hybrid Object Tracking Modes</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuncheng Lu, Yucen Shi, Aobo Li, Zehao Li, Junying Li, Bo Wang, Tony Tae-Hyoung Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17939" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17939</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/962df2fe1f424af4979b55e1a459771077ddad5484459248d7cbdb7f3d246823_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/962df2fe1f424af4979b55e1a459771077ddad5484459248d7cbdb7f3d246823_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A 96pJ/Frame/Pixel and 61pJ/Event Anti-UAV System with Hybrid Object Tracking Modes</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Karthik Prabhakar</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17943" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17943</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a630b7d9810838c6059574b3dae2d2f3fcc9c79699030e8640202f2dbcd2d4b0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a630b7d9810838c6059574b3dae2d2f3fcc9c79699030e8640202f2dbcd2d4b0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SuperFlow: Training Flow Matching Models with RL on the Fly</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kaijie Chen, Zhiyang Xu, Ying Shen, Zihao Lin, Yuguang Yao, Lifu Huang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17951" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17951</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/948003a9b36edb891b1eea74cedf1aaab2c086c912b2c476bae0259abbca38e3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/948003a9b36edb891b1eea74cedf1aaab2c086c912b2c476bae0259abbca38e3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SuperFlow: Training Flow Matching Models with RL on the Fly</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Bin Wang, Fadi Dornaika</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17954" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17954</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bf3a9a797cfd9bf487a467cfbf96c2912048bb9780e4ea911d8505de4a3fd09_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bf3a9a797cfd9bf487a467cfbf96c2912048bb9780e4ea911d8505de4a3fd09_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ellie Zhou, Jihoon Chung, Olga Russakovsky</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17953" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17953</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8fc5229281981de8f8ce6b4446b9b416a3bf03a5b7dfe015f7327ed14da6c6c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8fc5229281981de8f8ce6b4446b9b416a3bf03a5b7dfe015f7327ed14da6c6c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Modular Framework for Single-View 3D Reconstruction of Indoor Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuxiao Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17955" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17955</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/984f7066ad7f63ea4f496d0d617a7a83be0b5004c759374d9228f95333891ba4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/984f7066ad7f63ea4f496d0d617a7a83be0b5004c759374d9228f95333891ba4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Modular Framework for Single-View 3D Reconstruction of Indoor Environments</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shubham Kumar Nigam, Parjanya Aditya Shukla, Noel Shallum, Arnab Bhattacharya</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18004" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18004</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2ffa85f9a5ddd1bd5f673a211deae55a7bad9087838680e7b143e6daac52df8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2ffa85f9a5ddd1bd5f673a211deae55a7bad9087838680e7b143e6daac52df8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Omar Faruq Shikdar, Fahad Ahammed, B. M. Shahria Alam, Golam Kibria, Tawhidur Rahman, Nishat Tasnim Niloy</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17987" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17987</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e026b2918baadec02fb27d5fb57e2257968a98d72147f8159070f8c9fbb7d1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e026b2918baadec02fb27d5fb57e2257968a98d72147f8159070f8c9fbb7d1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Robotic VLA Benefits from Joint Learning with Motion Image Diffusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yu Fang, Kanchana Ranasinghe, Le Xue, Honglu Zhou, Juntao Tan, Ran Xu, Shelby Heinecke, Caiming Xiong, Silvio Savarese, Daniel Szafir, Mingyu Ding, Michael S. Ryoo, Juan Carlos Niebles</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18007" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18007</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b59654686bbf5b51e6cb8f755af962b88b4d3f2a65f9c991f5f10b64604828_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b59654686bbf5b51e6cb8f755af962b88b4d3f2a65f9c991f5f10b64604828_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Robotic VLA Benefits from Joint Learning with Motion Image Diffusion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Name That Part: 3D Part Segmentation and Naming</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Soumava Paul, Prakhar Kaushik, Ankit Vaidya, Anand Bhattad, Alan Yuille</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18003" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18003</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7489c4c88b9369f0e55ac4eb9e395d18b5597618ea2266650f451ea4d5f3eec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7489c4c88b9369f0e55ac4eb9e395d18b5597618ea2266650f451ea4d5f3eec_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Name That Part: 3D Part Segmentation and Naming</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18028" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18028</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30065ffa9a2b8c3aa56f4dd49b67477e394ef33e769980a2e7968c85edfa3346_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30065ffa9a2b8c3aa56f4dd49b67477e394ef33e769980a2e7968c85edfa3346_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Fakrul Islam Tushar, Ehsan Samei, Cynthia Rudin, Joseph Y. Lo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18038" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18038</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7823e2e22739636a655f10a5159ed96eedc679b68020943f440020f0658dad87_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7823e2e22739636a655f10a5159ed96eedc679b68020943f440020f0658dad87_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] FOODER: Real-time Facial Authentication and Expression Recognition</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sabri Mustafa Kahya, Muhammet Sami Yavuz, Boran Hamdi Sivrikaya, Eckehard Steinbach</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18057" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18057</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17048a3dede1c165a0f52aab61de33bef5559518cf2996ad61858f9a9d680457_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17048a3dede1c165a0f52aab61de33bef5559518cf2996ad61858f9a9d680457_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FOODER: Real-time Facial Authentication and Expression Recognition</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] YolovN-CBi: A Lightweight and Efficient Architecture for Real-Time Detection of Small UAVs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ami Pandat, Punna Rajasekhar, Gopika Vinod, Rohit Shukla</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18046" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18046</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74e5d6d01e4d7b6683268a6dfd04d4c061677c1530a7ce95eaaf2ce9e55a0600_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74e5d6d01e4d7b6683268a6dfd04d4c061677c1530a7ce95eaaf2ce9e55a0600_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> YolovN-CBi: A Lightweight and Efficient Architecture for Real-Time Detection of Small UAVs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] FPBench: A Comprehensive Benchmark of Multimodal Large Language Models for Fingerprint Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ekta Balkrishna Gavas, Sudipta Banerjee, Chinmay Hegde, Nasir Memon</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18073" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18073</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6335692d3a738711a3704dedcaa3eaa60dbe8b762e225fb2857789d12a428e0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6335692d3a738711a3704dedcaa3eaa60dbe8b762e225fb2857789d12a428e0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FPBench: A Comprehensive Benchmark of Multimodal Large Language Models for Fingerprint Analysis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shreshth Rajan, Raymond Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18082" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18082</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6dd5ca9b561b8ad179709036acde1f313672cfa0c00a271b9235f8ab0e640d0a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6dd5ca9b561b8ad179709036acde1f313672cfa0c00a271b9235f8ab0e640d0a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Changxu Duan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18115" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18115</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13ec88807faf2814dad94d8489b36894b8aa8c290f9026ca2108a6c17ac22ca6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13ec88807faf2814dad94d8489b36894b8aa8c290f9026ca2108a6c17ac22ca6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SERA-H: Beyond Native Sentinel Spatial Limits for High-Resolution Canopy Height Mapping</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Thomas Boudras, Martin Schwartz, Rasmus Fensholt, Martin Brandt, Ibrahim Fayad, Jean-Pierre Wigneron, Gabriel Belouze, Fajwel Fogel, Philippe Ciais</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18128" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18128</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40cedf819eb6eb6cd85f9bb6838fd612711be404b26144f5bf2363883a2eac28_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40cedf819eb6eb6cd85f9bb6838fd612711be404b26144f5bf2363883a2eac28_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SERA-H: Beyond Native Sentinel Spatial Limits for High-Resolution Canopy Height Mapping</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] EndoStreamDepth: Temporally Consistent Monocular Depth Estimation for Endoscopic Video Streams</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hao Li, Daiwei Lu, Jiacheng Wang, Robert J. Webster III, Ipek Oguz</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18159" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18159</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c6bfdccbbe4e580860cba1ef6a6436d8fc986b7e46d121db741672f2f3a233_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c6bfdccbbe4e580860cba1ef6a6436d8fc986b7e46d121db741672f2f3a233_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EndoStreamDepth: Temporally Consistent Monocular Depth Estimation for Endoscopic Video Streams</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Midhat Urooj, Ayan Banerjee, Sandeep Gupta</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18177" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18177</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7ef165e056ae631599bd024eb4341c57c1705258598a82662ae1166302f2947_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7ef165e056ae631599bd024eb4341c57c1705258598a82662ae1166302f2947_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Local Patches Meet Global Context: Scalable 3D Diffusion Priors for Computed Tomography Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Taewon Yang, Jason Hu, Jeffrey A. Fessler, Liyue Shen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18161" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18161</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6f2d7690da449f60ab28b9f1c8644a2829a779454fd531b2852837f43afe4bb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6f2d7690da449f60ab28b9f1c8644a2829a779454fd531b2852837f43afe4bb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Local Patches Meet Global Context: Scalable 3D Diffusion Priors for Computed Tomography Reconstruction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Atlas is Your Perfect Context: One-Shot Customization for Generalizable Foundational Medical Image Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ziyu Zhang, Yi Yu, Simeng Zhu, Ahmed Aly, Yunhe Gao, Ning Gu, Yuan Xue</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18176" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18176</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/011e5fe0a450b1348324ad1d5a24fbfca2367916d6e12d05481fe9aeccee423d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/011e5fe0a450b1348324ad1d5a24fbfca2367916d6e12d05481fe9aeccee423d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Atlas is Your Perfect Context: One-Shot Customization for Generalizable Foundational Medical Image Segmentation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Is There a Better Source Distribution than Gaussian? Exploring Source Distributions for Image Flow Matching</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Junho Lee, Kwanseok Kim, Joonseok Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18184" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18184</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4568908c3f989abda31d63ece857e924f12b9a87e165bc3d982eeccfbbad2aa2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4568908c3f989abda31d63ece857e924f12b9a87e165bc3d982eeccfbbad2aa2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Is There a Better Source Distribution than Gaussian? Exploring Source Distributions for Image Flow Matching</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ALIGN: Advanced Query Initialization with LiDAR-Image Guidance for Occlusion-Robust 3D Object Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Janghyun Baek, Mincheol Chang, Seokha Moon, Seung Joon Lee, Jinkyu Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18187" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18187</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1281826184eb88510283760e8d18262afdfcf1143f3ba9a7c22b684fb4f4489_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1281826184eb88510283760e8d18262afdfcf1143f3ba9a7c22b684fb4f4489_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ALIGN: Advanced Query Initialization with LiDAR-Image Guidance for Occlusion-Robust 3D Object Detection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MACE-Dance: Motion-Appearance Cascaded Experts for Music-Driven Dance Video Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kaixing Yang, Jiashu Zhu, Xulong Tang, Ziqiao Peng, Xiangyue Zhang, Puwei Wang, Jiahong Wu, Xiangxiang Chu, Hongyan Liu, Jun He</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18181" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18181</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f91052deb821e034255a2a5fe3873c583d9fcbf1ff48249b8457b8bc7b44e261_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f91052deb821e034255a2a5fe3873c583d9fcbf1ff48249b8457b8bc7b44e261_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MACE-Dance: Motion-Appearance Cascaded Experts for Music-Driven Dance Video Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Multi-Part Object Representations via Graph Structures and Co-Part Discovery</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Alex Foo, Wynne Hsu, Mong Li Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18192" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18192</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/691a99ce976b35cbbbe1a4ae36131ee76e0a4332b25068ac660d1ddb59eb7132_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/691a99ce976b35cbbbe1a4ae36131ee76e0a4332b25068ac660d1ddb59eb7132_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multi-Part Object Representations via Graph Structures and Co-Part Discovery</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Unsupervised Anomaly Detection with an Enhanced Teacher for Student-Teacher Feature Pyramid Matching</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mohammad Zolfaghari, Hedieh Sajedi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18219" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18219</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49c3ce25aa0d46d7576b119913f2eb1d74bf6b2d970566f7b3c4a55bb19063d9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49c3ce25aa0d46d7576b119913f2eb1d74bf6b2d970566f7b3c4a55bb19063d9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Unsupervised Anomaly Detection with an Enhanced Teacher for Student-Teacher Feature Pyramid Matching</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Stable and Efficient Single-Rollout RL for Multimodal Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rui Liu, Dian Yu, Lei Ke, Haolin Liu, Yujun Zhou, Zhenwen Liang, Haitao Mi, Pratap Tokekar, Dong Yu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18215" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18215</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81e83852cf5de78a73c53dc039a80d4328420c326e71217ed656de7348457003_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81e83852cf5de78a73c53dc039a80d4328420c326e71217ed656de7348457003_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Stable and Efficient Single-Rollout RL for Multimodal Reasoning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Investigating Spatial Attention Bias in Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Aryan Chaudhary, Sanchit Goyal, Pratik Narang, Dhruv Kumar</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18231" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18231</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be5cd89ea4d004fbb18410473c51ae236387bc088e028d1ddd84dbce2d703bfc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be5cd89ea4d004fbb18410473c51ae236387bc088e028d1ddd84dbce2d703bfc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Investigating Spatial Attention Bias in Vision-Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Multifaceted Exploration of Spatial Openness in Rental Housing: A Big Data Analysis in Tokyo&#x27;s 23 Wards</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Takuya OKi, Yuan Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18226" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18226</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa20e5bd064a3600a6a9399f88b0f303ee4cc42c7edd5c8236408c1ec3052eb8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa20e5bd064a3600a6a9399f88b0f303ee4cc42c7edd5c8236408c1ec3052eb8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multifaceted Exploration of Spatial Openness in Rental Housing: A Big Data Analysis in Tokyo&#x27;s 23 Wards</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SG-RIFE: Semantic-Guided Real-Time Intermediate Flow Estimation with Diffusion-Competitive Perceptual Quality</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pan Ben Wong, Chengli Wu, Hanyue Lu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18241" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18241</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/973299de069f19e1fee6c5ddbbadac21151ac4addddd917990024e02cf59b042_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/973299de069f19e1fee6c5ddbbadac21151ac4addddd917990024e02cf59b042_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SG-RIFE: Semantic-Guided Real-Time Intermediate Flow Estimation with Diffusion-Competitive Perceptual Quality</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shahram Najam Syed, Yitian Hu, Yuchao Yao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18237" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18237</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b04b692d1acf5e8524839243e2fe0778331620858ff93eb02166948c9684c4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b04b692d1acf5e8524839243e2fe0778331620858ff93eb02166948c9684c4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Spectral Discrepancy and Cross-modal Semantic Consistency Learning for Object Detection in Hyperspectral Image</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiao He, Chang Tang, Xinwang Liu, Wei Zhang, Zhimin Gao, Chuankun Li, Shaohua Qiu, Jiangfeng Xu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18245" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18245</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028cfc83b63f51bf798c2014f1dc8e1f4c786134910334880eb4c0cf340a5eb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028cfc83b63f51bf798c2014f1dc8e1f4c786134910334880eb4c0cf340a5eb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Spectral Discrepancy and Cross-modal Semantic Consistency Learning for Object Detection in Hyperspectral Image</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Who Can See Through You? Adversarial Shielding Against VLM-Based Attribute Inference Attacks</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yucheng Fan, Jiawei Chen, Yu Tian, Zhaoxia Yin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18264" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18264</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6181175b477e5c50e93e9a1a3a4690fb1673471073933b770754e208c7b0e776_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6181175b477e5c50e93e9a1a3a4690fb1673471073933b770754e208c7b0e776_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Who Can See Through You? Adversarial Shielding Against VLM-Based Attribute Inference Attacks</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Towards Ancient Plant Seed Classification: A Benchmark Dataset and Baseline Model</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rui Xing, Runmin Cong, Yingying Wu, Can Wang, Zhongming Tang, Fen Wang, Hao Wu, Sam Kwong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18247" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18247</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d341345aee67211dc7e3cea021a11051fd66231eb649e4da442fda6828319f0d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d341345aee67211dc7e3cea021a11051fd66231eb649e4da442fda6828319f0d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards Ancient Plant Seed Classification: A Benchmark Dataset and Baseline Model</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Building UI/UX Dataset for Dark Pattern Detection and YOLOv12x-based Real-Time Object Recognition Detection System</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Se-Young Jang, Su-Yeon Yoon, Jae-Woong Jung, Dong-Hun Lee, Seong-Hun Choi, Soo-Kyung Jun, Yu-Bin Kim, Young-Seon Ju, Kyounggon Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18269" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18269</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/367924eac7293dd60fe2ea42c0f8cc5f30a08d5958f71c4e9e1a77afcf26f521_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/367924eac7293dd60fe2ea42c0f8cc5f30a08d5958f71c4e9e1a77afcf26f521_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Building UI/UX Dataset for Dark Pattern Detection and YOLOv12x-based Real-Time Object Recognition Detection System</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Loom: Diffusion-Transformer for Interleaved Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mingcheng Ye, Jiaming Liu, Yiren Song</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18254" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18254</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2702848bca515645a4d065b0a7036ee7ddebe3d80478578da0afc6637e8987e1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2702848bca515645a4d065b0a7036ee7ddebe3d80478578da0afc6637e8987e1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Loom: Diffusion-Transformer for Interleaved Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] UniMPR: A Unified Framework for Multimodal Place Recognition with Arbitrary Sensor Configurations</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhangshuo Qi, Jingyi Xu, Luqi Cheng, Shichen Wen, Yiming Ma, Guangming Xiong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18279" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18279</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d97551a04523bb912a1f4f7a4089cc0954eacc215acb16ece3401441f9ae898e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d97551a04523bb912a1f4f7a4089cc0954eacc215acb16ece3401441f9ae898e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> UniMPR: A Unified Framework for Multimodal Place Recognition with Arbitrary Sensor Configurations</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Pyramidal Adaptive Cross-Gating for Multimodal Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zidong Gu, Shoufu Tian</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18291" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18291</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/686814403fba53ef7884ea37a0cf31a6148bda927776fcfca1291870ba0502e3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/686814403fba53ef7884ea37a0cf31a6148bda927776fcfca1291870ba0502e3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Pyramidal Adaptive Cross-Gating for Multimodal Detection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Eren Caglar, Amirkia Rafiei Oskooei, Mehmet Kutanoglu, Mustafa Keles, Mehmet S. Aktas</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18318" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18318</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff046f84477e998c712a0f584e02cdfbe6c1bef89652e720bfe7cbb5bb7764ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff046f84477e998c712a0f584e02cdfbe6c1bef89652e720bfe7cbb5bb7764ac_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MatE: Material Extraction from Single-Image via Geometric Prior</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zeyu Zhang, Wei Zhai, Jian Yang, Yang Cao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18312" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18312</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb74dfaa1e58347f80eafe1b8d6c9ccb8621df2345727b43170380c7a9ec7111_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb74dfaa1e58347f80eafe1b8d6c9ccb8621df2345727b43170380c7a9ec7111_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MatE: Material Extraction from Single-Image via Geometric Prior</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MatSpray: Fusing 2D Material World Knowledge on 3D Geometry</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Philipp Langsteiner, Jan-Niklas Dihlmann, Hendrik P.A. Lensch</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18314" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18314</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bc201c384a37d43b2bd10a9944a6df93bc871b921e058d53b3a80eb95aa3784_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bc201c384a37d43b2bd10a9944a6df93bc871b921e058d53b3a80eb95aa3784_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MatSpray: Fusing 2D Material World Knowledge on 3D Geometry</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A two-stream network with global-local feature fusion for bone age assessment</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Qiong Lou, Han Yang, Fang Lu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18331" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18331</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d26c3d7fded2a10e84ebad25bc6bb1810428c48d125c9ad4b3a401fe14ca66d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d26c3d7fded2a10e84ebad25bc6bb1810428c48d125c9ad4b3a401fe14ca66d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A two-stream network with global-local feature fusion for bone age assessment</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MCVI-SANet: A lightweight semi-supervised model for LAI and SPAD estimation of winter wheat under vegetation index saturation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhiheng Zhang, Jiajun Yang, Hong Sun, Dong Wang, Honghua Jiang, Yaru Chen, Tangyuan Ning</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18344" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18344</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f19d6aa9f347693da897453773133b3a6f0b66ee06e5f6bdf421bb24507e5f56_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f19d6aa9f347693da897453773133b3a6f0b66ee06e5f6bdf421bb24507e5f56_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MCVI-SANet: A lightweight semi-supervised model for LAI and SPAD estimation of winter wheat under vegetation index saturation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Enhancing 3D Semantic Scene Completion with a Refinement Module</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dunxing Zhang, Jiachen Lu, Han Yang, Lei Bao, Bo Song</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18363" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18363</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87b47e037d36c9a89056d6636a00bc4c1b54b6f312aac970b439024a3aa71b85_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87b47e037d36c9a89056d6636a00bc4c1b54b6f312aac970b439024a3aa71b85_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Enhancing 3D Semantic Scene Completion with a Refinement Module</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] RecurGS: Interactive Scene Modeling via Discrete-State Recurrent Gaussian Fusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wenhao Hu, Haonan Zhou, Zesheng Li, Liu Liu, Jiacheng Dong, Zhizhong Su, Gaoang Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18386" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18386</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e36e78691b7bf84af271eac170b5599e6d8dffe940baf4afb324df4c1c26545_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e36e78691b7bf84af271eac170b5599e6d8dffe940baf4afb324df4c1c26545_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RecurGS: Interactive Scene Modeling via Discrete-State Recurrent Gaussian Fusion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Automated Mosaic Tesserae Segmentation via Deep Learning Techniques</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Charilaos Kapelonis, Marios Antonakakis, Konstantinos Politof, Aristomenis Antoniadis, Michalis Zervakis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18406" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18406</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6893130bacdfd82f19e38a48fe6c1a981e8a6e139face8bc5ca4379fb98ab0c8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6893130bacdfd82f19e38a48fe6c1a981e8a6e139face8bc5ca4379fb98ab0c8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Automated Mosaic Tesserae Segmentation via Deep Learning Techniques</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Fei Song, Yi Li, Jiangmeng Li, Rui Wang, Changwen Zheng, Fanjiang Xu, Hui Xiong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18411" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18411</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c480d382f1a840aaa7b8f0ed1cc380b9ece2b0a04c01a7bfeaf7551356ef5a0c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c480d382f1a840aaa7b8f0ed1cc380b9ece2b0a04c01a7bfeaf7551356ef5a0c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Badr Moufad, Navid Bagheri Shouraki, Alain Oliviero Durmus, Thomas Hirtz, Eric Moulines, Jimmy Olsson, Yazid Janati</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18365" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18365</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb10f5246f2791c1a5c5d83727bb85643a1c89da44e041633e646e724bfa1d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb10f5246f2791c1a5c5d83727bb85643a1c89da44e041633e646e724bfa1d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Through the PRISm: Importance-Aware Scene Graphs for Image Retrieval</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dimitrios Georgoulopoulos, Nikolaos Chaidos, Angeliki Dimitriou, Giorgos Stamou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18407" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18407</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8250115733311080fc732003a6de9ee573531db16fb388825dac28ed45b0904f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8250115733311080fc732003a6de9ee573531db16fb388825dac28ed45b0904f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Through the PRISm: Importance-Aware Scene Graphs for Image Retrieval</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] E-RGB-D: Real-Time Event-Based Perception with Structured Light</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Seyed Ehsan Marjani Bajestani, Giovanni Beltrame</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18429" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18429</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adf7afb7ba214958716e0fa581dd6ddbddce2e0c6851dde58c99a8a067def71d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adf7afb7ba214958716e0fa581dd6ddbddce2e0c6851dde58c99a8a067def71d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> E-RGB-D: Real-Time Event-Based Perception with Structured Light</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Object-Centric Framework for Video Moment Retrieval</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zongyao Li, Yongkang Wong, Satoshi Yamazaki, Jianquan Liu, Mohan Kankanhalli</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18448" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18448</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aeaf50384c57c0a0975048f565d619000d31089ae116153752950a50d948d161_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aeaf50384c57c0a0975048f565d619000d31089ae116153752950a50d948d161_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Object-Centric Framework for Video Moment Retrieval</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Agent-Based Output Drift Detection for Breast Cancer Response Prediction in a Multisite Clinical Decision Support System</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xavier Rafael-Palou, Jose Munuera, Ana Jimenez-Pastor, Richard Osuala, Karim Lekadir, Oliver Diaz</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18450" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18450</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a54a0b377f2d61cdb058b9dd088fe0948517999354f7c502389a3f1149e8a3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a54a0b377f2d61cdb058b9dd088fe0948517999354f7c502389a3f1149e8a3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Agent-Based Output Drift Detection for Breast Cancer Response Prediction in a Multisite Clinical Decision Support System</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MeniMV: A Multi-view Benchmark for Meniscus Injury Severity Grading</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shurui Xu, Siqi Yang, Jiapin Ren, Zhong Cao, Hongwei Yang, Mengzhen Fan, Yuyu Sun, Shuyan Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18437" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18437</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82297f34fc5f7dfab35d4ab984e7ba607ad5ac2c849bffbec01d38cbdcf42e3d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82297f34fc5f7dfab35d4ab984e7ba607ad5ac2c849bffbec01d38cbdcf42e3d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MeniMV: A Multi-view Benchmark for Meniscus Injury Severity Grading</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jayant Lohia</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18453" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18453</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e4686d2a1c44f6a0f1d346c2e5401709f3d974dfd9062781f00cc8eb5d71952_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e4686d2a1c44f6a0f1d346c2e5401709f3d974dfd9062781f00cc8eb5d71952_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Plasticine: A Traceable Diffusion Model for Medical Image Translation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tianyang Zhanng, Xinxing Cheng, Jun Cheng, Shaoming Zheng, He Zhao, Huazhu Fu, Alejandro F Frangi, Jiang Liu, Jinming Duan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18455" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18455</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2148612b9f3ccd68ab3abdcc94c84b8f1b0d8d4ed68811c6de5496dd1c2352d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2148612b9f3ccd68ab3abdcc94c84b8f1b0d8d4ed68811c6de5496dd1c2352d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Plasticine: A Traceable Diffusion Model for Medical Image Translation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Santwana Sagnika, Manav Malhotra, Ishtaj Kaur Deol, Soumyajit Roy, Swarnav Kumar</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18500" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18500</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3d3024bb6cf729a0d0827d13bba185e065be8eb34bd4dffa40d58782bc4e5ab_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3d3024bb6cf729a0d0827d13bba185e065be8eb34bd4dffa40d58782bc4e5ab_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] STORM: Search-Guided Generative World Models for Robotic Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wenjun Lin, Jensen Zhang, Kaitong Cai, Keze Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18477" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18477</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79949cb474434bd024983169a211bb0a029e6412a974a6dd6f4ee7a51cf05349_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79949cb474434bd024983169a211bb0a029e6412a974a6dd6f4ee7a51cf05349_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> STORM: Search-Guided Generative World Models for Robotic Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Adaptive-VoCo: Complexity-Aware Visual Token Compression for Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiaoyang Guo, Keze Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18496" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18496</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff175aad1ff6f6b7f47faa5f2c01c3d0f7b27451e06bf33f2c763508f1ff6f0e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff175aad1ff6f6b7f47faa5f2c01c3d0f7b27451e06bf33f2c763508f1ff6f0e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Adaptive-VoCo: Complexity-Aware Visual Token Compression for Vision-Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] NASTaR: NovaSAR Automated Ship Target Recognition Dataset</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Benyamin Hosseiny, Kamirul Kamirul, Odysseas Pappas, Alin Achim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18503" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18503</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4ac29e833bc5eaed0a9881e81dd877d55f600f6bd8722c27c1466b58afc1378_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4ac29e833bc5eaed0a9881e81dd877d55f600f6bd8722c27c1466b58afc1378_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NASTaR: NovaSAR Automated Ship Target Recognition Dataset</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] GTMA: Dynamic Representation Optimization for OOD Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jensen Zhang, Ningyuan Liu, Keze Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18504" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18504</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec81480aabd317eb3778c36f956b818e8aed383ab7f9a13fe2c867243fcec025_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec81480aabd317eb3778c36f956b818e8aed383ab7f9a13fe2c867243fcec025_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GTMA: Dynamic Representation Optimization for OOD Vision-Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] WoundNet-Ensemble: A Novel IoMT System Integrating Self-Supervised Deep Learning and Multi-Model Fusion for Automated, High-Accuracy Wound Classification and Healing Progression Monitoring</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Moses Kiprono</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18528" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18528</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c30b62937b70850aaf6278e7cee47b3850562dc10f4c505a5d923c08c6e1e20c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c30b62937b70850aaf6278e7cee47b3850562dc10f4c505a5d923c08c6e1e20c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> WoundNet-Ensemble: A Novel IoMT System Integrating Self-Supervised Deep Learning and Multi-Model Fusion for Automated, High-Accuracy Wound Classification and Healing Progression Monitoring</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rahul Yumlembam, Biju Issac, Nauman Aslam, Eaby Kollonoor Babu, Josh Collyer, Fraser Kennedy</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18527" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18527</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1520c61491b8f395b60f64432e37eff57c8608eba74cd9029c6109d32db7554_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1520c61491b8f395b60f64432e37eff57c8608eba74cd9029c6109d32db7554_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Enhancing Medical Large Vision-Language Models via Alignment Distillation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Aofei Chang, Ting Wang, Fenglong Ma</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18554" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18554</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65cfdaf6c50eb13e1535902993b0e58d2ccb2d1d8a89304254b7eb116f2e3bec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65cfdaf6c50eb13e1535902993b0e58d2ccb2d1d8a89304254b7eb116f2e3bec_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Enhancing Medical Large Vision-Language Models via Alignment Distillation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Hierarchical Bayesian Framework for Multisource Domain Adaptation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Alexander M. Glandon, Khan M. Iftekharuddin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18553" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18553</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8faca3b573bb5e3938ff1e6d161ec550a7c61bbaaa60d1a247d4dd139982ad93_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8faca3b573bb5e3938ff1e6d161ec550a7c61bbaaa60d1a247d4dd139982ad93_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Hierarchical Bayesian Framework for Multisource Domain Adaptation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sumaiya Ali, Areej Alhothali, Ohoud Alzamzami, Sameera Albasri, Ahmed Abduljabbar, Muhammad Alwazzan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18573" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18573</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/387c39b67caa5e84daf0327dfb4c7a948d6d72c292a3404e6c7e8a2bcd6db7df_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/387c39b67caa5e84daf0327dfb4c7a948d6d72c292a3404e6c7e8a2bcd6db7df_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] OpenView: Empowering MLLMs with Out-of-view VQA</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Qixiang Chen, Cheng Zhang, Chi-Wing Fu, Jingwen Ye, Jianfei Cai</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18563" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18563</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd24136fa7bb319394718494b72886c4e4dd61a8ff3664079b5014b3c8e9d20_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd24136fa7bb319394718494b72886c4e4dd61a8ff3664079b5014b3c8e9d20_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> OpenView: Empowering MLLMs with Out-of-view VQA</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Weijie Zhou, Xuangtang Xiong, Ye Tian, Lijun Yue, Xinyu Wu, Wei Li, Chaoyang Zhao, Honghui Dong, Ming Tang, Jinqiao Wang, Zhengyou Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18571" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18571</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e48075d8fabfbd12f97c2605f729d021ebb805999e01bbf511f08a872cf4cbae_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e48075d8fabfbd12f97c2605f729d021ebb805999e01bbf511f08a872cf4cbae_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Commercial Vehicle Braking Optimization: A Robust SIFT-Trajectory Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhe Li, Kun Cheng, Hanyue Mo, Jintao Lu, Ziwen Kuang, Jianwen Ye, Lixu Xu, Xinya Meng, Jiahui Zhao, Shengda Ji, Shuyuan Liu, Mengyu Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18597" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18597</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e804718e966934e64cc22d02e796b1123866e2dd8d037801ef21cbcbee4c0537_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e804718e966934e64cc22d02e796b1123866e2dd8d037801ef21cbcbee4c0537_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Commercial Vehicle Braking Optimization: A Robust SIFT-Trajectory Approach</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jianglin Lu, Yuanwei Wu, Ziyi Zhao, Hongcheng Wang, Felix Jimenez, Abrar Majeedi, Yun Fu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18599" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18599</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/196c51c23a0210e0d5f61fa9aa109b4c1b71673440779c2559e8762b7020daf5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/196c51c23a0210e0d5f61fa9aa109b4c1b71673440779c2559e8762b7020daf5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Saeideh Yousefzadeh, Hamidreza Pourreza</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18613" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18613</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15155edf4c23c5fa3645a1383be98a1728e9025130895c36fb1d8c7a536e2335_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15155edf4c23c5fa3645a1383be98a1728e9025130895c36fb1d8c7a536e2335_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] PTTA: A Pure Text-to-Animation Framework for High-Quality Creation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ruiqi Chen, Kaitong Cai, Yijia Fan, Keze Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18614" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18614</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95e6d5eb29d8f200e9b4d751e2105c378ddc6c8efc5742b330b0ff8118931fb1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95e6d5eb29d8f200e9b4d751e2105c378ddc6c8efc5742b330b0ff8118931fb1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PTTA: A Pure Text-to-Animation Framework for High-Quality Creation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Adversarial Robustness in Zero-Shot Learning<!-- -->:An<!-- --> Empirical Study on Class and Concept-Level Vulnerabilities</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhiyuan Peng, Zihan Ye, Shreyank N Gowda, Yuping Yan, Haotian Xu, Ling Shao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18651" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18651</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/723c55bcd003bd92c1a8106676da980d908b4363b91caeab957599d3002e301d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/723c55bcd003bd92c1a8106676da980d908b4363b91caeab957599d3002e301d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Adversarial Robustness in Zero-Shot Learning<!-- -->:An<!-- --> Empirical Study on Class and Concept-Level Vulnerabilities</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pengxiang Ouyang, Qing Ma, Zheng Wang, Cong Bai</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18660" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18660</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06fe8e37dc2d1d1d6a5d48298128004f86b1be4d6ecbeca32f5c72786b1a535c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06fe8e37dc2d1d1d6a5d48298128004f86b1be4d6ecbeca32f5c72786b1a535c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SplatBright: Generalizable Low-Light Scene Reconstruction from Sparse Views via Physically-Guided Gaussian Enhancement</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yue Wen, Liang Song, Hesheng Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18655" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18655</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e36abe6a038f84b030a1de9a4b07a7b5e542151e56041f8a4107ffcc75a8c4fa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e36abe6a038f84b030a1de9a4b07a7b5e542151e56041f8a4107ffcc75a8c4fa_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SplatBright: Generalizable Low-Light Scene Reconstruction from Sparse Views via Physically-Guided Gaussian Enhancement</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Offline Reinforcement Learning for End-to-End Autonomous Driving</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chihiro Noguchi, Takaki Yamamoto</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18662" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18662</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48070c89f8318c57738214d175fd04c9e38d7e43e2838b599453ae0e31d8c27f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48070c89f8318c57738214d175fd04c9e38d7e43e2838b599453ae0e31d8c27f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Offline Reinforcement Learning for End-to-End Autonomous Driving</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Geometric-Photometric Event-based 3D Gaussian Ray Tracing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kai Kohyama, Yoshimitsu Aoki, Guillermo Gallego, Shintaro Shiba</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18640" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18640</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85632c631e489e2f789f1b5319b05b69e8e883a12a7b626de475c98e6066ef45_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85632c631e489e2f789f1b5319b05b69e8e883a12a7b626de475c98e6066ef45_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Geometric-Photometric Event-based 3D Gaussian Ray Tracing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SmartSight: Mitigating Hallucination in Video-LLMs Without Compromising Video Understanding via Temporal Attention Collapse</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yiming Sun, Mi Zhang, Feifei Li, Geng Hong, Min Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18671" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18671</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aba3b21e95cf3e99fe497ad62634d3e70efc41858e08b88889ea760e9fc41f24_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aba3b21e95cf3e99fe497ad62634d3e70efc41858e08b88889ea760e9fc41f24_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SmartSight: Mitigating Hallucination in Video-LLMs Without Compromising Video Understanding via Temporal Attention Collapse</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Uni-Neur2Img: Unified Neural Signal-Guided Image Generation, Editing, and Stylization via Diffusion Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiyue Bai, Ronghao Yu, Jia Xiu, Pengfei Zhou, Jie Xia, Peng Ji</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18635" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18635</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c6acb4dae57f2c9f4c60c94122091feee1beca126d52be260f7006e1f25433b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c6acb4dae57f2c9f4c60c94122091feee1beca126d52be260f7006e1f25433b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Uni-Neur2Img: Unified Neural Signal-Guided Image Generation, Editing, and Stylization via Diffusion Transformers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] brat: Aligned Multi-View Embeddings for Brain MRI Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Maxime Kayser, Maksim Gridnev, Wanting Wang, Max Bain, Aneesh Rangnekar, Avijit Chatterjee, Aleksandr Petrov, Harini Veeraraghavan, Nathaniel C. Swinburne</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18679" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18679</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e2d9cbeed3e332a02f5cc9adf5abc0ec370a32b059de24cd550cc930eb84a82_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e2d9cbeed3e332a02f5cc9adf5abc0ec370a32b059de24cd550cc930eb84a82_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> brat: Aligned Multi-View Embeddings for Brain MRI Analysis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] AsyncDiff: Asynchronous Timestep Conditioning for Enhanced Text-to-Image Diffusion Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Longhuan Xu, Feng Yin, Cunjian Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18675</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b77064b6897ecbdf7c4ad30fb422607730ad3fdb70df7e888f14b1c35318262c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b77064b6897ecbdf7c4ad30fb422607730ad3fdb70df7e888f14b1c35318262c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AsyncDiff: Asynchronous Timestep Conditioning for Enhanced Text-to-Image Diffusion Inference</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Study of Finetuning Video Transformers for Multi-view Geometry Tasks</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Huimin Wu, Kwang-Ting Cheng, Stephen Lin, Zhirong Wu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18684" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18684</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbb0d58e96608a91e501c2ec636326573d0b8bbd34db61dfabc4e1544edbaaef_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbb0d58e96608a91e501c2ec636326573d0b8bbd34db61dfabc4e1544edbaaef_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Study of Finetuning Video Transformers for Multi-view Geometry Tasks</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] EcoSplat: Efficiency-controllable Feed-forward 3D Gaussian Splatting from Multi-view Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jongmin Park, Minh-Quan Viet Bui, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18692" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18692</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbc186e81fb4bc0dba691524b3e057afe1a2f63cefe632fd49cd141b1ddd38ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbc186e81fb4bc0dba691524b3e057afe1a2f63cefe632fd49cd141b1ddd38ac_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EcoSplat: Efficiency-controllable Feed-forward 3D Gaussian Splatting from Multi-view Images</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Rectification Reimagined: A Unified Mamba Model for Image Correction and Rectangling with Prompts</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Linwei Qiu, Gongzhe Li, Xiaozhe Zhang, Qinlin Sun, Fengying Xie</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18718" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18718</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1fb34acfd0681b1e7d5e9dd5a9b2ead7a096ce7f062456abd5c1d666d63c9f6d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1fb34acfd0681b1e7d5e9dd5a9b2ead7a096ce7f062456abd5c1d666d63c9f6d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Rectification Reimagined: A Unified Mamba Model for Image Correction and Rectangling with Prompts</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Breast Cancer Recurrence Risk Prediction Based on Multiple Instance Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jinqiu Chen, Huyan Xu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18734" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18734</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/703d5b9807b107a0861def2e3d8e792e6ba0cafc487dd572e6b8472d4694e38c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/703d5b9807b107a0861def2e3d8e792e6ba0cafc487dd572e6b8472d4694e38c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Breast Cancer Recurrence Risk Prediction Based on Multiple Instance Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>M</mi><mn>3</mn></msup><mo>−</mo><mi>V</mi><mi>e</mi><mi>r</mi><mi>s</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">M^3-Verse</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mord mathnormal">erse</span></span></span></span>: A &quot;Spot the Difference&quot; Challenge for Large Multimodal Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kewei Wei, Bocheng Hu, Jie Cao, Xiaohan Chen, Zhengxi Lu, Wubing Xia, Weili Xu, Jiaao Wu, Junchen He, Mingyu Jia, Ciyun Zhao, Ye Sun, Yizhi Li, Zhonghan Zhao, Jian Zhang, Gaoang Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18735" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18735</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52a20f9bfaa32af67c363579cc4ad37ddbcaa7484f53c2ad3e6f6287dffcb22d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52a20f9bfaa32af67c363579cc4ad37ddbcaa7484f53c2ad3e6f6287dffcb22d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>M</mi><mn>3</mn></msup><mo>−</mo><mi>V</mi><mi>e</mi><mi>r</mi><mi>s</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">M^3-Verse</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mord mathnormal">erse</span></span></span></span>: A &quot;Spot the Difference&quot; Challenge for Large Multimodal Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] AMLID: An Adaptive Multispectral Landmine Identification Dataset for Drone-Based Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> James E. Gallagher, Edward J. Oughton</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18738" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18738</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd46cd4988eda7be0f3fc725e20d0cd6d84d2d855b2fe6ca9ada6da8ce6533a1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd46cd4988eda7be0f3fc725e20d0cd6d84d2d855b2fe6ca9ada6da8ce6533a1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AMLID: An Adaptive Multispectral Landmine Identification Dataset for Drone-Based Detection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tianrui Zhu, Shiyi Zhang, Zhirui Sun, Jingqi Tian, Yansong Tang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18741" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18741</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17ec1d3f6ff19284c38fae7fffb7c89e33c210bb632c6e29bab17704791956af_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17ec1d3f6ff19284c38fae7fffb7c89e33c210bb632c6e29bab17704791956af_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Context-Aware Network Based on Multi-scale Spatio-temporal Attention for Action Recognition in Videos</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiaoyang Li, Wenzhu Yang, Kanglin Wang, Tiebiao Wang, Qingsong Fei</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18750" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18750</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d25c33c0793bb9ac5e532b11ef64839818a8ed3877ba9c30ed6683982c402d3b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d25c33c0793bb9ac5e532b11ef64839818a8ed3877ba9c30ed6683982c402d3b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Context-Aware Network Based on Multi-scale Spatio-temporal Attention for Action Recognition in Videos</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] IPCV: Information-Preserving Compression for MLLM Visual Encoders</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuan Chen, Zichen Wen, Yuzhou Wu, Xuyang Liu, Shuang Chen, Junpeng Ma, Weijia Li, Conghui He, Linfeng Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18747" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18747</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23f93076434c2a627bcdaf65dfd58999db9105798eceb360e343a3f4018cc020_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23f93076434c2a627bcdaf65dfd58999db9105798eceb360e343a3f4018cc020_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> IPCV: Information-Preserving Compression for MLLM Visual Encoders</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kaican Li, Lewei Yao, Jiannan Wu, Tiezheng Yu, Jierun Chen, Haoli Bai, Lu Hou, Lanqing Hong, Wei Zhang, Nevin L. Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18745" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18745</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87556fdc09b55b65c0d472d205a384cc42453256afeb9e7a28db98178fe1d145_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87556fdc09b55b65c0d472d205a384cc42453256afeb9e7a28db98178fe1d145_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] In-Context Audio Control of Video Diffusion Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wenze Liu, Weicai Ye, Minghong Cai, Quande Liu, Xintao Wang, Xiangyu Yue</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18772" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18772</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2f6222ebfd1fddbc353b3e719e28fb0ea12ffea379c74f8f8539de1c07ea2e3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2f6222ebfd1fddbc353b3e719e28fb0ea12ffea379c74f8f8539de1c07ea2e3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> In-Context Audio Control of Video Diffusion Transformers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MaskFocus: Focusing Policy Optimization on Critical Steps for Masked Image Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Guohui Zhang, Hu Yu, Xiaoxiao Ma, Yaning Pan, Hang Xu, Feng Zhao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18766" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18766</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e30d5e631331f72f5d3daa88862f5f4de5bcba185997003c4eb4c2df4ba695e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e30d5e631331f72f5d3daa88862f5f4de5bcba185997003c4eb4c2df4ba695e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MaskFocus: Focusing Policy Optimization on Critical Steps for Masked Image Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Fanis Mathioulakis, Gorjan Radevski, Tinne Tuytelaars</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18784" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18784</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0254e0e9393c18241de33ec503d4cc8d75622e94c6a0d08ec4ee16da3e3b6b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0254e0e9393c18241de33ec503d4cc8d75622e94c6a0d08ec4ee16da3e3b6b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Guangtao Lyu, Chenghao Xu, Qi Liu, Jiexi Yan, Muli Yang, Fen Fang, Cheng Deng</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18804" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18804</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dab2ab51309551324dcfdba087c2e5ec1e6dfdae10633047423e7233f30fdf84_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dab2ab51309551324dcfdba087c2e5ec1e6dfdae10633047423e7233f30fdf84_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ziyuan Tao, Chuanzhi Xu, Sandaru Jayawardana, Wei Bao, Kanchana Thilakarathna, Teng Joon Lim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18809" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18809</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/396672768b4960b9fefe0f861b938a8b78842c8181043ded9d12c7e8f28dbdfe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/396672768b4960b9fefe0f861b938a8b78842c8181043ded9d12c7e8f28dbdfe_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Revealing Perception and Generation Dynamics in LVLMs: Mitigating Hallucinations via Validated Dominance Correction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Guangtao Lyu, Xinyi Cheng, Chenghao Xu, Qi Liu, Muli Yang, Fen Fang, Huilin Chen, Jiexi Yan, Xu Yang, Cheng Deng</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18813" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18813</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ecc3b83a3bbbf2f3d70f79eb80fb5574e88c49c36b11bed230f795915fb03_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ecc3b83a3bbbf2f3d70f79eb80fb5574e88c49c36b11bed230f795915fb03_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Revealing Perception and Generation Dynamics in LVLMs: Mitigating Hallucinations via Validated Dominance Correction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuxiao Yang, Hualian Sheng, Sijia Cai, Jing Lin, Jiahao Wang, Bing Deng, Junzhe Lu, Haoqian Wang, Jieping Ye</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18814" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18814</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a2ba0db72977cbda98db1501df497120d084239fa62014678ae702cb513369e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a2ba0db72977cbda98db1501df497120d084239fa62014678ae702cb513369e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Brain-Gen: Towards Interpreting Neural Signals for Stimulus Reconstruction Using Transformers and Latent Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hasib Aslam, Muhammad Talal Faiz, Muhammad Imran Malik</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18843" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18843</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c10b7d7d79ed94bbb6547ab2dcefe6d392d199280eab76c815bf57db3927efb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c10b7d7d79ed94bbb6547ab2dcefe6d392d199280eab76c815bf57db3927efb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Brain-Gen: Towards Interpreting Neural Signals for Stimulus Reconstruction Using Transformers and Latent Diffusion Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] VizDefender: Unmasking Visualization Tampering through Proactive Localization and Intent Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sicheng Song, Yanjie Zhang, Zixin Chen, Huamin Qu, Changbo Wang, Chenhui Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18853" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18853</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6df051754b8f594a7ef88a46e4855d2eb43c4c662b1afbca9997caf2b4fbad53_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6df051754b8f594a7ef88a46e4855d2eb43c4c662b1afbca9997caf2b4fbad53_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VizDefender: Unmasking Visualization Tampering through Proactive Localization and Intent Inference</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Application of deep learning approaches for medieval historical documents transcription</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Maksym Voloshchuk, Bohdana Zarembovska, Mykola Kozlenko</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18865" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18865</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bc697efc3bfd30e66b187f56c365b6a8add07756fb768bc77ba4586f1ab7d205_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bc697efc3bfd30e66b187f56c365b6a8add07756fb768bc77ba4586f1ab7d205_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Application of deep learning approaches for medieval historical documents transcription</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Cross-modal Counterfactual Explanations: Uncovering Decision Factors and Dataset Biases in Subjective Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Alina Elena Baia, Andrea Cavallaro</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18864" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18864</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54def5a8025f36fedd1db00274e2f13348c15e8cd933c948a5286d52b31223ae_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54def5a8025f36fedd1db00274e2f13348c15e8cd933c948a5286d52b31223ae_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Cross-modal Counterfactual Explanations: Uncovering Decision Factors and Dataset Biases in Subjective Classification</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Localising Shortcut Learning in Pixel Space via Ordinal Scoring Correlations for Attribution Representations (OSCAR)</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Akshit Achara, Peter Triantafillou, Esther Puyol-Antón, Alexander Hammers, Andrew P. King</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18888" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18888</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c85ecde5b204bf7cf4f7ce66885bc81ef00aee9a5651c7332b2bd485f6e54e2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c85ecde5b204bf7cf4f7ce66885bc81ef00aee9a5651c7332b2bd485f6e54e2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Localising Shortcut Learning in Pixel Space via Ordinal Scoring Correlations for Attribution Representations (OSCAR)</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kaidi Liang, Ke Li, Xianbiao Hu, Ruwen Qin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18878" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18878</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40cc111afbcdc76e8f9c40d867f3e3d92fefb4f06215bb877943f16f5fc7f761_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40cc111afbcdc76e8f9c40d867f3e3d92fefb4f06215bb877943f16f5fc7f761_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Thinking Beyond Labels: Vocabulary-Free Fine-Grained Recognition using Reasoning-Augmented LMMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dmitry Demidov, Zaigham Zaheer, Zongyan Han, Omkar Thawakar, Rao Anwer</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18897" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18897</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8993f53f872c636102637faace6bbf220931bba2b5c07be198751d05e23fa52_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8993f53f872c636102637faace6bbf220931bba2b5c07be198751d05e23fa52_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Thinking Beyond Labels: Vocabulary-Free Fine-Grained Recognition using Reasoning-Augmented LMMs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Delta-LLaVA: Base-then-Specialize Alignment for Token-Efficient Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mohamad Zamini, Diksha Shukla</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18910" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18910</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82dc1b903fe39645cfdac67e793b8aaf7d0b36f43bd3088781f3ec68c702dafb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82dc1b903fe39645cfdac67e793b8aaf7d0b36f43bd3088781f3ec68c702dafb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Delta-LLaVA: Base-then-Specialize Alignment for Token-Efficient Vision-Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Point What You Mean: Visually Grounded Instruction Policy</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hang Yu, Juntu Zhao, Yufeng Liu, Kaiyu Li, Cheng Ma, Di Zhang, Yingdong Hu, Guang Chen, Junyuan Xie, Junliang Guo, Junqiao Zhao, Yang Gao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18933" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18933</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0cc4da48f9bff61ef73842c7f9922cf58cd10179d0b3d6cb1241fbc052909cc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0cc4da48f9bff61ef73842c7f9922cf58cd10179d0b3d6cb1241fbc052909cc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Point What You Mean: Visually Grounded Instruction Policy</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Raina Panda, Daniel Fein, Arpita Singhal, Mark Fiore, Maneesh Agrawala, Matyas Bohacek</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18930" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18930</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17d9e52f35a5e302d613cba6423f95b6e9bb58c2b559fbd5a209c0516f8e2326_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17d9e52f35a5e302d613cba6423f95b6e9bb58c2b559fbd5a209c0516f8e2326_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Symmetrization of 3D Generative Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nicolas Caytuiro, Ivan Sipiran</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18953" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18953</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67504b808f29cea86fc99619a21f55d7a71b7b925241cf4a90f7273b07bddf83_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67504b808f29cea86fc99619a21f55d7a71b7b925241cf4a90f7273b07bddf83_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Symmetrization of 3D Generative Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] DVI: Disentangling Semantic and Visual Identity for Training-Free Personalized Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Guandong Li, Yijun Ding</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18964" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18964</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5f5e482fec6d1911329e83df08fb77c38f618acd901b6be5084bffe87124bfb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5f5e482fec6d1911329e83df08fb77c38f618acd901b6be5084bffe87124bfb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DVI: Disentangling Semantic and Visual Identity for Training-Free Personalized Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Total Curvature Regularization and its_Minimization for Surface and Image Smoothing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tianle Lu, Ke Chen, Yuping Duan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18968" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18968</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e49d05c1cc8ec54ae283a3da000454b18542cf8f4a1408bd3980e43be6a00e3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e49d05c1cc8ec54ae283a3da000454b18542cf8f4a1408bd3980e43be6a00e3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Total Curvature Regularization and its_Minimization for Surface and Image Smoothing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Cheng-Hong Chang, Pei-Hsuan Tsai</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18969" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18969</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05062b9ac64115654a255df578e1f3f61c6740d8e9db0b67ceca3387185661df_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05062b9ac64115654a255df578e1f3f61c6740d8e9db0b67ceca3387185661df_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zaidao Han, Risa Higashita, Jiang Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18954" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18954</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a277490a1036cca55f864049064f469e9f72d6c27becedf02e6e0afb0212e89_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a277490a1036cca55f864049064f469e9f72d6c27becedf02e6e0afb0212e89_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Towards AI-Guided Open-World Ecological Taxonomic Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Cheng Yaw Low, Heejoon Koo, Jaewoo Park, Kaleb Mesfin Asfaw, Meeyoung Cha</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18994" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18994</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/780199ccb79605e70d0938bc92678f9a79a328a23356a7c9a76644dcf9ab4dfe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/780199ccb79605e70d0938bc92678f9a79a328a23356a7c9a76644dcf9ab4dfe_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards AI-Guided Open-World Ecological Taxonomic Classification</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ryosuke Korekata, Quanting Xie, Yonatan Bisk, Komei Sugiura</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18987" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18987</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/456f5beecc1c2c45f598578f8491d702dc76e9f500bae44e01454f783dc10b05_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/456f5beecc1c2c45f598578f8491d702dc76e9f500bae44e01454f783dc10b05_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Gyeongrok Oh, Youngdong Jang, Jonghyun Choi, Suk-Ju Kang, Guang Lin, Sangpil Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18991" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18991</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca47fc4083c13826225296ae5380ec0a994e6b8b0a936b8582bb3acb510605f6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca47fc4083c13826225296ae5380ec0a994e6b8b0a936b8582bb3acb510605f6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Steering Vision-Language Pre-trained Models for Incremental Face Presentation Attack Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Haoze Li, Jie Zhang, Guoying Zhao, Stephen Lin, Shiguang Shan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19022" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19022</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/462a93cd76a387b34292a443a8359c9699b91dba8c90d7da0799846e9545ee6a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/462a93cd76a387b34292a443a8359c9699b91dba8c90d7da0799846e9545ee6a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Steering Vision-Language Pre-trained Models for Incremental Face Presentation Attack Detection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sihao Lin, Zerui Li, Xunyi Zhao, Gengze Zhou, Liuyi Wang, Rong Wei, Rui Tang, Juncheng Li, Hanqing Wang, Jiangmiao Pang, Anton van den Hengel, Jiajun Liu, Qi Wu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19021" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19021</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73e25aea53ce484be801e13db8703f7ac91dcd519e03aa8f1f11869324466841_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73e25aea53ce484be801e13db8703f7ac91dcd519e03aa8f1f11869324466841_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Automatic Neuronal Activity Segmentation in Fast Four Dimensional Spatio-Temporal Fluorescence Imaging using Bayesian Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ran Li, Pan Xiao, Kaushik Dutta, Youdong Guo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19032" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19032</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/caef29a5648e5f1039a8eef029aa3d268223d6542939af5d70efe56c4350e067_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/caef29a5648e5f1039a8eef029aa3d268223d6542939af5d70efe56c4350e067_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Automatic Neuronal Activity Segmentation in Fast Four Dimensional Spatio-Temporal Fluorescence Imaging using Bayesian Approach</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zelin Zhao, Xinyu Gong, Bangya Liu, Ziyang Song, Jun Zhang, Suhui Wu, Yongxin Chen, Hao Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19020" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19020</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7bd5a2abb18589060985877478ba075c83351c0fe7a3b61f7db28dccf9b3d5b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7bd5a2abb18589060985877478ba075c83351c0fe7a3b61f7db28dccf9b3d5b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Connor Kilrain, David Carlyn, Julia Chae, Sara Beery, Wei-Lun Chao, Jianyang Gu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19026" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19026</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9212089c44919f3c00bd82b13c0b39b407491c5d9de5c2c72ed790b2f0a0a2f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9212089c44919f3c00bd82b13c0b39b407491c5d9de5c2c72ed790b2f0a0a2f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Distinguishing Visually Similar Actions: Prompt-Guided Semantic Prototype Modulation for Few-Shot Action Recognition</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiaoyang Li, Mingming Lu, Ruiqi Wang, Hao Li, Zewei Le</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19036" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19036</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3cd50ba346b66d56f1f55a399d0c82cbfbd81d4c166ba855757c74c8346c7c2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3cd50ba346b66d56f1f55a399d0c82cbfbd81d4c166ba855757c74c8346c7c2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Distinguishing Visually Similar Actions: Prompt-Guided Semantic Prototype Modulation for Few-Shot Action Recognition</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] WaTeRFlow: Watermark Temporal Robustness via Flow Consistency</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Utae Jeong, Sumin In, Hyunju Ryu, Jaewan Choi, Feng Yang, Jongheon Jeong, Seungryong Kim, Sangpil Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19048" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19048</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f8776ce3ae3dfd5e014702d94d69af26362743bfccc9cf4f4e1fc818a95a539_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f8776ce3ae3dfd5e014702d94d69af26362743bfccc9cf4f4e1fc818a95a539_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> WaTeRFlow: Watermark Temporal Robustness via Flow Consistency</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] 6DAttack: Backdoor Attacks in the 6DoF Pose Estimation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jihui Guo, Zongmin Zhang, Zhen Sun, Yuhao Yang, Jinlin Wu, Fu Zhang, Xinlei He</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19058" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19058</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d7cd1a6d20a70ace7bef5d2b8b091492883b5588753a885d31218a86b7cb53e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d7cd1a6d20a70ace7bef5d2b8b091492883b5588753a885d31218a86b7cb53e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> 6DAttack: Backdoor Attacks in the 6DoF Pose Estimation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ruiqi Ma, Yu Yan, Chunhong Zhang, Minghao Yin, XinChao Liu, Zhihong Jin, Zheng Hu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19070" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19070</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d56f2f61fcc600eaa02837139337fdefe7e31f2b7f624524c972734f069b6c0f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d56f2f61fcc600eaa02837139337fdefe7e31f2b7f624524c972734f069b6c0f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Khanh Nguyen, Dasith de Silva Edirimuni, Ghulam Mubashar Hassan, Ajmal Mian</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19088" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19088</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d400fa28859b94a82c7a1ca0fe5b0ee2133535554ef479d696c7d0032c8503d3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d400fa28859b94a82c7a1ca0fe5b0ee2133535554ef479d696c7d0032c8503d3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Decoupled Generative Modeling for Human-Object Interaction Synthesis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hwanhee Jung, Seunggwan Lee, Jeongyoon Yoon, SeungHyeon Kim, Giljoo Nam, Qixing Huang, Sangpil Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19049" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19049</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efa5fbcac724949235f9de6d44ddfad055396c38cff93f03bbf6611e7504dfe4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efa5fbcac724949235f9de6d44ddfad055396c38cff93f03bbf6611e7504dfe4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Decoupled Generative Modeling for Human-Object Interaction Synthesis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ariel Lubonja, Pedro R. A. S. Bassi, Wenxuan Li, Hualin Qiao, Randal Burns, Alan L. Yuille, Zongwei Zhou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19091" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19091</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8b75e4c6b7e493a0c6d3b57468d0ae3edcb9efaeaaf9076c00744a6a04c7c6a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8b75e4c6b7e493a0c6d3b57468d0ae3edcb9efaeaaf9076c00744a6a04c7c6a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Mamba-Based Modality Disentanglement Network for Multi-Contrast MRI Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Weiyi Lyu, Xinming Fang, Jun Wang, Jun Shi, Guixu Zhang, Juncheng Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19095" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19095</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/660b7ce425295dabbd643561507450b5b391330b19646c8c466147c1770bdf63_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/660b7ce425295dabbd643561507450b5b391330b19646c8c466147c1770bdf63_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Mamba-Based Modality Disentanglement Network for Multi-Contrast MRI Reconstruction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Trifocal Tensor and Relative Pose Estimation with Known Vertical Direction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tao Li, Zhenbao Yu, Banglei Guan, Jianli Han, Weimin Lv, Friedrich Fraundorfer</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19110" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19110</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59e59cb599a47e950626af160c385ffc182ff74568bbebdcb198397b16efd9d1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59e59cb599a47e950626af160c385ffc182ff74568bbebdcb198397b16efd9d1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Trifocal Tensor and Relative Pose Estimation with Known Vertical Direction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hengyi Feng, Zeang Sheng, Meiyi Qiang, Wentao Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19115" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19115</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1ccbf3745011850a9bf45220f261e89ca40bd9c98025e6d92083fb946fbf5c6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1ccbf3745011850a9bf45220f261e89ca40bd9c98025e6d92083fb946fbf5c6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] GaussianImage++: Boosted Image Representation and Compression with 2D Gaussian Splatting</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tiantian Li, Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Jun Zhang, Yan Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19108" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19108</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a01ced772b8d2640ac7da9b4aa353acacbc0a7a10ccf022639e513769de09272_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a01ced772b8d2640ac7da9b4aa353acacbc0a7a10ccf022639e513769de09272_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GaussianImage++: Boosted Image Representation and Compression with 2D Gaussian Splatting</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pengxuan Yang, Ben Lu, Zhongpu Xia, Chao Han, Yinfeng Gao, Teng Zhang, Kun Zhan, XianPeng Lang, Yupeng Zheng, Qichao Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19133" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19133</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95928df29dd1d6db8d9cc4946cd472f8722d19286d4bfa47dd919093172f5948_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95928df29dd1d6db8d9cc4946cd472f8722d19286d4bfa47dd919093172f5948_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ruikai Li, Xinrun Li, Mengwei Xie, Hao Shan, Shoumeng Qiu, Xinyuan Chang, Yizhe Fan, Feng Xiong, Han Jiang, Yilong Ren, Haiyang Yu, Mu Xu, Yang Long, Varun Ojha, Zhiyong Cui</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19150" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19150</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aaaf65665b46ca412dac28df472cffd8faf9ff9cf5ae8945c3d13bbd791e3c88_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aaaf65665b46ca412dac28df472cffd8faf9ff9cf5ae8945c3d13bbd791e3c88_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wendong Bu, Kaihang Pan, Yuze Lin, Jiacheng Li, Kai Shen, Wenqiao Zhang, Juncheng Li, Jun Xiao, Siliang Tang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19159" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19159</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b0a157ededdb6863038e7005281e71ab57fdcaf96eba9b30702558285b1ed2e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b0a157ededdb6863038e7005281e71ab57fdcaf96eba9b30702558285b1ed2e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CycleChart: A Unified Consistency-Based Learning Framework for Bidirectional Chart Understanding and Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dazhen Deng, Sen Yang, Yuchen He, Yuan Tian, Yingcai Wu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19173" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19173</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4924c981ea9ee58543e98b565a1e8fca0e8ce65bad7464a4041f4c5ffc756918_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4924c981ea9ee58543e98b565a1e8fca0e8ce65bad7464a4041f4c5ffc756918_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CycleChart: A Unified Consistency-Based Learning Framework for Bidirectional Chart Understanding and Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Marios Thoma, Zenonas Theodosiou, Harris Partaourides, Vassilis Vassiliades, Loizos Michael, Andreas Lanitis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19190" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19190</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a96c05d7294228300f6794f5bbda416f7d11e2b7c58157c93485f6f6ba933ade_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a96c05d7294228300f6794f5bbda416f7d11e2b7c58157c93485f6f6ba933ade_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] InvCoSS: Inversion-driven Continual Self-supervised Learning in Medical Multi-modal Image Pre-training</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zihao Luo, Shaohao Rui, Zhenyu Tang, Guotai Wang, Xiaosong Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19213" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19213</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c90f4c95929734535ee76b711f8c5c23d3fb093f6e99a2ba0fea1407c036c313_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c90f4c95929734535ee76b711f8c5c23d3fb093f6e99a2ba0fea1407c036c313_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> InvCoSS: Inversion-driven Continual Self-supervised Learning in Medical Multi-modal Image Pre-training</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] HippMetric: A skeletal-representation-based framework for cross-sectional and longitudinal hippocampal substructural morphometry</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Na Gao, Chenfei Ye, Yanwu Yang, Anqi Li, Zhengbo He, Li Liang, Zhiyuan Liu, Xingyu Hao, Ting Ma, Tengfei Guo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19214" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19214</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e03fac6363ee3c142ec7100157cae8cca39cfd14894e066b219b9ace6425131_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e03fac6363ee3c142ec7100157cae8cca39cfd14894e066b219b9ace6425131_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HippMetric: A skeletal-representation-based framework for cross-sectional and longitudinal hippocampal substructural morphometry</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Towards Minimal Fine-Tuning of VLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tiange Luo, Lajanugen Logeswaran, Jaekyeom Kim, Justin Johnson, Honglak Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19219" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19219</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/420c22fc5b580697b05c90c8fbf0115c2cea8a82f971a19f125c0456c3405309_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/420c22fc5b580697b05c90c8fbf0115c2cea8a82f971a19f125c0456c3405309_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards Minimal Fine-Tuning of VLMs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] From Pixels to Predicates Structuring urban perception with scene graphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yunlong Liu, Shuyang Li, Pengyuan Liu, Yu Zhang, Rudi Stouffs</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19221" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19221</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/937a8aa60cbd8f9c8016e6a36a9941d4f6c5a8af94206f4d30a8f70eac213a2a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/937a8aa60cbd8f9c8016e6a36a9941d4f6c5a8af94206f4d30a8f70eac213a2a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> From Pixels to Predicates Structuring urban perception with scene graphs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Meng Chu, Senqiao Yang, Haoxuan Che, Suiyun Zhang, Xichen Zhang, Shaozuo Yu, Haokun Gui, Zhefan Rao, Dandan Tu, Rui Liu, Jiaya Jia</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19243" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19243</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/189a642ba98e396da7852dfdc680536e07570eee6c947c570c81130b8f827924_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/189a642ba98e396da7852dfdc680536e07570eee6c947c570c81130b8f827924_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Carla Crivoi, Radu Tudor Ionescu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19253" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19253</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d70a17b56ecda8937a9bd488550e13c9d9833f836ea7a0e16e024c8b5e950c5b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d70a17b56ecda8937a9bd488550e13c9d9833f836ea7a0e16e024c8b5e950c5b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] 3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xinyang Song, Libin Wang, Weining Wang, Zhiwei Li, Jianxin Sun, Dandan Zheng, Jingdong Chen, Qi Li, Zhenan Sun</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19271" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19271</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be02858b75b1adbf6d43c4ad543ed82d07789eb9f6bc5adee21b6c3bc2806e5d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be02858b75b1adbf6d43c4ad543ed82d07789eb9f6bc5adee21b6c3bc2806e5d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> 3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kyungwon Cho, Hanbyul Joo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19283" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19283</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eeffc3f64bbeee78f48fdd58e03a511d7a87a0d92465f1e6a49a7e082ffac166_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eeffc3f64bbeee78f48fdd58e03a511d7a87a0d92465f1e6a49a7e082ffac166_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ivan DeAndres-Tame, Chengwei Ye, Ruben Tolosana, Ruben Vera-Rodriguez, Shiqi Yu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19275" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19275</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d12748fd351c66664603a2df32d39e3ed4e526013cc4fc7f328dc001dea6e6a2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d12748fd351c66664603a2df32d39e3ed4e526013cc4fc7f328dc001dea6e6a2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] RMLer: Synthesizing Novel Objects across Diverse Categories via Reinforcement Mixing Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jun Li, Zikun Chen, Haibo Chen, Shuo Chen, Jian Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19300" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19300</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4417f7bedbf213cebde42bcbbe520cb32e68e60eec8444be79b07344e3b47020_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4417f7bedbf213cebde42bcbbe520cb32e68e60eec8444be79b07344e3b47020_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RMLer: Synthesizing Novel Objects across Diverse Categories via Reinforcement Mixing Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xu Zhang, Junyao Ge, Yang Zheng, Kaitai Guo, Jimin Liang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19302" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19302</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05bd6cba707b6fa3a38c669135a7bae69d4a297f430743568b2ec5258f28c554_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05bd6cba707b6fa3a38c669135a7bae69d4a297f430743568b2ec5258f28c554_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MAGIC: Achieving Superior Model Merging via Magnitude Calibration</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yayuan Li, Jian Zhang, Jintao Guo, Zihan Cheng, Lei Qi, Yinghuan Shi, Yang Gao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19320" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19320</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7204250ada52cfa8e70ee24634b9a58aab0085ac9d5854e5c672a585fb92a0a6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7204250ada52cfa8e70ee24634b9a58aab0085ac9d5854e5c672a585fb92a0a6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MAGIC: Achieving Superior Model Merging via Magnitude Calibration</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Marica Muffoletto, Uxio Hermida, Charlène Mauger, Avan Suinesiaputra, Yiyang Xu, Richard Burns, Lisa Pankewitz, Andrew D McCulloch, Steffen E Petersen, Daniel Rueckert, Alistair A Young</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19316" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19316</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2169c1336bc2f9517102921ae45f8684aaa37f9e051cbaf98dee4fa4fd0d1da4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2169c1336bc2f9517102921ae45f8684aaa37f9e051cbaf98dee4fa4fd0d1da4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hui Li, Jiayue Lyu, Fu-Yun Wang, Kaihui Cheng, Siyu Zhu, Jingdong Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19311" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19311</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/333652b938a5ecac737d2dfcea2bae93a2f072e15ac1c354ce9b2da1931714cc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/333652b938a5ecac737d2dfcea2bae93a2f072e15ac1c354ce9b2da1931714cc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Extended OpenTT Games Dataset: A table tennis dataset for fine-grained shot type and point outcome</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Moamal Fadhil Abdul, Jonas Bruun Hubrechts, Thomas Martini Jørgensen, Emil Hovad</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19327" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19327</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7996df4a8c51ad78b9cf6e38d68e28803f1e0e8374ec48e5258b82ca6726f085_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7996df4a8c51ad78b9cf6e38d68e28803f1e0e8374ec48e5258b82ca6726f085_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Extended OpenTT Games Dataset: A table tennis dataset for fine-grained shot type and point outcome</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] DeltaMIL: Gated Memory Integration for Efficient and Discriminative Whole Slide Image Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yueting Zhu, Yuehao Song, Shuai Zhang, Wenyu Liu, Xinggang Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19331" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19331</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09dc308a5637ec12a3c4efc8fea104a4feed83c0622643ceb02afbe80f82e58c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09dc308a5637ec12a3c4efc8fea104a4feed83c0622643ceb02afbe80f82e58c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DeltaMIL: Gated Memory Integration for Efficient and Discriminative Whole Slide Image Analysis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] GANeXt: A Fully ConvNeXt-Enhanced Generative Adversarial Network for MRI- and CBCT-to-CT Synthesis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Siyuan Mei, Yan Xia, Fuxin Fan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19336" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19336</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da8f1615b6e017ea5a7845dd78d938dfd6175022b2518294c8d72a1f24b2befa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da8f1615b6e017ea5a7845dd78d938dfd6175022b2518294c8d72a1f24b2befa_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GANeXt: A Fully ConvNeXt-Enhanced Generative Adversarial Network for MRI- and CBCT-to-CT Synthesis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhenyang Huang, Xiao Yu, Yi Zhang, Decheng Wang, Hang Ruan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19354" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19354</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe8395ab00c36c5c60a6a3cce591a67d23872c3d730ee745491ee87f86bf9993_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe8395ab00c36c5c60a6a3cce591a67d23872c3d730ee745491ee87f86bf9993_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Efficient Spike-driven Transformer for High-performance Drone-View Geo-Localization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhongwei Chen, Hai-Jun Rong, Zhao-Xu Yang, Guoqi Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19365" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19365</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e5753a79bc7c6daf06150580604d523fe5ee37173b2bd7e625292862bfda957a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e5753a79bc7c6daf06150580604d523fe5ee37173b2bd7e625292862bfda957a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Efficient Spike-driven Transformer for High-performance Drone-View Geo-Localization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hongwei Fan, Hang Dai, Jiyao Zhang, Jinzhou Li, Qiyang Yan, Yujie Zhao, Mingju Gao, Jinghang Wu, Hao Tang, Hao Dong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19390" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19390</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf1e9d4003951dba668a306d606416b006b75689695b9667afa58a3ba76ea89_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf1e9d4003951dba668a306d606416b006b75689695b9667afa58a3ba76ea89_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yueyao Chen, Kai-Ni Wang, Dario Tayupo, Arnaud Huaulm&#x27;e, Krystel Nyangoh Timoh, Pierre Jannin, Qi Dou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19387" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19387</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d09473d7af5f8db0e2099dcd5a18592d023dddc412cada060f4f05596921ff9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d09473d7af5f8db0e2099dcd5a18592d023dddc412cada060f4f05596921ff9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yujie Zhao, Hongwei Fan, Di Chen, Shengcong Chen, Liliang Chen, Xiaoqi Li, Guanghui Ren, Hao Dong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19402" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19402</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3585a78a8a01def680be454d4e0abbbbcf24961bd331f0f18d30d4fa2409128_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3585a78a8a01def680be454d4e0abbbbcf24961bd331f0f18d30d4fa2409128_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Non-Contrast CT Esophageal Varices Grading through Clinical Prior-Enhanced Multi-Organ Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiaoming Zhang, Chunli Li, Jiacheng Hao, Yuan Gao, Danyang Tu, Jianyi Qiao, Xiaoli Yin, Le Lu, Ling Zhang, Ke Yan, Yang Hou, Yu Shi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19415" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19415</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d823776fb42e2083e867607e00670f7745cc6a59e773b0a990924b705cbb63ce_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d823776fb42e2083e867607e00670f7745cc6a59e773b0a990924b705cbb63ce_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Non-Contrast CT Esophageal Varices Grading through Clinical Prior-Enhanced Multi-Organ Analysis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Fei Ge, Ying Huang, Jie Liu, Guixuan Zhang, Zhi Zeng, Shuwu Zhang, Hu Guan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19438" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19438</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c5f1cc5f1508ff7f3dac04d0e138fe9c30202bb132ef0c4725622de8caf6c5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c5f1cc5f1508ff7f3dac04d0e138fe9c30202bb132ef0c4725622de8caf6c5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yi Xin, Siqi Luo, Qi Qin, Haoxing Chen, Kaiwen Zhu, Zhiwei Zhang, Yangfan He, Rongchao Zhang, Jinbin Bai, Shuo Cao, Bin Fu, Junjun He, Yihao Liu, Yuewen Cao, Xiaohong Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19433" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19433</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e45156e486a1bc9ed274f1c77ca361208ca741b5f093796d4e95c9d983ba1b84_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e45156e486a1bc9ed274f1c77ca361208ca741b5f093796d4e95c9d983ba1b84_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Evelyn Zhang, Fufu Yu, Aoqi Wu, Zichen Wen, Ke Yan, Shouhong Ding, Biqing Qi, Linfeng Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19443" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19443</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b276a091b11e8296a53fd51f1160b3e57aa375c543150f9b00e1f03505b59fa1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b276a091b11e8296a53fd51f1160b3e57aa375c543150f9b00e1f03505b59fa1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Sign Language Recognition using Parallel Bidirectional Reservoir Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nitin Kumar Singh, Arie Rachmad Syulistyo, Yuichiro Tanaka, Hakaru Tamukoh</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19451" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19451</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/554347db8f25765f90ed0de383600422b9f8c928bbd223207e7df0e299098ce4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/554347db8f25765f90ed0de383600422b9f8c928bbd223207e7df0e299098ce4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Sign Language Recognition using Parallel Bidirectional Reservoir Computing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Emotion-Director: Bridging Affective Shortcut in Emotion-Oriented Image Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Guoli Jia, Junyao Hu, Xinwei Long, Kai Tian, Kaiyan Zhang, KaiKai Zhao, Ning Ding, Bowen Zhou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19479" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19479</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc7b54a9b2b78f7914a0108337ac26045e762e0d69fe4d901c3f358805a1715b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc7b54a9b2b78f7914a0108337ac26045e762e0d69fe4d901c3f358805a1715b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Emotion-Director: Bridging Affective Shortcut in Emotion-Oriented Image Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Dynamic Stream Network for Combinatorial Explosion Problem in Deformable Medical Image Registration</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shaochen Bi, Yuting He, Weiming Wang, Hao Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19486" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19486</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb129fee202e72128dcf5cd341766c1e138b9f80593cb33ba80612c8b99c355a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb129fee202e72128dcf5cd341766c1e138b9f80593cb33ba80612c8b99c355a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dynamic Stream Network for Combinatorial Explosion Problem in Deformable Medical Image Registration</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ziyang Song, Zelin Zang, Zuyao Chen, Xusheng Liang, Dong Yi, Jinlin Wu, Hongbin Liu, Jiebo Luo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19512" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19512</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d14f0ebe1771704c1788aacd2e3db88e7c3b990ef8113a061936422f9bb95889_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d14f0ebe1771704c1788aacd2e3db88e7c3b990ef8113a061936422f9bb95889_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Georgios Voulgaris</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19504" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19504</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b44fc34cd61f2154ab40ee342bff48a158e28f41d4da554eeb8a5175ecd28b9d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b44fc34cd61f2154ab40ee342bff48a158e28f41d4da554eeb8a5175ecd28b9d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Convolutional Neural Deferred Shader for Physics Based Rendering</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhuo He, Yingdong Ru, Qianying Liu, Paul Henderson, Nicolas Pugeault</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19522" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19522</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6cedf1c27957e3514f227025357a6a77d6e8cc183d8b8443ff380a0198a2b27e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6cedf1c27957e3514f227025357a6a77d6e8cc183d8b8443ff380a0198a2b27e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Convolutional Neural Deferred Shader for Physics Based Rendering</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SlicerOrbitSurgerySim: An Open-Source Platform for Virtual Registration and Quantitative Comparison of Preformed Orbital Plates</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chi Zhang, Braedon Gunn, Andrew M. Read-Fuller</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19534" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19534</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2dda742639cf4aa80038c7f5a4810b7a39a34acaa57f71b1646ebafce80e27e8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2dda742639cf4aa80038c7f5a4810b7a39a34acaa57f71b1646ebafce80e27e8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SlicerOrbitSurgerySim: An Open-Source Platform for Virtual Registration and Quantitative Comparison of Preformed Orbital Plates</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Multi-Modal Soccer Scene Analysis with Masked Pre-Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Marc Peral, Guillem Capellera, Luis Ferraz, Antonio Rubio, Antonio Agudo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19528" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19528</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca38cc1fee364a3f5a5912d1d267f89ff10d2261c0295e00eeca8f554a12e3d5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca38cc1fee364a3f5a5912d1d267f89ff10d2261c0295e00eeca8f554a12e3d5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multi-Modal Soccer Scene Analysis with Masked Pre-Training</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Moritz Böhle, Amélie Royer, Juliette Marrie, Edouard Grave, Patrick Pérez</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19535" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19535</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b20ae57f683974a6959785befd45010cb87dde73d9ccc345f16e11af116951e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b20ae57f683974a6959785befd45010cb87dde73d9ccc345f16e11af116951e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] StoryMem: Multi-shot Long Video Storytelling with Memory</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kaiwen Zhang, Liming Jiang, Angtian Wang, Jacob Zhiyuan Fang, Tiancheng Zhi, Qing Yan, Hao Kang, Xin Lu, Xingang Pan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19539" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19539</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a7a12b30bf051906255b90290fece33407e3691deaf21452976c48e1d52f546_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a7a12b30bf051906255b90290fece33407e3691deaf21452976c48e1d52f546_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> StoryMem: Multi-shot Long Video Storytelling with Memory</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ziqiao Peng, Yi Chen, Yifeng Ma, Guozhen Zhang, Zhiyao Sun, Zixiang Zhou, Youliang Zhang, Zhengguang Zhou, Zhaoxin Fan, Hongyan Liu, Yuan Zhou, Qinglin Lu, Jun He</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19546" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19546</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a48fa101dd9fce1c329f7df974385f6ec3d96f2ce0111f84aa53a212233bd01_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a48fa101dd9fce1c329f7df974385f6ec3d96f2ce0111f84aa53a212233bd01_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] BabyFlow: 3D modeling of realistic and expressive infant faces</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Antonia Alomar, Mireia Masias, Marius George Linguraru, Federico M. Sukno, Gemma Piella</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19560" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19560</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4bdcd1bc9dd6e1141b9b283f24968fbb4a40dc22257a719c5e16fbac178220f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4bdcd1bc9dd6e1141b9b283f24968fbb4a40dc22257a719c5e16fbac178220f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> BabyFlow: 3D modeling of realistic and expressive infant faces</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] No Data? No Problem: Robust Vision-Tabular Learning with Missing Values</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Marta Hasny, Laura Daza, Keno Bressem, Maxime Di Folco, Julia Schnabel</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19602" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19602</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/750e85c1bc605ca0702dd33fe3c21b8f7a30131f1bd4c08b119b6cd868aa6e98_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/750e85c1bc605ca0702dd33fe3c21b8f7a30131f1bd4c08b119b6cd868aa6e98_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> No Data? No Problem: Robust Vision-Tabular Learning with Missing Values</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Eric Zimmermann, Harley Wiltzer, Justin Szeto, David Alvarez-Melis, Lester Mackey</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19605" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19605</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8876f353bd3b6bc215381979aff16556177f0d7ca7e5b9cf3aa366d3fbd3c21d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8876f353bd3b6bc215381979aff16556177f0d7ca7e5b9cf3aa366d3fbd3c21d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jiaqi Peng, Wenzhe Cai, Yuqiang Yang, Tai Wang, Yuan Shen, Jiangmiao Pang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19629" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19629</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c789e71ef34ed4c917c96dfb4b01a72b2ebbbaf1b31aedd33c19890927a051c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c789e71ef34ed4c917c96dfb4b01a72b2ebbbaf1b31aedd33c19890927a051c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MapTrace: Scalable Data Generation for Route Tracing on Maps</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Artemis Panagopoulou, Aveek Purohit, Achin Kulshrestha, Soroosh Yazdani, Mohit Goyal</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19609" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19609</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7cc350a173ade821ef309bec6e6155158ae8332042895a8d6f45396c8b70c06_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7cc350a173ade821ef309bec6e6155158ae8332042895a8d6f45396c8b70c06_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MapTrace: Scalable Data Generation for Route Tracing on Maps</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Generative diffusion models for agricultural AI: plant image generation, indoor-to-outdoor translation, and expert preference alignment</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Da Tan, Michael Beck, Christopher P. Bidinosti, Robert H. Gulden, Christopher J. Henry</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19632" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19632</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2f66f3db49949bf517b4441efbd4d2391e38803e0ee9434c98db47da1306caf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2f66f3db49949bf517b4441efbd4d2391e38803e0ee9434c98db47da1306caf_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generative diffusion models for agricultural AI: plant image generation, indoor-to-outdoor translation, and expert preference alignment</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Argha Kamal Samanta, Harshika Goyal, Vasudha Joshi, Tushar Mungle, Pabitra Mitra</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19663" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19663</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cd268a98c5baac8b164ba733c255159614de6b969e8c6dd3f943fa74d98e5a1b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cd268a98c5baac8b164ba733c255159614de6b969e8c6dd3f943fa74d98e5a1b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] 4D Gaussian Splatting as a Learned Dynamical System</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Arnold Caleb Asiimwe, Carl Vondrick</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19648" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19648</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a72a5f6b1ca2d0116e20e704886e46a58e929e4b87b6d69b18361c69528e49d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a72a5f6b1ca2d0116e20e704886e46a58e929e4b87b6d69b18361c69528e49d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> 4D Gaussian Splatting as a Learned Dynamical System</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hanyang Kong, Xingyi Yang, Xiaoxu Zheng, Xinchao Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19678" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19678</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4f113f5d19fa4ca45dd649c8c074bdcd96d439b1640d1340c9f10070d3b5a62_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4f113f5d19fa4ca45dd649c8c074bdcd96d439b1640d1340c9f10070d3b5a62_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Over++: Generative Video Compositing for Layer Interaction Effects</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Luchao Qi, Jiaye Wu, Jun Myeong Choi, Cary Phillips, Roni Sengupta, Dan B Goldman</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19661" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19661</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e4b3add32e80abbfca0b3aa657bd70035f1f8f7db466fff302f7edb85698647_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e4b3add32e80abbfca0b3aa657bd70035f1f8f7db466fff302f7edb85698647_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Over++: Generative Video Compositing for Layer Interaction Effects</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mojtaba Safari, Shansong Wang, Vanessa L Wildman, Mingzhe Hu, Zach Eidex, Chih-Wei Chang, Erik H Middlebrooks, Richard L.J Qiu, Pretesh Patel, Ashesh B. Jania, Hui Mao, Zhen Tian, Xiaofeng Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19676" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19676</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/342e0f596d7de60763df762d3410c257d5603f80f4a044d27f87795d68b9c1dd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/342e0f596d7de60763df762d3410c257d5603f80f4a044d27f87795d68b9c1dd_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zixuan Ye, Quande Liu, Cong Wei, Yuanxing Zhang, Xintao Wang, Pengfei Wan, Kun Gai, Wenhan Luo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19686" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19686</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/843c501d9e520248d2e1a28597703cd3b54542449f629d50d194d54e77fa641e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/843c501d9e520248d2e1a28597703cd3b54542449f629d50d194d54e77fa641e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] VA-<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">π</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">π</span></span></span></span>: Variational Policy Alignment for Pixel-Aware Autoregressive Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xinyao Liao, Qiyuan He, Kai Xu, Xiaoye Qu, Yicong Li, Wei Wei, Angela Yao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19680" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19680</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce1823fb4399aafb9a993608f6bc9e7070ee1def4476bd9868d19ab0d1633442_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce1823fb4399aafb9a993608f6bc9e7070ee1def4476bd9868d19ab0d1633442_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VA-<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">π</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">π</span></span></span></span>: Variational Policy Alignment for Pixel-Aware Autoregressive Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Weichen Fan, Haiwen Diao, Quan Wang, Dahua Lin, Ziwei Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19693" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19693</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/546a4d6b5b3d2bcc5adcfd25b27a121f46760fd0f9e85b7b4c89467641629c03_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/546a4d6b5b3d2bcc5adcfd25b27a121f46760fd0f9e85b7b4c89467641629c03_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Zero-shot Reconstruction of In-Scene Object Manipulation from Video</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dixuan Lin, Tianyou Wang, Zhuoyang Pan, Yufu Wang, Lingjie Liu, Kostas Daniilidis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19684" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19684</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/967b97ff200c76a7e13dbcc9b4b2be1132807200b91f7312ad2eb4984f48bb88_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/967b97ff200c76a7e13dbcc9b4b2be1132807200b91f7312ad2eb4984f48bb88_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Zero-shot Reconstruction of In-Scene Object Manipulation from Video</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A curated UK rain radar data set for training and benchmarking nowcasting models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Viv Atureta, Rifki Priansyah Jasin, Stefan Siegert</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17924" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17924</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71fd337b26c3bfd8670295386b9f2b4df184043caa1e2c1b8a39442f6cdb1c15_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71fd337b26c3bfd8670295386b9f2b4df184043caa1e2c1b8a39442f6cdb1c15_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A curated UK rain radar data set for training and benchmarking nowcasting models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Apoorv Vyas, Heng-Jui Chang, Cheng-Fu Yang, Po-Yao Huang, Luya Gao, Julius Richter, Sanyuan Chen, Matt Le, Piotr Dollár, Christoph Feichtenhofer, Ann Lee, Wei-Ning Hsu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19687" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19687</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de3d411e351c730f5a4f6bcbb2b3a9ec59b568b77417127cf865aadf04270b18_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de3d411e351c730f5a4f6bcbb2b3a9ec59b568b77417127cf865aadf04270b18_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Disentangled representations via score-based variational autoencoders</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Benjamin S. H. Lyo, Eero P. Simoncelli, Cristina Savin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17127" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17127</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c4fff8035bd7179107df39681970450e10d3f516b687330e2caedbac602c68b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c4fff8035bd7179107df39681970450e10d3f516b687330e2caedbac602c68b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Disentangled representations via score-based variational autoencoders</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mingrui Wu, Zhaozhi Wang, Fangjinhua Wang, Jiaolong Yang, Marc Pollefeys, Tong Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19683" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19683</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7452b3f045d2d058f3a16f8690c6b4f168b374381f31024557431e4ca6b30cf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7452b3f045d2d058f3a16f8690c6b4f168b374381f31024557431e4ca6b30cf_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pablo Ruiz-Ponce, Sergio Escalera, José García-Rodríguez, Jiankang Deng, Rolandos Alexandros Potamias</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19692" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19692</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c358cce0c6057dff9e9db7532908f886d71d12899237ce5cdd1ce4b783108cd6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c358cce0c6057dff9e9db7532908f886d71d12899237ce5cdd1ce4b783108cd6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CytoDINO: Risk-Aware and Biologically-Informed Adaptation of DINOv3 for Bone Marrow Cytomorphology</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Aziz Muminov, Anne Pham</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17930" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17930</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7391864933852f8b3d970d31b51ddd7af845e9cf2ef6c035d939e6f6cca13967_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7391864933852f8b3d970d31b51ddd7af845e9cf2ef6c035d939e6f6cca13967_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CytoDINO: Risk-Aware and Biologically-Informed Adaptation of DINOv3 for Bone Marrow Cytomorphology</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Standardized Evaluation of Automatic Methods for Perivascular Spaces Segmentation in MRI -- MICCAI 2024 Challenge Results</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yilei Wu, Yichi Zhang, Zijian Dong, Fang Ji, An Sen Tan, Gifford Tan, Sizhao Tang, Huijuan Chen, Zijiao Chen, Eric Kwun Kei Ng, Jose Bernal, Hang Min, Ying Xia, Ines Vati, Liz Cooper, Xiaoyu Hu, Yuchen Pei, Yutao Ma, Victor Nozais, Ami Tsuchida, Pierre-Yves Hervé, Philippe Boutinaud, Marc Joliot, Junghwa Kang, Wooseung Kim, Dayeon Bak, Rachika E. Hamadache, Valeriia Abramova, Xavier Lladó, Yuntao Zhu, Zhenyu Gong, Xin Chen, John McFadden, Pek Lan Khong, Roberto Duarte Coello, Hongwei Bran Li, Woon Puay Koh, Christopher Chen, Joanna M. Wardlaw, Maria del C. Valdés Hernández, Juan Helen Zhou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18197" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18197</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/010f615acca3968f6c9a01a9f1e16da3691c03b0a86b7fb021c3877735493e22_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/010f615acca3968f6c9a01a9f1e16da3691c03b0a86b7fb021c3877735493e22_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Standardized Evaluation of Automatic Methods for Perivascular Spaces Segmentation in MRI -- MICCAI 2024 Challenge Results</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SLIM: Semantic-based Low-bitrate Image compression for Machines by leveraging diffusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hyeonjin Lee, Jun-Hyuk Kim, Jong-Seok Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18200" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18200</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9c567930d6d4ee43e33b9cf92ff8599d4af08021295eca7c47d016de7557640_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9c567930d6d4ee43e33b9cf92ff8599d4af08021295eca7c47d016de7557640_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SLIM: Semantic-based Low-bitrate Image compression for Machines by leveraging diffusion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Selective Phase-Aware Training of nnU-Net for Robust Breast Cancer Segmentation in Multi-Center DCE-MRI</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Beyza Zayim, Aissiou Ikram, Boukhiar Naima</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19225" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19225</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ddf6be7e15b5d6602d6e9aa40624abefc0e83cbee9e90302c334bd468ba1ea9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ddf6be7e15b5d6602d6e9aa40624abefc0e83cbee9e90302c334bd468ba1ea9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Selective Phase-Aware Training of nnU-Net for Robust Breast Cancer Segmentation in Multi-Center DCE-MRI</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Deep Learning for Primordial <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span>-mode Extraction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Eric Guzman, Joel Meyers</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19577" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19577</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9bbecd5a2eae70e7ea81960a62c36c37f9e7805f682cebee98cb7ab81bd67249_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9bbecd5a2eae70e7ea81960a62c36c37f9e7805f682cebee98cb7ab81bd67249_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Deep Learning for Primordial <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span>-mode Extraction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Rethinking Coupled Tensor Analysis for Hyperspectral Super-Resolution: Recoverable Modeling Under Endmember Variability</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Meng Ding, Xiao Fu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19489" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19489</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c4dd05c6080abf0291c4f46eccdfa18752df5e2fee1aad694dbdd4d50e61cd3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c4dd05c6080abf0291c4f46eccdfa18752df5e2fee1aad694dbdd4d50e61cd3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Rethinking Coupled Tensor Analysis for Hyperspectral Super-Resolution: Recoverable Modeling Under Endmember Variability</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ziqian Huang, Boxiao Yu, Siqi Li, Savas Ozdemir, Sangjin Bae, Jae Sung Lee, Guobao Wang, Kuang Gong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19584" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19584</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/35afd74d755844e3f7dd7c04116e75ba37dbf5e148c533c0e847939d30f5c6fc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/35afd74d755844e3f7dd7c04116e75ba37dbf5e148c533c0e847939d30f5c6fc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Niclas Griesshaber, Jochen Streb</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19675</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d49754318d6e9803642e66f7555bb2551be305239532d52cb9f0b2b55048ad6b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d49754318d6e9803642e66f7555bb2551be305239532d52cb9f0b2b55048ad6b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-24">2025-12-24<a href="#2025-12-24" class="hash-link" aria-label="Direct link to 2025-12-24" title="Direct link to 2025-12-24" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251224] PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Md Nahid Hasan Shuvo, Moinul Hossain</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19711" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19711</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d516fcc6c3de6b0e65c078e5e3f3dda23bfd77fbb9c5f4abfe2c509c2cb6dfe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d516fcc6c3de6b0e65c078e5e3f3dda23bfd77fbb9c5f4abfe2c509c2cb6dfe_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiangzhong Luo, Weichen Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19731" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19731</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c10d82314834148af959284b60a6d6f0fa9e36928106648626f8c43d4f07b79_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c10d82314834148af959284b60a6d6f0fa9e36928106648626f8c43d4f07b79_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Learning to Refocus with Video Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> SaiKiran Tedla, Zhoutong Zhang, Xuaner Zhang, Shumian Xin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19823" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19823</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d67b80f0c15466284688038780928e07f307c7269e0df0ded0424d3e770fcd6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d67b80f0c15466284688038780928e07f307c7269e0df0ded0424d3e770fcd6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning to Refocus with Video Diffusion Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] RANSAC Scoring Functions: Analysis and Reality Check</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> A. Shekhovtsov</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19850" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19850</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd50e25656a023b3c2995d13c741fc721d03038e35f7503eb116497b3cb8637d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd50e25656a023b3c2995d13c741fc721d03038e35f7503eb116497b3cb8637d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RANSAC Scoring Functions: Analysis and Reality Check</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Generating the Past, Present and Future from a Motion-Blurred Image</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> SaiKiran Tedla, Kelly Zhu, Trevor Canham, Felix Taubner, Michael S. Brown, Kiriakos N. Kutulakos, David B. Lindell</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19817" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19817</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d34df9bd29ac26646aab435f8fb3761d7ba6f54b4c9919286b04b5436dba1d0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d34df9bd29ac26646aab435f8fb3761d7ba6f54b4c9919286b04b5436dba1d0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generating the Past, Present and Future from a Motion-Blurred Image</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jong Wook Kim, Wonseok Roh, Ha Dam Baek, Pilhyeon Lee, Jonghyun Choi, Sangpil Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19871" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19871</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a4679f5f60c2af6c77a6712d0c27272bc4719706cb56235956b7caf4c1bac0f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a4679f5f60c2af6c77a6712d0c27272bc4719706cb56235956b7caf4c1bac0f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Unified Brain Surface and Volume Registration</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> S. Mazdak Abulnaga, Andrew Hoopes, Malte Hoffmann, Robin Magnet, Maks Ovsjanikov, Lilla Zöllei, John Guttag, Bruce Fischl, Adrian Dalca</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19928" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19928</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52472b7c53844ab7af247ff699ee1c659d2b3ff27e2e21ee8f187677824a5d8d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52472b7c53844ab7af247ff699ee1c659d2b3ff27e2e21ee8f187677824a5d8d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Unified Brain Surface and Volume Registration</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Houston H. Zhang, Tao Zhang, Baoze Lin, Yuanqi Xue, Yincheng Zhu, Huan Liu, Li Gu, Linfeng Ye, Ziqiang Wang, Xinxin Zuo, Yang Wang, Yuanhao Yu, Zhixiang Chi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19918" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19918</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afcd12646a75911cc37486f5af7f55491eaf6f54718182f47a36695579c55660_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afcd12646a75911cc37486f5af7f55491eaf6f54718182f47a36695579c55660_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Vehicle-centric Perception via Multimodal Structured Pre-training</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wentao Wu, Xiao Wang, Chenglong Li, Jin Tang, Bin Luo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19934" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19934</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25393280cf5e09ecc25c375a88de26bef6b6904fd90a6775cf7591ed8a615fe1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25393280cf5e09ecc25c375a88de26bef6b6904fd90a6775cf7591ed8a615fe1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Vehicle-centric Perception via Multimodal Structured Pre-training</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Block-Recurrent Dynamics in Vision Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mozes Jacobs, Thomas Fel, Richard Hakim, Alessandra Brondetta, Demba Ba, T. Andy Keller</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19941" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19941</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e10f9b4ab6d210902b2c5092ebfe1fcc82dd7a3d2032bfc97fbfadd16867c2a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e10f9b4ab6d210902b2c5092ebfe1fcc82dd7a3d2032bfc97fbfadd16867c2a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Block-Recurrent Dynamics in Vision Transformers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] SE360: Semantic Edit in 360<span class="katex-error" title="ParseError: KaTeX parse error: Expected group after &#x27;^&#x27; at position 1: ^̲" style="color:#cc0000">^</span> Panoramas via Hierarchical Data Construction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Haoyi Zhong, Fang-Lue Zhang, Andrew Chalmers, Taehyun Rhee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19943" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19943</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5acefee064893ec36480803c8a614ffcd70f5e22f389b691a22dff4da224abf7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5acefee064893ec36480803c8a614ffcd70f5e22f389b691a22dff4da224abf7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SE360: Semantic Edit in 360<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mo>∘</mo></msup></mrow><annotation encoding="application/x-tex">^\circ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6741em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6741em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∘</span></span></span></span></span></span></span></span></span></span></span> Panoramas via Hierarchical Data Construction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] HistoWAS: A Pathomics Framework for Large-Scale Feature-Wide Association Studies of Tissue Topology and Patient Outcomes</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuechen Yang, Junlin Guo, Yanfan Zhu, Jialin Yue, Junchao Zhu, Yu Wang, Shilin Zhao, Haichun Yang, Xingyi Guo, Jovan Tanevski, Laura Barisoni, Avi Z. Rosenberg, Yuankai Huo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19954" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19954</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebf1e5c1a2595dc81a69957123729d9aa0fa024a71bdfda164ce0ae6bf95d110_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebf1e5c1a2595dc81a69957123729d9aa0fa024a71bdfda164ce0ae6bf95d110_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HistoWAS: A Pathomics Framework for Large-Scale Feature-Wide Association Studies of Tissue Topology and Patient Outcomes</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] How Much 3D Do Video Foundation Models Encode?</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zixuan Huang, Xiang Li, Zhaoyang Lv, James M. Rehg</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19949" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19949</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b54fab192e555a908a0f7daf8d7992c85cd3241844d6a17c19d685c99c93dc5e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b54fab192e555a908a0f7daf8d7992c85cd3241844d6a17c19d685c99c93dc5e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> How Much 3D Do Video Foundation Models Encode?</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Peng Gao, Ke Li, Di Wang, Yongshan Zhu, Yiming Zhang, Xuemei Luo, Yifeng Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19990" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19990</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1691a202602f3bcf2d0e97787c25b8551ee2510f815f2c351ebacb3b3a37953_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1691a202602f3bcf2d0e97787c25b8551ee2510f815f2c351ebacb3b3a37953_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] WSD-MIL: Window Scale Decay Multiple Instance Learning for Whole Slide Image Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Le Feng, Li Xiao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19982" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19982</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1fecc5e0134d3112ae90b5c8ac61ae68bcd52b0697a6c32184a7fc10d72e0640_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1fecc5e0134d3112ae90b5c8ac61ae68bcd52b0697a6c32184a7fc10d72e0640_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> WSD-MIL: Window Scale Decay Multiple Instance Learning for Whole Slide Image Classification</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tamim Ahasan Rijon, Yeasin Arafath</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19989" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19989</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7415dab1086d45b2bb6f7a49ab33be753b3da047de23310de552ead99dc2448_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7415dab1086d45b2bb6f7a49ab33be753b3da047de23310de552ead99dc2448_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhenhao Li, Shaohan Yi, Zheng Liu, Leonartinus Gao, Minh Ngoc Le, Ambrose Ling, Zhuoran Wang, Md Amirul Islam, Zhixiang Chi, Yuanhao Yu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20000" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20000</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f2d07a108f93995c65cf13d740ca889c8e7dd494a7ef3c9222caba5e6ccf3c3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f2d07a108f93995c65cf13d740ca889c8e7dd494a7ef3c9222caba5e6ccf3c3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Blessing Agyei Kyem, Joshua Kofi Asamoah, Anthony Dontoh, Andrews Danyo, Eugene Denteh, Armstrong Aboah</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20011" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20011</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff4db8075e196efc52322813cedc23193239565c930612aec052f3acbb95eab5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff4db8075e196efc52322813cedc23193239565c930612aec052f3acbb95eab5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zepeng Xin, Kaiyu Li, Luodi Chen, Wanchen Li, Yuchen Xiao, Hui Qiao, Weizhan Zhang, Deyu Meng, Xiangyong Cao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20013" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20013</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24d12d8c96876ce1d14987d2507601d9688f72e9f86e06abfd01bc397241cbd6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24d12d8c96876ce1d14987d2507601d9688f72e9f86e06abfd01bc397241cbd6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chang Sun, Dongliang Xie, Bo Qin, Hong Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20032" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20032</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/800e8f7c8c22b74ebd9c804082b6c7f5a5c4998365cdbe511167991351910a98_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/800e8f7c8c22b74ebd9c804082b6c7f5a5c4998365cdbe511167991351910a98_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Anthony Dontoh, Stephanie Ivey, Armstrong Aboah</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20025" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20025</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8e8d7557e6f332097ab10b8c45895c9a3f2b19aa186bcd6c3071ac6e02ee8e7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8e8d7557e6f332097ab10b8c45895c9a3f2b19aa186bcd6c3071ac6e02ee8e7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">H^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Lin Li, Jiahui Li, Jiaming Lei, Jun Xiao, Feifei Shao, Long Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20029" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20029</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1aa2e8e4d0ae946b7e977e841084f034f6f83372776d08e5a2bf43a04e81003_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1aa2e8e4d0ae946b7e977e841084f034f6f83372776d08e5a2bf43a04e81003_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>{</mtext><mi>H</mi><msup><mo stretchy="false">}</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\text\{H\}^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">{</span></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ziwei Qin, Xuhui Song, Deqing Huang, Na Qin, Jun Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20026" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20026</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7dc5f8098baa15ab99d6ec49af6ea9b561b6b787a33ac1ec6e365ad910668048_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7dc5f8098baa15ab99d6ec49af6ea9b561b6b787a33ac1ec6e365ad910668048_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nguyen Lam Phu Quy, Pham Phu Hoa, Tran Chi Nguyen, Dao Sy Duy Minh, Nguyen Hoang Minh Ngoc, Huynh Trung Kiet</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20042" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20042</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9fb136c3c44d734ccd03ee7608dfc92d3612b466bfffc82e1d2efdfd79e5f161_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9fb136c3c44d734ccd03ee7608dfc92d3612b466bfffc82e1d2efdfd79e5f161_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] FlashLips: 100-FPS Mask-Free Latent Lip-Sync using Reconstruction Instead of Diffusion or GANs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Andreas Zinonos, Michał Stypułkowski, Antoni Bigata, Stavros Petridis, Maja Pantic, Nikita Drobyshev</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20033" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20033</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24426a84ed65236012992da21d9b902f793ee45db54516610ca6203c50494625_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24426a84ed65236012992da21d9b902f793ee45db54516610ca6203c50494625_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlashLips: 100-FPS Mask-Free Latent Lip-Sync using Reconstruction Instead of Diffusion or GANs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Progressive Learned Image Compression for Machine Perception</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jungwoo Kim, Jun-Hyuk Kim, Jong-Seok Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20070" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20070</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24993bfda343219d771658c8c9767048aec44f5b6784dcfb07bb075ba4304dd1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24993bfda343219d771658c8c9767048aec44f5b6784dcfb07bb075ba4304dd1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Progressive Learned Image Compression for Machine Perception</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hao Li, Fabian Deuser, Wenping Yin, Steffen Knoblauch, Wufan Zhao, Filip Biljecki, Yong Xue, Wei Huang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20056" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20056</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c37110ad3319d1434c17c04ae94a4dcfa94a278faf51360e359e1a063ce32e7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c37110ad3319d1434c17c04ae94a4dcfa94a278faf51360e359e1a063ce32e7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Haiyun Wei, Fan Lu, Yunwei Zhu, Zehan Zheng, Weiyi Xue, Lin Shao, Xudong Zhang, Ya Wu, Rong Fu, Guang Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20105" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20105</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a2dee7bb51f34affab3884be56b1bfb46b76c6a66e40400ed09830c5aabd0b0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a2dee7bb51f34affab3884be56b1bfb46b76c6a66e40400ed09830c5aabd0b0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Subrata Kumer Paula, Dewan Nafiul Islam Noora, Rakhi Rani Paula, Md. Ekramul Hamidb, Fahmid Al Faridc, Hezerul Abdul Karimd, Md. Maruf Al Hossain Princee, Abu Saleh Musa Miahb</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20104" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20104</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e8b67a33af7843f8ee6ec84eb87891b7d12b57e4af5fa1f77bce5e76a6b92b0d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e8b67a33af7843f8ee6ec84eb87891b7d12b57e4af5fa1f77bce5e76a6b92b0d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jinyoung Choi, Youngchae Kwon, Injung Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20088" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20088</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1946d7c3329c37db07556d963544aad7dbfeda194c515e4debdc6e3754d8920_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1946d7c3329c37db07556d963544aad7dbfeda194c515e4debdc6e3754d8920_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Alireza Moayedikia, Sattar Dorafshan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20113" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20113</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00b50254f658c253763f9df9bd57707873bdb0982863020f26c263961aa323d8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00b50254f658c253763f9df9bd57707873bdb0982863020f26c263961aa323d8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Niraj Prakash Kini, Shiau-Rung Tsai, Guan-Hsun Lin, Wen-Hsiao Peng, Ching-Wen Ma, Jenq-Neng Hwang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20128" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20128</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b40cd83392a8ac698c9e282aef2bd70809bcd4957965aa0c36506eb68c77ec30_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b40cd83392a8ac698c9e282aef2bd70809bcd4957965aa0c36506eb68c77ec30_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mohammad Helal Uddin, Liam Seymour, Sabur Baidya</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20120" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20120</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f965376a2586790dd27548936a060ee2437b3c5085b58331952dfddc1265b29f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f965376a2586790dd27548936a060ee2437b3c5085b58331952dfddc1265b29f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Cyrus Vachha, Yixiao Kang, Zach Dive, Ashwat Chidambaram, Anik Gupta, Eunice Jun, Bjoern Hartmann</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20129" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20129</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8400f1033a93635b0a5b706c568585f557f04cf8533a2b77ce1c55f08e14bef6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8400f1033a93635b0a5b706c568585f557f04cf8533a2b77ce1c55f08e14bef6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jingqi Tian, Yiheng Du, Haoji Zhang, Yuji Wang, Isaac Ning Lee, Xulong Bai, Tianrui Zhu, Jingxuan Niu, Yansong Tang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20117" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20117</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a290a5343ed508cf106c79abdb63608b4b4b69f2e0d42fc218565b45b8eb64f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a290a5343ed508cf106c79abdb63608b4b4b69f2e0d42fc218565b45b8eb64f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Thanh-Tung Le, Tuan Pham, Tung Nguyen, Deying Kong, Xiaohui Xie, Stephan Mandt</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20107" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20107</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/668ef8c168df89e0d5190bbd1ac0a2d6de0affc034e4a50d8e050242fd138dd8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/668ef8c168df89e0d5190bbd1ac0a2d6de0affc034e4a50d8e050242fd138dd8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Retrieval-augmented Prompt Learning for Pre-trained Foundation Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiang Chen, Yixin Ou, Quan Feng, Lei Li, Piji Li, Haibo Ye, Sheng-Jun Huang, Shuofei Qiao, Shumin Deng, Huajun Chen, Ningyu Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20145" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20145</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028ab8e7171d7f497cbdc48e136d419e8642bf876111786382aee29b15d0992_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028ab8e7171d7f497cbdc48e136d419e8642bf876111786382aee29b15d0992_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Retrieval-augmented Prompt Learning for Pre-trained Foundation Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Robert van de Ven, Trim Bresilla, Bram Nelissen, Ard Nieuwenhuizen, Eldert J. van Henten, Gert Kootstra</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20148" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20148</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0476ec59995b5f3259a9b2ab1456d67ddbbb750902c5acd3d684600b97c47598_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0476ec59995b5f3259a9b2ab1456d67ddbbb750902c5acd3d684600b97c47598_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] CoDi -- an exemplar-conditioned diffusion model for low-shot counting</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Grega Šuštar, Jer Pelhan, Alan Lukežič, Matej Kristan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20153" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20153</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19ee3eb6d09129b7fef5f1398fe3a279f203f7ff1a53a416574b63094eeefdf5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19ee3eb6d09129b7fef5f1398fe3a279f203f7ff1a53a416574b63094eeefdf5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CoDi -- an exemplar-conditioned diffusion model for low-shot counting</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sofian Chaybouti, Sanath Narayan, Yasser Dahou, Phúc H. Lê Khac, Ankit Singh, Ngoc Dung Huynh, Wamiq Reyaz Para, Hilde Kuehne, Hakim Hacid</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20157" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20157</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61612d4352aae9f7b2e5272f0843ecd4da94205a817c26dac3655aa0b24e73b2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61612d4352aae9f7b2e5272f0843ecd4da94205a817c26dac3655aa0b24e73b2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hao Guo, Xugong Qin, Jun Jie Ou Yang, Peng Zhang, Gangyan Zeng, Yubo Li, Hailun Lin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20174" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20174</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a74295652803d99c4b0631ef38e9684d12032ddd9797f217033f356497ff647_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a74295652803d99c4b0631ef38e9684d12032ddd9797f217033f356497ff647_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Generative Latent Coding for Ultra-Low Bitrate Image Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhaoyang Jia, Jiahao Li, Bin Li, Houqiang Li, Yan Lu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20194" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20194</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e95ca206803e3acc79be8a04fea7c616d1d6d5dc840a97528ce68341f589c143_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e95ca206803e3acc79be8a04fea7c616d1d6d5dc840a97528ce68341f589c143_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generative Latent Coding for Ultra-Low Bitrate Image Compression</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiangxuan Ren, Zhongdao Wang, Pin Tang, Guoqing Wang, Jilai Zheng, Chao Ma</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20217" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20217</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8799680440a14b4dded747c223ca374b82c834b527822c9fff101d2247c1d65d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8799680440a14b4dded747c223ca374b82c834b527822c9fff101d2247c1d65d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] JDPNet: A Network Based on Joint Degradation Processing for Underwater Image Enhancement</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tao Ye, Hongbin Ren, Chongbing Zhang, Haoran Chen, Xiaosong Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20213" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20213</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c290a68b3d97be1a375bfedfbc89e4ad87f03c4cfd59b51b99adeb151186d5ed_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c290a68b3d97be1a375bfedfbc89e4ad87f03c4cfd59b51b99adeb151186d5ed_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> JDPNet: A Network Based on Joint Degradation Processing for Underwater Image Enhancement</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] How I Met Your Bias: Investigating Bias Amplification in Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nathan Roos, Ekaterina Iakovleva, Ani Gjergji, Vito Paolo Pastore, Enzo Tartaglione</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20233" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20233</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3c43be0c18a78b4304c027f45372c35a663531d4f9536a15c2b4ca0dbb155b2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3c43be0c18a78b4304c027f45372c35a663531d4f9536a15c2b4ca0dbb155b2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> How I Met Your Bias: Investigating Bias Amplification in Diffusion Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xuanyu Hu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20249" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20249</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1c6a732f9a0082b695f01f79b2bcc47f909daa0f9d50a6af4d86a633213b328_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1c6a732f9a0082b695f01f79b2bcc47f909daa0f9d50a6af4d86a633213b328_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] IndicDLP: A Foundational Dataset for Multi-Lingual and Multi-Domain Document Layout Parsing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Oikantik Nath, Sahithi Kukkala, Mitesh Khapra, Ravi Kiran Sarvadevabhatla</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20236" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20236</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/027b9426177f33cb782197baba41e405dda8dcde86fe03cd16387612d6f69294_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/027b9426177f33cb782197baba41e405dda8dcde86fe03cd16387612d6f69294_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> IndicDLP: A Foundational Dataset for Multi-Lingual and Multi-Domain Document Layout Parsing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jinghao Shi, Jianing Song</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20255" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20255</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2b3920e5370f3c041a4e84894da06ffa2a4f371a30e7ead753e72b679159943_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2b3920e5370f3c041a4e84894da06ffa2a4f371a30e7ead753e72b679159943_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Daniele Cardullo, Simone Teglia, Irene Amerini</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20257" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20257</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77044e7e1eb0cbde388f554efc56df1d6faee9e6b4ec8be9ca9c2664befbf208_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77044e7e1eb0cbde388f554efc56df1d6faee9e6b4ec8be9ca9c2664befbf208_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Degradation-Aware Metric Prompting for Hyperspectral Image Restoration</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Binfeng Wang, Di Wang, Haonan Guo, Ying Fu, Jing Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20251" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20251</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4f8150acede0f77c8c88a8f6399592fa53d83830252a9609aa77972be050f28_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4f8150acede0f77c8c88a8f6399592fa53d83830252a9609aa77972be050f28_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Degradation-Aware Metric Prompting for Hyperspectral Image Restoration</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>D</mi><msup><mo stretchy="false">}</mo><mo stretchy="false">{</mo></msup><mn>3</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{D\}^\{3\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mopen mtight">{</span></span></span></span></span></span></span></span><span class="mord">3</span><span class="mclose">}</span></span></span></span>{ETOR}: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>D</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{D\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose">}</span></span></span></span>ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>D</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{D\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose">}</span></span></span></span>ebiasing for Weakly-Supervised Camouflaged Object <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>D</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{D\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose">}</span></span></span></span>etection with Scribble Annotations</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jiawei Ge, Jiuxin Cao, Xinyi Li, Xuelin Zhu, Chang Liu, Bo Liu, Chen Feng, Ioannis Patras</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20260" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20260</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee0606d3fe6e6ae30bbf9417baa7948dab878cfcc6b70e36031424c107aafb81_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee0606d3fe6e6ae30bbf9417baa7948dab878cfcc6b70e36031424c107aafb81_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>D</mi><msup><mo stretchy="false">}</mo><mo stretchy="false">{</mo></msup><mn>3</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{D\}^\{3\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mopen mtight">{</span></span></span></span></span></span></span></span><span class="mord">3</span><span class="mclose">}</span></span></span></span>{ETOR}: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>D</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{D\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose">}</span></span></span></span>ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>D</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{D\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose">}</span></span></span></span>ebiasing for Weakly-Supervised Camouflaged Object <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>D</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{D\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose">}</span></span></span></span>etection with Scribble Annotations</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] UbiQVision: Quantifying Uncertainty in XAI for Image Recognition</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Akshat Dubey, Aleksandar Anžel, Bahar İlgen, Georges Hattab</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20288" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20288</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6106b87e48429449f0b11c2765bdedb47797d7822e4b38ad3a9fe7f94b92dacb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6106b87e48429449f0b11c2765bdedb47797d7822e4b38ad3a9fe7f94b92dacb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> UbiQVision: Quantifying Uncertainty in XAI for Image Recognition</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ji-Hoon Kim, Junseok Ahn, Doyeop Kwak, Joon Son Chung, Shinji Watanabe</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20296" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20296</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/217e6d95bfc0e38be8f5a7356d3e869c3b3a5b9d4daf05b6435c3c756df247c6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/217e6d95bfc0e38be8f5a7356d3e869c3b3a5b9d4daf05b6435c3c756df247c6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhongyu Xia, Wenhao Chen, Yongtao Wang, Ming-Hsuan Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20299" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20299</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0961ae49fbc925ad5eacb6602aeec6cfdfabae07b252a044cd181bdb5a47746_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0961ae49fbc925ad5eacb6602aeec6cfdfabae07b252a044cd181bdb5a47746_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Qingdong He, Xueqin Chen, Yanjie Pan, Peng Tang, Pengcheng Xu, Zhenye Gan, Chengjie Wang, Xiaobin Hu, Jiangning Zhang, Yabiao Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20340" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20340</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce852f1d570d91487912f5d0abf0a93c49ad00eeecfdd840d9dbb9025ff8fd3a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce852f1d570d91487912f5d0abf0a93c49ad00eeecfdd840d9dbb9025ff8fd3a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> V. Kovalev, A. Kuvshinov, A. Buzovkin, D. Pokidov, D. Timonin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20362" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20362</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9180090a8d8d701e63ea635b11cc6a39fed9f252167568980aefb8786a66ef43_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9180090a8d8d701e63ea635b11cc6a39fed9f252167568980aefb8786a66ef43_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Field-Space Attention for Structure-Preserving Earth System Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Maximilian Witte, Johannes Meuer, Étienne Plésiat, Christopher Kadow</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20350" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20350</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8623c14b77fd771294e1b936a58143cff6a715df1b4a5026fe2d59708383e6e2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8623c14b77fd771294e1b936a58143cff6a715df1b4a5026fe2d59708383e6e2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Field-Space Attention for Structure-Preserving Earth System Transformers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Linking Faces and Voices Across Languages: Insights from the FAME 2026 Challenge</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Marta Moscati, Ahmed Abdullah, Muhammad Saad Saeed, Shah Nawaz, Rohan Kumar Das, Muhammad Zaigham Zaheer, Junaid Mir, Muhammad Haroon Yousaf, Khalid Mahmood Malik, Markus Schedl</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20376" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20376</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/483debde320d1a401b91478496a4b53ca3f74d52718353845af8598a16bf6167_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/483debde320d1a401b91478496a4b53ca3f74d52718353845af8598a16bf6167_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Linking Faces and Voices Across Languages: Insights from the FAME 2026 Challenge</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Linfei Li, Lin Zhang, Zhong Wang, Ying Shen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20377" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20377</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06dc77546a31f168b83f87c8efbfe002590b29ab252385b482db477a74d615ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06dc77546a31f168b83f87c8efbfe002590b29ab252385b482db477a74d615ac_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> YuChe Hsu, AnJui Wang, TsaiChing Ni, YuanFu Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20387" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20387</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d9aa5293bf4ca8e839b122277f1484ffb43b6211d54741d7eb7f58312f5509_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d9aa5293bf4ca8e839b122277f1484ffb43b6211d54741d7eb7f58312f5509_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Chain-of-Anomaly Thoughts with Large Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pedro Domingos, João Pereira, Vasco Lopes, João Neves, David Semedo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20417" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20417</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e26e3aba92227ff2a24c227033ffa15e8e191cf437a0a97c819748f4dfb7c94_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e26e3aba92227ff2a24c227033ffa15e8e191cf437a0a97c819748f4dfb7c94_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Chain-of-Anomaly Thoughts with Large Vision-Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Simplifying Multi-Task Architectures Through Task-Specific Normalization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mihai Suteu, Ovidiu Serban</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20420" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20420</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d24e6af533166dd44855079b97d7233c65878aa61e02267e65f4e306467d04b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d24e6af533166dd44855079b97d7233c65878aa61e02267e65f4e306467d04b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Simplifying Multi-Task Architectures Through Task-Specific Normalization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Junho Yoon, Jaemo Jung, Hyunju Kim, Dongman Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20409" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20409</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d4c944d1aa99e38d69c83c388503646f62271c8bb649aafcafb57ad0ba7c7d0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d4c944d1aa99e38d69c83c388503646f62271c8bb649aafcafb57ad0ba7c7d0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Abdullah Al Shafi, Abdul Muntakim, Pintu Chandra Shill, Rowzatul Zannat, Abdullah Al-Amin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20431" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20431</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/905e502b77110f2fdb7bcfd863c906242998a2915d794e39c3da2377a2743392_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/905e502b77110f2fdb7bcfd863c906242998a2915d794e39c3da2377a2743392_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] High Dimensional Data Decomposition for Anomaly Detection of Textured Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ji Song, Xing Wang, Jianguo Wu, Xiaowei Yue</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20432" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20432</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1df48b800de53f9fff1712283209aa73147125a9725200492a8bcc4cd35e7182_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1df48b800de53f9fff1712283209aa73147125a9725200492a8bcc4cd35e7182_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> High Dimensional Data Decomposition for Anomaly Detection of Textured Images</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Anh Dao, Manh Tran, Yufei Zhang, Xiaoming Liu, Zijun Cui</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20451" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20451</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dab892d1698a54ca5bea9ec4d990ec556b33cc4b339ac925ef5d18dba7b5dbae_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dab892d1698a54ca5bea9ec4d990ec556b33cc4b339ac925ef5d18dba7b5dbae_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> James E. Gallagher, Edward J. Oughton, Jana Kosecka</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20487" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20487</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09ac139242f7c2d01b6739f6aa8ae3ee3439e687ec6c7b420e17bccf33e9de40_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09ac139242f7c2d01b6739f6aa8ae3ee3439e687ec6c7b420e17bccf33e9de40_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] SirenPose: Dynamic Scene Reconstruction via Geometric Supervision</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kaitong Cai, Jensen Zhang, Jing Yang, Keze Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20531" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20531</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbf03f3fbe032434f49cece08cea9e779ea7ea57fa26c8f00eeed2e7c9080766_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbf03f3fbe032434f49cece08cea9e779ea7ea57fa26c8f00eeed2e7c9080766_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SirenPose: Dynamic Scene Reconstruction via Geometric Supervision</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shengchao Zhou, Yuxin Chen, Yuying Ge, Wei Huang, Jiehong Lin, Ying Shan, Xiaojuan Qi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20557" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20557</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e481c21d437a92edaec37fae6af73e02495508b9597f95d16d784b230f3165c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e481c21d437a92edaec37fae6af73e02495508b9597f95d16d784b230f3165c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Gorjan Radevski</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20501" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20501</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e4fc93af34bdcb98669d1a3c900d03900cb8c1359b89368e444c1c70848cdd4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e4fc93af34bdcb98669d1a3c900d03900cb8c1359b89368e444c1c70848cdd4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yiming Zhao, Yuanpeng Gao, Yuxuan Luo, Jiwei Duan, Shisong Lin, Longfei Xiong, Zhouhui Lian</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20479" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20479</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4093d97a2621866450d2ab0e47bbdaa622a6fe81e981843cdd087e11e4301902_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4093d97a2621866450d2ab0e47bbdaa622a6fe81e981843cdd087e11e4301902_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Long Nguyen, Micha Fauth, Bernhard Jaeger, Daniel Dauner, Maximilian Igl, Andreas Geiger, Kashyap Chitta</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20563" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20563</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f62c50da026de5eec286e01930af394650fb8b9c309ff48cd8f733ee9ca220b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f62c50da026de5eec286e01930af394650fb8b9c309ff48cd8f733ee9ca220b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kaitong Cai, Jusheng Zhang, Jing Yang, Yijia Fan, Pengtao Xie, Jian Wang, Keze Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20561" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20561</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72707d221b008970aee7befd580ee702951e338a32b995af62d9c893fe16e7e1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72707d221b008970aee7befd580ee702951e338a32b995af62d9c893fe16e7e1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Anna Šárová Mikeštíková, Médéric Fourmy, Martin Cífka, Josef Sivic, Vladimir Petrik</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20538" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20538</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f6450c33e31646a32cf9129dad4fbbcc84ead81c36e23e134b1b0f9930b9db8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f6450c33e31646a32cf9129dad4fbbcc84ead81c36e23e134b1b0f9930b9db8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mingwei Tang, Jiahao Nie, Guang Yang, Ziqing Cui, Jie Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20556" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20556</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7eb3ac652519f5f42206d482e71edb24ef458336500aad066aa021c53b988cca_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7eb3ac652519f5f42206d482e71edb24ef458336500aad066aa021c53b988cca_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dhruv Anand, Ehsan Shareghi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20595" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20595</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb95f031dbfb3f7bfc348a6d3e77d5c3ae7e2408f06dd57529b77764de0ce0b7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb95f031dbfb3f7bfc348a6d3e77d5c3ae7e2408f06dd57529b77764de0ce0b7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] FedPOD: the deployable units of training for federated learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Daewoon Kim, Si Young Yie, Jae Sung Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20610" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20610</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e8e1f095bc0447c82283e16e3fb09acd3ae3773107b9096b3a06a1659a978e0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e8e1f095bc0447c82283e16e3fb09acd3ae3773107b9096b3a06a1659a978e0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FedPOD: the deployable units of training for federated learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LongVideoAgent: Multi-Agent Reasoning with Long Videos</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20618" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20618</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e38fba460d45fac945020bc323269f04211978c3e2e544d86072d5b062f40958_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e38fba460d45fac945020bc323269f04211978c3e2e544d86072d5b062f40958_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LongVideoAgent: Multi-Agent Reasoning with Long Videos</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Active Intelligence in Video Avatars via Closed-loop World Modeling</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xuanhua He, Tianyu Yang, Ke Cao, Ruiqi Wu, Cheng Meng, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Qifeng Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20615" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20615</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/507ac9f976cb24dc0dc72fb35cca194ceb4a25829fd305c6f45493a8a9531c76_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/507ac9f976cb24dc0dc72fb35cca194ceb4a25829fd305c6f45493a8a9531c76_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Active Intelligence in Video Avatars via Closed-loop World Modeling</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Repurposing Video Diffusion Transformers for Robust Point Tracking</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Soowon Son, Honggyu An, Chaehyun Kim, Hyunah Ko, Jisu Nam, Dahyun Chung, Siyoon Jin, Jung Yi, Jaewon Min, Junhwa Hur, Seungryong Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20606" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20606</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415d9c5c4a502667ccd87f6d4f24a9276a6e9f5e7f729b850e01ea33b4b6681a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415d9c5c4a502667ccd87f6d4f24a9276a6e9f5e7f729b850e01ea33b4b6681a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Repurposing Video Diffusion Transformers for Robust Point Tracking</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] SpatialTree: How Spatial Abilities Branch Out in MLLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuxi Xiao, Longfei Li, Shen Yan, Xinhang Liu, Sida Peng, Yunchao Wei, Xiaowei Zhou, Bingyi Kang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20617" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20617</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c18a98475f6303ed360b86b5c0ec7ec6b4ab088ad0e70f925606628c827b46d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c18a98475f6303ed360b86b5c0ec7ec6b4ab088ad0e70f925606628c827b46d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SpatialTree: How Spatial Abilities Branch Out in MLLMs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] SemanticGen: Video Generation in Semantic Space</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jianhong Bai, Xiaoshi Wu, Xintao Wang, Fu Xiao, Yuanxing Zhang, Qinghe Wang, Xiaoyu Shi, Menghan Xia, Zuozhu Liu, Haoji Hu, Pengfei Wan, Kun Gai</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20619" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20619</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77e0d53ce289ecacefa061dc5d124f7b98c80f5b9417f83e06a4ccaafb10e8a8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77e0d53ce289ecacefa061dc5d124f7b98c80f5b9417f83e06a4ccaafb10e8a8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SemanticGen: Video Generation in Semantic Space</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] SAM Audio: Segment Anything in Audio</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Bowen Shi, Andros Tjandra, John Hoffman, Helin Wang, Yi-Chiao Wu, Luya Gao, Julius Richter, Matt Le, Apoorv Vyas, Sanyuan Chen, Christoph Feichtenhofer, Piotr Dollár, Wei-Ning Hsu, Ann Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18099" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18099</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4568ea657a5ec8dc7d7944a03205d941e2bc9d054f9586938d2caa09e0b272e6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4568ea657a5ec8dc7d7944a03205d941e2bc9d054f9586938d2caa09e0b272e6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SAM Audio: Segment Anything in Audio</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Dual-Encoder Transformer-Based Multimodal Learning for Ischemic Stroke Lesion Segmentation Using Diffusion MRI</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Muhammad Usman, Azka Rehman, Muhammad Mutti Ur Rehman, Abd Ur Rehman, Muhammad Umar Farooq</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20436" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20436</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efdfcdd8483dd2617be0701188cb1e86708fc680b1516caff77f5075ed2baf49_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efdfcdd8483dd2617be0701188cb1e86708fc680b1516caff77f5075ed2baf49_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dual-Encoder Transformer-Based Multimodal Learning for Ischemic Stroke Lesion Segmentation Using Diffusion MRI</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Snapshot 3D image projection using a diffractive decoder</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Cagatay Isil, Alexander Chen, Yuhang Li, F. Onuralp Ardic, Shiqi Chen, Che-Yung Shen, Aydogan Ozcan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20464" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20464</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f05e75cf0d45d4f4a6b73b6c248e120e8372e756e8b025e6f78b8ce6098b183_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f05e75cf0d45d4f4a6b73b6c248e120e8372e756e8b025e6f78b8ce6098b183_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Snapshot 3D image projection using a diffractive decoder</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yujia Fu, Zhiyu Dong, Tianwen Qian, Chenye Zheng, Danian Ji, Linhai Zhuo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20374" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20374</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b125441b1639c4b3867045b3843d2e46f162ef46f6b24b77dee1c01e02ade5a8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b125441b1639c4b3867045b3843d2e46f162ef46f6b24b77dee1c01e02ade5a8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-25">2025-12-25<a href="#2025-12-25" class="hash-link" aria-label="Direct link to 2025-12-25" title="Direct link to 2025-12-25" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251225] MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [multimodal knowledge graph, cross-modal reasoning, visual document understanding, retrieval-augmented generation, entity-centric structure]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chi-Hsiang Hsiao, Yi-Cheng Wang, Tzung-Sheng Lin, Yi-Ren Yeh, Chu-Song Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> National Taiwan University, E.SUN Financial Holding Co., Ltd., National Kaohsiung Normal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20626" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20626</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a multimodal knowledge graph-based RAG framework that integrates visual cues into KG construction, retrieval, and answer generation for cross-modal reasoning. 2. Addresses the limitation of existing text-only KG-RAG methods by automatically building KGs that capture text-to-figure and figure-to-figure relationships. 3. Demonstrates superior performance over existing RAG approaches on both textual and multimodal question-answering tasks through comprehensive experiments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/425d6eb853edb40749e686474d27dc018d8a86017a4cd69160f9ac2081d36385_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/425d6eb853edb40749e686474d27dc018d8a86017a4cd69160f9ac2081d36385_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces MegaRAG, a multimodal knowledge graph-based retrieval-augmented generation method designed to overcome the limitations of text-only RAG systems in understanding complex, long-form visual documents. It integrates visual information into the knowledge graph construction and retrieval process to enable better cross-modal reasoning. Experimental results show it consistently outperforms existing RAG methods on various question-answering tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] MaskOpt: A Large-Scale Mask Optimization Dataset to Advance AI in Integrated Circuit Manufacturing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [mask optimization, optical proximity correction, inverse lithography technique, deep learning, benchmark dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuting Hu, Lei Zhuang, Hua Xiang, Jinjun Xiong, Gi-Joon Nam</p>
</li>
<li class="">
<p><strong>institution:</strong> University at Buffalo, IBM Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20655" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20655</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MaskOpt, a large-scale benchmark dataset for AI-driven mask optimization constructed from real 45nm IC designs, addressing limitations of synthetic data. 2. The dataset preserves standard-cell hierarchy and includes varying context window sizes to enable cell- and context-aware model training. 3. Provides comprehensive benchmarks by evaluating state-of-the-art deep learning models, revealing performance trade-offs and validating the importance of contextual and cell-aware inputs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce9bc1ac552258257c450a9bc4d4c4ae0264c958efae291f56fc44af367bf07a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce9bc1ac552258257c450a9bc4d4c4ae0264c958efae291f56fc44af367bf07a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents MaskOpt, a large-scale dataset built from real integrated circuit designs to advance deep learning for mask optimization in semiconductor manufacturing. It addresses the limitations of existing synthetic datasets by including cell hierarchy and surrounding context. Benchmarking results demonstrate the dataset&#x27;s utility and highlight the critical role of context and cell information for accurate mask generation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [Low-Rank Adaptation (LoRA), parameter-efficient fine-tuning, rank adaptation, mobile vision language model, dynamic scheduling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuanhao Xi, Xiaohuan Bing, Ramin Yahyapour</p>
</li>
<li class="">
<p><strong>institution:</strong> Liaoning Technical University, University of Göttingen, Gesellschaft für Wissenschaftliche Datenverarbeitung mbH Göttingen</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20674" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20674</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes HyDRA, a parameter-efficient fine-tuning framework for mobile VLMs that implements hierarchical and dynamic rank scheduling. 2. Introduces hierarchical optimization with coarse-grained (layer-level) and fine-grained (intra-layer) rank assignment. 3. Employs dynamic adjustment via an end-to-end automatic optimization using a lightweight performance model to determine ranks during fine-tuning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9589d36616e78e4826e61d2dbafa4ccc718075f3659352d4d01c1b6f4795a02a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9589d36616e78e4826e61d2dbafa4ccc718075f3659352d4d01c1b6f4795a02a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces HyDRA, a parameter-efficient fine-tuning framework for mobile Vision Language Models (VLMs) that addresses the computational inefficiency of standard LoRA by implementing hierarchical and dynamic rank scheduling. The method uses a two-pronged optimization strategy and a lightweight performance model to adjust ranks automatically. Experiments show HyDRA outperforms baselines, achieving a 4.7% average improvement without extra parameters and sometimes surpassing full fine-tuning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] VL4Gaze: Unleashing Vision-Language Models for Gaze Following</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [gaze following], [vision-language models, gaze understanding, visual question answering, benchmark dataset, multi-task learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shijing Wang, Chaoqun Cui, Yaping Huang, Hyung Jin Chang, Yihua Cheng</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Jiaotong University, Institute of Automation, Chinese Academy of Sciences, University of Birmingham</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20735" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20735</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces VL4Gaze, the first large-scale benchmark for evaluating and training vision-language models on gaze understanding. 2. Formulates gaze understanding as a unified VQA problem across four complementary tasks: gaze object description, gaze direction description, gaze point location, and ambiguous question recognition. 3. Demonstrates that targeted multi-task supervision on VL4Gaze substantially improves VLMs&#x27; gaze understanding capabilities, which do not reliably emerge from general pre-training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48a1d18f4099c1ab4f26169d964e14f98c077ef87b0bc95c0bf4868b43217b89_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48a1d18f4099c1ab4f26169d964e14f98c077ef87b0bc95c0bf4868b43217b89_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of gaze understanding in vision-language models by introducing VL4Gaze, a large-scale benchmark dataset with 489K QA pairs. It evaluates VLMs on four gaze-related VQA tasks and finds that task-specific fine-tuning on this dataset is crucial for achieving reliable performance, as general-purpose VLMs struggle with gaze inference.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [neural architecture search, hardware-aware search, edge detection, TinyML, waste detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tony Tran, Bin Hu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Houston</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20746" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20746</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an iterative hardware-aware neural architecture search (NAS) framework that alternates between backbone and neck/head optimization for efficient object detection under TinyML constraints. 2. Introduces a population passthrough mechanism and an accuracy predictor to reduce search cost and improve the stability of the evolutionary search. 3. Delivers a family of deployment-ready detectors (TrashDets) that significantly improve efficiency (energy, latency, power) and accuracy on microcontroller hardware compared to existing TinyML baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4dbb2f9a6c26aed837d78c4cfa78db76890d85a7fadbb49f8592bc1ae30894e1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4dbb2f9a6c26aed837d78c4cfa78db76890d85a7fadbb49f8592bc1ae30894e1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses efficient waste detection for edge/IoT devices under TinyML constraints. It proposes an iterative hardware-aware neural architecture search framework to generate a family of efficient detectors called TrashDets. The resulting models achieve higher accuracy with fewer parameters and significantly reduce energy consumption and latency on resource-constrained microcontrollers.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [semantic scene completion], [semantic scene completion, 3D reconstruction, aerial perspective, benchmark dataset, label transfer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Markus Gross, Sai B. Matha, Aya Fahmy, Rui Song, Daniel Cremers, Henri Meess</p>
</li>
<li class="">
<p><strong>institution:</strong> Fraunhofer IVI, TU Munich, MCML, UCLA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20770" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20770</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/markus-42/occufly" target="_blank" rel="noopener noreferrer" class="">https://github.com/markus-42/occufly</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces OccuFly, the first real-world, camera-based aerial Semantic Scene Completion (SSC) benchmark dataset captured at different altitudes and seasons. 2. Proposes a LiDAR-free data generation framework that automates label transfer from 2D masks to 3D point clouds, minimizing manual annotation. 3. Benchmarks state-of-the-art methods on the new dataset and highlights unique challenges of aerial viewpoints.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/292af942047832eaba7423020289566350434f49381259d1665a57faf1a20881_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/292af942047832eaba7423020289566350434f49381259d1665a57faf1a20881_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of aerial datasets for Semantic Scene Completion (SSC) by introducing OccuFly, a camera-based benchmark captured from UAVs. The authors propose a framework that uses traditional 3D reconstruction and 2D mask lifting to automate 3D annotation. The resulting dataset enables benchmarking and reveals specific challenges for 3D perception from elevated viewpoints.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [nullable prompts, mixed-supervision, vision-language models, breast ultrasound segmentation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Raja Mallina, Bryar Shareef</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Nevada, Las Vegas</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20783" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20783</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes NullBUS, a multimodal mixed-supervision framework for BUS segmentation that can learn from images both with and without prompts in a single model. 2. Introduces nullable prompts, implemented as learnable null embeddings with presence masks, to handle missing text metadata by enabling fallback to image-only evidence. 3. Demonstrates state-of-the-art performance on a unified pool of three public BUS datasets under mixed prompt availability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9c68929834a07c86ac43fe9856efa137aa11c30a231a02ea87272f2b689e4f7f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9c68929834a07c86ac43fe9856efa137aa11c30a231a02ea87272f2b689e4f7f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that many public breast ultrasound datasets lack reliable text or spatial prompts, which limits the training of promptable segmentation models. It proposes NullBUS, a framework that uses nullable global-local prompts to learn from both prompted and prompt-free images. The method achieves state-of-the-art segmentation performance on a unified evaluation of three public datasets, showing robustness under mixed prompt availability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Learning to Sense for Driving: Joint Optics-Sensor-Model Co-Design for Semantic Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [semantic segmentation], [optics-sensor co-design, RAW-to-task pipeline, learnable color filter array, differentiable simulation, autonomous driving]</p>
</li>
<li class="">
<p><strong>authors:</strong> Reeshad Khan amd John Gauch</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Arkansas</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20815" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20815</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A task-driven, end-to-end co-design framework that jointly optimizes optics (via lens models), sensor parameters (learnable color filter arrays, noise, quantization), and a lightweight semantic segmentation network for autonomous driving. 2. Demonstrates consistent performance improvements (mIoU) on KITTI-360, particularly for challenging classes, by adapting image acquisition directly to semantic structure rather than human-viewable imagery. 3. Shows that the robustness gains are achieved with a compact, efficient model (~1M parameters, ~28 FPS), proving the deployability of the full-stack co-optimization approach on edge devices.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98485ccaafcb09f4e55fbe828874d818d5caf3b68c7d2d4f86250cb41081aebe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98485ccaafcb09f4e55fbe828874d818d5caf3b68c7d2d4f86250cb41081aebe_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a joint optics-sensor-model co-design framework for autonomous driving perception. It unifies differentiable models of optics, sensor noise, and quantization with a lightweight segmentation network into an end-to-end RAW-to-task pipeline, optimized directly for semantic accuracy. The method outperforms traditional fixed pipelines, especially in challenging conditions, while maintaining high efficiency suitable for edge deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [adaptive preprocessing, visual token reduction, inference efficiency, content-aware cropping, FastVLM]</p>
</li>
<li class="">
<p><strong>authors:</strong> Putu Indah Githa Cahyani, Komang David Dananjaya Suartana, Novanto Yudistira</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Brawijaya</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20839" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20839</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/kmdavidds/mlfastlm" target="_blank" rel="noopener noreferrer" class="">https://github.com/kmdavidds/mlfastlm</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content to reduce redundancy. 2. Integrated the method with FastVLM without modifying its architecture or requiring retraining, maintaining model integrity. 3. Demonstrated significant efficiency improvements, including over 50% reduction in per-image inference time and over 55% reduction in visual token count on a DocVQA subset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffbed91a603765fa692e3234db5637487cb3f57c201db5e6263fed6a6c03b2ca_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffbed91a603765fa692e3234db5637487cb3f57c201db5e6263fed6a6c03b2ca_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high inference latency and computational cost of Vision-Language Models (VLMs) when processing high-resolution images by proposing an adaptive visual preprocessing method. This method dynamically adjusts input resolution and cropping based on image content to reduce visual redundancy before encoding, integrated seamlessly with FastVLM. Experimental results show over 50% reduction in per-image inference time and over 55% reduction in visual token count, proving it as an effective lightweight strategy for deployment efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [bioimage analysis], [multi-channel microscopy, cellular morphology, pre-training dataset, channel-adaptive models, heterogeneous data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vidit Agrawal, John Peters, Tyler N. Thompson, Mohammad Vali Sanian, Chau Pham, Nikita Moshkov, Arshad Kazi, Aditya Pillai, Jack Freeman, Byunguk Kang, Samouil L. Farhi, Ernest Fraenkel, Ron Stewart, Lassi Paavolainen, Bryan A. Plummer, Juan C. Caicedo</p>
</li>
<li class="">
<p><strong>institution:</strong> Morgridge Institute for Research, University of Wisconsin-Madison, Institute for Molecular Medicine Finland (FIMM), University of Helsinki, Boston University, Institute of Computational Biology (Helmholtz Munich), Massachusetts Institute of Technology, Broad Institute of MIT and Harvard</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20833" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20833</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces CHAMMI-75, a novel open-access dataset of heterogeneous, multi-channel microscopy images curated from 75 diverse biological studies. 2. Proposes the use of this dataset to investigate and develop channel-adaptive models for cellular morphology that can process any microscopy image type. 3. Demonstrates experimentally that pre-training with the diverse CHAMMI-75 dataset improves performance on multi-channel bioimaging tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3554315333d73be844495c47c136c201cae29e405327a25f6e11fc1488c5f4f9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3554315333d73be844495c47c136c201cae29e405327a25f6e11fc1488c5f4f9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that models for quantifying cell morphology are typically specialized to single microscopy types, limiting their reuse. It proposes CHAMMI-75, a diverse, multi-channel microscopy image dataset, and shows that pre-training with it improves model performance by enabling channel-adaptability and handling heterogeneous modalities. The work aims to pave the way for more generalizable cellular morphology models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [content-aware retrieval, FAISS, neural talking-head synthesis, local deployment, multimodal interaction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Zabirul Islam, Md Motaleb Hossen Manik, Ge Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Rensselaer Polytechnic Institute</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20858" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20858</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A fully local, privacy-preserving interactive video engine that integrates avatar synthesis, content retrieval, and multimodal interaction. 2. A content-aware retrieval mechanism combining semantic similarity with timestamp alignment to surface contextually relevant lecture segments. 3. A system design employing lightweight models, FAISS, and segmented synthesis with progressive preloading to achieve real-time responsiveness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edf9b6fce27313fb183b0fbb9a8b32f719d83748e14517c97b10d89c253c6cac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edf9b6fce27313fb183b0fbb9a8b32f719d83748e14517c97b10d89c253c6cac_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents ALIVE, a system that transforms passive lecture videos into interactive experiences by using local ASR, LLMs, and neural avatars to generate lecture content, a content-aware retrieval mechanism to find relevant segments, and enabling real-time Q&amp;A. It demonstrates that combining multimodal AI with local, content-aware retrieval can create engaging and pedagogically valuable interactive learning environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [360-degree video, implicit neural representations, viewport decoding, spatial-temporal affine transform, video compression]</p>
</li>
<li class="">
<p><strong>authors:</strong> Daichi Arai, Kyohei Unno, Yasuko Sugito, Yuichi Kusakabe</p>
</li>
<li class="">
<p><strong>institution:</strong> Science &amp; Technology Research Laboratories, NHK</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20871" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20871</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of the entire panoramic frame for 360-degree videos. 2. Introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. 3. Achieves significant reductions in memory consumption (7x) and increases in decoding speed (2.5x) compared to prior work HNeRV, while improving image quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77be849ee4606db0adeefbd5d6ebfff328fcdd9b300007aa2dc353aaea4af87e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77be849ee4606db0adeefbd5d6ebfff328fcdd9b300007aa2dc353aaea4af87e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high memory usage and slow decoding of applying implicit neural representations (NeRV) to high-resolution 360-degree videos. It proposes NeRV360, a framework that integrates viewport extraction directly into the decoding process using a conditional spatial-temporal affine transform module. Experiments show NeRV360 significantly reduces memory consumption and increases decoding speed while delivering better image quality compared to prior methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [YOLOv11, 3D-DIoU, multi-view fusion, GPR, FDTD]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haotian Lv, Chao Li, Jiangbo Dai, Yuhui Zhang, Zepeng Fan, Yiqiu Tan, Dawei Wang, Binglei Xie</p>
</li>
<li class="">
<p><strong>institution:</strong> Harbin Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20866" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20866</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a B/C/D-Scan three-view joint analysis strategy and a feature evaluation method validated by FDTD simulations and real data. 2. Developed the DCO-YOLO framework, integrating DySample, CGLU, and OutlookAttention into YOLOv11 to enhance small-scale pipeline feature extraction. 3. Introduced a 3D-DIoU spatial feature matching algorithm with 3D geometric constraints to automate multi-view annotation association and resolve single-view ambiguities.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5ecaa60b98d3c92e7c85d9cd5e1f9894fba8806dbb24358189ef30201b7b93c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5ecaa60b98d3c92e7c85d9cd5e1f9894fba8806dbb24358189ef30201b7b93c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a lightweight framework for 3D underground pipeline detection using multi-view 2D GPR images. The method integrates an improved YOLO-based detection model (DCO-YOLO) with a novel 3D-DIoU spatial matching algorithm for multi-view fusion. Experiments on real urban data show the framework achieves high accuracy, recall, and mAP, outperforming the baseline and offering a reliable solution for pipeline recognition and localization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [cross-modal re-identification], [Parameter-Efficient Fine-Tuning, Vision Foundation Models, Domain Representation Injection, feature-space adaptation, cross-modality]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tingfeng Xian, Wenlve Zhou, Zhiheng Zhou, Zhelin Li</p>
</li>
<li class="">
<p><strong>institution:</strong> South China University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20892" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20892</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/TingfengXian/DRI" target="_blank" rel="noopener noreferrer" class="">https://github.com/TingfengXian/DRI</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Parameter-Efficient Fine-Tuning (PEFT) strategy called Domain Representation Injection (DRI) that operates in the feature space instead of the weight space. 2. Introduces a lightweight Offset Encoder and a Modulator to extract and adapt domain-specific representations, which are then injected into a frozen Vision Foundation Model to bridge modality gaps. 3. Achieves state-of-the-art performance on cross-modal ship re-identification with minimal trainable parameters, demonstrating the effectiveness of feature-space adaptation for limited-capacity models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39fa98c450498929c15e9f360b859ef2bfff59f65e629afdd04c76cb67a0fe53_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39fa98c450498929c15e9f360b859ef2bfff59f65e629afdd04c76cb67a0fe53_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of cross-modal ship re-identification by proposing a feature-space adaptation method called Domain Representation Injection (DRI). Instead of fine-tuning model weights, DRI injects learned domain-specific features into a frozen Vision Foundation Model to bridge the modality gap. The method achieves state-of-the-art results with very few trainable parameters, showing its efficiency and effectiveness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] DGSAN: Dual-Graph Spatiotemporal Attention Network for Pulmonary Nodule Malignancy Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [spatiotemporal attention, graph neural network, multimodal fusion, pulmonary nodule classification, feature encoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiao Yu, Zhaojie Fang, Guanyu Zhou, Yin Shen, Huoling Luo, Ye Li, Ahmed Elazab, Xiang Wan, Ruiquan Ge, Changmiao Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Hangzhou Dianzi University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20898" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20898</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/lcbkmm/DGSAN" target="_blank" rel="noopener noreferrer" class="">https://github.com/lcbkmm/DGSAN</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a Dual-Graph Spatiotemporal Attention Network (DGSAN) for pulmonary nodule malignancy prediction. 2. Introduced a Dual-Graph Construction method and a Hierarchical Cross-Modal Graph Fusion Module for effective multimodal feature integration. 3. Compiled a novel multimodal dataset named NLST-cmst to support related research.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/422fb811704c808454d49662b57428a8e4f2132f4f9eb28d4def2caf51204b49_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/422fb811704c808454d49662b57428a8e4f2132f4f9eb28d4def2caf51204b49_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of inefficient multimodal fusion in pulmonary nodule malignancy prediction. It proposes a Dual-Graph Spatiotemporal Attention Network (DGSAN) that uses a Global-Local Feature Encoder and a hierarchical graph fusion module to integrate temporal and multimodal data. Experiments show DGSAN outperforms state-of-the-art methods with high computational efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Benchmarking and Enhancing VLM for Compressed Image Understanding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [vision-language models], [image compression, benchmark, vision-language models, adaptor, generalization gap]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zifu Zhang, Tongda Xu, Siqi Li, Shengxi Li, Yue Zhang, Mai Xu, Yan Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Beihang University, Beijing University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20901" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20901</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the first comprehensive benchmark to evaluate Vision-Language Models (VLMs) on compressed images, covering multiple codecs and tasks with over a million images. 2. Analyzes the performance gap, categorizing it into information loss and VLM generalization failure, and identifies that only the generalization gap is mitigable. 3. Proposes a universal VLM adaptor that improves model performance across various codecs and bitrates by 10%-30%.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39011e6665de56b73804a40551ab504f1a26f1c50146d14d5c84a2c6c5586f47_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39011e6665de56b73804a40551ab504f1a26f1c50146d14d5c84a2c6c5586f47_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the performance drop of Vision-Language Models (VLMs) when processing low-bitrate compressed images. It introduces a large-scale benchmark to evaluate this issue, analyzes the sources of the performance gap, and proposes a universal adaptor to enhance VLM robustness. The proposed adaptor successfully improves VLM performance on compressed images by 10%-30%, bridging the gap between VLMs and compressed data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D visual grounding], [panoramic rendering, vision-language model (VLM), 3D scene understanding, multi-modal fusion, viewpoint selection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Seongmin Jung, Seongho Choi, Gunwoo Jeon, Minsu Cho, Jongwoo Lim</p>
</li>
<li class="">
<p><strong>institution:</strong> Seoul National University, Pohang University of Science and Technology (POSTECH)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20907" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20907</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://choiseongho-h.github.io/PanoGrounder" target="_blank" rel="noopener noreferrer" class="">https://choiseongho-h.github.io/PanoGrounder</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel 3D visual grounding framework that uses panoramic scene renderings as an intermediate 2D-3D representation, enabling direct use of powerful pretrained 2D Vision-Language Models (VLMs) with minimal adaptation. 2. Introduces a three-stage pipeline that strategically places panoramic viewpoints, performs per-view grounding with a VLM, and fuses predictions into a final 3D bounding box. 3. Demonstrates state-of-the-art performance on standard benchmarks (ScanRefer, Nr3D) and superior generalization to unseen datasets and text paraphrases.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb61d36c0dc9249907d446e1e54dd427e25dfcfa7af988fd4093a0199335a6e7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb61d36c0dc9249907d446e1e54dd427e25dfcfa7af988fd4093a0199335a6e7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of limited generalization in 3D Visual Grounding (3DVG) by introducing PanoGrounder, a framework that bridges 2D and 3D using panoramic scene renderings. The method leverages pretrained 2D Vision-Language Models for reasoning on these panoramic views and fuses predictions into a 3D bounding box. It achieves state-of-the-art results and shows strong generalization capabilities.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Self-supervised Multiplex Consensus Mamba for General Image Fusion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image fusion], [Mamba, Self-supervised Learning, Mixture of Experts, Cross-modal Scanning, Contrastive Learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yingying Wang, Rongjin Zhuang, Hui Zheng, Xuanhua He, Ke Cao, Xiaotong Tu, Xinghao Ding</p>
</li>
<li class="">
<p><strong>institution:</strong> Xiamen University, The Hong Kong University of Science and Technology, University of Science and Technology of China</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20921" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20921</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed the SMC-Mamba framework, featuring a Modality-Agnostic Feature Enhancement (MAFE) module for detail preservation and global representation enhancement. 2. Introduced the Multiplex Consensus Cross-modal Mamba (MCCM) module for dynamic expert collaboration and efficient cross-modal information integration. 3. Designed a Bi-level Self-supervised Contrastive Learning Loss (BSCL) to preserve high-frequency information and boost downstream task performance without extra computational cost.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91945f65768f96b0613d0702a5b5373c8b78673452a1717dc1624fdddc50aa4b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91945f65768f96b0613d0702a5b5373c8b78673452a1717dc1624fdddc50aa4b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes SMC-Mamba, a self-supervised Mamba-based framework for general image fusion. It introduces novel modules for feature enhancement and cross-modal consensus, along with a contrastive learning loss, to efficiently integrate information from various modalities. Experiments show it outperforms state-of-the-art methods across multiple fusion tasks and downstream vision applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D scene understanding], [3D Gaussian Splatting, open-vocabulary segmentation, quantile rendering, feature rendering, real-time rendering]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yoonwoo Jeong, Cheng Sun, Frank Wang, Minsu Cho, Jaesung Choe</p>
</li>
<li class="">
<p><strong>institution:</strong> NVIDIA, POSTECH</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20927" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20927</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that sparsely samples only dominant Gaussians along a ray to efficiently handle high-dimensional features. 2. Proposes Gaussian Splatting Network (GS-Net), a generalizable 3D neural network that integrates Q-Render to predict Gaussian features. 3. Achieves state-of-the-art performance on benchmarks (ScanNet, LeRF) while enabling real-time rendering with a ~43.7x speedup for 512-D feature maps.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9eadb39e7ce78a2787efbad5a7c95d49fedbdfefe4e08690405521c36873eb3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9eadb39e7ce78a2787efbad5a7c95d49fedbdfefe4e08690405521c36873eb3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of efficiently rendering high-dimensional features (e.g., 512-D CLIP features) for open-vocabulary segmentation in 3D Gaussian Splatting. The authors propose Quantile Rendering (Q-Render), which sparsely samples influential Gaussians, and integrate it into a generalizable network called GS-Net. The method outperforms existing approaches on standard datasets and achieves significant speedups, enabling real-time performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [visual reasoning], [visual programming, spatial reasoning, tool induction, transductive learning, 3D scene understanding]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shengguang Wu, Xiaohan Wang, Yuhui Zhang, Hao Zhu, Serena Yeung-Levy</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20934" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20934</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://transductive-visualprogram.github.io/" target="_blank" rel="noopener noreferrer" class="">https://transductive-visualprogram.github.io/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Transductive Visual Programming (TVP), a novel framework that builds new tools from experiential solutions rather than speculative induction., 2. Introduces a closed-loop system with an evolving Tool Library and an Example Library, enabling self-improvement through experience., 3. Demonstrates state-of-the-art performance on spatial reasoning benchmarks and shows that transductively learned tools are used more frequently and generalize better.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6374488a70a5d9147002f5652452c2f63ea3698c6660c54123c36fc9deef3991_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6374488a70a5d9147002f5652452c2f63ea3698c6660c54123c36fc9deef3991_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of spatial reasoning in 3D scenes by proposing Transductive Visual Programming (TVP), a framework that learns reusable higher-level tools by abstracting patterns from its own successful solutions. This experience-driven approach outperforms existing methods and GPT-4o on benchmarks, showing more effective tool discovery and strong generalization to unseen tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [amodal completion], [multi-agent reasoning, semantic planning, visual synthesis, MAC-Score, chain-of-thought]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hongxing Fan, Shuyu Zhao, Jiayang Ao, Lu Sheng</p>
</li>
<li class="">
<p><strong>institution:</strong> Beihang University, The University of Melbourne</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20936" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20936</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://fanhongxing.github.io/remac-page" target="_blank" rel="noopener noreferrer" class="">https://fanhongxing.github.io/remac-page</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A Collaborative Multi-Agent Reasoning Framework that decouples Semantic Planning from Visual Synthesis for stable, single-pass amodal completion. 2. Integration of a self-correcting Verification Agent and a Diverse Hypothesis Generator to improve reasoning and handle ambiguity. 3. Introduction of the MAC-Score, a novel human-aligned evaluation metric for assessing inferred invisible content.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70d3435136a9939f15c6fefdbf2a3700e29be4992fac1f670aa29c0aa24f6e92_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70d3435136a9939f15c6fefdbf2a3700e29be4992fac1f670aa29c0aa24f6e92_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of inference instability and error accumulation in amodal completion by proposing a reasoning-driven framework. The method employs specialized agents for upfront semantic planning before visual synthesis, and introduces a new evaluation metric. Experiments show the framework outperforms state-of-the-art methods on multiple datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Beyond Artifacts: Real-Centric Envelope Modeling for Reliable AI-Generated Image Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [ai-generated image detection], [real-centric envelope modeling, distribution modeling, chain degradations, benchmark, generalization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ruiqi Liu, Yi Han, Zhengbo Zhang, Liwei Yao, Zhiyuan Yan, Jialiang Shen, ZhiJin Chen, Boyi Sun, Lubin Weng, Jing Dong, Yan Wang, Shu Wu</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences; Tsinghua University; Peking University; The University of Sydney; Southwest University; Shanghai Second Polytechnic University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20937" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20937</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new paradigm called Real-centric Envelope Modeling (REM) that shifts the detection focus from learning generator-specific artifacts to modeling the robust distribution of real images. 2. Introduces a method that uses feature-level perturbations in self-reconstruction to generate near-real samples and an envelope estimator with cross-domain consistency to learn a boundary around the real image manifold. 3. Builds a comprehensive benchmark named RealChain that covers various generators and simulates real-world degradation chains to evaluate detector robustness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e6b1c54aadd8c3872d06b909d6de02c5d6a8a1aafa2a19375b0c325e7c62c51_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e6b1c54aadd8c3872d06b909d6de02c5d6a8a1aafa2a19375b0c325e7c62c51_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that existing AI-generated image detectors overfit to generator artifacts and fail under real-world degradations. It proposes a new method called Real-centric Envelope Modeling (REM) that learns the robust distribution of real images instead, and introduces a new benchmark, RealChain, for evaluation. The results show REM achieves significant performance improvements and maintains strong generalization on degraded images.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Generalization of Diffusion Models Arises with a Balanced Representation Space</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [representation learning, denoising autoencoder, memorization detection, representation steering, generalization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zekai Zhang, Xiao Li, Xiang Li, Lianghe Shi, Meng Wu, Molei Tao, Qing Qu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Michigan, Georgia Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20963" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20963</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://deepthink-umich.github.io" target="_blank" rel="noopener noreferrer" class="">https://deepthink-umich.github.io</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a theoretical analysis linking memorization and generalization in diffusion models to specific representation structures (&quot;spiky&quot; vs. &quot;balanced&quot;) using a two-layer ReLU DAE model. 2. Validates the theoretical findings on real-world unconditional and text-to-image diffusion models, showing the practical emergence of these representation regimes. 3. Proposes a representation-based method for detecting memorization and a training-free editing technique for precise control via representation steering.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58c26c58c94d92dfa7924dfd1c702b8c3239c0a63a4e6ef6f0eaaf42d7dc410a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58c26c58c94d92dfa7924dfd1c702b8c3239c0a63a4e6ef6f0eaaf42d7dc410a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes the distinction between memorization and generalization in diffusion models through representation learning. It theoretically proves that memorization leads to &quot;spiky&quot; representations while generalization yields &quot;balanced&quot; ones, and validates this on real models. Based on these insights, the authors propose methods for memorization detection and training-free image editing, highlighting the central role of good representations for meaningful generation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] XGrid-Mapping: Explicit Implicit Hybrid Grid Submaps for Efficient Incremental Neural LiDAR Mapping</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [neural radiance fields], [hybrid grid, VDB structure, submap-based organization, distillation-based alignment, dynamic removal]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zeqing Song, Zhongmiao Yan, Junyuan Deng, Songpengcheng Xia, Xiang Mu, Jingyi Xu, Qi Wu, Ling Pei</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University (inferred from author email domains, e.g., likely sjtu.edu.cn)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20976" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20976</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes XGrid-Mapping, a hybrid grid framework combining sparse explicit grids for geometric priors and implicit dense grids for rich scene representation to enable efficient neural LiDAR mapping. 2. Introduces a submap-based organization coupled with a VDB structure to reduce computational load and support large-scale incremental mapping. 3. Presents a distillation-based overlap alignment strategy and a dynamic removal module to ensure submap consistency and enhance robustness and sampling efficiency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4f609d4aace0ae5130d408518726a0efd356768fd806198569bd7240de6d7ef_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4f609d4aace0ae5130d408518726a0efd356768fd806198569bd7240de6d7ef_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes XGrid-Mapping, a hybrid explicit-implicit grid framework for efficient incremental neural LiDAR mapping. It uses submaps with a VDB structure and a distillation-based alignment strategy to achieve real-time performance and superior mapping quality, outperforming existing state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multi-camera tracking], [spatial RAG, beam search, CARLA simulator]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yujin Noh, Inho Jake Park, Chigon Hwang</p>
</li>
<li class="">
<p><strong>institution:</strong> Kwangwoon University, Gwangju Institute of Science and Technology (GIST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20975" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20975</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SPOT, a map-guided LLM agent for zero-shot vehicle tracking in multi-CCTV blind spots without prior training. 2. Introduces a method to structure road and CCTV data as spatial documents using chunking for real-time LLM querying. 3. Combines map data with vehicle dynamics to perform beam search at intersections for predicting the next CCTV location.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28d25f4afe3b9ea5d43a1eab9aa50328a7e33a926ce2a3183f35335ab5710c43_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28d25f4afe3b9ea5d43a1eab9aa50328a7e33a926ce2a3183f35335ab5710c43_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes SPOT, a method that uses a map-guided LLM agent to track vehicles across blind spots in multi-CCTV environments. It converts spatial map and camera data into a queryable format for the LLM and uses vehicle motion data with beam search to predict where a vehicle will reappear. Experiments in CARLA show it maintains continuous trajectories better than existing techniques.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] X-ray Insights Unleashed: Pioneering the Enhancement of Multi-Label Long-Tail Data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image synthesis], [diffusion model, data augmentation, long-tail learning, chest X-ray, inpainting]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xinquan Yang, Jinheng Xie, Yawen Huang, Yuexiang Li, Huimin Huang, Hao Zheng, Xian Wu, Yefeng Zheng, Linlin Shen</p>
</li>
<li class="">
<p><strong>institution:</strong> Tencent, Shenzhen University, National University of Singapore, Westlake University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20980" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20980</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel data synthesis pipeline that uses a diffusion model trained on normal X-rays to inpaint head-class lesions, thereby augmenting tail-class data. 2. The introduction of a Large Language Model Knowledge Guidance (LKG) module to aid the inpainting process. 3. The proposal of a Progressive Incremental Learning (PIL) strategy to stabilize the fine-tuning of the inpainting model.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0edf9e433c973b8803a686a6312cf73fb691cb44807918973120eb68c5308b2a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0edf9e433c973b8803a686a6312cf73fb691cb44807918973120eb68c5308b2a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of diagnosing rare, long-tail pulmonary anomalies in chest X-rays by proposing a data augmentation pipeline. The method trains a diffusion model on normal X-rays and uses it to inpaint common lesions from diseased images, preserving rare lesions for training, and is guided by an LLM and a progressive learning strategy. Evaluations on MIMIC and CheXpert datasets show the method achieves state-of-the-art performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] PUFM++: Point Cloud Upsampling via Enhanced Flow Matching</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [point cloud upsampling], [flow matching, two-stage strategy, adaptive time scheduler, on-manifold constraints, recurrent interface network]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhi-Song Liu, Chenhang He, Roland Maier, Andreas Rupp</p>
</li>
<li class="">
<p><strong>institution:</strong> Lappeenranta-Lahti University of Technology LUT, The Hong Kong Polytechnic University, Karlsruhe Institute of Technology (KIT), Saarland University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20988" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20988</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Holmes-Alan/Enhanced_PUFM" target="_blank" rel="noopener noreferrer" class="">https://github.com/Holmes-Alan/Enhanced_PUFM</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A two-stage flow-matching strategy for improved geometric fidelity and distribution approximation, 2. A data-driven adaptive time scheduler to accelerate and stabilize inference, 3. The incorporation of on-manifold constraints and a recurrent interface network (RIN) to enhance surface alignment and feature interaction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76eef67ea87c98d8de86d38ef9deebb9dc41ba30f94748973adbad178bbaab09_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76eef67ea87c98d8de86d38ef9deebb9dc41ba30f94748973adbad178bbaab09_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents PUFM++, an enhanced flow-matching framework for generating dense and accurate point clouds from sparse, noisy inputs. It introduces a two-stage flow strategy, an adaptive time scheduler, on-manifold constraints, and a recurrent network to improve fidelity, robustness, and efficiency. Experiments show it achieves state-of-the-art performance in point cloud upsampling.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video representation learning], [autoregressive pretraining, next-frame prediction, flow-matching, context-isolated predictor, generative pretraining]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jinghan Li, Yang Jin, Hao Jiang, Yadong Mu, Yang Song, Kun Xu</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21004" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21004</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Singularity0104/NExT-Vid" target="_blank" rel="noopener noreferrer" class="">https://github.com/Singularity0104/NExT-Vid</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes NExT-Vid, a novel autoregressive visual generative pretraining framework using masked next-frame prediction to jointly model images and videos. 2. Introduces a context-isolated autoregressive predictor to decouple semantic representation learning from target decoding. 3. Employs a conditioned flow-matching decoder to enhance the quality and diversity of generated frames.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/680ed2b04d0f3e4083f2df212eb836dbdb953f63f6593cd3dbdaba933611f46c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/680ed2b04d0f3e4083f2df212eb836dbdb953f63f6593cd3dbdaba933611f46c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of existing visual generative pretraining methods, which often neglect temporal information, by proposing NExT-Vid, an autoregressive framework for next-frame prediction. The method uses a context-isolated predictor and a flow-matching decoder to learn effective video representations. Experiments show it outperforms previous generative pretraining methods on downstream classification tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Granular-ball Guided Masking: Structure-aware Data Augmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [data augmentation], [Granular-ball Computing, structure-aware masking, information dropping, hierarchical masking]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuyin Xia, Fan Chen, Dawei Dai, Meng Yang, Junwei Han, Xinbo Gao, Guoyin Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Chongqing University of Posts and Telecommunications</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21011" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21011</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Granular-ball Guided Masking (GBGM), a novel structure-aware data augmentation method guided by Granular-ball Computing (GBC). 2. Introduces a coarse-to-fine hierarchical masking process that adaptively preserves semantically rich regions while suppressing redundant areas. 3. Demonstrates the method&#x27;s model-agnostic nature and effectiveness through extensive experiments on multiple benchmarks, showing improvements in classification and reconstruction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/20fc1c10cf1817c8407a18f117844ed1bf0dd4159dfb95e173f2060df3d75c18_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/20fc1c10cf1817c8407a18f117844ed1bf0dd4159dfb95e173f2060df3d75c18_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of structural awareness in existing mask-based data augmentation methods, which can discard essential semantics. The authors propose Granular-ball Guided Masking (GBGM), a strategy that uses Granular-ball Computing to guide a hierarchical masking process, preserving important regions. Experiments show GBGM improves model robustness and performance across various benchmarks and architectures.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] FluencyVE: Marrying Temporal-Aware Mamba with Bypass Attention for Video Editing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video editing], [Mamba, Stable Diffusion, temporal attention, low-rank approximation, one-shot editing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mingshu Cai, Yixuan Li, Osamu Yoshie, Yuya Ieiri</p>
</li>
<li class="">
<p><strong>institution:</strong> Waseda University, Southeast University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21015" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21015</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FluencyVE, a one-shot video editing method that integrates the Mamba module to replace temporal attention layers for improved temporal consistency and reduced computation. 2. Employs low-rank approximation matrices for query and key weights and a weighted averaging technique during training to preserve generative power while lowering computational burden. 3. Demonstrates effective editing of various video attributes, subjects, and locations in real-world videos through experiments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3419f702725d02a1b18daa40499c9c2d89fb38a4a84ff67afde4aaf53c6e943_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3419f702725d02a1b18daa40499c9c2d89fb38a4a84ff67afde4aaf53c6e943_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenges of temporal inconsistency and high computational cost in text-driven video editing. It proposes FluencyVE, a method that integrates the Mamba module into a Stable Diffusion-based model to replace temporal attention, and uses low-rank approximation and weighted averaging to maintain performance while reducing overhead. Experiments show the method&#x27;s effectiveness in diverse video editing tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] MVInverse: Feed-forward Multi-view Inverse Rendering in Seconds</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [inverse rendering], [multi-view consistency, feed-forward network, attention mechanism, consistency-based finetuning, intrinsic decomposition]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiangzuo Wu, Chengwei Ren, Jun Zhou, Xiu Li, Yuan Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Hong Kong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21003" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21003</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://maddog241.github.io/mvinverse-page/" target="_blank" rel="noopener noreferrer" class="">https://maddog241.github.io/mvinverse-page/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A feed-forward multi-view inverse rendering framework that jointly predicts consistent geometry and material properties from RGB image sequences in a single forward pass. 2. A model architecture that uses alternating attention across views to capture both intra-view lighting and inter-view material consistency. 3. A consistency-based finetuning strategy using unlabeled real-world videos to improve generalization and robustness in real-world conditions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a474ea3dff714bb84e38df3458cdbb2ec256329a9c182d4c1eeb5f20afad380_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a474ea3dff714bb84e38df3458cdbb2ec256329a9c182d4c1eeb5f20afad380_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces MVInverse, a fast feed-forward framework for multi-view inverse rendering that predicts consistent scene properties like albedo and normals from RGB images. It uses cross-view attention for coherent reasoning and a novel finetuning strategy on real videos to improve generalization. Experiments show it achieves state-of-the-art performance in consistency and quality while being much faster than optimization-based methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [adversarial defense], [talking face generation, 3D neural field, adversarial perturbation, video defense, spatial-frequency optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rui-qing Sun, Xingshan Yao, Tian Lan, Hui-Yang Zhao, Jia-Ling Shi, Chen-Hao Cui, Zhijing Wu, Chen Yang, Xian-Ling Mao</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Institute of Technology, Alibaba International Digital Commerce</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21019" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21019</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Richen7418/VDF" target="_blank" rel="noopener noreferrer" class="">https://github.com/Richen7418/VDF</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel video defense framework that perturbs the 3D information acquisition process of talking face generation models to protect portrait videos. 2. A similarity-guided parameter sharing mechanism to achieve high computational efficiency (47x acceleration). 3. A multi-scale dual-domain attention module to jointly optimize perturbations in both spatial and frequency domains for robustness and high visual fidelity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba8dbfa36170ca60f99422da322b8d87e90e6f7380199d6d5fb91ee653784372_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba8dbfa36170ca60f99422da322b8d87e90e6f7380199d6d5fb91ee653784372_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the privacy threat posed by 3D-field talking face generation models by proposing an efficient video defense framework. The method introduces a parameter-sharing mechanism for speed and a dual-domain attention module to perturb 3D information while preserving video quality. Experiments show it offers strong defense, is 47x faster than baselines, and is robust against common attacks and transformations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Multi-Attribute guided Thermal Face Image Translation based on Latent Diffusion Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [heterogeneous face recognition], [latent diffusion model, multi-attribute classifier, Self-attn Mamba]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mingshu Cai, Osamu Yoshie, Yuya Ieiri</p>
</li>
<li class="">
<p><strong>institution:</strong> Waseda University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21032" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21032</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel latent diffusion-based model for generating high-quality visible face images from thermal inputs while preserving identity features. 2. Incorporates a multi-attribute classifier to extract key facial attributes from visible images to mitigate feature loss during translation. 3. Introduces the Self-attn Mamba module to enhance global modeling of cross-modal features and improve inference speed.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fef179e889d0a26d06a1c9b52dedf4221a1040e9e221b0c19692c72d9430246_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fef179e889d0a26d06a1c9b52dedf4221a1040e9e221b0c19692c72d9430246_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of performance degradation in face recognition models when applied to thermal infrared images due to domain shift. It proposes a new method using a latent diffusion model guided by a multi-attribute classifier and a Self-attn Mamba module to translate thermal images into high-quality visible images that preserve identity. The approach achieves state-of-the-art performance in image quality and identity preservation on benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Next-Scale Prediction: A Self-Supervised Approach for Real-World Image Denoising</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image denoising], [self-supervised learning, blind-spot network, pixel-shuffle downsampling, real-world noise, super-resolution]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yiwen Shan, Haiyu Zhao, Peng Hu, Xi Peng, Yuanbiao Gou</p>
</li>
<li class="">
<p><strong>institution:</strong> Sichuan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21038" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21038</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Next-Scale Prediction (NSP), a novel self-supervised paradigm that decouples noise decorrelation from detail preservation by constructing cross-scale training pairs for blind-spot networks. 2. Introduces a method where the network takes low-resolution, fully decorrelated sub-images as input to predict high-resolution targets, alleviating the trade-off between noise removal and detail retention. 3. Demonstrates that the proposed NSP framework naturally supports super-resolution of noisy images as a by-product, without requiring retraining or architectural modification.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fd7469e2ff88e84d5b9f4c335e67c32a82564aabaef8eb11c7625010499218a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fd7469e2ff88e84d5b9f4c335e67c32a82564aabaef8eb11c7625010499218a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge in self-supervised real-world image denoising where existing methods struggle to balance noise decorrelation and detail preservation. It introduces Next-Scale Prediction (NSP), a method that uses cross-scale training pairs to decouple these two objectives, allowing a blind-spot network to learn from decorrelated low-resolution inputs to predict clean high-resolution outputs. Experiments show NSP achieves state-of-the-art performance and can also perform super-resolution on noisy images without additional training.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] A Large-Depth-Range Layer-Based Hologram Dataset for Machine Learning-Based 3D Computer-Generated Holography</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [computer-generated holography], [hologram dataset, amplitude projection, angular spectrum method, RGB-D images, super-resolution]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jaehong Lee, You Chan No, YoungWoo Kim, Duksu Kim</p>
</li>
<li class="">
<p><strong>institution:</strong> Korea University of Technology &amp; Education (KOREATECH)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21040" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21040</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces KOREATECH-CGH, a large-scale public dataset of 6,000 RGB-D and complex hologram pairs with wide resolution and depth range for ML-CGH. 2. Proposes amplitude projection, a post-processing technique to enhance hologram reconstruction fidelity at large depth ranges by replacing amplitude components while preserving phase. 3. Validates the dataset&#x27;s utility by demonstrating its effectiveness for training state-of-the-art ML models on tasks like hologram generation and super-resolution.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52ea7153ddcddf48944d10c7358fc7dea7c5e544520bd72d9c3d2634cf167b35_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52ea7153ddcddf48944d10c7358fc7dea7c5e544520bd72d9c3d2634cf167b35_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of high-quality datasets for machine learning-based computer-generated holography (ML-CGH) by introducing KOREATECH-CGH, a large-scale dataset of RGB-D and hologram pairs. The authors also propose a novel amplitude projection technique to improve hologram quality at large depth ranges. The dataset and method are shown to enable effective training of ML models and achieve superior reconstruction quality compared to prior work.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Matrix Completion Via Reweighted Logarithmic Norm Minimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [low-rank matrix completion], [reweighted logarithmic norm, matrix completion, ADMM, nonconvex optimization, image inpainting]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhijie Wang, Liangtian He, Qinghua Zhang, Jifei Miao, Liang-Jian Deng, Jun Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> The affiliations are not explicitly listed in the provided content. Based on the author names (Zhijie Wang, Liangtian He, Qinghua Zhang, Jifei Miao, Liang-Jian Deng, Jun Liu), it is not possible to reliably infer a single main institution without the full paper.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21050" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21050</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel reweighted matrix logarithmic norm (RMLN) as a more accurate nonconvex surrogate for the rank function. 2. Develops an ADMM-based optimization algorithm that works for any p in (0,1], removing a restriction present in prior work. 3. Demonstrates superior performance in image inpainting experiments compared to state-of-the-art low-rank matrix completion methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81808ff3d4d128d0df5f11ee05651c74a2691f1ac5943d560a7091bdb5c374c8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81808ff3d4d128d0df5f11ee05651c74a2691f1ac5943d560a7091bdb5c374c8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the suboptimal solutions from nuclear norm minimization in low-rank matrix completion by proposing a novel reweighted logarithmic norm as a better nonconvex surrogate. The resulting optimization problem is solved efficiently using ADMM. Experiments on image inpainting show the proposed method outperforms existing state-of-the-art approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object pose tracking], [event camera, optical flow, 6DoF pose estimation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zibin Liu, Banglei Guan, Yang Shang, Shunkun Liang, Zhenbao Yu, Qifeng Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Defense Technology, Wuhan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21053" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21053</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel optical flow-guided 6DoF object pose tracking method using an event camera. 2. Introduces a 2D-3D hybrid feature extraction strategy to detect corners and edges from events and object models. 3. Establishes correlation between features by searching for corner optical flow and minimizing distances between corners and edges for iterative pose optimization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9212058f75e634f43650bad542c684ec2b99117ed801e190d2e670146ed2f351_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9212058f75e634f43650bad542c684ec2b99117ed801e190d2e670146ed2f351_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a new method for 6DoF object pose tracking using an event camera. The method uses optical flow to guide the association between extracted 2D-3D hybrid features (corners and edges) and iteratively optimizes the object pose. Experiments on simulated and real event data show the method outperforms existing event-based approaches in accuracy and robustness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D human pose estimation], [3D sign language reconstruction, biomechanical accuracy, hand and body pose priors, monocular video, SMPL-X]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kaustubh Kundu, Hrishav Bakul Barua, Lucy Robertson-Bell, Zhixi Cai, Kalin Stefanov</p>
</li>
<li class="">
<p><strong>institution:</strong> Monash University, TCS Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21054" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21054</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/kaustesseract/DexAvatar" target="_blank" rel="noopener noreferrer" class="">https://github.com/kaustesseract/DexAvatar</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel framework (DexAvatar) for reconstructing biomechanically accurate 3D hand and body poses from monocular sign language videos. 2. The use of learned 3D hand and body pose priors to guide the reconstruction and overcome challenges like self-occlusion and motion blur. 3. Demonstrating strong performance on the SGNify benchmark, achieving a 35.11% improvement over the state-of-the-art.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/594bef871fe9a00d58a9f3f12c9a0b4bf4f66d3d738bd3f02dedcbad04bdcd25_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/594bef871fe9a00d58a9f3f12c9a0b4bf4f66d3d738bd3f02dedcbad04bdcd25_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces DexAvatar, a framework that uses learned 3D hand and body pose priors to reconstruct accurate 3D sign language poses from monocular videos. It addresses the limitations of existing 2D datasets and noisy 3D estimations. The method significantly outperforms prior work on the SGNify motion capture benchmark.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Multimodal Skeleton-Based Action Representation Learning via Decomposition and Composition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [skeleton-based action recognition], [multimodal fusion, self-supervised learning, decomposition and composition, skeleton data, representation learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hongsong Wang, Heng Fei, Bingxuan Dai, Jie Gui</p>
</li>
<li class="">
<p><strong>institution:</strong> Southeast University, Purple Mountain Laboratories</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21064" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21064</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel self-supervised multimodal skeleton-based action representation learning framework named &quot;Decomposition and Composition&quot;. 2. Introduces a Decomposition strategy to decompose fused multimodal features into distinct unimodal features and align them with ground truth unimodal counterparts. 3. Introduces a Composition strategy that integrates multiple unimodal features as self-supervised guidance to enhance multimodal representation learning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c579dcce18ac587c13f2fd1178f66dd2922786e4c3434ff17ad5319761f7aa1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c579dcce18ac587c13f2fd1178f66dd2922786e4c3434ff17ad5319761f7aa1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a self-supervised framework called &quot;Decomposition and Composition&quot; for multimodal skeleton-based action recognition. It aims to effectively utilize the complementarity of different modalities while maintaining computational efficiency. Experiments on major datasets show the method achieves a strong balance between performance and cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [robotic grasping], [language-guided grasping, cross-modal fusion, coarse-to-fine learning, CLIP embeddings, dynamic convolution]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zebin Jiang, Tianle Jin, Xiangtong Yao, Alois Knoll, Hu Cao</p>
</li>
<li class="">
<p><strong>institution:</strong> Technical University of Munich</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21065" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21065</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hierarchical cross-modal fusion pipeline that progressively injects linguistic cues into visual feature reconstruction for fine-grained visual-semantic alignment., 2. Introduces a language-conditioned dynamic convolution head (LDCH) that adaptively mixes convolution experts based on sentence-level features for instruction-adaptive predictions., 3. Presents a final refinement module to enhance grasp consistency and robustness in complex scenes, validated on real robotic platforms.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2da981b4739ec7852bb087293e335e475a0b336479e9ff3e69ca100703c7262_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2da981b4739ec7852bb087293e335e475a0b336479e9ff3e69ca100703c7262_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes LGGD, a language-guided grasp detection method using a coarse-to-fine learning paradigm with hierarchical cross-modal fusion and a language-conditioned dynamic convolution head. It achieves superior performance on benchmark datasets and demonstrates effective real-world robotic manipulation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Beyond Pixel Simulation: Pathology Image Generation via Diagnostic Semantic Tokens and Prototype Control</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image generation], [diagnostic semantic tokens, prototype control, multi-stream control, pathology MLLM, Patho-FID]</p>
</li>
<li class="">
<p><strong>authors:</strong> Minghao Han, YiChen Liu, Yizhou Liu, Zizhi Chen, Jingqun Tang, Xuecheng Wu, Dingkang Yang, Lihua Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Fudan University, University of Science and Technology Beijing, Xi&#x27;an Jiaotong University, ByteDance, Fysics Intelligence Technologies Co., Ltd.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21058" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21058</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed UniPath, a semantics-driven pathology image generation framework with Multi-Stream Control (Raw-Text, High-Level Semantics, and Prototype streams) for fine-grained, diagnosis-aware control. 2. Curated a large-scale 2.65M image-text corpus and a high-quality 68K subset to address data scarcity in computational pathology. 3. Established a comprehensive four-tier evaluation hierarchy tailored for pathology and demonstrated state-of-the-art performance, including a 51% improvement in Patho-FID.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54421c653fc67f5d29e7359f606ed3b8f53d435c83275a71b19858309287fe45_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54421c653fc67f5d29e7359f606ed3b8f53d435c83275a71b19858309287fe45_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces UniPath, a framework for generating pathology images with fine-grained semantic control by leveraging diagnostic understanding through multi-stream conditioning and a prototype bank. It addresses key challenges like data scarcity and terminological heterogeneity by curating large datasets and using a frozen pathology MLLM to distill robust semantic tokens. Experiments show UniPath achieves state-of-the-art performance, significantly outperforming prior methods in both image quality and semantic fidelity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [visual place recognition], [multi-view, transformer, 3D representation, feature aggregation, geometry-grounded]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tianchen Deng, Xun Chen, Ziming Li, Hongming Shen, Danwei Wang, Javier Civera, Hesheng Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, University of Zaragoza, Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21078" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21078</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/dtc111111/UniPR-3D" target="_blank" rel="noopener noreferrer" class="">https://github.com/dtc111111/UniPR-3D</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. 2. Proposes a method that jointly leverages 2D and 3D tokens from a VGGT backbone and designs dedicated aggregation modules for each. 3. Incorporates both single- and multi-frame aggregation schemes with a variable-length sequence retrieval strategy to enhance generalization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f608df655d14ba7ae116d2888176d7d1cb60fee1d08eaf9644d0c320c42e0fe6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f608df655d14ba7ae116d2888176d7d1cb60fee1d08eaf9644d0c320c42e0fe6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces UniPR-3D, a novel Visual Place Recognition (VPR) method that uses a Visual Geometry Grounded Transformer (VGGT) backbone to integrate multi-view information. It constructs descriptors by aggregating both 2D and 3D tokens with dedicated modules and supports variable-length sequence retrieval. Experiments show that UniPR-3D achieves state-of-the-art performance, outperforming existing single- and multi-view baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Hierarchical Modeling Approach to Fast and Accurate Table Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [document analysis], [table recognition, multi-task learning, non-causal attention, parallel inference, hierarchical modeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Takaya Kawakatsu</p>
</li>
<li class="">
<p><strong>institution:</strong> Preferred Networks, Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21083" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21083</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel multi-task model utilizing non-causal attention to capture the entire table structure. 2. A parallel inference algorithm for cell content recognition that significantly speeds up inference. 3. A hierarchical modeling approach that extends the performance of multi-task models while addressing their speed and explainability limitations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd3e215f24ffc72ff43528e39e86c65289bbfb5de75955d48f0284f277f0089e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd3e215f24ffc72ff43528e39e86c65289bbfb5de75955d48f0284f277f0089e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the slow inference speed and lack of explainability in existing table recognition models. It proposes a new multi-task model with non-causal attention for global structure understanding and a parallel inference algorithm to decode cell contents simultaneously, achieving both superior accuracy and a 10x+ speedup on large public datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] UniRec-0.1B: Unified Text and Formula Recognition with 0.1B Parameters</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [document parsing], [unified recognition, hierarchical supervision, semantic-decoupled tokenizer, lightweight model, multi-level recognition]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yongkun Du, Zhineng Chen, Yazhen Xie, Weikang Baiand Hao Feng, Wei Shi, Yuchen Su, Can Huang, Yu-Gang Jiang</p>
</li>
<li class="">
<p><strong>institution:</strong> Fudan University, ByteDance</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21095" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21095</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Topdu/OpenOCR" target="_blank" rel="noopener noreferrer" class="">https://github.com/Topdu/OpenOCR</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed UniRec-0.1B, a lightweight unified model with only 0.1B parameters for multi-level text and formula recognition. 2. Introduced a hierarchical supervision training strategy to address structural variability across different recognition levels. 3. Developed a semantic-decoupled tokenizer to separate text and formula representations, mitigating semantic entanglement.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d562a464fcb112a02846c5c1d5056088c1b9a37f7ece91f77cfeea28939b63b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d562a464fcb112a02846c5c1d5056088c1b9a37f7ece91f77cfeea28939b63b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes UniRec-0.1B, a lightweight vision-language model for unified text and formula recognition across multiple document levels. To address challenges like structural variability and semantic entanglement, the method introduces hierarchical supervision training and a semantic-decoupled tokenizer, trained on a new 40M-sample dataset. Experiments show it outperforms existing general and expert models while being 2-9x faster.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal generation evaluation], [text-to-audio-video, cross-modal alignment, MLLM-as-a-Judge, benchmark, evaluation framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhe Cao, Tao Wang, Jiaming Wang, Yanghai Wang, Yuanxing Zhang, Jialu Chen, Miao Deng, Jiahao Wang, Yubin Guo, Chenxi Liao, Yize Zhang, Zhaoxiang Zhang, Jiaheng Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanjing University, Kuaishou Technology, Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21094" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21094</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/NJU-LINK/T2AV-Compass" target="_blank" rel="noopener noreferrer" class="">https://github.com/NJU-LINK/T2AV-Compass</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces T2AV-Compass, a unified benchmark with 500 diverse and complex prompts for evaluating Text-to-Audio-Video generation systems. 2. Proposes a dual-level evaluation framework combining objective signal-level metrics with a subjective MLLM-as-a-Judge protocol. 3. Provides extensive evaluation of 11 T2AV systems, revealing significant gaps in realism and cross-modal consistency, establishing a diagnostic testbed for future research.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d57ae1315c90b252f611ac78bcd0cff28f02c7dc2648194faa7e1614fe031d1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d57ae1315c90b252f611ac78bcd0cff28f02c7dc2648194faa7e1614fe031d1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the fragmented evaluation of Text-to-Audio-Video (T2AV) generation by proposing T2AV-Compass, a unified benchmark and evaluation framework. It combines objective metrics for quality and alignment with a subjective MLLM-as-a-Judge protocol. The evaluation of 11 systems shows they still fall short of human-level realism and synchronization, highlighting the benchmark&#x27;s value for advancing the field.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D avatar generation], [3D Gaussian Splatting, analytic rigging, texel-space deformation, hybrid representation, head reenactment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jaeseong Lee, Junyeong Ahn, Taewoong Kang, Jaegul Choo</p>
</li>
<li class="">
<p><strong>institution:</strong> KAIST, Hanyang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21099" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21099</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A hybrid avatar representation (TexAvatars) that combines analytic rigging for geometric grounding with texel-space neural regression for spatial continuity. 2. A method that predicts Gaussian attributes in UV space via CNNs but drives 3D deformation using mesh-aware Jacobians, enabling smooth transitions across mesh boundaries. 3. The model demonstrates improved generalization, stability, and capture of fine-grained expression details (e.g., wrinkles, mouth cavity) under extreme poses and expressions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8320fd6b1a6f131ad704b82b65143269040ab86b9b67005a1edf15fca8097f6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8320fd6b1a6f131ad704b82b65143269040ab86b9b67005a1edf15fca8097f6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces TexAvatars, a method for creating drivable 3D head avatars by hybridizing analytic rigging with texel-space neural regression to improve generalization to unseen expressions. It predicts local attributes in UV space but uses mesh-aware Jacobians for 3D deformation, separating semantic modeling from geometric control. The approach achieves state-of-the-art performance in challenging reenactment scenarios, capturing fine details with high fidelity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] FreeInpaint: Tuning-free Prompt Alignment and Visual Rationality Enhancement in Image Inpainting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image inpainting], [diffusion models, latent optimization, prompt alignment, visual rationality, plug-and-play]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chao Gong, Dong Li, Yingwei Pan, Jingjing Chen, Ting Yao, Tao Mei</p>
</li>
<li class="">
<p><strong>institution:</strong> Fudan University, HiDream.ai Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21104" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21104</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/CharlesGong12/FreeInpaint" target="_blank" rel="noopener noreferrer" class="">https://github.com/CharlesGong12/FreeInpaint</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a plug-and-play, tuning-free approach (FreeInpaint) that optimizes diffusion latents during inference to enhance image faithfulness., 2. Introduces a prior-guided noise optimization method to steer model attention towards valid inpainting regions by optimizing the initial noise., 3. Designs a composite guidance objective tailored for inpainting to direct the denoising process, improving both prompt alignment and visual rationality by optimizing intermediate latents.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4367afc5cacaf5baa9675d465f870aa3305d6715ef1ced7b9a233ea1cdf470b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4367afc5cacaf5baa9675d465f870aa3305d6715ef1ced7b9a233ea1cdf470b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge in text-guided image inpainting of aligning generated content with user prompts while maintaining visual fidelity. It proposes FreeInpaint, a tuning-free method that optimizes diffusion latents during inference using a prior-guided noise optimization and a composite guidance objective. Extensive experiments demonstrate its effectiveness in enhancing both prompt alignment and visual rationality compared to existing methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [precipitation nowcasting, latent diffusion model, spatio-temporal prediction, variational autoencoder, conditioning network]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shi Quan Foo, Chi-Ho Wong, Zhihan Gao, Dit-Yan Yeung, Ka-Hing Wong, Wai-Kin Wong</p>
</li>
<li class="">
<p><strong>institution:</strong> The Hong Kong University of Science and Technology, Hong Kong Observatory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21118" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21118</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/sqfoo/stldm_official" target="_blank" rel="noopener noreferrer" class="">https://github.com/sqfoo/stldm_official</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes STLDM, a novel two-stage diffusion-based architecture for precipitation nowcasting that combines deterministic forecasting with generative enhancement. 2. Introduces an end-to-end learning framework that jointly trains a Variational Autoencoder, a conditioning network, and a latent diffusion model. 3. Demonstrates superior performance and improved inference efficiency compared to state-of-the-art methods on multiple radar datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3f04a94a2a07676690c2f108f7a602a6b052c1463701d217a319cd81e05ecce_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3f04a94a2a07676690c2f108f7a602a6b052c1463701d217a319cd81e05ecce_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of precipitation nowcasting, where deterministic models produce blurry predictions and generative models suffer from poor accuracy. It proposes STLDM, a spatio-temporal latent diffusion model that decomposes the task into a deterministic forecasting stage and a generative enhancement stage. Experiments show STLDM outperforms state-of-the-art methods while being more efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] MarineEval: Assessing the Marine Intelligence of Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [vision-language models], [marine intelligence, domain-specific evaluation, benchmark dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> YuK-Kwan Wong, Tuan-An To, Jipeng Zhang, Ziqiang Zheng, Sai-Kit Yeung</p>
</li>
<li class="">
<p><strong>institution:</strong> Hong Kong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21126" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21126</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://marineeval.hkustvgd.com" target="_blank" rel="noopener noreferrer" class="">https://marineeval.hkustvgd.com</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MarineEval, the first large-scale marine domain-specific dataset and benchmark for evaluating Vision-Language Models (VLMs). 2. Constructs a diverse dataset with 2,000 image-based QA pairs, covering 7 task dimensions and 20 capacity dimensions, validated by marine domain experts. 3. Provides a comprehensive benchmark evaluation of 17 existing VLMs, revealing their significant limitations in handling domain-specific marine questions and highlighting areas for future improvement.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98ea3a0e94d1e540398b18f5612700e51ce9e190b312a741182b20da5ecfacac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98ea3a0e94d1e540398b18f5612700e51ce9e190b312a741182b20da5ecfacac_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether existing general-purpose Vision-Language Models (VLMs) can act as domain experts for marine science. To evaluate this, the authors construct MarineEval, a large-scale, expert-verified benchmark dataset of 2,000 marine image-question-answer pairs. The benchmark reveals that current VLMs perform poorly on this domain-specific task, indicating a significant gap and need for future research in specialized VLM capabilities.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] ORCA: Object Recognition and Comprehension for Archiving Marine Species</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multi-modal benchmark], [marine visual understanding, object detection, instance captioning, visual grounding, open-vocabulary]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuk-Kwan Wong, Haixin Liang, Zeyu Ma, Yiwei Chen, Ziqiang Zheng, Rinaldi Gotama, Pascal Sebastian, Lauren D. Sparks, Sai-Kit Yeung</p>
</li>
<li class="">
<p><strong>institution:</strong> Hong Kong University of Science and Technology, University of Electronic Science and Technology of China, Indo Ocean Foundation</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21150" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21150</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://orca.hkustvgd.com" target="_blank" rel="noopener noreferrer" class="">https://orca.hkustvgd.com</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces ORCA, a comprehensive multi-modal benchmark dataset for marine species with 14,647 images, 42,217 bounding boxes, and 22,321 expert-verified instance captions. 2. Formulates and evaluates three core computer vision tasks (object detection, instance captioning, visual grounding) on the benchmark to align domain challenges with well-defined tasks. 3. Provides an extensive evaluation of 18 state-of-the-art models, highlighting key challenges like species diversity and morphological overlap, establishing a baseline for future research.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfd1cc706ce3b8fe57cf7dcd49a501ba304d0ea7e587a81bddd8288ab2c69be7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfd1cc706ce3b8fe57cf7dcd49a501ba304d0ea7e587a81bddd8288ab2c69be7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents ORCA, a multi-modal benchmark dataset for marine visual understanding, addressing the lack of systematic data and task formulation in the domain. It evaluates 18 state-of-the-art models on tasks like object detection and captioning, revealing significant challenges due to species diversity and domain-specific demands. The work establishes a comprehensive benchmark to advance research in automated marine ecosystem monitoring.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [CLIP, multimodal fusion, parameter-efficient fine-tuning, vision-language alignment, anatomical structure]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gaoren Lin, Huangxuan Zhao, Yuan Xiong, Lefei Zhang, Bo Du, Wentao Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> Wuhan University (Inferred from authors Gaoren Lin, Lefei Zhang, Bo Du, Wentao Zhu, who are known to be affiliated with Wuhan University. Huangxuan Zhao and Yuan Xiong are likely from the same group.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21135" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21135</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Semantic-Structural Synergy Encoder (SSE) that augments CLIP&#x27;s ViT with a CNN branch to preserve fine-grained anatomical structures. 2. Introduces a Domain-Augmented Text Encoder (DATE) that injects medical knowledge from large language models to better model complex clinical descriptions. 3. Designs a Vision-Language Calibration Module (VLCM) to refine cross-modal correspondence in a unified feature space, addressing domain-specific semantic misalignment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b11d6061b6a08f8b03b2395e16fc0847c1b68ab4e388d93a886ee875211fcf44_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b11d6061b6a08f8b03b2395e16fc0847c1b68ab4e388d93a886ee875211fcf44_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes TGC-Net, a parameter-efficient CLIP-based framework for text-guided medical image segmentation. It addresses CLIP&#x27;s limitations in medical imaging by introducing modules for structural refinement, medical knowledge injection, and cross-modal calibration. Experiments on five datasets show state-of-the-art performance with fewer trainable parameters.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Towards Arbitrary Motion Completing via Hierarchical Continuous Representation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [motion representation and generation], [Implicit Neural Representations, hierarchical temporal encoding, parametric activation, continuous representation, motion interpolation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chenghao Xu, Guangtao Lyu, Qi Liu, Jiexi Yan, Muli Yang, Cheng Deng</p>
</li>
<li class="">
<p><strong>institution:</strong> Hohai University, Xidian University, Institute for Infocomm Research (I2R), A*STAR</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21183" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21183</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel hierarchical continuous representation framework (PA-HiRes) for human motion sequences based on Implicit Neural Representations (INRs). 2. Introduces a hierarchical temporal encoding mechanism to capture intricate motion patterns at multiple temporal scales. 3. Integrates a custom parametric activation function, powered by Fourier transformations, into the MLP decoder to enhance the expressiveness and accuracy of the continuous motion model.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d8371d82733eca85c31c35a7b6beb75b440d219ddb44ee03521161e6e3f48a6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d8371d82733eca85c31c35a7b6beb75b440d219ddb44ee03521161e6e3f48a6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of fixed-frame-rate human motion data by proposing a continuous representation model called PA-HiRes. The method uses a hierarchical implicit neural representation with a novel parametric activation function to enable interpolation and extrapolation of motion at arbitrary frame rates. Extensive evaluations show the approach is effective and robust for representing complex motion behaviors.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D shape generation], [diffusion models, geometric refinement, watertight processing, voxel-based refinement, RoPE]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tanghui Jia, Dongyu Yan, Dehao Hao, Yang Li, Kaiyi Zhang, Xianyi He, Lanjiong Li, Jinnan Chen, Lutao Jiang, Qishen Yin, Long Quan, Ying-Cong Chen, Li Yuan</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University, The Hong Kong University of Science and Technology, National University of Singapore</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21185" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21185</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A comprehensive data processing pipeline for 3D datasets, featuring a novel watertight processing method and high-quality filtering to improve geometric quality. 2. A two-stage 3D diffusion framework that decouples spatial localization from geometric detail synthesis, using voxel-based refinement with RoPE-encoded positional anchors. 3. Training and evaluation demonstrating competitive performance with existing open-source methods using only publicly available datasets and limited resources.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73c8cc72abe4a497351656b82b6735dbe1550a6d18e5c93d7aa031162f07f81e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73c8cc72abe4a497351656b82b6735dbe1550a6d18e5c93d7aa031162f07f81e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces UltraShape 1.0, a two-stage diffusion framework for generating high-fidelity 3D shapes. It first synthesizes a coarse structure and then refines it using a novel method that decouples spatial localization from detail synthesis via voxel queries and RoPE encoding. The approach, supported by an improved data processing pipeline, achieves competitive geometry generation quality using public datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] A Turn Toward Better Alignment: Few-Shot Generative Adaptation with Equivariant Feature Rotation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [few-shot image generation], [domain adaptation, feature rotation, Lie Group, equivariant space, generative adversarial networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chenghao Xu, Qi Liu, Jiexi Yan, Muli Yang, Cheng Deng</p>
</li>
<li class="">
<p><strong>institution:</strong> Hohai University, Xidian University, Institute for Infocomm Research (I2R), A*STAR</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21174" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21174</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Equivariant Feature Rotation (EFR), a novel adaptation strategy that aligns source and target domains within a self-rotated proxy feature space to bridge the domain gap. 2. Introduces adaptive rotations within a parameterized Lie Group to transform features into an equivariant space, preserving intra-domain structural information without distortion. 3. Demonstrates through comprehensive experiments that the method significantly enhances generative performance in the target domain compared to existing approaches.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b06e02ddfdad8176f73c53632a928df39be42b51a926b7d00530f341c4b6945e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b06e02ddfdad8176f73c53632a928df39be42b51a926b7d00530f341c4b6945e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of few-shot image generation, where existing methods struggle to align source and target domains effectively due to strict or relaxed constraints. The authors propose Equivariant Feature Rotation (EFR), a method that uses learnable rotations in a Lie Group to map features into an equivariant proxy space for better alignment. Experiments show that EFR significantly improves generative performance in the target domain.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [visual reasoning], [vision-language models, benchmark, perceptual reasoning, compositional reasoning, abstraction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Brigitta Malagurski Törtei, Yasser Dahou, Ngoc Dung Huynh, Wamiq Reyaz Para, Phúc H. Lê Khac, Ankit Singh, Sofian Chaybouti, Sanath Narayan</p>
</li>
<li class="">
<p><strong>institution:</strong> Technology Innovation Institute, Tuebingen AI Center/University of Tuebingen</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21194" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21194</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces VisRes Bench, a new benchmark for evaluating visual reasoning in naturalistic settings without language supervision. 2. Proposes a structured evaluation across three levels of complexity (perceptual, single-attribute, multi-attribute) to isolate distinct reasoning abilities. 3. Reveals that state-of-the-art VLMs perform near random under subtle perceptual perturbations, exposing their limited abstraction beyond pattern recognition.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da4a0d93325d1e003056f13f48074769efe0198717d9c263282ebe51f3848352_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da4a0d93325d1e003056f13f48074769efe0198717d9c263282ebe51f3848352_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces VisRes Bench, a benchmark designed to evaluate the visual reasoning capabilities of Vision-Language Models (VLMs) without relying on linguistic cues. It tests models across three levels of increasing complexity—perceptual completion, rule-based inference, and compositional reasoning—using over 19,000 controlled images. The main finding is that current VLMs perform poorly, near random chance, under perceptual perturbations, indicating a lack of genuine abstract visual reasoning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Schrödinger&#x27;s Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [robot navigation], [zero-shot object navigation, trajectory-conditioned 3D imagination, occlusion-aware planning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yu He, Da Huang, Zhenyang Liu, Zixiao Gu, Qiang Sun, Guangnan Ye, Yanwei Fu</p>
</li>
<li class="">
<p><strong>institution:</strong> Fudan University, Shanghai Jiao Tong University, Shanghai University of International Business and Economics, Shanghai Innovation Institute</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21201" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21201</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://heyu322.github.io/Schrodinger-Navigator.github.io/" target="_blank" rel="noopener noreferrer" class="">https://heyu322.github.io/Schrodinger-Navigator.github.io/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Schrödinger&#x27;s Navigator, a novel navigation framework that models unobserved space as an ensemble of plausible future worlds to handle uncertainty. 2. Introduced a trajectory-conditioned 3D world model that imagines future observations along candidate paths to see beyond occlusions and anticipate risks. 3. Developed a method to fuse imagined 3D observations into a navigation map to update a value map, guiding the policy toward safer, less-occluded routes for better object tracking.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34e56028ea40f6f3b4a9150683288695e8b7fd2724c676c4f7f56f07367b4fb3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34e56028ea40f6f3b4a9150683288695e8b7fd2724c676c4f7f56f07367b4fb3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of zero-shot object navigation in cluttered environments with occlusions and moving targets. It proposes Schrödinger&#x27;s Navigator, a framework that samples candidate trajectories and uses a 3D imagination model to predict future observations, enabling the robot to plan safer paths and locate hidden objects. Experiments on a quadruped robot show the method outperforms baselines in success rate and localization in occlusion-heavy settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Latent Implicit Visual Reasoning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal learning], [visual reasoning tokens, task-agnostic, implicit supervision]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kelvin Li, Chuyi Shang, Leonid Karlinsky, Rogerio Feris, Trevor Darrell, Roei Herzig</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Berkeley, Xero, MIT-IBM Watson AI Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21218" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21218</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a task-agnostic mechanism for training Large Multimodal Models (LMMs) to discover and use visual reasoning tokens without explicit supervision. 2. Enables models to adaptively re-encode images for a task, extracting relevant visual information without hand-crafted intermediate steps. 3. Achieves state-of-the-art results on diverse vision-centric tasks and generalizes well to multi-task instruction tuning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42f15cd73a3268dae7d708e796f38593c4f7159611e5b226fceb4458a983b525_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42f15cd73a3268dae7d708e796f38593c4f7159611e5b226fceb4458a983b525_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the limitation of text-centric Large Multimodal Models (LMMs) in handling vision-heavy reasoning tasks. It proposes a method that allows LMMs to learn and use visual reasoning tokens implicitly, without needing supervised intermediate visual steps. This approach outperforms direct fine-tuning and achieves state-of-the-art performance on a range of challenging visual reasoning tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [runtime safety guardrail, executable safety logic, hybrid reasoning, temporal safety predicate, context-aware safety predicate]</p>
</li>
<li class="">
<p><strong>authors:</strong> Le Wang, Zonghao Ying, Xiao Yang, Quanchen Zou, Zhenfei Yin, Tianlin Li, Jian Yang, Yaodong Yang, Aishan Liu, Xianglong Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Beihang University, Beijing University of Posts and Telecommunications, 360 AI Security Lab, The University of Sydney, Nanyang Technological University, Peking University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21220" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21220</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes RoboSafe, a hybrid reasoning runtime safeguard for embodied agents using executable predicate-based safety logic. 2. Introduces a Backward Reflective Reasoning module to infer temporal safety predicates from recent trajectories and trigger replanning. 3. Introduces a Forward Predictive Reasoning module to anticipate risks by generating context-aware safety predicates from long-term memory and observations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb68be6c6803ce76bf0036925bc1f629db0f2d1ae9be491b5ab0a450b37d1e5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb68be6c6803ce76bf0036925bc1f629db0f2d1ae9be491b5ab0a450b37d1e5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the vulnerability of vision-language model-driven embodied agents to hazardous instructions in dynamic environments. It proposes RoboSafe, a runtime safety system that uses hybrid reasoning with backward reflection and forward prediction to generate executable safety logic. Experiments show RoboSafe significantly reduces hazardous actions while maintaining task performance, and its practicality is validated on physical robots.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image-text retrieval], [event-centric entity extraction, BM25, BEiT-3, two-stage retrieval]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dao Sy Duy Minh, Huynh Trung Kiet, Nguyen Lam Phu Quy, Phu-Hoa Pham, Tran Chi Nguyen</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science - VNUHCM</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21221" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21221</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval" target="_blank" rel="noopener noreferrer" class="">https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a lightweight two-stage retrieval pipeline for event-based image retrieval. 2. Leverages event-centric entity extraction to incorporate temporal and contextual signals for efficient candidate filtering. 3. Combines BM25-based filtering with BEiT-3 reranking to achieve high accuracy on the OpenEvents benchmark.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3fccaf1dfde41ddfe9c023d8a1f41a9f87c4a0899f25d8a475702d3f368a129_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3fccaf1dfde41ddfe9c023d8a1f41a9f87c4a0899f25d8a475702d3f368a129_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of retrieving images from natural language descriptions in complex, real-world scenarios. It proposes a two-stage method that first filters candidates using BM25 on extracted event entities, then reranks them with a BEiT-3 model. The approach significantly outperforms prior baselines on the OpenEvents benchmark, demonstrating the effectiveness of combining lightweight entity guidance with deep multimodal modeling.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Human Motion Estimation with Everyday Wearables</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human motion capture], [multimodal learning, teacher-student framework, egocentric vision, inertial measurement units (IMUs), sim-to-real gap]</p>
</li>
<li class="">
<p><strong>authors:</strong> Siqi Zhu, Yixuan Li, Junfu Li, Qi Wu, Zan Wang, Haozhe Ma, Wei Liang</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Institute of Technology, Yangtze Delta Region Academy of Beijing Institute of Technology, Shenzhen MSU-BIT University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21209" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21209</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://pie-lab.cn/EveryWear/" target="_blank" rel="noopener noreferrer" class="">https://pie-lab.cn/EveryWear/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes EveryWear, a lightweight and practical full-body motion capture system using only common consumer wearables (smartphone, smartwatch, earbuds, smart glasses) without requiring explicit calibration. 2. Introduces Ego-Elec, a large-scale, real-world dataset with 9 hours of data covering 56 daily activities in 17 environments, providing ground-truth 3D motion annotations. 3. Presents a multimodal teacher-student framework that fuses visual cues from egocentric cameras with inertial signals, trained directly on real-world data to eliminate the sim-to-real gap.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ced793b8564fa6f3ddfc9f17a175ca19ac32fba9b281a74c4be9046c4484d995_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ced793b8564fa6f3ddfc9f17a175ca19ac32fba9b281a74c4be9046c4484d995_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes EveryWear, a human motion estimation method that uses everyday wearables like smartphones and smartwatches in a multimodal teacher-student framework. It introduces a real-world dataset, Ego-Elec, for training and benchmarking. Experiments show the method outperforms baselines, offering a practical solution for full-body motion capture.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] SegMo: Segment-aligned Text to 3D Human Motion Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human motion generation], [segment alignment, contrastive learning, text-to-motion, fine-grained correspondence, shared embedding space]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bowen Dang, Lin Wu, Xiaohang Yang, Zheng Yuan, Zhixiang Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Sheffield, University of Glasgow, Queen Mary University of London</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21237" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21237</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel segment-aligned framework (SegMo) for text-to-3D-motion generation, decomposing both text and motion into temporally ordered segments for fine-grained alignment. 2. Introduces a method for fine-grained text-motion alignment using contrastive learning to create a shared embedding space for text and motion segments. 3. Demonstrates improved performance on standard benchmarks and shows the framework&#x27;s applicability to retrieval tasks like motion grounding and motion-to-text retrieval.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39d0fd9df6a0e35165dba62659aeda0ea42d69345d39516d1c7d95fc5895ac0a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39d0fd9df6a0e35165dba62659aeda0ea42d69345d39516d1c7d95fc5895ac0a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of coarse alignment in text-to-3D human motion generation by proposing SegMo, a framework that decomposes text and motion into segments and aligns them using contrastive learning. The method achieves improved performance on the HumanML3D dataset and enables retrieval-style applications. The results show that fine-grained segment alignment enhances the accuracy and realism of generated motions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [adversarial attacks], [hard-label black-box attacks, query efficiency, ray search optimization, Nesterov&#x27;s Accelerated Gradient, momentum-based optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xinjie Xu, Shuyu Cheng, Dongwei Xu, Qi Xuan, Chen Ma</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University of Technology, Binjiang Institute of Artificial Intelligence, ZJUT, JQ Investments</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21241" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21241</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/machanic/hard_label_attacks" target="_blank" rel="noopener noreferrer" class="">https://github.com/machanic/hard_label_attacks</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed ARS-OPT, a momentum-based algorithm inspired by Nesterov&#x27;s Accelerated Gradient to accelerate the convergence of ray search in hard-label attacks. 2. Introduced PARS-OPT, which further enhances ARS-OPT by incorporating surrogate-model priors into the gradient estimation. 3. Provided theoretical convergence guarantees for the proposed methods and demonstrated superior query efficiency over 13 state-of-the-art approaches on ImageNet and CIFAR-10.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/302b574932ba227c13854e5c9c2cab3d7cf8f1ed29ee145fbc73062632304cd4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/302b574932ba227c13854e5c9c2cab3d7cf8f1ed29ee145fbc73062632304cd4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high query cost of hard-label black-box adversarial attacks by optimizing ray search. The authors propose ARS-OPT, a momentum-based algorithm, and its enhanced version PARS-OPT, which uses surrogate-model priors, to accelerate convergence. Experiments show the methods outperform 13 existing approaches in query efficiency on standard datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [Diffusion Transformers (DiT), intermediate-conditioning, Segment-wise Auto-Regressive (SAR), Direct Preference Optimization (DPO), one-shot video]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiawei Liu, Junqiao Li, Jiangfan Deng, Gen Li, Siyu Zhou, Zetao Fang, Shanshan Lao, Zengde Deng, Jianing Zhu, Tingting Ma, Jiayi Li, Yunqiu Wang, Qian He, Xinglong Wu</p>
</li>
<li class="">
<p><strong>institution:</strong> ByteDance</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21252" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21252</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://dreamontage.github.io/DreaMontage" target="_blank" rel="noopener noreferrer" class="">https://dreamontage.github.io/DreaMontage</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a lightweight intermediate-conditioning mechanism into the DiT architecture with an Adaptive Tuning strategy for robust arbitrary-frame control. 2. Developed a Visual Expression SFT stage and a Tailored DPO scheme to enhance visual fidelity, motion rationality, and transition smoothness. 3. Designed a memory-efficient Segment-wise Auto-Regressive (SAR) inference strategy for generating long-duration videos.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b17202b644882c3a0dbe4c4001d4778edc5131e61435a474ca0e440fd66482a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b17202b644882c3a0dbe4c4001d4778edc5131e61435a474ca0e440fd66482a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces DreaMontage, a framework for generating seamless, long-duration &quot;one-shot&quot; videos guided by arbitrary user-provided frames. It achieves this by integrating an intermediate-conditioning mechanism into a Diffusion Transformer, employing specialized fine-tuning and preference optimization, and using a segment-wise auto-regressive inference strategy. The method produces visually coherent and expressive one-shot videos efficiently.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [attention supervision, conditional control, video diffusion, layout controlnet, 3D-aware layout]</p>
</li>
<li class="">
<p><strong>authors:</strong> Weiqi Li, Zehao Zhang, Liang Lin, Guangrun Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Sun Yat-sen University, Yonsei University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21268" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21268</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ACD (Attention-Conditional Diffusion), a novel framework for direct conditional control in video diffusion models via attention supervision. 2. Introduces a sparse 3D-aware object layout as an efficient conditioning signal and a dedicated Layout ControlNet. 3. Presents an automated annotation pipeline for scalable layout integration.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb3dd27adaccdaa06447db98898dc4dfc553a4184249c20523f8f6b4642036c3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb3dd27adaccdaa06447db98898dc4dfc553a4184249c20523f8f6b4642036c3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the limited controllability in existing video diffusion models by proposing ACD, a framework that directly aligns the model&#x27;s attention maps with external control signals. This method uses a novel sparse 3D-aware layout and a Layout ControlNet to achieve superior alignment with conditioning inputs while maintaining video quality. Experiments show ACD establishes an effective paradigm for conditional video synthesis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] AnyAD: Unified Any-Modality Anomaly Detection in Incomplete Multi-Sequence MRI</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [anomaly detection, missing modality, feature alignment, prototype learning, multi-sequence MRI]</p>
</li>
<li class="">
<p><strong>authors:</strong> Changwei Wu, Yifei Chen, Yuxin Du, Mingxuan Liu, Jinying Zong, Beining Wu, Jie Dong, Feiwei Qin, Yunkang Cao, Qiyuan Tian</p>
</li>
<li class="">
<p><strong>institution:</strong> Hangzhou Dianzi University, Tsinghua University, Hunan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21264" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21264</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/wuchangw/AnyAD" target="_blank" rel="noopener noreferrer" class="">https://github.com/wuchangw/AnyAD</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A unified any-modality anomaly detection framework for incomplete multi-sequence MRI. 2. A dual-pathway encoder with feature distribution alignment to handle severe modality dropout. 3. An Intrinsic Normal Prototypes (INPs) extractor and guided decoder to reconstruct normal patterns and amplify anomalies.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d759b100d62183be434837a07a310b265daec4b80f872064c9f08fd1e67db57_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d759b100d62183be434837a07a310b265daec4b80f872064c9f08fd1e67db57_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes AnyAD, a unified framework for anomaly detection in brain MRI that works robustly with any available combination of imaging modalities, even when some are missing. The method uses a feature alignment mechanism and normal prototype-guided reconstruction to adapt to incomplete data without retraining. Experiments show it outperforms state-of-the-art methods across multiple datasets and modality combinations, establishing a scalable approach for real-world clinical use.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [diffusion transformer, factorized generation, grid-based diffusion, super-resolution, long sequence]</p>
</li>
<li class="">
<p><strong>authors:</strong> Snehal Singh Tomar, Alexandros Graikos, Arjun Krishna, Dimitris Samaras, Klaus Mueller</p>
</li>
<li class="">
<p><strong>institution:</strong> Stony Brook University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21276" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21276</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a factorized generation approach for image sequences, separating coarse sequence generation at low resolution from individual frame refinement at high resolution. 2. Introduces a method to train a model on grid images of subsampled frames, effectively extending a 2D image generator to a 3D sequence generator without architectural changes. 3. Demonstrates superior synthesis quality, improved sequence coherence, high-fidelity generation of arbitrary-length sequences, and increased efficiency (at least 2x faster inference) across diverse datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b10d59f334dda58c40390950d7473806d1453d042361c2e6ae7ef6e5e1a680df_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b10d59f334dda58c40390950d7473806d1453d042361c2e6ae7ef6e5e1a680df_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses inefficiencies in generating long image sequences by proposing GriDiT, a method that factorizes the process. It first generates a coarse, low-resolution sequence using a grid-based Diffusion Transformer, then super-resolves each frame individually. This approach achieves higher quality, better coherence, and at least twice the inference speed compared to state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [surgical scene segmentation], [spiking neural networks, video transformer, masked autoencoding, real-time inference, surgical video]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shihao Zou, Jingjing Li, Wei Ji, Jincai Huang, Kai Wang, Guo Dan, Weixin Si, Yi Pan</p>
</li>
<li class="">
<p><strong>institution:</strong> Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; University of Alberta; Yale University; Southern University of Science and Technology; Nanfang Hospital Southern Medical University; Shenzhen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21284" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21284</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SpikeSurgSeg, the first spike-driven video Transformer framework for surgical scene segmentation with real-time potential on non-GPU platforms. 2. Introduces a surgical-scene masked autoencoding pretraining strategy for SNNs using layer-wise tube masking to learn robust spatiotemporal representations from limited labeled data. 3. Designs a lightweight spike-driven segmentation head that maintains temporal consistency and the low-latency characteristics of SNNs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67a3926b4197163c38fdd24a63b06867e674a7f4f039c4976e75df11c771f6b6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67a3926b4197163c38fdd24a63b06867e674a7f4f039c4976e75df11c771f6b6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of real-time surgical scene segmentation in resource-constrained environments by proposing SpikeSurgSeg, a spike-driven video Transformer based on Spiking Neural Networks (SNNs). The method uses a masked autoencoding pretraining strategy and a lightweight segmentation head to achieve efficient inference. Experiments show it matches the accuracy of state-of-the-art ANN models while reducing latency by at least 8x and achieving over 20x acceleration compared to foundation models, demonstrating its potential for time-critical surgical applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Post-Processing Mask-Based Table Segmentation for Structural Coordinate Extraction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [document image analysis], [table structure extraction, mask-based segmentation, Gaussian convolution, signal processing, Cell-Aware Segmentation Accuracy (CASA)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Suren Bandara</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Moratuwa</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21287" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21287</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel multi-scale signal-processing method that models row/column transitions as 1D signals and uses Gaussian convolution with increasing variances for robust edge detection from imperfect table masks. 2. Introduces a statistical thresholding technique applied after convolution to suppress noise while preserving stable structural edges, improving boundary localization. 3. Demonstrates robustness to resolution variations through zero-padding and scaling strategies, significantly boosting layout-aware accuracy (CASA) on a standard benchmark.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4ec531bbcda4dfdb5e4b7b3bbe3d643d9a1e7bf829357622807bc23e628f0b58_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4ec531bbcda4dfdb5e4b7b3bbe3d643d9a1e7bf829357622807bc23e628f0b58_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of accurately extracting table row and column boundaries from noisy or low-resolution document images. It proposes a post-processing method that converts table masks into 1D signals, applies multi-scale Gaussian convolution and statistical thresholding to detect edges, and maps peaks back to image coordinates. The approach improves Cell-Aware Segmentation Accuracy from 67% to 76% on PubLayNet-1M, showing robustness to image degradation and producing structured outputs suitable for downstream analysis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] AndroidLens: Long-latency Evaluation with Nested Sub-targets for Android GUI Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [GUI agents, mobile automation, long-latency tasks, evaluation benchmark, Average Task Progress (ATP)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yue Cao, Yingyao Wang, Pi Bu, Jingxuan Xing, Wei Jiang, Zekun Zhu, Junpeng Ma, Sashuai Zhou, Tong Lu, Jun Song, Yu Cheng, Yuning Jiang, Bo Zheng</p>
</li>
<li class="">
<p><strong>institution:</strong> Alibaba Group, Nanjing University, Fudan University, Zhejiang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21302" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21302</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/alibaba/AndroidLens" target="_blank" rel="noopener noreferrer" class="">https://github.com/alibaba/AndroidLens</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces AndroidLens, a challenging benchmark with 571 long-latency tasks across 38 real-world domains in Chinese and English environments. 2. Proposes a static evaluation that preserves real-world anomalies and allows multiple valid paths to reduce bias. 3. Designs a dynamic evaluation with a milestone-based scheme for fine-grained progress measurement via Average Task Progress (ATP).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/662a645c6b32441afe19665c8ae67f9217e2983c70a42ab2e9430700bb7e5305_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/662a645c6b32441afe19665c8ae67f9217e2983c70a42ab2e9430700bb7e5305_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces AndroidLens, a new evaluation framework designed to assess mobile GUI agents on complex, long-latency tasks derived from real-world scenarios. The framework features both static and dynamic evaluation methods, including a novel milestone-based progress metric. The evaluation results show that even state-of-the-art models perform poorly, achieving only a 12.7% success rate, highlighting significant challenges in real-world mobile automation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Does the Data Processing Inequality Reflect Practice? On the Utility of Low-Level Tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [information theory, statistical learning], [data processing inequality, Bayes classifier, low-level processing, classification accuracy, finite sample analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Roy Turgeman, Tom Tirer</p>
</li>
<li class="">
<p><strong>institution:</strong> Bar-Ilan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21315" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21315</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A theoretical proof that for any finite number of training samples, there exists a pre-classification processing that improves classification accuracy, even for a classifier converging to the optimal Bayes classifier. 2. An analysis of how class separation, training set size, and class balance affect the relative gain from low-level pre-processing. 3. Empirical validation of the theory on a synthetic setup and on practical deep classifiers with denoising/encoding tasks on benchmark datasets, showing consistent trends.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59be928ca6eaa4e86b8cd3fb9b4f9e66ff3aed7a604e05c3f63b00a3ca0c56bd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59be928ca6eaa4e86b8cd3fb9b4f9e66ff3aed7a604e05c3f63b00a3ca0c56bd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the practical utility of performing low-level signal processing tasks (like denoising) before classification, which seemingly contradicts the information-theoretic Data Processing Inequality. Through a theoretical binary classification analysis and empirical studies, it demonstrates that for finite training samples, such pre-processing can improve accuracy, with the benefit influenced by dataset characteristics like size and noise level.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [computational pathology], [tile contextualization, transformer, masked modeling, whole slide image, foundation model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Varun Belagali, Saarthak Kapse, Pierre Marza, Srijan Das, Zilinghan Li, Sofiène Boutaj, Pushpak Pati, Srikar Yellapragada, Tarak Nath Nandi, Ravi K Madduri, Joel Saltz, Prateek Prasanna, Stergios Christodoulidis Maria Vakalopoulou, Dimitris Samaras</p>
</li>
<li class="">
<p><strong>institution:</strong> Stony Brook University, MICS/CentraleSupélec/Université Paris-Saclay, UNC Charlotte, Argonne National Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21331" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21331</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://cvlab-stonybrook.github.io/TICON/" target="_blank" rel="noopener noreferrer" class="">https://cvlab-stonybrook.github.io/TICON/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces TICON, a unified transformer-based model to contextualize tile embeddings from any tile-level foundation model for computational pathology. 2. Uses a masked modeling objective during pretraining to simultaneously unify and enrich representations from diverse tile encoders. 3. Demonstrates that TICON-contextualized embeddings achieve state-of-the-art results on multiple tile-level and slide-level benchmarks and enables a highly data-efficient slide-level foundation model.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd2cc5f1b4233e238b13cc9ecf555914fae34743ffd9e8646b2f8300433d9479_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd2cc5f1b4233e238b13cc9ecf555914fae34743ffd9e8646b2f8300433d9479_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of interpreting small image tiles in computational pathology without their larger slide-level context. It proposes TICON, a transformer-based model that contextualizes embeddings from any tile-level foundation model using a masked modeling pretraining objective. The results show that TICON significantly improves performance across various tasks and enables a slide-level foundation model that outperforms prior work while using far less data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Streaming Video Instruction Tuning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video understanding], [streaming video, instruction tuning, real-time assistant, temporal reasoning, multimodal LLM]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiaer Xia, Peixian Chen, Mengdan Zhang, Xing Sun, Kaiyang Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> Hong Kong Baptist University, Tencent Youtu Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21334" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21334</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/maifoundations/Streamo" target="_blank" rel="noopener noreferrer" class="">https://github.com/maifoundations/Streamo</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Streamo, a real-time streaming video LLM for general-purpose interactive assistance across diverse tasks. 2. Constructs Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. 3. Demonstrates strong performance in temporal reasoning and generalization across streaming benchmarks, bridging offline models and real-time assistants.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/683a0c8f318f6ed415484df41515d10ec5ce6b48a1d38eba2ff127258aac07cd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/683a0c8f318f6ed415484df41515d10ec5ce6b48a1d38eba2ff127258aac07cd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Streamo, a real-time streaming video LLM trained on a large-scale instruction dataset (Streamo-Instruct-465K) to perform diverse tasks like narration and question answering on continuous video. The model shows strong temporal reasoning and generalization, bridging the gap between offline video models and real-time multimodal assistants.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Fast SAM2 with Text-Driven Token Pruning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [token pruning, video object segmentation, vision transformer, inference efficiency, text guidance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Avilasha Mandal, Chaoning Zhang, Fachrina Dewi Puspitasari, Xudong Wang, Jiaquan Zhang, Caiyan Qin, Guoqing Wang, Yang Yang, Heng Tao Shen</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China, Indian Institute of Technology Delhi, Harbin Institute of Technology (Shenzhen)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21333" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21333</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A text-guided token pruning framework that reduces token density before temporal propagation in SAM2 without modifying the core architecture. 2. A lightweight routing mechanism for token ranking that integrates local visual context, semantic relevance from text, and uncertainty cues. 3. Demonstrates significant improvements in inference speed (up to 42.50% faster) and memory usage (37.41% lower) while maintaining competitive segmentation performance on video benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/135ec6d87d21a48705862639dd6d546443e4868af716ccfb3932e1ca4a9ba7d9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/135ec6d87d21a48705862639dd6d546443e4868af716ccfb3932e1ca4a9ba7d9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high computational cost of the Segment Anything Model 2 (SAM2) for video segmentation by proposing a text-driven token pruning method. The method selectively removes less informative visual tokens before temporal processing using a routing mechanism that considers visual, textual, and uncertainty information. This achieves substantially faster inference and lower memory usage with minimal impact on segmentation accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Flow Gym</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [optical flow / particle image velocimetry], [flow-field quantification, synthetic data generation, JAX, reinforcement learning environment, benchmarking toolkit]</p>
</li>
<li class="">
<p><strong>authors:</strong> Francesco Banelli, Antonio Terpin, Alan Bonomi, Raffaello D&#x27;Andrea</p>
</li>
<li class="">
<p><strong>institution:</strong> ETH Zürich</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20642" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20642</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/antonioterpin/flowgym" target="_blank" rel="noopener noreferrer" class="">https://github.com/antonioterpin/flowgym</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Flow Gym, a unified toolkit for research and deployment of flow-field quantification methods, inspired by OpenAI Gym. 2. Provides a modular, stateless interface for testing, training, and deploying both learning-based and classical algorithms using a synthetic image generation engine (SynthPix). 3. Offers stable JAX re-implementations and integrations of existing algorithms for standardized benchmarking.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e5173f537ce94701f33cf868d525b0c8117477e440d08e96c43c778b59915a4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e5173f537ce94701f33cf868d525b0c8117477e440d08e96c43c778b59915a4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents Flow Gym, a toolkit designed to standardize the development and evaluation of algorithms for quantifying flow fields from particle images. It provides a unified, modular interface inspired by reinforcement learning environments, enabling easy testing and training of methods using synthetic data. The main outcome is a framework that facilitates reproducible research and benchmarking in flow-field quantification.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [vision-language models], [popularity bias, ordinal regression, multi-modal benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Li-Zhong Szu-Tu, Ting-Lin Wu, Chia-Jui Chang, He Syu, Yu-Lun Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> National Yang Ming Chiao Tung University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21337" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21337</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://sytwu.github.io/BeyondMemo/" target="_blank" rel="noopener noreferrer" class="">https://sytwu.github.io/BeyondMemo/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces YearGuessr, the largest open benchmark dataset for building age estimation with multi-modal attributes (55,546 images, construction years, GPS, page-view counts). 2. Frames construction year prediction as an ordinal regression task and proposes popularity-aware interval accuracy metrics to quantify bias. 3. Benchmarks 30+ models, including YearCLIP, exposing significant popularity bias in VLMs where accuracy is up to 34% higher on famous vs. ordinary buildings.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6c6407a1e5e9ce1a54aebfb3d9bee052114a5e56feb5e9df1910a7031677d3c6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6c6407a1e5e9ce1a54aebfb3d9bee052114a5e56feb5e9df1910a7031677d3c6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper exposes a popularity bias in vision-language models (VLMs), where models perform much better on famous buildings than ordinary ones, indicating reliance on memorization rather than generalizable understanding. To study this, the authors introduce YearGuessr, a large multi-modal benchmark dataset for building age prediction framed as ordinal regression, and propose metrics to measure bias. Their evaluation confirms VLMs excel on popular items but struggle with unrecognized subjects, revealing a critical flaw in reasoning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [video generation, inference acceleration, redundancy elimination, autoregressive framework, feature caching]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haonan Qiu, Shikun Liu, Zijian Zhou, Zhaochong An, Weiming Ren, Zhiheng Liu, Jonas Schult, Sen He, Shoufa Chen, Yuren Cong, Tao Xiang, Ziwei Liu, Juan-Manuel Perez-Rua</p>
</li>
<li class="">
<p><strong>institution:</strong> Meta AI, Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21338" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21338</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="http://haonanqiu.com/projects/HiStream.html" target="_blank" rel="noopener noreferrer" class="">http://haonanqiu.com/projects/HiStream.html</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a three-axis redundancy elimination framework (spatial, temporal, timestep) for efficient high-resolution video generation. 2. Proposes a dual-resolution caching mechanism for spatial compression and a chunk-by-chunk strategy with a fixed-size anchor cache for temporal compression. 3. Demonstrates state-of-the-art visual quality with up to 107.5x faster denoising compared to baselines, making 1080p video generation practical.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfd3105f76a9173c10ae0b43fda34a955ef34708de3f700fa0fc7c67d1f3ab3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfd3105f76a9173c10ae0b43fda34a955ef34708de3f700fa0fc7c67d1f3ab3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the computational bottleneck of high-resolution video generation by proposing HiStream, an autoregressive framework that eliminates redundancy across spatial, temporal, and timestep dimensions. The method achieves significant speedups (up to 107.5x) with negligible quality loss, making high-fidelity 1080p video generation scalable and practical.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Equivariant Multiscale Learned Invertible Reconstruction for Cone Beam CT: From Simulated to Real Data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image reconstruction], [rotational equivariance, multiscale reconstruction, learned invertible primal-dual, quasi-Monte Carlo simulation, cone beam CT]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nikita Moriakov, Efstratios Gavves, Jonathan H. Mason, Carmen Seller-Oria, Jonas Teuwen, Jan-Jakob Sonke</p>
</li>
<li class="">
<p><strong>institution:</strong> Netherlands Cancer Institute, University of Amsterdam, Elekta Limited</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21180" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21180</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed LIRE++, an end-to-end rotationally-equivariant multiscale learned invertible primal-dual scheme for fast and memory-efficient CBCT reconstruction. 2. Developed a fast quasi-Monte Carlo CBCT projection simulator to generate training data. 3. Demonstrated improved performance on both synthetic and real clinical data compared to deep learning baselines and a state-of-the-art proprietary hybrid method.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80fc637f4006372b13c2de48295a289562b18e569dfcb797d3ad0d9157751aa8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80fc637f4006372b13c2de48295a289562b18e569dfcb797d3ad0d9157751aa8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of low image quality in Cone Beam CT (CBCT) by proposing LIRE++, a deep learning-based reconstruction method. LIRE++ incorporates rotational equivariance and multiscale processing for efficiency and was trained using a custom fast simulator. It shows improved reconstruction quality over existing methods on both simulated and real clinical data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2026-01-01T04:50:55.000Z" itemprop="dateModified">Jan 1, 2026</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/cs_CV/20251215-20251221"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251215-20251221 (cs.CV)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/daily/cscv/20251229-20260104"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251229-20260104 (cs.CV)</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-22" class="table-of-contents__link toc-highlight">2025-12-22</a></li><li><a href="#2025-12-23" class="table-of-contents__link toc-highlight">2025-12-23</a></li><li><a href="#2025-12-24" class="table-of-contents__link toc-highlight">2025-12-24</a></li><li><a href="#2025-12-25" class="table-of-contents__link toc-highlight">2025-12-25</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>