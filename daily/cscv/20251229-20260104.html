<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_CV/20251229-20260104" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251229-20260104 (cs.CV) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/cscv/20251229-20260104"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251229-20260104 (cs.CV) | AI头条"><meta data-rh="true" name="description" content="2025-12-29"><meta data-rh="true" property="og:description" content="2025-12-29"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/cscv/20251229-20260104"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cscv/20251229-20260104" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cscv/20251229-20260104" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.CV","item":"https://jokebear666.github.io/ai_toutiao/daily/cscv"},{"@type":"ListItem","position":3,"name":"20251229-20260104 (cs.CV)","item":"https://jokebear666.github.io/ai_toutiao/daily/cscv/20251229-20260104"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.647bd1ff.css">
<script src="/ai_toutiao/assets/js/runtime~main.f73b021c.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.6153aad8.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/arxiv-daily"><span title="Arxiv每日论文" class="linkLabel_WmDU">Arxiv每日论文</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Collapse sidebar category &#x27;cs.CV&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cs_CV/20251215-20251221"><span title="20251215-20251221 (cs.CV)" class="linkLabel_WmDU">20251215-20251221 (cs.CV)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cscv/20251222-20251228"><span title="20251222-20251228 (cs.CV)" class="linkLabel_WmDU">20251222-20251228 (cs.CV)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/cscv/20251229-20260104"><span title="20251229-20260104 (cs.CV)" class="linkLabel_WmDU">20251229-20260104 (cs.CV)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csoh"><span title="cs.OH" class="categoryLinkLabel_W154">cs.OH</span></a><button aria-label="Expand sidebar category &#x27;cs.OH&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/daily/cscv"><span>cs.CV</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251229-20260104 (cs.CV)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251229-20260104 (cs.CV)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-29">2025-12-29<a href="#2025-12-29" class="hash-link" aria-label="Direct link to 2025-12-29" title="Direct link to 2025-12-29" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251229] Understanding Virality: A Rubric based Vision-Language Model Framework for Short-Form Edutainment Evaluation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video understanding], [Vision-Language Models, engagement prediction, multimodal features, YouTube Shorts, regression-based evaluator]</p>
</li>
<li class="">
<p><strong>authors:</strong> Arnav Gupta, Gurekas Singh Sahney, Hardik Rathi, Abhishek Chandwani, Ishaan Gupta, Pratik Narang, Dhruv Kumar</p>
</li>
<li class="">
<p><strong>institution:</strong> Birla Institute of Technology and Science, Pilani; GenimeLabs</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21402" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21402</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A large-scale curated dataset of 11,000 YouTube Shorts videos for edutainment engagement modeling. 2. An unsupervised multimodal framework using VLMs to extract audiovisual features without handcrafted selection. 3. An explainable, regression-based evaluator that predicts engagement with strong correlation and provides feature-level interpretability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bb4f5ae92f21f2e03de0476c0d49f5d4facf563d20f5bbb707cb9ed5960cd88_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bb4f5ae92f21f2e03de0476c0d49f5d4facf563d20f5bbb707cb9ed5960cd88_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of evaluating short-form edutainment videos by moving beyond traditional quality metrics to predict human engagement. The proposed method uses Vision-Language Models to extract unsupervised audiovisual features, clusters them, and trains a regression model to predict engagement. The results show strong correlation with actual engagement, offering a scalable and interpretable evaluation framework.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [tool-use framework, vision-language model, interpretability, data-efficiency, tool bottleneck model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Christina Liu, Alan Q. Wang, Joy Hsu, Jiajun Wu, Ehsan Adeli</p>
</li>
<li class="">
<p><strong>institution:</strong> California Institute of Technology, Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21414" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21414</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/christinaliu2020/tool-bottleneck-framework" target="_blank" rel="noopener noreferrer" class="">https://github.com/christinaliu2020/tool-bottleneck-framework</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed the Tool Bottleneck Framework (TBF) for medical image understanding, which uses a learned Tool Bottleneck Model (TBM) to compose tool outputs instead of text-based composition. 2. Introduced a strategy for the TBM to handle arbitrary VLM tool selections, enabling flexible and robust prediction. 3. Demonstrated that the framework improves performance, interpretability, and data-efficiency, matching or surpassing deep learning classifiers and other tool-use frameworks, especially in data-limited scenarios.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that existing tool-use frameworks for image understanding perform poorly on medical images due to their reliance on text to compose specialized tools. The authors propose the Tool Bottleneck Framework (TBF), which uses a vision-language model to select tools and a learned neural network (the Tool Bottleneck Model) to fuse their outputs. Evaluations on histopathology and dermatology tasks show TBF performs on par with or better than existing methods, offering gains in data-limited settings and more interpretable predictions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Scalable Deep Subspace Clustering Network</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [subspace clustering], [landmark-based approximation, self-expression, spectral clustering, linear complexity, convolutional auto-encoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nairouz Mrabah, Mohamed Bouguessa, Sihem Sami</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Quebec at Montreal</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21434" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21434</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a deep subspace clustering framework (SDSNet) that achieves linear O(n) computational complexity, breaking the traditional O(n^3) bottleneck. 2. Introduces a landmark-based approximation to avoid constructing the full n×n affinity matrix, combined with joint optimization of auto-encoder reconstruction and self-expression objectives. 3. Enables direct spectral clustering on factorized representations, integrating convolutional auto-encoders with subspace-preserving constraints for efficient and accurate clustering.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high computational cost (O(n^3)) of traditional subspace clustering methods. It proposes SDSNet, a scalable deep learning framework that uses landmark approximation and joint optimization to achieve linear O(n) complexity. Experiments show SDSNet maintains clustering quality comparable to state-of-the-art methods while being significantly more efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [multi-annotator segmentation, skin lesion segmentation, dermoscopy, consensus masks, annotator metadata]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kumar Abhishek, Jeremy Kawahara, Ghassan Hamarneh</p>
</li>
<li class="">
<p><strong>institution:</strong> Simon Fraser University, AIP Labs</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21472" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21472</a></p>
</li>
<li class="">
<p><strong>code:</strong> /githubsfu-mial/IMAplusplus</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces IMA++, the largest publicly available multi-annotator skin lesion segmentation dataset with 17,684 masks across 14,967 dermoscopic images. 2. Provides rich annotator metadata (e.g., skill level, tool) enabling research on annotator-specific modeling and metadata analysis. 3. Offers curated data partitions, consensus segmentation masks, and a detailed dataset analysis.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c892cf532aaf3e65ad92c26d2ef6701dc5d629cb5bc4b453893916992cd6089_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c892cf532aaf3e65ad92c26d2ef6701dc5d629cb5bc4b453893916992cd6089_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces IMA++, a large-scale public dataset to address the lack of multi-annotator data for dermoscopic skin lesion segmentation. The dataset contains thousands of images with multiple segmentation masks and annotator metadata, facilitating research into modeling annotator variability. It is presented as the largest publicly available resource of its kind for this medical imaging domain.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [Ground Penetrating Radar (GPR), Multi-modal Chain Feature Fusion (MCFF), Global Attention Mechanism (GAM), DCGAN, transfer learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haotian Lv, Yuhui Zhang, Jiangbo Dai, Hanli Wu, Jiaji Wang, Dawei Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Harbin Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21452" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21452</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a DCGAN-based data augmentation strategy to synthesize high-fidelity GPR images, mitigating data scarcity. 2. Designed a novel Multi-modal Chain and Global Attention Network (MCGA-Net) integrating Multi-modal Chain Feature Fusion (MCFF) and a Global Attention Mechanism (GAM) for enhanced defect representation. 3. Utilized MS COCO transfer learning to fine-tune the backbone network, accelerating convergence and improving model generalization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4debe9b33028e70ed06ab5d1f340e5cb76dcb8d09f7adf0d8195a2422c90668_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4debe9b33028e70ed06ab5d1f340e5cb76dcb8d09f7adf0d8195a2422c90668_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the subjective and inefficient interpretation of Ground Penetrating Radar (GPR) images for road defect detection by proposing a comprehensive framework. The method combines DCGAN-based data augmentation, a novel MCGA-Net architecture with feature fusion and attention mechanisms, and transfer learning. The proposed model achieves high precision, recall, and robustness, establishing a new paradigm for automated GPR-based defect detection.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] CCAD: Compressed Global Feature Conditioned Anomaly Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [anomaly detection], [global feature conditioning, adaptive compression, reconstruction-based anomaly detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiao Jin, Liang Diao, Qixin Xiao, Yifan Hu, Ziqi Zhang, Yuchen Liu, Haisong Gu</p>
</li>
<li class="">
<p><strong>institution:</strong> Columbia University, University of Michigan, University of California at Berkeley, Stevens Institute of Technology, VisionX LLC</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21459" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21459</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/chloeqxq/CCAD" target="_blank" rel="noopener noreferrer" class="">https://github.com/chloeqxq/CCAD</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CCAD, a novel method that synergizes reconstruction-based and representation-based anomaly detection by using compressed global features as a conditioning modality for the reconstruction model. 2. Introduces an adaptive compression mechanism to enhance model generalization and training efficiency. 3. Contributes a reorganized and re-annotated version of the DAGM 2007 dataset to validate the method&#x27;s effectiveness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of anomaly detection in industrial settings with limited anomalous data by proposing CCAD, a method that combines the strengths of reconstruction-based and representation-based approaches. CCAD conditions a reconstruction model on adaptively compressed global features, leading to improved generalization and training efficiency. Experiments show that CCAD outperforms state-of-the-art methods in AUC and achieves faster convergence.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image retrieval], [polyp re-identification, gated progressive fusion, multimodal feature fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Suncheng Xiang, Xiaoyang Wang, Junjie Jiang, Hejia Wang, Dahong Qian</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, Peking University, Shanghai Fifth People&#x27;s Hospital</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21476" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21476</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/JeremyXSC/GPF-Net" target="_blank" rel="noopener noreferrer" class="">https://github.com/JeremyXSC/GPF-Net</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1) Proposes a novel multimodal feature fusion framework named GPF-Net for polyp re-identification. 2) Introduces a gated progressive fusion strategy for layer-wise refinement of semantic information through multi-level feature interactions. 3) Demonstrates state-of-the-art performance on standard benchmarks, showing the benefit of multimodal fusion over unimodal methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75fe09ad1f753b61f2c30ab37c16d0deef5060ca95658846bc6dcaf6bb4c53f9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75fe09ad1f753b61f2c30ab37c16d0deef5060ca95658846bc6dcaf6bb4c53f9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of colonoscopic polyp re-identification, where coarse high-level features harm small object matching. The authors propose GPF-Net, a Gated Progressive Fusion network that selectively fuses multi-level features using gates. Experiments show this multimodal approach outperforms state-of-the-art unimodal ReID models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Generative Multi-Focus Image Fusion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image fusion], [multi-focus image fusion, latent diffusion models, generative restoration, StackMFF, IFControlNet]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xinzhe Xie, Buyu Guo, Bolin Li, Shuangyan He, Yanzhen Gu, Qingyan Jiang, Peiliang Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Donghai Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21495" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21495</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Xinzhe99/StackMFF-Series" target="_blank" rel="noopener noreferrer" class="">https://github.com/Xinzhe99/StackMFF-Series</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel two-stage generative framework (GMFF) for multi-focus image fusion, combining deterministic fusion with generative restoration. 2. Introduces a generative restoration stage using IFControlNet, a latent diffusion model, to reconstruct missing content and eliminate edge artifacts. 3. Demonstrates state-of-the-art performance, particularly in handling complex scenarios where not all scene areas are in focus in any input image.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32d197875e157fe73427be3804a8f00aec57f7112b871c69141003d4a9a92477_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32d197875e157fe73427be3804a8f00aec57f7112b871c69141003d4a9a92477_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes GMFF, a two-stage framework for multi-focus image fusion. It first uses StackMFF V4 for deterministic fusion and then employs a latent diffusion model (IFControlNet) for generative restoration to recover missing details and remove artifacts. Experiments show GMFF achieves state-of-the-art performance, especially for complex multi-focal content.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-view clustering], [incomplete multi-view clustering, missing pattern tree, decision ensemble, knowledge distillation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenyuan Yang, Jie Xu, Hongqing He, Jiangzhang Gan, Xiaofeng Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China, Hainan University, Singapore University of Technology and Design, Guangxi Normal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21510" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21510</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a missing-pattern tree model to group data into decision sets for fully utilizing available multi-view pairs, addressing the pair under-utilization issue. 2. Introduces a multi-view decision ensemble module that uses uncertainty-based weighting to aggregate robust clustering decisions from different sets. 3. Designs an ensemble-to-individual knowledge distillation module to transfer ensemble knowledge to view-specific models, promoting mutual optimization between ensemble and individual modules.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of incomplete multi-view clustering (IMVC) where inconsistent missing patterns hinder performance. It proposes a framework called TreeEIC that groups data by missing pattern, performs clustering within each group, ensembles the results with uncertainty weighting, and uses knowledge distillation to refine individual models. Experiments show TreeEIC achieves state-of-the-art performance and robustness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image forensics], [fixed-threshold evaluation, CNN-ViT hybrid, gated fusion, frequency-domain features, cross-domain detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Ashik Khan, Arafat Alam Jion</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology Kharagpur, Chittagong University of Engineering and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21512" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21512</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a fixed-threshold evaluation protocol for AI-generated image detection to provide deployment-relevant robustness estimates by preventing per-condition threshold retuning. 2. Proposed a lightweight CNN-ViT hybrid model with gated fusion and optional frequency enhancement for cross-domain detection across photos and art. 3. Conducted a systematic analysis revealing a forensic-semantic spectrum, showing CNNs are fragile to compression while ViTs are robust, and that semantic patterns in artistic content are more reliable detection cues.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca89224e09c82fcbaf5bf9db1a5d9fb5df38a1947dd8ae3ddcb0e9c385e126aa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca89224e09c82fcbaf5bf9db1a5d9fb5df38a1947dd8ae3ddcb0e9c385e126aa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of misleading robustness estimates in AI-generated image detection by proposing a fixed-threshold evaluation protocol. The authors develop a hybrid CNN-ViT model with gated fusion and evaluate it across photos and art, finding that ViTs are more robust to post-processing like compression due to semantic pattern recognition, and that detection is fundamentally easier on artistic content than photorealistic images.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] SVBench: Evaluation of Video Generation Models on Social Reasoning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [social reasoning, benchmark, agent-based pipeline, VLM judge, multi-agent interaction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenshuo Peng, Gongxuan Wang, Tianmeng Yang, Chuanhao Li, Xiaojie Xu, Hui He, Kaipeng Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Shanghai AI Laboratory, Peking University, Harbin Institute of Technology, The Hong Kong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21507" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21507</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Gloria2tt/SVBench-Evaluation" target="_blank" rel="noopener noreferrer" class="">https://github.com/Gloria2tt/SVBench-Evaluation</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the first benchmark (SVBench) for evaluating social reasoning in video generation, grounded in developmental and social psychology. 2. Develops a fully training-free, agent-based pipeline to synthesize and evaluate diverse social reasoning scenarios. 3. Conducts a large-scale evaluation of seven state-of-the-art video generation models, revealing their systematic failures in social cognition.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415eaf890354c8803ae31303b38b5679ebed73bd981f8860ef00ba636922568d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415eaf890354c8803ae31303b38b5679ebed73bd981f8860ef00ba636922568d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces SVBench, the first benchmark for evaluating social reasoning in video generation models. It uses an agent-based pipeline to create and assess videos based on psychological paradigms, finding that while current models are visually realistic, they fail at understanding intentions, beliefs, and social norms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [parameter-efficient training, frozen encoders, adapters, LoRA, BitFit]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Ashik Khan, Md Nahid Siddique</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology Kharagpur, Florida International University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21508" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21508</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrated that parameter-efficient training (PET) methods with frozen encoders achieve superior performance (AUROC 0.892-0.908) compared to full fine-tuning (AUROC 0.770) for multimodal chest X-Ray classification using only 2.51% of trainable parameters. 2. Conducted controlled, budget-matched experiments revealing that performance gains stem primarily from efficient parameter allocation rather than cross-modal synergy, as vision-only models outperformed multimodal models under the same parameter budget. 3. Identified and quantified a tractable limitation of PET methods—degraded model calibration (ECE: 0.29-0.34)—and suggested post-hoc calibration as a solution for clinical deployment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3511bf13e05579029e8458f9b63afdf7f5fb3ed3a02050a524e53a4fcb570084_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3511bf13e05579029e8458f9b63afdf7f5fb3ed3a02050a524e53a4fcb570084_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates parameter-efficient training (PET) strategies for multimodal chest X-Ray classification to reduce computational costs. By freezing pre-trained encoders and training only small modules like adapters or LoRA under a fixed parameter budget, the methods significantly outperform full fine-tuning. The main conclusion is that frozen encoder PET provides superior discrimination at a fraction of the cost, though calibration correction is needed for clinical use.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [diffusion models], [GRPO, mode collapse, diversity-aware reward, spectral clustering, structure-aware regularization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Henglin Liu, Huijuan Huang, Jing Wang, Chang Liu, Xiu Li, Xiangyang Ji</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Kuaishou Technology (Kling Team), Sun Yat-sen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21514" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21514</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and analyzes the mode collapse problem in GRPO-based image generation from both reward modeling and generation dynamics perspectives. 2. Proposes a distributional creativity bonus reward based on semantic grouping via spectral clustering to encourage novel visual modes. 3. Introduces a structure-aware regularization that applies stronger constraints during early-stage denoising to preserve diversity without sacrificing quality optimization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862e58c2beed3d5383235af01560a2dc06bc384250083e4027ddff5a3aa32368_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862e58c2beed3d5383235af01560a2dc06bc384250083e4027ddff5a3aa32368_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the mode collapse problem in GRPO-based image generation, where models produce homogenized outputs. The proposed DiverseGRPO method introduces a diversity-aware reward based on semantic clustering and a structure-aware regularization to preserve generation diversity. Experiments show the method significantly improves semantic diversity while maintaining image quality, establishing a better quality-diversity trade-off.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] MuS-Polar3D: A Benchmark Dataset for Computational Polarimetric 3D Imaging under Multi-Scattering Conditions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D reconstruction], [polarization imaging, computational imaging, underwater imaging, benchmark dataset, multi-scattering]</p>
</li>
<li class="">
<p><strong>authors:</strong> Puyun Wang, Kaimin Yu, Huayang He, Xianyu Wu</p>
</li>
<li class="">
<p><strong>institution:</strong> Fuzhou University, Research Institute of Highway, Ministry of Transport</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21513" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21513</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/WangPuyun/MuS-Polar3D" target="_blank" rel="noopener noreferrer" class="">https://github.com/WangPuyun/MuS-Polar3D</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MuS-Polar3D, the first publicly available benchmark dataset for quantitative turbidity underwater polarization-based 3D imaging, featuring 42 objects captured under seven controlled scattering conditions and five viewpoints. 2. Proposes a two-stage computational imaging pipeline that decouples underwater 3D reconstruction into descattering followed by 3D reconstruction. 3. Provides extensive evaluations using multiple baseline methods, demonstrating the dataset&#x27;s effectiveness for fair algorithm comparison under complex scattering conditions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c66fd636f62e6b85894934c19824730a7d5f2125cecd8c2495b556ef0c4c42_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c66fd636f62e6b85894934c19824730a7d5f2125cecd8c2495b556ef0c4c42_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of diverse public datasets for polarization-based underwater 3D imaging by introducing MuS-Polar3D, a benchmark dataset captured under controlled multi-scattering conditions. The authors also propose a two-stage computational imaging pipeline (descattering then 3D reconstruction) for this task. The dataset enables accurate reconstruction and fair evaluation, with experiments achieving a best mean angular error of 15.49 degrees.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-view clustering], [contrastive learning, incomplete multi-view data, noise-robust clustering, graph-guided learning, imputation-free]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hongqing He, Jie Xu, Wenyuan Yang, Yonghua Zhu, Guoqiu Wen, Xiaofeng Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> Guangxi Normal University, University of Electronic Science and Technology of China, Singapore University of Technology and Design, Hainan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21516" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21516</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a global-graph guided contrastive learning strategy to address the rare-paired sample issue in incomplete multi-view data by constructing a global affinity graph to form new sample pairs. 2. Introduces a local-graph weighted contrastive learning mechanism to mitigate the mis-paired sample issue caused by noise, using local neighbors to generate adaptive weights for pair-wise contrast. 3. Integrates these strategies into a unified, imputation-free framework for effective clustering on both incomplete and noisy multi-view data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of rare-paired and mis-paired samples in contrastive learning for multi-view clustering on incomplete and noisy data. It proposes a unified framework combining global-graph guided contrastive learning to explore complementary information and local-graph weighted contrastive learning to adaptively handle noisy pairs. Experiments show the method outperforms state-of-the-art approaches on both incomplete and noisy data settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Hierarchy-Aware Fine-Tuning of Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal learning], [hierarchical classification, vision-language models, efficient fine-tuning, LoRA, Tree-Path KL Divergence]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiayu Li, Rajesh Gangireddy, Samet Akcay, Wei Cheng, Juhua Hu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Washington, Intel</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21529" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21529</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an efficient hierarchy-aware fine-tuning framework for Vision-Language Models (VLMs) that updates only a few parameters. 2. Introduces two novel loss functions: Tree-Path KL Divergence (TP-KL) for vertical consistency along label paths and Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) for horizontal consistency among sibling classes. 3. Demonstrates consistent improvements in Full-Path Accuracy and reduced Tree-based Inconsistency Error across multiple hierarchical benchmarks with minimal parameter overhead.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b60d2fc3dd0ad92b5ac8857f844fed2dbe51e280dfb702c26028d91c14fd92_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b60d2fc3dd0ad92b5ac8857f844fed2dbe51e280dfb702c26028d91c14fd92_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of adapting large Vision-Language Models (VLMs) to hierarchical classification tasks efficiently. The proposed method combines two novel hierarchy-aware loss functions (TP-KL and HiSCE) with lightweight LoRA adaptation to enforce structural consistency in predictions. Experiments show the approach improves accuracy and reduces inconsistency across taxonomy levels with minimal computational cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Vision Transformers are Circulant Attention Learners</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [vision transformers], [circulant attention, block circulant matrix with circulant blocks (BCCB), computational complexity, vision transformers, fast Fourier transform (FFT)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dongchen Han, Tianyu Li, Ziyi Wang, Gao Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21542" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21542</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/LeapLabTHU/Circulant-Attention" target="_blank" rel="noopener noreferrer" class="">https://github.com/LeapLabTHU/Circulant-Attention</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies that self-attention matrices in vision Transformers often approximate Block Circulant matrices with Circulant Blocks (BCCB). 2. Proposes a novel Circulant Attention paradigm that explicitly models the attention map as its nearest BCCB matrix. 3. Introduces an efficient computation algorithm using 2D Discrete Fourier Transform (DFT) to achieve O(N log N) complexity while largely preserving the modeling capacity of standard self-attention.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14a3b6c919de65b8d25da885c52023b90262e94f46956f6aed612d5e62b5f19b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14a3b6c919de65b8d25da885c52023b90262e94f46956f6aed612d5e62b5f19b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the quadratic computational complexity of self-attention in vision Transformers by proposing Circulant Attention. The method models the attention matrix as a Block Circulant matrix with Circulant Blocks (BCCB) and leverages Fast Fourier Transform for efficient O(N log N) computation. Experiments show it effectively reduces computation cost while maintaining model performance, offering a promising alternative to standard self-attention.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Toward Intelligent Scene Augmentation for Context-Aware Object Placement and Sponsor-Logo Integration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image editing], [context-aware object insertion, sponsor-product logo augmentation, vision-language models, diffusion models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Unnati Saraswat, Tarun Rao, Namah Gupta, Shweta Swami, Shikhar Sharma, Prateek Narang, Dhruv Kumar</p>
</li>
<li class="">
<p><strong>institution:</strong> Birla Institute of Technology and Science, Pilani</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21560" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21560</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces two new tasks for advertising: context-aware object insertion and sponsor-product logo augmentation. 2. Builds two new datasets with annotations for category, placement, and sponsor-product labels to support these tasks. 3. Identifies and addresses a research gap in jointly performing category reasoning, placement prediction, object generation, and seamless compositing for scene augmentation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0959c405e8f65b70f4fa16189d8d2817513788b4aa2715490a9136daff2672b7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0959c405e8f65b70f4fa16189d8d2817513788b4aa2715490a9136daff2672b7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces two new tasks for intelligent image editing in advertising: context-aware object insertion and sponsor-product logo augmentation. It proposes to address these tasks using vision-language models and diffusion models, and builds new datasets to support them. The main conclusion is that a significant research gap exists in systems that can jointly reason about context, placement, generation, and compositing for realistic scene augmentation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image inpainting], [object removal, dataset-free, test-time adaptation, multimodal large-language model, background-aware reasoning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sanghyun Jo, Donghwan Lee, Eunji Jung, Seong Je Oh, Kyungsu Kim</p>
</li>
<li class="">
<p><strong>institution:</strong> Seoul National University, OGQ</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21545" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21545</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel dataset-free framework, EraseLoRA, that replaces attention manipulation with background-aware reasoning and test-time adaptation for object removal. 2. Introduces Background-aware Foreground Exclusion (BFE), which uses an MLLM to separate target foreground, non-target foregrounds, and clean background from a single image-mask pair without supervision. 3. Introduces Background-aware Reconstruction with Subtype Aggregation (BRSA), a test-time optimization that treats background subtypes as complementary pieces and enforces their consistent integration through reconstruction and alignment objectives.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f86516075c3f56dc02ca42051c7afa7154583c986a395447b42dd8dc4bf354_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f86516075c3f56dc02ca42051c7afa7154583c986a395447b42dd8dc4bf354_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of object removal in inpainting, where existing dataset-free methods often regenerate unwanted objects or disrupt details. It proposes EraseLoRA, a framework that uses an MLLM to separate image components and performs test-time optimization to reconstruct the background coherently. The method shows consistent improvements over dataset-free baselines and competitive results against dataset-driven approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Exploration of Reproducible Generated Image Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image forensics], [AIGC detection, reproducibility, generalizability, diffusion models, binary classification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yihang Duan</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in the provided content. (Author name only, no affiliation or email domain provided)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21562" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21562</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and analyzes the root causes of poor reproducibility in AIGC image detection research, citing omitted experimental details and overfitting to generator-specific features. 2. Provides empirical evidence for the reproducibility issue by constructing a test dataset and reproducing a representative detection method, demonstrating performance drops under cross-generator testing. 3. Proposes reference directions for the research community to improve reproducibility and generalizability, such as more comprehensive disclosure of experimental details.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c9df3d6873df2ea90e378adf52625583637b76319eb9a55ebf11b8f17abf1fc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c9df3d6873df2ea90e378adf52625583637b76319eb9a55ebf11b8f17abf1fc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the reproducibility and generalizability challenges in AI-Generated Content (AIGC) image detection. By reviewing key literature, building a test dataset, and reproducing a detection method, it identifies causes like omitted experimental details and model overfitting. The study concludes that while basic performance can be reproduced, detection fails when preprocessing disrupts key features or when testing across different generators, highlighting the need for better methodological disclosure and robustness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Towards Long-window Anchoring in Vision-Language Model Distillation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [knowledge distillation, long-context, rotary position embeddings (RoPE), attention mechanism, vision-language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haoyi Zhou, Shuo Li, Tianyu Chen, Qi Song, Chonghan Gao, Jianxin Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Beihang University, Zhongguancun Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21576" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21576</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies the problem of limited effective context windows in small, distilled vision-language models despite using identical positional embeddings and architectures as their larger counterparts. 2. Proposes LAid, a novel distillation method featuring progressive distance-weighted attention matching and learnable RoPE response gain modulation to transfer long-range attention mechanisms. 3. Demonstrates that LAid-distilled models achieve significantly longer effective context windows (up to 3.2x) while maintaining performance on standard benchmarks, and provides spectral analysis showing successful transfer of low-frequency attention components.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4663870a97f8c352e6cd352d0f9f9be365648a5545642796d256fa99c7ddcd4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4663870a97f8c352e6cd352d0f9f9be365648a5545642796d256fa99c7ddcd4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that small, distilled vision-language models have much shorter effective context windows than their large teacher models. The authors propose LAid, a new distillation method that transfers long-range attention capabilities via progressive attention matching and learnable RoPE modulation. Their method successfully extends the context window of small models by up to 3.2 times while preserving performance on standard benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] LLM-Free Image Captioning Evaluation in Reference-Flexible Settings</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image captioning evaluation], [LLM-free evaluation, reference-flexible, supervised metric, image-caption similarity, human-annotated dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shinnosuke Hirano, Yuiga Wada, Kazuki Matsuda, Seitaro Otsuki, Komei Sugiura</p>
</li>
<li class="">
<p><strong>institution:</strong> Keio University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21582" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21582</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Pearl, a novel LLM-free supervised metric for image captioning that works in both reference-based and reference-free settings. 2. Introduces a novel mechanism to learn representations of image-caption and caption-caption similarities. 3. Constructs a large-scale human-annotated dataset for metric evaluation, comprising ~333k judgments from over 2k annotators across 75k+ images.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad4cf942f6b913d144878df5c7f41a2b4ea396044f5286c9aeed1071cfa693a1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad4cf942f6b913d144878df5c7f41a2b4ea396044f5286c9aeed1071cfa693a1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the issues of bias in LLM-based and the limited performance of existing LLM-free metrics for evaluating image captions. It proposes Pearl, a fast, LLM-free supervised metric that learns joint image-caption and caption-caption similarity representations and is applicable to both reference-based and reference-free settings. Pearl outperforms other LLM-free metrics on multiple benchmarks, demonstrating higher alignment with human judgment without the neutrality and speed concerns of LLM-based approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] UltraLBM-UNet: Ultralight Bidirectional Mamba-based Model for Skin Lesion Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [Mamba, state-space model, knowledge distillation, lightweight model, U-Net]</p>
</li>
<li class="">
<p><strong>authors:</strong> Linxuan Fan, Juntao Jiang, Weixuan Liu, Zhucun Xue, Jiajun Lv, Jiangning Zhang, Yong Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in the provided content. Could be inferred from author names but no affiliations/domains are given.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21584" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21584</a></p>
</li>
<li class="">
<p><strong>code:</strong> this https URL</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a lightweight Global–Local Multi-branch Perception Module integrating global context and local features. 2. Demonstrates a bidirectional Mamba with shared weights for enhanced information flow without extra parameters. 3. Introduces a hybrid knowledge distillation strategy to create an ultra-compact, high-performance variant.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089b376dfef7caeeae8830bbdef8bf95a5c6ff909a643481b728550d7ff3d937_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089b376dfef7caeeae8830bbdef8bf95a5c6ff909a643481b728550d7ff3d937_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes UltraLBM-UNet, a lightweight model for skin lesion segmentation that combines a bidirectional Mamba for global modeling with multi-branch local feature perception. It achieves state-of-the-art accuracy on benchmark datasets with minimal parameters and computational cost, and a distilled variant further compresses the model, making it suitable for point-of-care deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal learning], [harmful meme detection, large multimodal model, agent self-improvement, label-free adaptation, contrastive learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jian Lang, Rongpei Hong, Ting Zhong, Leiting Chen, Qiang Gao, Fan Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China, Southwestern University of Finance and Economics</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21598" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21598</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ALARM, the first label-free harmful meme detection framework using LMM agent self-improvement. 2. Introduces a Confidence-based Explicit Meme Identification mechanism to isolate and pseudo-label explicit memes. 3. Introduces a Pairwise Learning Guided Agent Self-Improvement paradigm that uses contrastive pairs to refine the agent&#x27;s ability to handle complex memes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c54d55d69468e93b3a2275f2a3f89c24499aa76dbe87423ea8069fb10f1df9f1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c54d55d69468e93b3a2275f2a3f89c24499aa76dbe87423ea8069fb10f1df9f1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes ALARM, a label-free framework for detecting harmful memes by using a Large Multimodal Model agent that self-improves by learning from easy, explicit examples to handle complex, subtle ones. The method outperforms label-driven approaches on three datasets, demonstrating strong adaptability to evolving online content.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] GaussianEM: Model compositional and conformational heterogeneity using 3D Gaussians</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [cryo-EM image analysis], [3D Gaussians, conformational heterogeneity, two-encoder-one-decoder, pseudo-atomic model, cryo-EM]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bintao He, Yiran Cheng, Hongjia Li, Xiang Gao, Xin Gao, Fa Zhang, Renmin Han</p>
</li>
<li class="">
<p><strong>institution:</strong> Shandong University, Ningxia Medical University, Beijing Institute of Technology, King Abdullah University of Science and Technology (KAUST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21599" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21599</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GaussianEM, a novel Gaussian pseudo-atomic framework for modeling compositional and conformational heterogeneity from cryo-EM images. 2. Introduces a two-encoder-one-decoder architecture to map images to individual Gaussian components and represent variability through Gaussian parameter changes. 3. Bridges the gap between density-based models and atomic models, providing an intuitive and interpretable description of conformational changes while preserving local structural consistency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b95f9f3fcbc3c6429b2ee462e30233f1b388c03440a44273a62612612e5e7244_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b95f9f3fcbc3c6429b2ee462e30233f1b388c03440a44273a62612612e5e7244_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces GaussianEM, a method that uses 3D Gaussians to model both compositional and conformational heterogeneity in proteins from cryo-EM images. It employs a two-encoder-one-decoder architecture to map images to Gaussian components, representing structural changes through parameter variations. The method is shown to be effective on both simulated and experimental datasets, offering an interpretable bridge between density maps and atomic models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data in Emergency and Critical Care</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [imbalanced classification], [XGBoost, TabNet, TabResNet, Bayesian hyperparameter search, class imbalance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yusuf Brima, Marcellin Atemkeng</p>
</li>
<li class="">
<p><strong>institution:</strong> Osnabrück University, Rhodes University, National Institute for Theoretical and Computational Sciences (NITheCS)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21602" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21602</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Systematic evaluation of robustness and scalability for classical ML and deep learning models on imbalanced clinical tabular data. 2. Introduction of TabResNet, a custom lightweight residual network designed as a computationally efficient alternative to TabNet for real-time clinical use. 3. Evidence-based finding that tree-based ensemble methods (especially XGBoost) offer the most stable and scalable performance for high-stakes, time-sensitive clinical environments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper evaluates the robustness and scalability of machine learning models on imbalanced clinical data from emergency and critical care settings. It proposes TabResNet, a lightweight deep learning alternative to TabNet, and compares it against tree-based methods like XGBoost. The main conclusion is that tree-based ensemble models, particularly XGBoost, provide the most stable performance and computational scalability for practical clinical deployment, outperforming more complex deep learning architectures in these environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] TAMEing Long Contexts in Personalization: Towards Training-Free and State-Aware MLLM Personalized Assistant</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [MLLM personalization, long-context, training-free, state-aware, retrieval-augmented generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rongpei Hong, Jian Lang, Ting Zhong, Yong Wang, Fan Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China, Aiwen Technology Co., Ltd.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21616" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21616</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LCMP, the first Long-Context MLLM Personalization benchmark for evaluating personalized dialogues. 2. Introduces TAME, a novel training-free and state-aware framework using double memories to manage concept variations. 3. Presents the RA2G (Retrieve-then-Align Augmented Generation) paradigm to align retrieved knowledge with current queries for better interaction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0916d89499cb78545669c049322f7cc673c2b69f516444a870dd8869bb13521_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0916d89499cb78545669c049322f7cc673c2b69f516444a870dd8869bb13521_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of existing MLLM personalization methods in handling long-context dialogues. It proposes a training-free, state-aware framework called TAME, which uses double memories and a novel retrieval-augmentation paradigm (RA2G) to manage personalized concepts over time. Experiments on the new LCMP benchmark show TAME achieves the best performance, enabling evolving interactions in long-context scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [novel view synthesis], [diffusion models, 3D Gaussian Splatting, auto-regressive restoration, context-aware inpainting, view synthesis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhiyuan Liu, Daocheng Fu, Pinlong Cai, Lening Wang, Ying Liu, Yilong Ren, Botian Shi, Jianqiang Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Shanghai Artificial Intelligence Laboratory, Beihang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21618" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21618</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SymDrive, a unified diffusion-based framework for joint high-quality rendering and scene editing in autonomous driving simulation. 2. Introduces a Symmetric Auto-regressive Online Restoration paradigm for recovering fine-grained details and generating consistent lateral views. 3. Leverages the restoration capability for a training-free harmonization mechanism, treating vehicle insertion as context-aware inpainting to ensure lighting and shadow consistency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> SymDrive addresses the challenge of creating high-fidelity and controllable 3D driving simulators by proposing a unified diffusion-based framework. It uses a symmetric auto-regressive online restoration method to enhance novel-view synthesis and a training-free harmonization mechanism for realistic vehicle insertion. The method achieves state-of-the-art performance in both rendering quality and scene editing realism.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] CausalFSFG: Rethinking Few-Shot Fine-Grained Visual Categorization from Causal Perspective</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [fine-grained visual categorization], [causal intervention, structural causal model, few-shot learning, interventional multi-scale encoder, interventional masked feature reconstruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhiwen Yang, Jinglin Xu, Yuxin Pen</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University, University of Science and Technology Beijing</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21617" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21617</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/PKU-ICST-MIPL/CausalFSFG_TMM" target="_blank" rel="noopener noreferrer" class="">https://github.com/PKU-ICST-MIPL/CausalFSFG_TMM</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a causal perspective to address the confounding effect of support samples in FS-FGVC, framing the task as a causal inference problem. 2. Introduces a novel CausalFSFG method with two key components: an Interventional Multi-Scale Encoder (IMSE) for sample-level intervention and an Interventional Masked Feature Reconstruction (IMFR) module for feature-level intervention. 3. Demonstrates state-of-the-art performance on standard FS-FGVC datasets (CUB-200-2011, Stanford Dogs, Stanford Cars) through extensive experiments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdf9390290fc2d16f99be55ee74ab9d2d794c0c3343cf9ea92ae99e679f1d456_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdf9390290fc2d16f99be55ee74ab9d2d794c0c3343cf9ea92ae99e679f1d456_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies that the support sample set acts as a confounding variable in Few-Shot Fine-Grained Visual Categorization (FS-FGVC), introducing bias and spurious correlations. To address this, the authors propose CausalFSFG, a method that uses causal intervention via a sample-level and a feature-level module to reveal true causal relationships. The approach achieves new state-of-the-art results on multiple benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image editing], [StyleGAN2, CLIP, L1 regularization, latent space manipulation, attribute disentanglement]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mutiara Shabrina, Nova Kurnia Putri, Jefri Satria Ferdiansyah, Sabita Khansa Dewi, Novanto Yudistira</p>
</li>
<li class="">
<p><strong>institution:</strong> Universitas Brawijaya</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21637" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21637</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An empirical analysis identifying that L2-based regularization in the PPE framework leads to dense latent updates and attribute leakage. 2. The introduction of a sparsity-based constraint using L1 regularization (referred to as an ultra-strict layer masking strategy) for latent space manipulation. 3. Demonstration that the proposed approach enforces more focused edits, reducing unintended changes and better preserving facial identity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d3d2b6fa78e88b5a6e668468f53e16660057860c14bd7c81ab11e3e9b86d455_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d3d2b6fa78e88b5a6e668468f53e16660057860c14bd7c81ab11e3e9b86d455_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of attribute entanglement in text-guided image editing. It analyzes the PPE framework and proposes a new method using L1 regularization to enforce sparsity in latent space updates. The results show this approach achieves more controlled edits with less unintended semantic leakage.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal weather modeling], [multimodal foundation model, weather generation, weather understanding, Chain-of-Thought, self-attention]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhiwang Zhou, Yuandong Pu, Xuming He, Yidi Liu, Yixin Chen, Junchao Gong, Xiang Zhuang, Wanghan Xu, Qinglong Cao, Shixiang Tang, Yihao Liu, Wenlong Zhang, Lei Bai</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai AI Laboratory, Tongji University, Shanghai Jiao Tong University, Zhejiang University, University of Science and Technology of China, UCLA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21643" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21643</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Zhouzone/OmniWeather" target="_blank" rel="noopener noreferrer" class="">https://github.com/Zhouzone/OmniWeather</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Omni-Weather, the first unified multimodal foundation model that integrates weather generation and understanding in a single architecture. 2. Introduces a Chain-of-Thought dataset for causal reasoning in weather generation to improve interpretability and perceptual quality. 3. Demonstrates through experiments that generative and understanding tasks in weather can mutually enhance each other, achieving state-of-the-art performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df6816c446631f92d0b9ba66f4d2f4ddda08200edc161b65555c257b6a7b7a85_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df6816c446631f92d0b9ba66f4d2f4ddda08200edc161b65555c257b6a7b7a85_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the gap between weather prediction and interpretation by proposing Omni-Weather, a unified multimodal foundation model that combines generation and understanding using a shared self-attention mechanism and a novel Chain-of-Thought dataset. The model achieves state-of-the-art results in both tasks, showing that they can mutually benefit each other.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D object grounding], [temporal multimodal grounding, LiDAR-image fusion, language-conditioned decoding, UniScene representation, NuPrompt benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Fudan University, Huawei Technologies Ltd.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21641" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21641</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes TrackTeller, a unified temporal multimodal framework for 3D grounding that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning. 2. Introduces a shared UniScene representation aligned with textual semantics to generate language-aware 3D proposals. 3. Demonstrates significant performance improvements on the NuPrompt benchmark, including a 70% relative gain in Average Multi-Object Tracking Accuracy and a 3.15-3.4x reduction in False Alarm Frequency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc60054c622874cc83217afc715702b65eb56126f722620ffb7004f46ebe296d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc60054c622874cc83217afc715702b65eb56126f722620ffb7004f46ebe296d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of grounding natural language references to objects in dynamic 3D driving scenes, which often depend on recent motion or behavior. The authors propose TrackTeller, a framework that fuses LiDAR and camera data with language, builds a unified scene representation, and uses temporal reasoning to refine object identification. Experiments show that TrackTeller significantly outperforms existing baselines in language-grounded tracking accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [deepfake detection], [mechanistic interpretability, sparse autoencoder, forensic manifold analysis, feature selectivity, vision-language model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Subramanyam Sahoo, Jared Junkin</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Berkeley, Johns Hopkins University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21670" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21670</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/SubramanyamSahoo/The-Deepfake-Detective" target="_blank" rel="noopener noreferrer" class="">https://github.com/SubramanyamSahoo/The-Deepfake-Detective</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a novel mechanistic interpretability framework specifically for deepfake detection, combining sparse autoencoder analysis with forensic manifold analysis. 2. Demonstrates that only a small fraction of latent features are actively used in each layer of the model for detection. 3. Shows that geometric properties of the model&#x27;s feature manifold (e.g., intrinsic dimensionality, curvature) vary systematically with different types of deepfake artifacts, linking learned features to specific forensic cues.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the &quot;black box&quot; nature of deepfake detectors by proposing a mechanistic interpretability framework that analyzes a vision-language model&#x27;s internal representations. The method uses sparse autoencoders and a novel forensic manifold analysis to probe how the model&#x27;s features respond to forensic artifacts. The key findings are that detection relies on a sparse set of features and that the geometry of the feature manifold correlates with specific artifact types, providing a step towards more interpretable and robust detectors.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [YOLO-NAS, YOLOv8, perception, autonomous vehicles, custom dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jalal Khan</p>
</li>
<li class="">
<p><strong>institution:</strong> United Arab Emirates University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21673" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21673</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal understanding], [perceptual-level image understanding, multimodal large language models, domain-adaptive pre-training, task-aligned reinforcement learning, unified benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuo Cao, Jiayang Li, Xiaohui Li, Yuandong Pu, Kaiwen Zhu, Yuanting Gao, Siqi Luo, Yi Xin, Qi Qin, Yu Zhou, Xiangyu Chen, Wenlong Zhang, Bin Fu, Yu Qiao, Yihao Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai AI Laboratory, University of Science and Technology of China, Peking University, Shanghai Jiao Tong University, Tsinghua University, Nanjing University, Sun Yat-sen University, Tele-AI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21675</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/thunderbolt215/UniPercept" target="_blank" rel="noopener noreferrer" class="">https://github.com/thunderbolt215/UniPercept</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes UniPercept-Bench, a unified benchmark for perceptual-level image understanding across aesthetics, quality, structure, and texture with hierarchical definitions and large-scale datasets. 2. Develops a strong baseline model, UniPercept, trained via Domain-Adaptive Pre-Training and Task-Aligned Reinforcement Learning for robust generalization. 3. Demonstrates that UniPercept outperforms existing MLLMs on perceptual tasks and can serve as a plug-and-play reward model for text-to-image generation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e93f4678ef30567f8d2c7b2034256f3795d2cf297d9c6c013681b974accd77a7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e93f4678ef30567f8d2c7b2034256f3795d2cf297d9c6c013681b974accd77a7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limited ability of Multimodal Large Language Models (MLLMs) to perceive perceptual-level image features. It introduces UniPercept, a unified framework and benchmark for perceptual-level image understanding across aesthetics, quality, structure, and texture, trained with Domain-Adaptive Pre-Training and Task-Aligned RL. The proposed model outperforms existing MLLMs and can be used as a reward model for image generation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [blockchain provenance, vision-language models, semantic extraction, reproducibility, educational AI]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Motaleb Hossen Manik, Md Zabirul Islam, Ge Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Rensselaer Polytechnic Institute</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21684" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21684</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces SlideChain, a blockchain-backed provenance framework for verifiable integrity in multimodal semantic extraction from educational content. 2. Presents the SlideChain Slides Dataset, a curated corpus of 1,117 medical imaging lecture slides, and conducts the first systematic analysis of semantic disagreement across four state-of-the-art VLMs. 3. Demonstrates practical scalability, perfect tamper detection, and deterministic reproducibility through evaluation of gas usage, throughput, and auditability on a local EVM-compatible blockchain.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369d8533474f19127055b5c4bf7fad048caff38e6ab3e9aca56aa3a6039a79a4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369d8533474f19127055b5c4bf7fad048caff38e6ab3e9aca56aa3a6039a79a4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces SlideChain, a framework that uses blockchain to provide verifiable provenance for semantic outputs from vision-language models (VLMs) on educational slides. It analyzes discrepancies across VLMs and shows that SlideChain ensures tamper-evident auditability and reproducibility for AI-assisted instructional systems. The results indicate it is a practical step toward trustworthy multimodal educational pipelines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [graph neural network, contrastive learning, few-shot learning, cross-domain, structural consistency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuntian Bo, Tao Zhou, Zechao Li, Haofeng Zhang, Ling Shao</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanjing University of Science and Technology, University of Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21683" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21683</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/primebo1/C-Graph" target="_blank" rel="noopener noreferrer" class="">https://github.com/primebo1/C-Graph</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Structural Prior Graph (SPG) layer to capture and transfer target-category node dependencies for global structure modeling. 2. Introduces a Subgraph Matching Decoding (SMD) mechanism that leverages semantic node relations to guide prediction. 3. Designs a Confusion-minimizing Node Contrast (CNC) loss to reduce node ambiguity and subgraph heterogeneity via contrastive learning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9571022ab0a47d7b3509a2620b8081c3b3ecf53a3d4371337ce5b1b8a68a57c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9571022ab0a47d7b3509a2620b8081c3b3ecf53a3d4371337ce5b1b8a68a57c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of cross-domain few-shot medical image segmentation, where existing methods degrade performance by filtering out domain-specific information. The proposed C-Graph framework leverages the structural consistency of medical images, modeling features as graphs with novel SPG layers, SMD decoding, and a CNC loss. It achieves state-of-the-art cross-domain performance while preserving strong source-domain accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Analyzing the Mechanism of Attention Collapse in VGGT from a Dynamics Perspective</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D reconstruction], [attention collapse, degenerate diffusion, token-merging, mean-field PDE, VGGT]</p>
</li>
<li class="">
<p><strong>authors:</strong> Huan Li, Longjun Luo, Yuling Shi, Xiaodong Gu</p>
</li>
<li class="">
<p><strong>institution:</strong> Huazhong University of Science and Technology, Guangdong University of Technology, Shanghai Jiao Tong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21691" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21691</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a rigorous mathematical explanation for the attention collapse phenomenon in VGGT by modeling it as a degenerate diffusion process. 2. Derives a closed-form mean-field partial differential equation that quantitatively predicts the observed rank profile and convergence rate of token features. 3. Explains why the token-merging remedy delays the collapse by slowing the effective diffusion coefficient, offering a principled lens for future scalable 3D-vision transformers.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81a116424b9dbb97e00548c11b9b6393d53b336e80df62d4caf4add021d7d599_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81a116424b9dbb97e00548c11b9b6393d53b336e80df62d4caf4add021d7d599_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes the attention collapse problem in Visual Geometry Grounded Transformers (VGGT) for 3D reconstruction. It models the global self-attention iteration as a degenerate diffusion process, proving that token features converge to a Dirac measure and deriving a mean-field PDE to predict the collapse. The theory explains the empirical observations and shows how token-merging mitigates the issue by reducing the diffusion rate.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] ShinyNeRF: Digitizing Anisotropic Appearance in Neural Radiance Fields</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [neural rendering], [Neural Radiance Fields, anisotropic specular reflections, Anisotropic Spherical Gaussian, von Mises-Fisher distribution, material editing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Albert Barreiro, Roger Marí, Rafael Redondo, Gloria Haro, Carles Bosch</p>
</li>
<li class="">
<p><strong>institution:</strong> Eurecat, Centre Tecnològic de Catalunya; Universitat Pompeu Fabra; Universitat de Vic - UCC</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21692" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21692</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces ShinyNeRF, a novel NeRF framework capable of modeling both isotropic and anisotropic specular reflections. 2. Proposes a method to jointly estimate physical surface properties (normals, tangents, specular concentration, anisotropy) by approximating outgoing radiance with a mixture of isotropic von Mises-Fisher distributions. 3. Achieves state-of-the-art performance in digitizing anisotropic materials and enables interpretable material property editing.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad12a4dd31b4ec9b89dafef4a6d4d851fe0aa09b5e3d8c6f918d085ee2acda50_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad12a4dd31b4ec9b89dafef4a6d4d851fe0aa09b5e3d8c6f918d085ee2acda50_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces ShinyNeRF, a novel Neural Radiance Fields framework designed to accurately model anisotropic specular reflections, such as those on brushed metals, which previous methods struggled with. The method learns an approximation of outgoing radiance using a mixture of isotropic von Mises-Fisher distributions to jointly estimate physical surface properties. Experimental results show it achieves state-of-the-art performance and enables plausible material editing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Prior-AttUNet: Retinal OCT Fluid Segmentation Based on Normal Anatomical Priors and Attention Gating</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [anatomical prior, attention mechanism, variational autoencoder, densely connected blocks, spatial pyramid pooling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Li Yang, Yuting Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Wannan Medical College</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21693" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21693</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hybrid dual-path architecture integrating a generative prior pathway (using a VAE) with a segmentation network to provide multi-scale normative anatomical priors. 2. Introduces an anatomical prior-guided triple-attention mechanism to dynamically modulate feature importance across decoding stages for improved boundary delineation. 3. Embeds densely connected blocks and spatial pyramid pooling modules in the segmentation backbone to enhance contextual feature extraction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85dc59e9646562af5e8c3595b08a9f5064375d7976961a2ab7b04fe287a243d1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85dc59e9646562af5e8c3595b08a9f5064375d7976961a2ab7b04fe287a243d1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Prior-AttUNet, a model for segmenting macular edema fluid in OCT images. It uses a dual-path architecture with generative anatomical priors and a novel triple-attention mechanism to improve accuracy, especially at boundaries, and achieves high Dice scores across multiple imaging devices while maintaining low computational cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] BeHGAN: Bengali Handwritten Word Generation from Plain Text Using Generative Adversarial Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [handwritten text generation], [Generative Adversarial Networks, Handwritten Text Generation, Bengali, Dataset, Pre-processing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md. Rakibul Islam, Md. Kamrozzaman Bhuiyan, Safwan Muntasir, Arifur Rahman Jawad, Most. Sharmin Sultana Samu</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in provided text. Affiliation/domain cannot be reliably inferred.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21694" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21694</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a method for generating Bengali handwritten words from plain text using GANs, addressing a significant research gap. 2. Developed and used a novel, self-collected dataset of Bengali handwriting from approximately 500 diverse individuals. 3. Demonstrated the ability to produce diverse and realistic handwritten outputs through the described approach.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8b36b4afe805283b88409b54815e1bd251762075786246ae741c57cb65c191a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8b36b4afe805283b88409b54815e1bd251762075786246ae741c57cb65c191a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of research on Bengali handwritten text generation by proposing a GAN-based method to generate words from plain text. The authors created a new dataset of Bengali handwriting samples from hundreds of contributors. The work successfully generates diverse handwritten outputs and contributes to advancing research in this area for the Bengali language.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [ai-generated image detection], [Fast Fourier Transform, CLIP, hybrid system, spectral features, semantic features]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md. Zahid Hossain, Most. Sharmin Sultana Samu, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman, Md. Rakibul Islam</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in provided content. Affiliation/email domain not present.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21695" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21695</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FUSE, a novel hybrid detection system that fuses spectral (FFT-based) and semantic (CLIP-based) features into a joint representation. 2. Introduces a progressive two-stage training strategy to effectively learn the fused representation. 3. Demonstrates state-of-the-art robustness and generalization across multiple high-fidelity image generators and diverse benchmarks, particularly on challenging datasets like Chameleon.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9517071d67c92839f8173192ae8096327651e4417f7ed69aefae08dc78c34838_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9517071d67c92839f8173192ae8096327651e4417f7ed69aefae08dc78c34838_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of robustly detecting AI-generated images by proposing FUSE, a hybrid method that combines spectral features from Fast Fourier Transform with semantic features from CLIP&#x27;s vision encoder. The fused features are trained progressively in two stages. The approach achieves strong generalization and state-of-the-art performance across multiple datasets and generators, highlighting the benefit of integrating spectral and semantic cues.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human motion prediction], [Mixture of Experts, Mamba, spatiotemporal dependencies, computational efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zheng Yin, Chengjian Li, Xiangbo Shu, Meiqi Cao, Rui Yan, Jinhui Tang</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanjing University of Science and Technology, Nanjing Forestry University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21707" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21707</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/alanyz106/ST-MoE" target="_blank" rel="noopener noreferrer" class="">https://github.com/alanyz106/ST-MoE</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Spatiotemporal-Untrammelled Mixture of Experts (ST-MoE) model to flexibly capture complex spatio-temporal dependencies in multi-person motion. 2. Introduces four distinct spatiotemporal experts based on bidirectional spatiotemporal Mamba to achieve parameter sharing and efficiency. 3. Demonstrates superior accuracy, a 41.38% reduction in model parameters, and a 3.6x training speedup on benchmark datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfad49ae18e732f3537bb8bddb97086ffd94c3c545ca0a735b6cfe245b14f6f0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfad49ae18e732f3537bb8bddb97086ffd94c3c545ca0a735b6cfe245b14f6f0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of existing multi-person motion prediction methods, which suffer from inflexible spatiotemporal representations and high computational costs. The authors propose the ST-MoE model, which uses a mixture of Mamba-based experts to adaptively capture spatiotemporal patterns. The method achieves higher accuracy while being more parameter-efficient and faster to train than state-of-the-art approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] RAPTOR: Real-Time High-Resolution UAV Video Prediction with Efficient Video Attention</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video prediction], [Efficient Video Attention (EVA), spatiotemporal factorization, real-time inference, on-device AI, training curriculum]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhan Chen, Zile Guo, Enze Zhu, Peirong Zhang, Xiaoxuan Liu, Lei Wang, Yidan Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Aerospace Information Research Institute, Chinese Academy of Sciences; University of Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21710" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21710</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Thelegendzz/RAPTOR" target="_blank" rel="noopener noreferrer" class="">https://github.com/Thelegendzz/RAPTOR</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces RAPTOR, a single-pass video prediction architecture that avoids the latency and error accumulation of iterative models like diffusion., 2. Proposes Efficient Video Attention (EVA), a novel module that factorizes spatiotemporal modeling to achieve linear time and memory complexity, enabling high-resolution (512^2) processing., 3. Presents a 3-stage training curriculum that progressively refines predictions from coarse structure to sharp, temporally coherent details.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca32784d3d86e53d15479ff35e2982c66656147309d47e2856d7c2334de35f97_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca32784d3d86e53d15479ff35e2982c66656147309d47e2856d7c2334de35f97_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the trilemma in video prediction between speed, resolution, and quality by introducing RAPTOR, a model featuring a novel Efficient Video Attention (EVA) module for linear-complexity spatiotemporal modeling. RAPTOR achieves real-time, high-resolution prediction on edge hardware, setting new state-of-the-art results on benchmark datasets and significantly improving UAV navigation success rates.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] AstraNav-World: World Model for Foresight Control and Consistency</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [world models], [world model, diffusion model, embodied navigation, foresight control, vision-language policy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Junjun Hu, Jintao Chen, Haochen Bai, Minghua Luo, Shichao Xie, Ziyi Chen, Fei Liu, Zedong Chu, Xinda Xue, Botao Ren, Xiaolong Wu, Mu Xu, Shanghang Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Amap Alibaba, Peking University (PKU), Tsinghua University (THU)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21714" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21714</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://astra-amap.github.io/AstraNav-World.github.io/" target="_blank" rel="noopener noreferrer" class="">https://astra-amap.github.io/AstraNav-World.github.io/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. 2. Introduces a training paradigm with bidirectional constraints, optimizing for both action-conditioned visual prediction and visual-conditioned trajectory derivation to ensure executability and physical consistency. 3. Demonstrates improved navigation performance and exceptional zero-shot generalization in real-world testing, showing the model captures transferable spatial and dynamic understanding.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15f89f809a2e6654a0c9645caeda1ab33306e62b8276228d05e7467ef63fc5b8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15f89f809a2e6654a0c9645caeda1ab33306e62b8276228d05e7467ef63fc5b8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes AstraNav-World, a unified world model that integrates a diffusion-based video generator with a vision-language policy to jointly predict future visual scenes and plan action sequences. This bidirectional, synchronized approach mitigates cumulative errors from decoupled pipelines. Experiments show it improves navigation accuracy and success rates, and exhibits strong zero-shot generalization to real-world scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [autoregressive video generation, KV caching, sliding window attention, temporal knot, chunk-wise generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Steven Xiao, XIndi Zhang, Dechao Meng, Qi Wang, Peng Zhang, Bang Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tongyi Lab, Alibaba Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21734" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21734</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://humanaigc.github.io/knot_forcing_demo_page/" target="_blank" rel="noopener noreferrer" class="">https://humanaigc.github.io/knot_forcing_demo_page/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A chunk-wise generation strategy with global identity preservation via cached KV states and local temporal modeling using sliding window attention. 2. A temporal knot module that overlaps adjacent chunks and uses image-to-video conditioning to smooth inter-chunk motion transitions. 3. A &quot;running ahead&quot; mechanism that dynamically updates the reference frame&#x27;s temporal coordinate to maintain long-term coherence.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3db0cbfa541f28563a8f65a9f7c0b4cb0b909e1c62fbec822e091b0ad44811c7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3db0cbfa541f28563a8f65a9f7c0b4cb0b909e1c62fbec822e091b0ad44811c7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Knot Forcing, a streaming framework for real-time, infinite portrait animation. It addresses error accumulation and motion discontinuities in autoregressive video diffusion models through chunk-wise generation, a temporal knot module, and a running-ahead mechanism. The method achieves high-fidelity, temporally consistent animation with real-time performance on consumer GPUs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] SyncAnyone: Implicit Disentanglement via Progressive Self-Correction for Lip-Syncing in the wild</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [lip-syncing, diffusion transformer, two-stage learning, inpainting, self-correction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xindi Zhang, Dechao Meng, Steven Xiao, Qi Wang, Peng Zhang, Bang Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tongyi Lab, Alibaba Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21736" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21736</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://humanaigc.github.io/sync_anyone_demo_page/" target="_blank" rel="noopener noreferrer" class="">https://humanaigc.github.io/sync_anyone_demo_page/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel two-stage learning framework (SyncAnyone) for lip-syncing that decouples accurate motion modeling from high-fidelity generation. 2. A Stage 2 mask-free tuning pipeline that uses a self-generated pseudo-dataset to correct artifacts from the initial mask-based training. 3. Demonstrates state-of-the-art performance in visual quality, temporal coherence, and identity preservation for in-the-wild scenarios.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/825b387e09612033170e3bd5c2f7bedd0f32c2a644514ee397fd09b64088653e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/825b387e09612033170e3bd5c2f7bedd0f32c2a644514ee397fd09b64088653e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of generating high-quality, audio-synchronized lip movements in videos without disrupting the spatiotemporal context or background. The proposed method, SyncAnyone, uses a two-stage framework: first training a diffusion-based video transformer for masked mouth inpainting, and then fine-tuning it mask-free on self-generated data to correct artifacts. Experiments show it achieves state-of-the-art results in lip-syncing for challenging, real-world videos.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [continual learning], [entropy scaling, catastrophic forgetting, stability-plasticity dilemma, dynamic feedback, layer-wise control]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hengyi Wu, Zhenyi Wang, Heng Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Maryland, College Park, University of Central Florida</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21743" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21743</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel entropy-aware continual learning method that uses a dynamic feedback mechanism to regulate each layer based on its entropy, addressing underfitting and overfitting. 2. Introduces Self-Adaptive Entropy Scaling, which adaptively adjusts regularization strength per layer to encourage convergence to wider local minima for better generalization. 3. Presents a complementary adaptive training mechanism that modulates layer plasticity based on performance, preserving knowledge in high-performing layers while amplifying updates in underperforming ones.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses catastrophic forgetting in continual learning by proposing a dynamic feedback mechanism that regulates each neural network layer based on its entropy. This entropy-aware method reduces entropy in high-entropy layers to prevent underfitting and increases it in low-entropy layers to prevent overfitting, promoting convergence to wider minima for improved generalization. The approach is general and integrates with existing replay- and regularization-based methods, showing substantial performance gains over state-of-the-art baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [brain-computer interface (BCI)], [EEG, TSception, Adaptive Average Pooling, spatiotemporal features, drowsiness detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gourav Siddhad, Anurag Singh, Rajkumar Saini, Partha Pratim Roy</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology Roorkee, OP Jindal University, Luleå University of Technology, Indian Institute of Technology Dhanbad</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21747" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21747</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a Modified TSception architecture with a five-layer temporal refinement strategy to capture multi-scale brain dynamics. 2. Introduced Adaptive Average Pooling for structural flexibility to handle varying EEG input dimensions and a two-stage fusion mechanism for optimized spatiotemporal feature integration. 3. Demonstrated improved performance stability (reduced confidence interval) on the SEED-VIG dataset and state-of-the-art generalizability on the STEW mental workload dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43d71afc9fcee5064adfe94f1fb1d9f8a1c55ccd8d1c759dcd1b5801b9d23f46_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43d71afc9fcee5064adfe94f1fb1d9f8a1c55ccd8d1c759dcd1b5801b9d23f46_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a Modified TSception deep learning model for robust EEG-based detection of driver drowsiness and mental workload. The key modifications include a multi-layer temporal refinement strategy and Adaptive Average Pooling, which improve the model&#x27;s stability and ability to handle varying input sizes. The model achieves comparable accuracy with significantly better stability on a drowsiness dataset and state-of-the-art results on a mental workload dataset, demonstrating its effectiveness for reliable cognitive state monitoring.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network for Multimodal Liver Tumor Segmentation from Unpaired Datasets</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [Quaternion Neural Networks, Cross-Attention, Unpaired Data, Multimodal Learning, Explainable AI]</p>
</li>
<li class="">
<p><strong>authors:</strong> Arunkumar V, Firos V M, Senthilkumar S, Gangadharan G R</p>
</li>
<li class="">
<p><strong>institution:</strong> Anna University, National Institute of Technology Tiruchirappalli</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21760" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21760</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an Adaptive Quaternion Cross-Fusion (A-QCF) block for bidirectional knowledge transfer between unpaired CT and MRI data streams., 2. Introduces a unified segmentation model (A-QCF-Net) that leverages Quaternion Neural Networks to build a shared feature space from separate datasets., 3. Demonstrates significant performance gains over strong unimodal baselines and validates clinical relevance through explainability analysis.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebec19949c3fad65f84d0acee71e271ec2d8caca288cc4ecf24768e9533dbbe8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebec19949c3fad65f84d0acee71e271ec2d8caca288cc4ecf24768e9533dbbe8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of training multimodal segmentation models with unpaired datasets. It proposes A-QCF-Net, which uses Quaternion Neural Networks and an adaptive cross-fusion block to enable knowledge transfer between separate CT and MRI data. The method significantly outperforms unimodal baselines and provides a viable paradigm for leveraging large, unpaired medical image archives.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3d medical image analysis], [Masked Autoencoder, Swin Transformer, Self-Supervised Learning, 3D Vision Transformer, Structural Priority Loss]</p>
</li>
<li class="">
<p><strong>authors:</strong> Evgeny Alves Limarenko, Anastasiia Studenikina</p>
</li>
<li class="">
<p><strong>institution:</strong> Moscow Institute of Physics and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21769" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21769</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed BertsWin, a hybrid architecture combining full BERT-style token masking with Swin Transformer windows to preserve 3D spatial topology during SSL pre-training. 2. Introduced a structural priority loss function to enhance learning. 3. Demonstrated significant acceleration in semantic convergence (5.8x) and a 15-fold reduction in training epochs to reach SOTA fidelity when combined with the GradientConductor optimizer, without increasing computational FLOPs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the difficulty of applying standard Masked Autoencoders to 3D medical images, which lose spatial context. It proposes BertsWin, a hybrid architecture that maintains a full 3D token grid using Swin Transformer windows and a structural loss. The method achieves much faster convergence and state-of-the-art reconstruction fidelity for 3D CBCT scans without extra computational cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Inference-based GAN Video Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [VAE-GAN, Markov chain, long video generation, temporal consistency, encoder-decoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jingbo Yang, Adrian G. Bors</p>
</li>
<li class="">
<p><strong>institution:</strong> University of York</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21776" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21776</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new video generator, Encoder GAN3 (EncGAN3), which is a VAE-GAN hybrid structure that incorporates inference capabilities into an adversarial-based unconditional video generator. 2. Introduces a novel, memory-efficient approach to generate long videos (hundreds/thousands of frames) by extending the base VAE-GAN model. 3. Leverages a Markov chain framework with a recall mechanism, where each state is a short VAE-GAN generator, to sequentially connect video sub-sequences and ensure temporal continuity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5d6cd59a07e4478b6b21e586a92b15f90e237037b758a29738bf31e46f9843c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5d6cd59a07e4478b6b21e586a92b15f90e237037b758a29738bf31e46f9843c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of generating long, high-quality videos, a task where existing models suffer from quality degradation. The proposed method first introduces a VAE-GAN hybrid video generator (EncGAN3) and then extends it using a Markov chain framework to sequentially generate short video clips, enabling the creation of temporally consistent long videos. The main conclusion is that this approach overcomes the temporal scaling limitation and allows for memory-efficient generation of long video sequences.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video scene segmentation], [vision-language model, multimodal reasoning, context-focus window, confidence score extraction, explainable AI]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nimrod Berman, Adam Botach, Emanuel Ben-Baruch, Shunit Haviv Hakimi, Asaf Gendler, Ilan Naiman, Erez Yosef, Igor Kviatkovsky</p>
</li>
<li class="">
<p><strong>institution:</strong> Ben-Gurion University, Amazon Prime Video, Tel-Aviv University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21778" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21778</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Scene-VLM, the first fine-tuned vision-language model framework for video scene segmentation, enabling multimodal reasoning across visual and textual cues. 2. Proposes a method to extract confidence scores from VLM token-level logits, enabling controllable precision-recall trade-offs. 3. Demonstrates the model can be aligned to generate coherent natural-language rationales for its boundary decisions with minimal supervision.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d88d54ab6607412e3eb29cafcea4e88a9b83dcabc3bd75f5b4eca365e6ada2b5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d88d54ab6607412e3eb29cafcea4e88a9b83dcabc3bd75f5b4eca365e6ada2b5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Scene-VLM, a novel vision-language model framework for segmenting long videos into coherent scenes by jointly processing visual frames, dialogue, and metadata. It predicts boundaries sequentially with a context-focus window and can provide confidence scores and explanations. The method achieves state-of-the-art performance on benchmarks like MovieNet, significantly outperforming previous methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [image captioning], [scientific figure captioning, large-scale dataset, domain-specific training, human evaluation, large language models (LLMs)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ting-Hao K.Huang, Ryan A. Rossi, Sungchul Kim, Tong Yu, Ting-Yao E. Hsu, Ho Yin, C. Lee Giles</p>
</li>
<li class="">
<p><strong>institution:</strong> The Pennsylvania State University, Adobe Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21789" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21789</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Creation and continuous updating of a large-scale, real-world dataset of scientific figure-caption pairs from arXiv papers. 2. Conducting extensive evaluations, both automatic and human, on generated and author-written captions to assess quality. 3. Developing interactive systems and launching annual challenges to advance the field and help scientists write better captions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper reviews the SciCap project&#x27;s first five years, which focused on generating and evaluating captions for scientific figures. The core method involved building a large-scale dataset from arXiv and exploring domain-specific training, similar to models like SciBERT, for captioning. The conclusion outlines key lessons learned and proposes future research directions to address unsolved challenges in the field.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [diffusion models], [Parameter-Efficient Fine-Tuning, Mixture of Low-rank Experts, Instruction-Guided Routing, Multi-Conditional Generation, Diffusion Transformers]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jinqi Xiao, Qing Yan, Liming Jiang, Zichuan Liu, Hao Kang, Shen Sang, Tiancheng Zhi, Jing Liu, Cheng Yang, Xin Lu, Bo Yuan</p>
</li>
<li class="">
<p><strong>institution:</strong> ByteDance Inc., Rutgers University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21788" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21788</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/yanq095/InstructMoLE" target="_blank" rel="noopener noreferrer" class="">https://github.com/yanq095/InstructMoLE</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces InstructMoLE, a framework using an Instruction-Guided Mixture of Low-Rank Experts for multi-conditional image generation., 2. Proposes Instruction-Guided Routing (IGR), a global routing signal derived from user instructions to select a coherent expert council for all tokens, addressing spatial fragmentation and semantic drift., 3. Introduces an output-space orthogonality loss to promote expert functional diversity and prevent representational collapse.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11bd8639cb632e86b35367843cf453bbc6a02fb6882f88ff7441c78b42b653ce_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11bd8639cb632e86b35367843cf453bbc6a02fb6882f88ff7441c78b42b653ce_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of task interference in multi-conditional image generation when using monolithic PEFT adapters like LoRA. It proposes InstructMoLE, a novel framework that uses global instruction-guided routing to select a consistent mixture of low-rank experts, combined with an orthogonality loss for diversity, which outperforms existing methods on benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] AI for Mycetoma Diagnosis in Histopathological Images: The MICCAI 2024 Challenge</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [histopathological images, image segmentation, grain detection, deep learning, mycetoma classification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hyam Omar Ali, Sahar Alhesseen, Lamis Elkhair, Adrian Galdran, Ming Feng, Zhixiang Xiong, Zengming Lin, Kele Xu, Liang Hu, Benjamin Keel, Oliver Mills, James Battye, Akshay Kumar, Asra Aslam, Prasad Dutande, Ujjwal Baid, Bhakti Baheti, Suhas Gajre, Aravind Shrenivas Murali, Eung-Joo Lee, Ahmed Fahal, Rachid Jennane</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Khartoum, Orleans University, Tongji University, National University of Defense Technology, University of Leeds</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21792" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21792</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Organized the mAIcetoma challenge to advance AI solutions for mycetoma diagnosis. 2. Provided the standardized Mycetoma database (MyData) for model development. 3. Demonstrated the effectiveness of automated deep learning models for segmenting grains and classifying mycetoma types from histopathological images.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2608e467b615afdd887a03b7ca003d5d2989f1640e4dc84304a9ee0f5a6e979_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2608e467b615afdd887a03b7ca003d5d2989f1640e4dc84304a9ee0f5a6e979_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents the mAIcetoma challenge, which aimed to develop AI models for automating the diagnosis of mycetoma by segmenting grains and classifying disease types from histopathological images. Multiple teams proposed various deep learning architectures, which were evaluated on a provided standardized dataset. The results showed that the models achieved high segmentation accuracy and significant performance in classification, highlighting the potential of AI to assist in diagnosing this neglected tropical disease.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Diffusion Posterior Sampling for Super-Resolution under Gaussian Measurement Noise</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image super-resolution], [diffusion posterior sampling, single-image super-resolution, inverse problems, measurement consistency, unconditional diffusion prior]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abu Hanif Muhammad Syarubany</p>
</li>
<li class="">
<p><strong>institution:</strong> Korea Advanced Institute of Science &amp; Technology (KAIST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21797" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21797</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Implementation and evaluation of diffusion posterior sampling (DPS) for single-image super-resolution under a known degradation model. 2. An ablation study analyzing the impact of guidance scale and noise level on reconstruction quality, identifying an optimal configuration. 3. Demonstration that balancing the diffusion prior and measurement-gradient strength enables stable, high-quality reconstructions without model retraining.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc3596dd62ab28bf4cee2aec80e4147a24811ace57a7d9d76802c0ea0767bbdd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc3596dd62ab28bf4cee2aec80e4147a24811ace57a7d9d76802c0ea0767bbdd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the use of diffusion posterior sampling (DPS) for single-image super-resolution under Gaussian noise. The method combines an unconditional diffusion prior with a likelihood-guided update to enforce measurement consistency during sampling. The main finding is that moderate guidance, at a specific scale and noise level, yields the best reconstruction quality, highlighting the importance of balancing prior and data fidelity for stable results.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] CellMamba: Adaptive Mamba for Accurate and Efficient Cell Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [Mamba, Triple-Mapping Adaptive Coupling (TMAC), Adaptive Mamba Head, biomedical instance detection, VSSD backbone]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ruochen Liu, Yi Tian, Jiahao Wang, Hongbin Liu, Xianxu Hou, Jingxin Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Liverpool, National University of Singapore, Xi&#x27;an Jiaotong-Liverpool University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21803" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21803</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Triple-Mapping Adaptive Coupling (TMAC) module that splits channels into parallel branches with dual idiosyncratic and one consensus attention map for enhanced spatial discriminability. 2. Designs an Adaptive Mamba Head that fuses multi-scale features via learnable weights to handle varying object sizes robustly. 3. Introduces CellMamba, a lightweight one-stage detector built on a VSSD backbone with CellMamba Blocks, achieving superior accuracy and efficiency on biomedical cell detection datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3fe9b8372a27eedc5a8e2e1150bcdf6cf461c195f9ed154d8890662de191da_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3fe9b8372a27eedc5a8e2e1150bcdf6cf461c195f9ed154d8890662de191da_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes CellMamba, a lightweight one-stage detector for cell detection in pathological images. It introduces a novel Triple-Mapping Adaptive Coupling (TMAC) module and an Adaptive Mamba Head to improve spatial discriminability and multi-scale feature fusion. Experiments show CellMamba outperforms CNN, Transformer, and Mamba baselines in accuracy while being more efficient in model size and inference speed.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] S&amp;P 500 Stock&#x27;s Movement Prediction using CNN</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [financial time series forecasting], [Convolutional Neural Network (CNN), multivariate raw data, stock movement prediction, historical data matrices, S&amp;P 500]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rahul Gupta</p>
</li>
<li class="">
<p><strong>institution:</strong> None (No affiliation or email domain provided in the given content)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21804" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21804</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the application of Convolutional Neural Networks (CNNs), typically used for image classification, to the problem of stock movement prediction by treating multivariate historical stock data as image-like matrices. 2. Utilizes raw, unprocessed market data including events like stock splits and dividends, instead of relying on pre-engineered financial features. 3. Demonstrates a flexible prediction framework that can be applied at different levels: individual stocks, sectors, or entire portfolios.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d6197655292ac8e55d8c7606c8c3cfe730f7f8dad4004b93d4a9b3a8d8f457_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d6197655292ac8e55d8c7606c8c3cfe730f7f8dad4004b93d4a9b3a8d8f457_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the problem of predicting stock price movements for the S&amp;P 500 index. The core method involves using a Convolutional Neural Network (CNN) to analyze multivariate historical stock data, which is structured as image-like matrices, without extensive feature engineering. The approach shows promising results and offers a flexible model for stock, sector, or portfolio-level predictions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [adversarial attacks], [entropy-guided attacks, vision-language models, adversarial robustness, harmful content generation, transferability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mengqi He, Xinyu Tian, Xin Shen, Jinhong Ni, Shu Zou, Zhaoyuan Yang, Jing Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Australian National University, The University of Queensland, GE Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21815" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21815</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies that only a small fraction (~20%) of high-entropy tokens (critical decision points) disproportionately govern output trajectories in autoregressive VLMs, challenging the prior assumption of equal token importance. 2. Proposes a selective attack strategy that concentrates adversarial perturbations on these high-entropy positions, achieving strong semantic degradation with a smaller perturbation budget and inducing 35-49% of benign outputs to become harmful. 3. Demonstrates that vulnerable high-entropy forks recur across diverse VLMs, enabling feasible transferable attacks (17-26% harmful rates), and proposes the Entropy-bank Guided Adversarial attack (EGA) method which achieves high attack success rates (93-95%) while exposing new safety weaknesses.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper reveals that adversarial attacks on Vision-Language Models (VLMs) can be made more efficient and dangerous by targeting only the critical high-entropy tokens during autoregressive generation, rather than all tokens. The proposed Entropy-bank Guided Adversarial attack (EGA) concentrates perturbations on these positions, achieving high attack success and converting many benign outputs into harmful ones, while also showing significant transferability across models. This exposes a critical and previously overlooked vulnerability in current VLM safety mechanisms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [autonomous driving perception], [multimodal fusion, multi-view cooperative perception, spatiotemporal modeling, V2X communication, deformable attention]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhenwei Yang, Yibo Ai, Weidong Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology Beijing, National Center for Materials Service Safety</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21831" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21831</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes XET-V2X, an end-to-end tracking framework that unifies multi-view multimodal sensing within a shared spatiotemporal representation for V2X collaboration. 2. Introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention to efficiently align heterogeneous viewpoints and modalities. 3. Demonstrates robust performance on real-world and simulated V2X datasets, showing consistent improvements in detection and tracking under varying communication delays.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2257ca524cd2e350a5c2abf7059415ea35045bbf6cb45991c73d32410116cb9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2257ca524cd2e350a5c2abf7059415ea35045bbf6cb45991c73d32410116cb9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of reliable 3D perception in autonomous driving under occlusions and communication delays by proposing XET-V2X, an end-to-end framework that fuses multi-view images and point clouds using a novel dual-layer spatial cross-attention module. The method achieves effective cross-modal interaction and reduces computational overhead. Experiments show it delivers robust and temporally stable detection and tracking performance in complex V2X scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Scalable Class-Incremental Learning Based on Parametric Neural Collapse</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [class-incremental learning], [parametric neural collapse, equiangular tight frame, knowledge distillation, adaptive expansion, feature alignment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chuangxin Zhang, Guangfeng Lin, Enhui Zhao, Kaiyang Liao, Yajun Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in the provided content. Affiliation information is not included.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21845" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21845</a></p>
</li>
<li class="">
<p><strong>code:</strong> this https URLETF2</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a scalable class-incremental learning method (SCL-PNC) based on parametric neural collapse, enabling demand-driven, minimal-cost backbone expansion via an adapt-layer. 2. Introduces a dynamic parametric Equiangular Tight Frame (ETF) classifier to address class misalignment caused by evolving class distributions. 3. Presents a parallel expansion framework with a knowledge distillation algorithm to counteract feature drift and ensure feature consistency across expansion modules.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses challenges in class-incremental learning, such as overfitting and catastrophic forgetting, by proposing SCL-PNC. This method uses a parametric neural collapse framework with an adaptive backbone expansion and a dynamic ETF classifier to efficiently handle growing categories while maintaining feature consistency via distillation. Experiments show the method is effective and efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Breaking Alignment Barriers: TPS-Driven Semantic Correlation Learning for Alignment-Free RGB-T Salient Object Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [salient object detection], [RGB-T, unaligned images, Thin-Plate Spline, MobileViT, Mamba]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lupiao Hu, Fasheng Wang, Fangmei Chen, Fuming Sun, Haojie Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Dalian Minzu University, Shandong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21856" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21856</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/HTUTU2/TPS-SCL" target="_blank" rel="noopener noreferrer" class="">https://github.com/HTUTU2/TPS-SCL</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Thin-Plate Spline Alignment Module (TPSAM) to mitigate spatial discrepancies between unaligned RGB-T modalities, addressing local deformations better than homography estimation. 2. Designs a Semantic Correlation Constraint Module (SCCM) to hierarchically constrain salient features and suppress background interference during alignment. 3. Introduces a Cross-Modal Correlation Module (CMCM) to fully explore and integrate inter-modal dependencies, enhancing detection performance in a lightweight architecture using MobileViT and Mamba.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcfc4f61262c973564dfeb3e9591537a78f5165f01a7fece4325e5961bd3fe89_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcfc4f61262c973564dfeb3e9591537a78f5165f01a7fece4325e5961bd3fe89_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the performance degradation of RGB-T salient object detection (SOD) methods on real-world, unaligned image pairs. It proposes TPS-SCL, a lightweight network that uses a Thin-Plate Spline module for alignment, semantic correlation constraints, and cross-modal fusion. Experiments show the method achieves state-of-the-art performance among lightweight SOD methods and outperforms mainstream RGB-T SOD approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image embedding], [conditional image embedding, large vision-language model, training-free, image similarity, hidden state]</p>
</li>
<li class="">
<p><strong>authors:</strong> Masayuki Kawarada, Kosuke Yamada, Antonio Tejero-de-Pablos, Naoto Inoue</p>
</li>
<li class="">
<p><strong>institution:</strong> CyberAgent</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21860" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21860</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/CyberAgentAILab/DIOR_conditional_image_embeddings" target="_blank" rel="noopener noreferrer" class="">https://github.com/CyberAgentAILab/DIOR_conditional_image_embeddings</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DIOR, a novel training-free framework for generating conditional image embeddings using Large Vision-Language Models (LVLMs)., 2. Introduces a method that prompts an LVLM to describe an image with a single word based on a given condition and uses the last token&#x27;s hidden state as the embedding., 3. Demonstrates superior performance over existing training-free baselines (e.g., CLIP) and even methods requiring additional training on conditional similarity tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dc3046fafcb5e9e27821f45ec139d58a5fbb5098fe33ea6a51f89167ca9df74_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dc3046fafcb5e9e27821f45ec139d58a5fbb5098fe33ea6a51f89167ca9df74_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of generating image embeddings that focus on specific textual conditions, which standard models like CLIP cannot do. It proposes DIOR, a training-free method that uses a Large Vision-Language Model to produce a one-word description based on a condition and extracts its hidden state as the conditional embedding. Experiments show DIOR outperforms both training-free and training-required baselines on conditional image similarity tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Fast Inference of Visual Autoregressive Model with Adjacency-Adaptive Dynamical Draft Trees</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [speculative decoding, draft tree, inference acceleration, autoregressive image generation, dynamic tree structure]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haodong Lei, Hongsong Wang, Xin Geng, Liang Wang, Pan Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> Southeast University, Institute of Automation Chinese Academy of Sciences, Singapore Management University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21857" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21857</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Haodong-Lei-Ray/ADT-Tree" target="_blank" rel="noopener noreferrer" class="">https://github.com/Haodong-Lei-Ray/ADT-Tree</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identified the key obstacle of applying speculative decoding to visual AR models: inconsistent acceptance rates across draft trees due to spatially varying token prediction difficulty. 2. Proposed ADT-Tree, an adjacency-adaptive dynamic draft tree that dynamically adjusts tree depth and width based on adjacent token states and prior acceptance rates. 3. Demonstrated significant speedups (over 3x) on benchmarks and seamless integration with relaxed sampling methods for further acceleration.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b9ff13149fa2d6733868b2125e7af2ae06239a529152a057b80d0d6f357ccf3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b9ff13149fa2d6733868b2125e7af2ae06239a529152a057b80d0d6f357ccf3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the slow inference of visual autoregressive models by proposing ADT-Tree, a dynamic draft tree method that adapts its structure to image region complexity. It achieves over 3x speedup on standard benchmarks and can be combined with other sampling techniques for additional gains.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [feature-level fusion, convolutional neural networks, diabetic retinopathy screening, EfficientNet, DenseNet]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan</p>
</li>
<li class="">
<p><strong>institution:</strong> North South University, Korea Institute of Oriental Medicine, American International University–Bangladesh</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21861" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21861</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] EasyOmnimatte: Taming Pretrained Inpainting Diffusion Models for End-to-End Video Layered Decomposition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video matting], [video omnimatte, diffusion models, LoRA, DiT blocks, dual-expert]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yihan Hu, Xuelin Chen, Xiaodong Cun</p>
</li>
<li class="">
<p><strong>institution:</strong> Great Bay University, Adobe Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21865" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21865</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/GVCLab/EasyOmnimatte" target="_blank" rel="noopener noreferrer" class="">https://github.com/GVCLab/EasyOmnimatte</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the first unified, end-to-end video omnimatte method by finetuning a pretrained video inpainting diffusion model. 2. Introduces a Dual-Expert strategy (Effect Expert and Quality Expert) that selectively applies LoRA to specific DiT blocks to capture foreground-associated effects and refine alpha mattes. 3. Achieves state-of-the-art performance in quality and efficiency by using experts at different denoising steps, eliminating the need for two full diffusion passes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/200afe63944dc4d9793ba3dadc45f6ca120880dcbf65f86982757bb158120f60_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/200afe63944dc4d9793ba3dadc45f6ca120880dcbf65f86982757bb158120f60_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces EasyOmnimatte, a method for video layered decomposition that finetunes a pretrained video inpainting diffusion model with a novel Dual-Expert strategy to efficiently produce high-quality alpha mattes with associated effects. It significantly outperforms existing methods in both speed and quality, enabling various downstream video editing tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [autoregressive image generation, dynamic tokenization, next-token prediction entropy, patch merging, training efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Divyansh Srivastava, Akshay Mehra, Pranav Maneriker, Debopam Sanyal, Vishnu Raj, Vijay Kamarshi, Fan Du, Joshua Kimball</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, San Diego, Dolby Laboratories</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21867" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21867</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces DPAR, a novel decoder-only autoregressive model that dynamically aggregates image tokens into variable-sized patches for efficient generation, using next-token prediction entropy as a merging criterion. 2. Demonstrates that training with dynamic patches yields patch-boundary-robust representations, enabling inference with larger patches for further efficiency. 3. Achieves significant reductions in token count (up to 2.06x) and training FLOPs (up to 40%) while improving image quality (FID) by up to 27.1% compared to fixed-tokenization baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dac1942675af4bd3638ebec23734e3fed0239b066b673633b094708fa3d2d26f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dac1942675af4bd3638ebec23734e3fed0239b066b673633b094708fa3d2d26f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the computational inefficiency of fixed tokenization in autoregressive image generation, where token count grows quadratically with resolution. It proposes DPAR, a method that dynamically merges image tokens into larger patches based on the information content predicted by a lightweight model&#x27;s entropy, allocating more compute to complex regions. This approach reduces training costs by up to 40% and improves generated image quality (FID) by up to 27.1% compared to standard models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [visual localization], [early-fusion, sparse mask attention, pose tokenizer, VGGT backbone, multi-view geometry]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tianchen Deng, Wenhua Wu, Kunzhen Wu, Guangming Wang, Siting Zhu, Shenghai Yuan, Xun Chen, Guole Shen, Zhe Liu, Hesheng Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, Nanyang Technological University, Cambridge University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21883" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21883</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/dtc111111/Reloc-VGGT" target="_blank" rel="noopener noreferrer" class="">https://github.com/dtc111111/Reloc-VGGT</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the first visual localization framework with an early-fusion mechanism for multi-view spatial integration. 2. Proposes a novel sparse mask attention strategy to reduce computational complexity and enable real-time performance. 3. Presents a pose tokenizer and projection module to better exploit spatial relationships from multiple database views.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a9b7693fa823e25dd55dc558d5c57f7d31f7adebc941d9ff45f7ef2ad7c6508_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a9b7693fa823e25dd55dc558d5c57f7d31f7adebc941d9ff45f7ef2ad7c6508_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Reloc-VGGT, a visual re-localization framework that uses an early-fusion mechanism and a VGGT backbone to integrate multi-view 3D geometry for robust pose estimation. It introduces a sparse mask attention strategy for efficiency and is trained on millions of image pairs. The method achieves strong accuracy, generalization, and real-time performance across diverse datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [fMRI, foundation model, data-efficient, training-efficient, hierarchical encoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mo Wang, Junfeng Xia, Wenhao Ye, Enyu Liu, Kaining Peng, Jianfeng Feng, Quanying Liu, Hongkai Wen</p>
</li>
<li class="">
<p><strong>institution:</strong> Southern University of Science and Technology, University of Warwick, Fudan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21881" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21881</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SLIM-Brain, a novel atlas-free foundation model for fMRI analysis that addresses the dual bottlenecks of data- and training-efficiency. 2. Introduces a two-stage adaptive design featuring a lightweight temporal extractor for saliency ranking and a 4D hierarchical encoder (Hiera-JEPA) that learns from selected windows and uses aggressive masking. 3. Demonstrates state-of-the-art performance across seven benchmarks while requiring significantly less pre-training data (4k sessions) and GPU memory (~30% of traditional methods).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03335d41a7bea8c473f46251d91847fc35908f4f15a206682290034deaf6ef92_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03335d41a7bea8c473f46251d91847fc35908f4f15a206682290034deaf6ef92_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the data- and training-efficiency bottlenecks in fMRI foundation models. It proposes SLIM-Brain, a two-stage model that uses a temporal extractor to select salient data windows and a hierarchical encoder to learn from them efficiently. The method achieves state-of-the-art results on multiple tasks using far less pre-training data and computational resources than traditional approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] CrownGen: Patient-customized Crown Generation via Point Diffusion Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D shape generation], [diffusion model, point cloud, dental crown, generative framework, boundary prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Juyoung Bae, Moo Hyun Son, Jiale Peng, Wanting Qu, Wener Chen, Zelin Qiu, Kaixin Li, Xiaojuan Chen, Yifan Lin, Hao Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Hong Kong University of Science and Technology, University of Hong Kong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21890" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21890</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel generative framework (CrownGen) that automates patient-customized dental crown design. 2. A method using a denoising diffusion model on a tooth-level point cloud representation with a boundary prediction module for spatial priors. 3. Validation through a large-scale quantitative benchmark and a clinical study showing the generated crowns are non-inferior to manual designs and reduce design time.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2baf8cabd0677f151bf313fb7831de809fe9ecc0a70533ba9d2cffabe95b064b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2baf8cabd0677f151bf313fb7831de809fe9ecc0a70533ba9d2cffabe95b064b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents CrownGen, a framework that automates dental crown design using a point diffusion model. It uses a boundary prediction module and a generative module to create patient-customized crowns in a single pass. Clinical validation shows the generated crowns match manual quality while significantly reducing design time.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] High-Fidelity and Long-Duration Human Image Animation with Diffusion Transformer</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human image animation], [diffusion transformer, hybrid implicit guidance, position shift adaptive module, skeleton alignment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shen Zheng, Jiaran Cai, Yuansheng Guan, Shenneng Huang, Xingpei Ma, Junjie Cao, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang, Xiao-Ping Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Guangzhou Quwan Network Technology, Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21905" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21905</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Designed a set of hybrid implicit guidance signals and a sharpness guidance factor to incorporate detailed facial and hand features for high-fidelity generation. 2. Proposed a Position Shift Adaptive Module (time-aware position shift fusion) to enable video generation of arbitrary length. 3. Introduced a novel data augmentation strategy and a skeleton alignment model to mitigate the impact of human shape variations across identities.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dcd7ed4538aa6140dc40b848f4f018648b9407793f582523c563f79c97253f1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dcd7ed4538aa6140dc40b848f4f018648b9407793f582523c563f79c97253f1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of generating high-fidelity and long-duration human animation videos by proposing a Diffusion Transformer (DiT)-based framework. The method introduces novel guidance mechanisms for facial/hand details, a module for arbitrary-length generation, and techniques to handle identity shape variations. Experimental results show it outperforms existing state-of-the-art approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal action recognition], [human-centric graph representation learning, attention-based post calibration, spatiotemporal graph, multimodal fusion, skeleton-guided sampling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zeyu Liang, Hailun Xia, Naichuan Zheng</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing University of Posts and Telecommunications</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21916" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21916</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes PAN, the first human-centric graph representation learning framework for multimodal action recognition, which models RGB patches containing human joints as spatiotemporal graphs to align with skeleton data and suppress redundancy. 2. Introduces an attention-based post calibration module to reduce the framework&#x27;s dependency on high-quality skeletal data with minimal performance cost. 3. Presents two model variants, PAN-Ensemble (dual-path with late fusion) and PAN-Unified (single-network unified learning), both achieving state-of-the-art performance on three benchmark datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ada463935a334d688b0ec740d70f3f4578fc9000b1c19263667ad6c53ea2b18_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ada463935a334d688b0ec740d70f3f4578fc9000b1c19263667ad6c53ea2b18_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of effectively fusing heterogeneous RGB and skeleton data for action recognition by proposing PAN, a human-centric graph learning framework. PAN represents RGB patches containing human joints as spatiotemporal graphs, enabling semantically coherent multimodal fusion and reducing RGB redundancy. The proposed method achieves state-of-the-art performance on three datasets through its two variants, PAN-Ensemble and PAN-Unified.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [unsupervised anomaly detection, disentangled representation, pseudo-healthy image reconstruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tao Yang, Xiuying Wang, Hao Liu, Guanzhong Gong, Lian-Ming Wu, Yu-Ping Wang, Lisheng Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21924" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21924</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a disentangled representation module to decouple brain MRI into imaging-invariant anatomical information and imaging-specific information, improving model generalizability across multi-modality and multi-center data. 2. Designed an edge-to-image restoration module that reconstructs high-quality pseudo-healthy images from edge information, suppressing the propagation of abnormal residuals. 3. Introduced brain anatomical priors and a differentiable one-hot encoding operator to constrain and stabilize the disentanglement learning process.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/785ff0154b41353c2fdfb9a61f66c2f97de1b49c0e9dbba451d335e3a8460e71_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/785ff0154b41353c2fdfb9a61f66c2f97de1b49c0e9dbba451d335e3a8460e71_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of generalizability and performance in unsupervised anomaly detection for brain MRI. It proposes a new framework that disentangles anatomical from imaging information and reconstructs pseudo-healthy images from edges, achieving state-of-the-art results on multi-center datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] AutoPP: Towards Automated Product Poster Generation and Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image generation], [product poster generation, click-through rate optimization, isolated direct preference optimization, AutoPP1M dataset, unified design module]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiahao Fan, Yuxin Qin, Wei Feng, Yanyin Chen, Yaoyu Li, Ao Ma, Yixiu Li, Li Zhuang, Haoyi Bian, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law</p>
</li>
<li class="">
<p><strong>institution:</strong> JD.COM</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21921" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21921</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/JD-GenX/AutoPP" target="_blank" rel="noopener noreferrer" class="">https://github.com/JD-GenX/AutoPP</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An automated pipeline (AutoPP) for end-to-end product poster generation and optimization, requiring only basic product information as input. 2. A novel optimization method that uses systematic element replacement and Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to specific poster elements. 3. The creation and release of AutoPP1M, the largest dataset for product poster generation and optimization, containing one million posters and user feedback.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces AutoPP, an automated pipeline that generates product posters from basic product information and then optimizes them for higher Click-Through Rate (CTR) using online feedback and a novel Isolated Direct Preference Optimization technique. It is supported by a large-scale dataset, AutoPP1M. Experiments show that AutoPP achieves state-of-the-art performance in both offline and online evaluations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Data relativistic uncertainty framework for low-illumination anime scenery image enhancement</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [low-light image enhancement], [Data Relativistic Uncertainty, Unsupervised Learning, EnlightenGAN, Anime Scenery, Domain Gap]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yiquan Gao, John See</p>
</li>
<li class="">
<p><strong>institution:</strong> Heriot-Watt University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21944" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21944</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Automated Discovery of Parsimonious Spectral Indices via Normalized Difference Polynomials</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [remote sensing, vegetation classification], [normalized difference polynomials, spectral indices, feature selection, Sentinel-2, illumination invariance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ali Lotfi, Adam Carter, Thuan Ha, Mohammad Meysami, Kwabena Nketia, Steve Shirtliffe</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Saskatchewan, New Mexico State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21948" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21948</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces an automated framework for generating a structured search space of spectral indices using pairwise normalized differences and polynomial combinations up to a fixed degree. 2. Employs feature selection methods (ANOVA filtering, recursive elimination, L1-regularized SVM) to select small, interpretable sets of indices that maintain high classification accuracy. 3. Demonstrates the effectiveness of the approach on a real-world task (Kochia detection), showing that simple degree-2 product indices achieve high accuracy and are deployable on platforms like Google Earth Engine.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/511f8067b02514f2cb415c41eb77aefa341affb7b0beba8fc78dcc37ebff9508_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/511f8067b02514f2cb415c41eb77aefa341affb7b0beba8fc78dcc37ebff9508_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces an automated method to discover simple spectral indices for vegetation classification by generating polynomial combinations of pairwise normalized differences and applying feature selection. The method was tested on detecting Kochia with Sentinel-2 imagery, where a single degree-2 index achieved over 96% accuracy. The results show that discriminative signals often come from spectral interactions, and the resulting indices are simple enough for direct deployment in cloud platforms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-modal robustness], [multi-modal large language model, input perturbation, training-free calibration, denoising, benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dunyuan XU, Xikai Yang, Yaoqian Li, Juzheng Miao, Jinpeng Li, Pheng-Ann Heng</p>
</li>
<li class="">
<p><strong>institution:</strong> The Chinese University of Hong Kong (CUHK)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21964" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21964</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A systematic analysis of the impact of various visual and textual perturbations on medical MLLMs, revealing their sensitivity. 2. A novel training-free Inherent-enhanced Multi-modal Calibration (IMC) framework that leverages MLLMs&#x27; own capabilities for robustness enhancement. 3. The introduction of two specific methods within IMC: Perturbation-aware Denoising Calibration (PDC) for visual noise and a Self-instantiated Multi-agent System (SMS) for textual noise.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252aad7806c9998b636d3335e0a4e90e437f57a51e2a942eb6d7b871a984ef2b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252aad7806c9998b636d3335e0a4e90e437f57a51e2a942eb6d7b871a984ef2b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the vulnerability of Medical Multi-modal Large Language Models (MLLMs) to input noise like imaging artifacts and text errors. It proposes a training-free framework called Inherent-enhanced Multi-modal Calibration (IMC) that uses the model&#x27;s own vision encoder and self-assessment capabilities to denoise inputs. Experiments on a new benchmark show the method achieves state-of-the-art robustness, enhancing clinical applicability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] LVLM-Aided Alignment of Task-Specific Vision Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [model alignment and interpretability], [LVLM-VA, spurious correlations, explainable AI (XAI), vision-language model, human-in-the-loop]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alexander Koebler, Lukas Kuhn, Ingo Thon, Florian Buettner</p>
</li>
<li class="">
<p><strong>institution:</strong> Goethe University Frankfurt, Siemens AG, German Cancer Research Center (DKFZ)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21985" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21985</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces LVLM-Aided Visual Alignment (LVLM-VA), a novel method for aligning small task-specific vision models with human domain knowledge using a Large Vision Language Model (LVLM). 2. Proposes a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling efficient expert-model interaction. 3. Demonstrates that the method effectively reduces the model&#x27;s dependence on spurious features and group-specific biases without requiring fine-grained, instance-level feedback.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c85ee676094853ba26d7711ac1d50d0ab994498bc2f91f2aaab4f1104d3222_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c85ee676094853ba26d7711ac1d50d0ab994498bc2f91f2aaab4f1104d3222_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of small task-specific vision models relying on spurious correlations, which leads to brittle real-world performance. The authors propose LVLM-Aided Visual Alignment (LVLM-VA), a method that uses a Large Vision Language Model to create a bidirectional interface between human domain knowledge and the model, translating explanations and critiques. The method is shown to significantly improve model alignment with human specifications and reduce dependence on spurious features across synthetic and real-world datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Lightweight Multi-Scale Attention Framework for Real-Time Spinal Endoscopic Instance Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [instance segmentation], [re-parameterized convolution, efficient multi-scale attention, lightweight multi-task head]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qi Lai, JunYan Li, Qiang Cai, Lei Wang, Tao Yan, XiaoKun Liang</p>
</li>
<li class="">
<p><strong>institution:</strong> The affiliations include IEEE members, suggesting an academic or research institution, but specific names are not fully listed in the provided text. Based on the GitHub URL pattern, a likely institution is not explicitly stated in the given content.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21984" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21984</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/hhwmortal/PELD-Instance-segmentation" target="_blank" rel="noopener noreferrer" class="">https://github.com/hhwmortal/PELD-Instance-segmentation</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed LMSF-A, a lightweight multi-scale attention framework co-designed across backbone, neck, and head for real-time instance segmentation. 2. Introduced the C2f-Pro backbone module combining RepViT-style re-parameterized convolution with efficient multi-scale attention for multi-branch training and fast inference. 3. Released a new clinically reviewed PELD dataset for spinal endoscopic instance segmentation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ddb0a22c2e72d9e15e574413f3c78c60ca63b7152d6320583b76bc00a64110f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ddb0a22c2e72d9e15e574413f3c78c60ca63b7152d6320583b76bc00a64110f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of real-time instance segmentation in spinal endoscopy, where factors like a narrow field of view and hardware constraints make deployment difficult. The authors propose LMSF-A, a lightweight framework featuring a re-parameterized backbone, a feature fusion neck, and a shared head, which achieves competitive accuracy with only 1.8M parameters. The method is validated on a new clinical dataset and shows good generalization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [hallucination mitigation, adversarial parametric editing, parameter clustering, visual-language models, activation dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji, Zhongshi He</p>
</li>
<li class="">
<p><strong>institution:</strong> Chongqing University, Xinjiang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21999" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21999</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/hujiayu1223/ALEAHallu" target="_blank" rel="noopener noreferrer" class="">https://github.com/hujiayu1223/ALEAHallu</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel adversarial parametric editing framework (ALEAHallu) following an Activate-Locate-Edit Adversarially paradigm for mitigating hallucinations in VLMs. 2. Introduces a method to construct an activation dataset with grounded and hallucinatory response pairs and identifies critical hallucination-prone parameter clusters via differential hidden state analysis. 3. Demonstrates the framework&#x27;s effectiveness by fine-tuning identified parameter clusters with adversarially tuned prefixes to force the model to prioritize visual evidence over linguistic priors.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the hallucination problem in Vision-Language Models (VLMs), where models generate content misaligned with visual inputs due to over-reliance on linguistic priors. The authors propose ALEAHallu, an adversarial parametric editing framework that identifies and fine-tunes hallucination-prone parameter clusters using adversarially optimized prompts to enhance visual grounding. Evaluations show the method significantly reduces hallucinations in both generative and discriminative VLM tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [multimodal large language models (MLLMs), slow-fast inference, adaptive perception, visual grounding, lightweight agent]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sarthak Mehrotra, Sairam V C Rebbapragada, Mani Hemanth Reddy Bonthu, Vineeth N Balasubramanian</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology, Bombay, Indian Institute of Technology, Hyderabad</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22009" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22009</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces iSHIFT, a lightweight (2.5B parameter) agent framework that integrates implicit chain-of-thought reasoning with a perception control module. 2. Proposes a novel slow-fast hybrid inference mechanism, allowing the model to adaptively switch between a detailed, high-precision &quot;slow mode&quot; and an efficient &quot;fast mode&quot; based on task demands. 3. Employs special perception tokens to dynamically guide the model&#x27;s visual attention to relevant screen regions, enabling precise visual grounding for GUI interaction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6067c294acdf08cb7927ad962f2d0d2c4f3efd938837b5cdb9ca1bdad4019422_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6067c294acdf08cb7927ad962f2d0d2c4f3efd938837b5cdb9ca1bdad4019422_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of building efficient yet precise GUI agents by proposing iSHIFT, a lightweight MLLM agent. iSHIFT uses an adaptive slow-fast inference mechanism and perception tokens to dynamically balance efficiency and accuracy for GUI tasks. Despite its small 2.5B size, it achieves state-of-the-art performance on multiple benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [vision-and-language navigation], [spatiotemporal context modeling, slot-based compression, prompt-guided multimodal integration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wen Jiang, Li Wang, Kangyao Huang, Wei Fan, Jinyuan Liu, Shaoyu Liu, Hongwei Duan, Bin Xu, Xiangyang Ji</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Institute of Technology, Tsinghua University, Dalian University of Technology, Xidian University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22010" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22010</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A slot-based historical image compression module to distill multi-view historical observations into fixed-length contextual representations. 2. A spatiotemporal trajectory encoding module to capture the temporal dynamics and spatial structure of UAV trajectories. 3. A prompt-guided multimodal integration module to fuse spatiotemporal context with current observations for robust waypoint prediction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c00484796a9df425f1884ea8ae22314b0592466ebe305303cf7f1f0c467b528_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c00484796a9df425f1884ea8ae22314b0592466ebe305303cf7f1f0c467b528_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes LongFly, a framework for long-horizon UAV vision-and-language navigation that addresses the challenge of modeling spatiotemporal context. The method integrates a history-aware modeling strategy with modules for compressing past observations, encoding trajectories, and fusing multimodal information. Experimental results show it outperforms state-of-the-art baselines in navigation success metrics across seen and unseen environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [Human-Computer Interaction (HCI)], [Gestural Interaction, Physics Simulation, Air-drawn Sketches, VR Content Creation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiangwen Zhang, Xiaowei Dai, Runnan Chen, Xiaoming Chen, Zeke Zexi Hu</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Technology and Business University, The University of Sydney</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22016" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22016</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel VR interaction framework (SketchPlay) that combines air-drawn sketches and gestures to create dynamic, physically realistic scenes. 2. A method that uses sketches to capture object/scene structure and gestures to convey physical cues (velocity, force) for defining motion and behavior. 3. Enables the generation of complex physical phenomena (rigid body motion, elastic deformation, cloth dynamics) through an intuitive, controller-free creation process.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79eefb65bc566df3d83196a3500892e4d09f26b4aa064a68193142a9ec59b0c0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79eefb65bc566df3d83196a3500892e4d09f26b4aa064a68193142a9ec59b0c0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes SketchPlay, a VR framework that allows users to create physically realistic dynamic scenes by sketching objects in the air and using gestures to define their motion. This method combines structural and dynamic intent to simulate phenomena like rigid body and cloth dynamics. The approach is shown to be more expressive and offer a better user experience than text-driven methods, lowering the barrier for non-expert creators.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Patch-Discontinuity Mining for Generalized Deepfake Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [deepfake detection], [patch-discontinuity, feature space redistribution, classification-invariant feature augmentation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Huanhuan Yuan, Yang Ping, Zhengqin Xu, Junyi Cao, Shuai Jia, Chao Ma</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, Chinese Academy of Military Science</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22027" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22027</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://gendf.github.io/" target="_blank" rel="noopener noreferrer" class="">https://gendf.github.io/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a deepfake-specific representation learning (DSRL) scheme to capture patch-discontinuity patterns in fake images and continuity in real ones. 2. Introduced a feature space redistribution (FSR) scheme to separately optimize the distributions of real and fake features, mitigating domain mismatch. 3. Designed a classification-invariant feature augmentation (CIFAug) strategy to enhance generalization by expanding feature scopes orthogonally to the classification direction without adding trainable parameters.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f93245fe55a81a22e8997db22f045beb4494e864df08fd145bf36088517c580_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f93245fe55a81a22e8997db22f045beb4494e864df08fd145bf36088517c580_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes GenDF, a generalized deepfake detection framework that transfers a large-scale vision model to detect fake faces by mining patch-discontinuity patterns. It introduces three key components—deepfake-specific representation learning, feature space redistribution, and a parameter-free feature augmentation strategy—to improve generalization to unseen forgery methods. The method achieves state-of-the-art cross-domain performance with only 0.28M trainable parameters, demonstrating high efficiency and effectiveness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [backdoor attacks], [video segmentation foundation models, backdoor attack, two-stage training, gradient analysis, attention shift]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zongmin Zhang, Zhen Sun, Yifan Liao, Wenhan Dong, Xinlei He, Xingshuo Han, Shengmin Xu, Xinyi Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Hong Kong University of Science and Technology (Guangzhou), Nanjing University of Aeronautics and Astronautics, Fujian Normal University, Jinan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22046" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22046</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies the ineffectiveness of classic backdoor attacks on prompt-driven Video Segmentation Foundation Models (VSFMs) and provides analysis via gradient and attention maps. 2. Proposes BadVSFM, the first dedicated backdoor attack framework for VSFMs, using a novel two-stage training strategy to separate clean and triggered representations. 3. Demonstrates strong, controllable attack performance across multiple models and datasets while preserving clean-task accuracy, and shows the vulnerability persists against existing defenses.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a7e9ff0ffc63f2085d6ca65db8e87f14fed435be5d8a51fd3d1265b22b668b1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a7e9ff0ffc63f2085d6ca65db8e87f14fed435be5d8a51fd3d1265b22b668b1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies that standard backdoor attacks fail on prompt-driven Video Segmentation Foundation Models (VSFMs) like SAM2. To solve this, the authors propose BadVSFM, a two-stage backdoor attack framework that successfully implants a controllable backdoor by steering the encoder and decoder separately. Experiments show the attack is effective and evades current defenses, revealing a significant security vulnerability in VSFMs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [autoregressive distillation, adversarial refinement, real-time streaming, reference-anchored positional re-encoding, consistency-aware discriminator]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhiyao Sun, Ziqiao Peng, Yifeng Ma, Yi Chen, Zhengguang Zhou, Zixiang Zhou, Guozhen Zhang, Youliang Zhang, Yuan Zhou, Qinglin Lu, Yong-Jin Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Renmin University of China, Tencent Hunyuan, Nanjing University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22065" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22065</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://streamavatar.github.io" target="_blank" rel="noopener noreferrer" class="">https://streamavatar.github.io</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A two-stage autoregressive adaptation and acceleration framework (autoregressive distillation + adversarial refinement) to adapt a non-causal human video diffusion model for real-time, interactive streaming. 2. Three novel components to ensure long-term stability and consistency: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. 3. A one-shot, interactive human avatar model capable of generating both natural talking and listening behaviors with coherent full-body gestures, surpassing existing methods in quality, efficiency, and interaction naturalness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of making diffusion-based human avatar generation suitable for real-time, interactive streaming. It proposes StreamAvatar, a two-stage framework that adapts a high-fidelity human video diffusion model using autoregressive distillation and adversarial refinement, incorporating novel components for long-term consistency. The method achieves state-of-the-art performance in generating high-resolution, full-body interactive avatars with natural talking/listening behaviors in real-time.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] MAI-UI Technical Report: Real-World Centric Foundation GUI Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [GUI agent, device-cloud collaboration, online reinforcement learning, self-evolving data pipeline, foundation model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hanzhang Zhou, Xu Zhang, Panrong Tong, Jianan Zhang, Liangyu Chen, Quyu Kong, Chenglin Cai, Chen Liu, Yue Wang, Jingren Zhou, Steven Hoi</p>
</li>
<li class="">
<p><strong>institution:</strong> Tongyi Lab, Alibaba Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22047" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22047</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Tongyi-MAI/MAI-UI" target="_blank" rel="noopener noreferrer" class="">https://github.com/Tongyi-MAI/MAI-UI</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A self-evolving data pipeline that expands GUI navigation data to include user interaction and tool calls. 2. A native device-cloud collaboration system that routes task execution based on state to improve efficiency and privacy. 3. An online RL framework with optimizations for scaling parallel environments and context length.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/819d67f31c44f0847dff819fbdf82773fea54c76d001f429e9e731b2ba253b85_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/819d67f31c44f0847dff819fbdf82773fea54c76d001f429e9e731b2ba253b85_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents MAI-UI, a family of foundation GUI agents designed to address challenges in real-world deployment, such as limited interaction and dynamic environments. Its unified methodology includes a self-evolving data pipeline, a device-cloud collaboration system, and an online RL framework. MAI-UI achieves state-of-the-art performance on multiple GUI grounding and mobile navigation benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Yume-1.5: A Text-Controlled Interactive World Generation Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [interactive world generation, long-video generation, attention distillation, context compression, text-controlled generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiaofeng Mao, Zhen Li, Chuanhao Li, Xiaojie Xu, Kaining Ying, Tong He, Jiangmiao Pang, Yu Qiao, Kaipeng Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai AI Laboratory, Fudan University, Shanghai Innovation Institute</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22096" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22096</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/stdstu12/YUME" target="_blank" rel="noopener noreferrer" class="">https://github.com/stdstu12/YUME</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A long-video generation framework integrating unified context compression with linear attention. 2. A real-time streaming acceleration strategy using bidirectional attention distillation and an enhanced text embedding scheme. 3. A text-controlled method for generating world events.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f7ffd3f0a90ba67551ade4e28abf8e27d5d08c106e463f85f9447011008416b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f7ffd3f0a90ba67551ade4e28abf8e27d5d08c106e463f85f9447011008416b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Yume-1.5, a framework to address challenges in generating interactive, explorable worlds using diffusion models, such as large model size and slow inference. The method introduces a novel architecture combining context compression, attention distillation, and text-based event control to enable real-time, keyboard-controlled world generation from text or images. The work concludes with a public codebase demonstrating the feasibility of text-controlled interactive world creation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Learning Association via Track-Detection Matching for Multi-Object Tracking</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multi-object tracking], [tracking-by-detection, link prediction, association learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Momir Adžemović</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Belgrade</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22105" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22105</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Robotmurlock/TDLP" target="_blank" rel="noopener noreferrer" class="">https://github.com/Robotmurlock/TDLP</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Track-Detection Link Prediction (TDLP), a tracking-by-detection method that learns data association via link prediction between tracks and detections, eliminating handcrafted heuristics. 2. Demonstrates that TDLP achieves state-of-the-art performance across multiple benchmarks, outperforming both traditional tracking-by-detection and end-to-end methods. 3. Provides analysis showing link prediction is more effective than metric learning for association, especially when handling heterogeneous features like bounding boxes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d61776d860da3a903f769a1427363e91df9be50d56b83666c73ac36008099062_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d61776d860da3a903f769a1427363e91df9be50d56b83666c73ac36008099062_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes TDLP, a tracking-by-detection method that learns to associate object detections across video frames by predicting links between existing tracks and new detections. This approach avoids handcrafted rules while remaining computationally efficient. Experiments show it outperforms state-of-the-art methods, and analysis indicates link prediction is superior to metric learning for this association task.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [visual question answering], [perceptual shaping, KL-consistency, KL-separation, evidence-preserving view, evidence-ablated view]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Yujiu Yang, Rui Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Microsoft Research, Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22120" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22120</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/zss02/BiPS" target="_blank" rel="noopener noreferrer" class="">https://github.com/zss02/BiPS</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Bi-directional Perceptual Shaping (BiPS), a novel training paradigm that uses question-conditioned masked views to generate bidirectional &quot;where-to-look&quot; signals. 2. Introduces a dual KL constraint mechanism: a KL-consistency constraint to encourage complete visual evidence coverage and a KL-separation constraint to discourage text-only shortcuts and enforce fine-grained visual reliance. 3. Demonstrates strong performance improvements and out-of-domain generalization across multiple benchmarks without adding inference-time cost.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74efcdb66ae5cc2a16fd7b59382dce3e78d24b03e998520275db9a96986fa158_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74efcdb66ae5cc2a16fd7b59382dce3e78d24b03e998520275db9a96986fa158_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that large vision-language models often overlook fine-grained visual evidence and rely on text shortcuts. It proposes Bi-directional Perceptual Shaping (BiPS), a training method that uses KL constraints on evidence-preserving and evidence-ablated image views to shape the model&#x27;s perception. The method significantly boosts the performance of a base VLM model and shows strong generalization to unseen data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] ProEdit: Inversion-based Editing From Prompts Done Right</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image editing], [inversion-based editing, KV-mix, Latents-Shift, plug-and-play, flow inversion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhi Ouyang, Dian Zheng, Xiao-Ming Wu, Jian-Jian Jiang, Kun-Yu Lin, Jingke Meng, Wei-Shi Zheng</p>
</li>
<li class="">
<p><strong>institution:</strong> Sun Yat-sen University, CUHK MMLab, Nanyang Technological University, The University of Hong Kong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22118" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22118</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://isee-laboratory.github.io/ProEdit/" target="_blank" rel="noopener noreferrer" class="">https://isee-laboratory.github.io/ProEdit/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes KV-mix to mix source and target KV features in the attention mechanism, reducing source influence in edited regions while preserving background. 2. Introduces Latents-Shift to perturb the source latent in the edited region, eliminating the influence of the inverted latent during sampling. 3. Presents a plug-and-play design that can be integrated into existing inversion-based editing methods like RF-Solver, FireFlow, and UniEdit.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/878121cf01366a9dcec34198113a6d864c4ea37e9b41c39e221a71b26e72039f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/878121cf01366a9dcec34198113a6d864c4ea37e9b41c39e221a71b26e72039f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem in inversion-based visual editing where over-reliance on source image information hinders accurate attribute changes. The proposed method, ProEdit, introduces two techniques: KV-mix in the attention aspect and Latents-Shift in the latent aspect, to mitigate source influence. It achieves state-of-the-art performance on image and video editing benchmarks and is designed as a plug-and-play module compatible with existing methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Graph-Augmented knowledge Distillation based Dual-Stream Vision Transformer with Region-Aware Attention for Gastrointestinal Disease Classification with Explainable AI</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [Knowledge Distillation, Vision Transformer, Swin Transformer, Explainable AI, Wireless Capsule Endoscopy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Assaduzzaman, Nushrat Jahan Oyshi, Eram Mahamud</p>
</li>
<li class="">
<p><strong>institution:</strong> Daffodil International University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21372" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21372</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a hybrid dual-stream teacher model combining Swin Transformer for global context and Vision Transformer for local features. 2. Developed a compact Tiny-ViT student model via knowledge distillation to balance accuracy and efficiency for clinical deployment. 3. Validated the model&#x27;s clinical relevance through extensive interpretability analysis using Grad-CAM, LIME, and Score-CAM.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cd724a13af61d1181b015dc7b5fe86e10cc834beac172261c5bebe54e371bc3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cd724a13af61d1181b015dc7b5fe86e10cc834beac172261c5bebe54e371bc3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of classifying gastrointestinal diseases from endoscopic images by proposing a knowledge distillation framework. A high-capacity dual-stream teacher model guides a compact Tiny-ViT student, achieving near-perfect accuracy on WCE datasets while providing explainable predictions. The work offers an efficient and interpretable solution suitable for resource-constrained clinical environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [diffusion models, generative modeling, evidence lower bound, residual learning, two-stage framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Takuro Kutsuna</p>
</li>
<li class="">
<p><strong>institution:</strong> Toyota Central R&amp;D Labs., Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21593" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21593</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image deblurring], [lightweight network, real-time inference, edge deployment, U-shaped architecture, motion blur]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhuoyu Wu, Wenhui Ou, Qiawei Zheng, Jiayan Yang, Quanjun Wang, Wenqi Fang, Zheng Wang, Yongkui Yang, Heshan Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Unifyware Co., Ltd.; Monash University; Hong Kong University of Science and Technology; Shenzhen Infynova Co., Ltd.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21975" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21975</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/ReaganWu/RT-Focuser" target="_blank" rel="noopener noreferrer" class="">https://github.com/ReaganWu/RT-Focuser</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a lightweight U-shaped network (RT-Focuser) with a Lightweight Deblurring Block (LD) for edge-aware feature extraction. 2. Introduces a Multi-Level Integrated Aggregation module (MLIA) for encoder feature integration. 3. Designs a Cross-source Fusion Block (X-Fuse) for progressive decoder refinement.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d65834719412d7909823764ddbf757cadad16b1b273fd3e67cf177cad638b9d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d65834719412d7909823764ddbf757cadad16b1b273fd3e67cf177cad638b9d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes RT-Focuser, a lightweight U-shaped neural network designed for real-time image deblurring on edge devices. It achieves a balance between speed and accuracy with only 5.85M parameters, running at over 140 FPS on both GPU and mobile platforms. The model demonstrates strong potential for deployment in real-time applications like autonomous driving and UAV perception.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] The Color-Clinical Decoupling: Why Perceptual Calibration Fails Clinical Biomarkers in Smartphone Dermatology</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical imaging], [colorimetric calibration, clinical biomarkers, Individual Typology Angle (ITA), Melanin Index, intraclass correlation coefficient (ICC)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sungwoo Kang</p>
</li>
<li class="">
<p><strong>institution:</strong> Korea University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21988" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21988</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and defines the &quot;color-clinical decoupling&quot; phenomenon, where perceptual color accuracy does not guarantee reliability of clinical biomarkers. 2. Demonstrates that anatomical variance (facial region) is a dominant source of color variance, significantly outweighing device effects, challenging single-patch calibration. 3. Provides a mathematical explanation for the decoupling by showing the ITA biomarker&#x27;s disproportionate sensitivity to noise in the b* color channel.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fbc8b90f041b184cdc9f22e1063fa065b2e1f8b098825058b11e381c89ffee7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fbc8b90f041b184cdc9f22e1063fa065b2e1f8b098825058b11e381c89ffee7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the assumption that colorimetric calibration ensures reliable clinical biomarker extraction in smartphone dermatology. Using a large dataset and standard calibration, it finds that while color error is reduced, biomarker reliability remains poor, a phenomenon termed &quot;color-clinical decoupling,&quot; primarily due to anatomical variance and formula sensitivity. The conclusion is that current calibration standards are insufficient for clinical use and require region-aware protocols.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-30">2025-12-30<a href="#2025-12-30" class="hash-link" aria-label="Direct link to 2025-12-30" title="Direct link to 2025-12-30" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251230] SlimEdge: Lightweight Distributed DNN Deployment on Constrained Hardware</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [Structured Pruning, Multi-Objective Optimization, Edge Inference, MVCNN, View-Adaptive Compression]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mahadev Sunil Kumar, Arnab Raha, Debayan Das, Gopakumar G, Amitava Mukherjee</p>
</li>
<li class="">
<p><strong>institution:</strong> Accenture PLC, Intel Corporation, Indian Institute of Science, Amrita Vishwa Vidyapeetham, Birla Institute of Technology and Science</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22136" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22136</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a framework for lightweight DNN deployment that integrates structured pruning with multi-objective optimization to meet heterogeneous hardware constraints. 2. Demonstrates the framework on MVCNN by quantifying the contribution of individual views to accuracy for view-adaptive pruning budget allocation. 3. Shows experimentally that the compressed models meet user-specified accuracy and memory bounds while achieving 1.2x to 5.0x inference speedup across diverse hardware.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7687c3e58bfa2b22573745dffb608fb7c36a0c339dd9216829f78a284f51e662_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7687c3e58bfa2b22573745dffb608fb7c36a0c339dd9216829f78a284f51e662_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of deploying large DNNs on resource-constrained edge devices. It proposes SlimEdge, a method that combines structured pruning and multi-objective optimization to compress models like MVCNN while preserving task performance. The results show that this approach successfully meets specified accuracy and memory constraints while significantly reducing inference latency on various edge hardware platforms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning from human feedback (rlhf)], [reward model, video generation, reward hacking, bradley-terry loss, hierarchical attention]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiesong Lian, Ruizhe Zhong, Zixiang Zhou, Xiaoyue Mi, Yixue Hao, Yuan Zhou, Qinglin Lu, Long Hu, Junchi Yan</p>
</li>
<li class="">
<p><strong>institution:</strong> Huazhong University of Science and Technology, Shanghai Jiao Tong University, Tencent Hunyuan, University of Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22170" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22170</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A data collection and pairing strategy using single-item binary annotations and cross-prompt pairing to reduce labeling noise. 2. A Hierarchical Progressive Query Attention mechanism for better feature aggregation in the reward model architecture. 3. A modified Bradley-Terry loss function that accommodates win-tie scenarios to regularize the score distribution and mitigate reward hacking.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04ea37599d144feb85d3d1e8303d5d59adfabb579e172f06aef976d3783ee4ef_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04ea37599d144feb85d3d1e8303d5d59adfabb579e172f06aef976d3783ee4ef_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes SoliReward, a systematic framework to improve reward models for video generation alignment. It addresses data noise and reward hacking through a new data annotation strategy, a hierarchical attention architecture, and a modified loss function. The approach shows improved performance on benchmarks for video quality and enhances the effectiveness of post-training video models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Real-Time American Sign Language Recognition Using 3D Convolutional Neural Networks and LSTM: Architecture, Training, and Deployment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [sign language recognition], [3D CNN, LSTM, real-time processing, spatial-temporal features, edge deployment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dawnena Key</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Denver</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22177" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22177</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A hybrid 3D CNN-LSTM architecture for capturing spatial and temporal features in ASL signs. 2. A training methodology using multiple complementary datasets (WLASL, ASL-LEX, expert-annotated signs). 3. A deployment architecture supporting both cloud (AWS) and edge (OAK-D camera) inference.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a6733a99335eeae8c4efc856acc6d6c875e74bc5925841089b5198403d1d3b0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a6733a99335eeae8c4efc856acc6d6c875e74bc5925841089b5198403d1d3b0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a real-time American Sign Language recognition system that uses a hybrid 3D CNN and LSTM architecture to process video streams. The system, trained on multiple datasets, achieves high F1-scores and is deployed for practical use on both cloud and edge devices.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Characterizing Motion Encoding in Video Diffusion Timesteps</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [video diffusion models, timestep analysis, motion-appearance disentanglement, motion transfer, one-shot customization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vatsal Baherwani, Yixuan Ren, Abhinav Shrivastava</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Maryland</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22175" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22175</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a quantitative proxy and conducts a large-scale study to systematically characterize how motion is encoded across the denoising timesteps of video diffusion models, identifying distinct motion-dominant and appearance-dominant regimes. 2. Derives an operational motion-appearance boundary in timestep space, turning a widely used empirical heuristic into a spatiotemporal disentanglement principle. 3. Simplifies the one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ff2b9cdf8da1e9fbc9ae04ff2d085e89388ee26b1689338abbb853b17970bed_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ff2b9cdf8da1e9fbc9ae04ff2d085e89388ee26b1689338abbb853b17970bed_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates how motion is encoded across the denoising timesteps of text-to-video diffusion models. By quantifying the trade-off between appearance editing and motion preservation when injecting new conditions, the authors identify early timesteps as motion-dominant and later ones as appearance-dominant. This characterization enables a simplified, effective method for one-shot motion transfer without extra modules.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [dimensionality reduction], [Locally Linear Embedding (LLE), AI-enhanced LLE, medical data analysis, medical billing, transcription]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hassan Khalid, Muhammad Mahad Khaliq, Muhammad Jawad Bashir</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Science and Technology (NUST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22182" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22182</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an innovative integration of AI with Locally Linear Embedding (LLE) to handle high-dimensional medical data. 2. Develops a comprehensive mathematical model for the AI-enhanced LLE technique. 3. Demonstrates the model&#x27;s application in real-world healthcare scenarios, showing significant improvements in data processing accuracy and operational efficiency for medical billing and transcription.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces an AI-enhanced Locally Linear Embedding (LLE) model to improve the analysis of high-dimensional medical data. The method is applied to automate and enhance medical billing and transcription services. The results show significant improvements in processing accuracy and operational efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Unbiased Visual Reasoning with Controlled Visual Inputs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal reasoning], [vision-language models, spurious correlations, information bottleneck, reinforcement learning, modular reasoning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhaonan Li, Shijie Lu, Fei Wang, Jacob Dineen, Xiao Ye, Zhikun Xu, Siyi Liu, Young Min Cho, Bangzheng Li, Daniel Chang, Kenny Nguyen, Qizheng Yang, Muhao Chen, Ben Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> Arizona State University, University of Southern California, University of Pennsylvania, University of California, Davis</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22183" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22183</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes VISTA, a modular framework that decouples visual perception from reasoning using an explicit information bottleneck to control visual inputs. 2. Introduces a training method using reinforcement learning (GRPO) on a small curated dataset to align the reasoner with unbiased visual evidence. 3. Demonstrates improved robustness against spurious correlations, transferability across VLM sensors, and enhanced interpretability in reasoning traces.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f546535b9c36b7873d0f685328a4f4a8e058e6a4788639c170f69e073d8f9e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f546535b9c36b7873d0f685328a4f4a8e058e6a4788639c170f69e073d8f9e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of vision-language models (VLMs) relying on spurious correlations rather than causal visual evidence. It proposes VISTA, a modular framework that separates perception (via a frozen VLM) from reasoning (via an LLM) using controlled queries and trains the reasoner with reinforcement learning. The method shows significant gains in robustness on benchmarks like SpuriVerse while maintaining competitive performance on other tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [computational pathology], [Multiple Instance Learning, Hook Tokens, Linear Complexity, Multimodal Initialization, Hook Diversity Loss]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xitong Ling, Minxi Ouyang, Xiaoxiao Li, Jiawen Li, Ying Chen, Yuxuan Sun, Xinrui Chen, Tian Guan, Xiaoping Liu, Yonghong He</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Xiamen University, Westlake University, Wuhan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22188" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22188</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/lingxitong/HookMIL" target="_blank" rel="noopener noreferrer" class="">https://github.com/lingxitong/HookMIL</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes HookMIL, a context-aware MIL framework using learnable hook tokens for structured contextual aggregation with linear computational complexity. 2. Introduces a multimodal initialization strategy for hook tokens using visual, textual, and spatial priors to accelerate convergence and improve representation. 3. Presents a Hook Diversity Loss and a hook-to-hook communication mechanism to encourage token specialization and refine interactions while minimizing redundancy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/125d35cff1b2c94d5e736ab81d897743e3d0b37d50d5ba6ce441aa77f8bc620e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/125d35cff1b2c94d5e736ab81d897743e3d0b37d50d5ba6ce441aa77f8bc620e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the loss of context in traditional MIL and the high computational cost of transformer-based MIL for whole-slide image analysis. It proposes HookMIL, a framework that uses learnable hook tokens for efficient, linear-complexity context modeling, enhanced by multimodal initialization and specialized loss functions. Experiments on four public datasets show that HookMIL achieves state-of-the-art performance with improved efficiency and interpretability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder for High-Sensitivity Intracrania Aneurysm Screening</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [dual-encoder, data augmentation, transfer learning, intracranial aneurysm detection, Grad-CAM]</p>
</li>
<li class="">
<p><strong>authors:</strong> Antara Titikhsha, Divyanshu Tak</p>
</li>
<li class="">
<p><strong>institution:</strong> Carnegie Mellon University, Harvard Medical School, Mass General Brigham, Dana-Farber Cancer Institute, Brigham and Women’s Hospital</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22185" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22185</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/antitikhsha/SAMM2D" target="_blank" rel="noopener noreferrer" class="">https://github.com/antitikhsha/SAMM2D</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced SAMM2D, a scale-aware multi-modal 2D dual-encoder framework for high-sensitivity intracranial aneurysm screening. 2. Demonstrated through ablation that data augmentation degrades performance when using a strong pretrained backbone, challenging a common assumption in low-data medical imaging. 3. Showed the model achieves 95% sensitivity (surpassing average radiologist performance) and provides interpretable visualizations (Grad-CAM) with clinically relevant focus.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d68de6f079250367c97085c42d9cd0408759f4a0e86134b480f2002a580665_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d68de6f079250367c97085c42d9cd0408759f4a0e86134b480f2002a580665_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces SAMM2D, a dual-encoder framework for detecting intracranial aneurysms in medical images. The key finding is that, contrary to common practice, data augmentation harms performance when using a strong ImageNet-pretrained backbone, suggesting robust pretraining is more beneficial than complex augmentation in low-data medical settings. The model achieves high sensitivity and demonstrates cost-saving potential in clinical screening.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Tiny-YOLOSAM: Fast Hybrid Image Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image segmentation], [Segment Anything Model (SAM), YOLO, hybrid prompting, inference acceleration, object detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kenneth Xu, Songhan Wu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Michigan</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22193" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22193</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Tiny-YOLOSAM, a fast hybrid segmentation pipeline that combines YOLO object detection with TinySAM for efficient prompting. 2. Introduces a targeted sparse prompting strategy that samples points only in regions not covered by detector-guided masks to improve coverage. 3. Demonstrates significant improvements in both segmentation coverage (AR, mIoU) and inference speed (4.7x faster) on COCO val2017 compared to the baseline TinySAM &quot;segment-everything&quot; mode.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a0001bbf996fbac2c566ca31f8e7ca42f4e6abdc06cc1a2d649111b5442d799_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a0001bbf996fbac2c566ca31f8e7ca42f4e6abdc06cc1a2d649111b5442d799_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high computational cost of the Segment Anything Model (SAM) for full-scene segmentation. It proposes Tiny-YOLOSAM, a hybrid method that uses a YOLO detector to generate box prompts for salient objects and supplements uncovered areas with sparse point prompts. The approach achieves a 4.7x speedup and significantly better coverage compared to the baseline, offering a practical alternative to dense &quot;segment-everything&quot; prompting.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Quadrant Segmentation VLM with Few-Shot Adaptation and OCT Learning-based Explainability Methods for Diabetic Retinopathy</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [Vision-Language Model (VLM), Few-Shot Learning, Grad-CAM, Multimodal Explainability, Diabetic Retinopathy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shivum Telang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Pittsburgh Rangos Research Center, North Allegheny Senior High School</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22197" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22197</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel multimodal explainability model that combines fundus and OCT images for Diabetic Retinopathy severity classification, mimicking an ophthalmologist&#x27;s reasoning. 2. Introduces a Vision-Language Model (VLM) with few-shot learning adaptation to analyze lesion distributions within retinal quadrants and generate natural language explanations. 3. Develops a method to generate paired Grad-CAM heatmaps across both imaging modalities to visually highlight regions contributing to the classification decision.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbd85ade1402c194ad4e8570cc627a2a90022d01c5dd2aebc1d16c608b3b733a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbd85ade1402c194ad4e8570cc627a2a90022d01c5dd2aebc1d16c608b3b733a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of explainability in AI models for Diabetic Retinopathy (DR) diagnosis by proposing a multimodal Vision-Language Model (VLM). The model uses few-shot learning to analyze lesion distributions in retinal quadrants from both fundus and OCT images and generates paired Grad-CAM heatmaps and natural language explanations for its severity classifications. This approach aims to provide a more interpretable and clinically practical tool for DR diagnostics.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [Convolutional Neural Network, SHAP, LIME, Saliency Maps, Malaria Diagnosis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md. Ismiel Hossen Abir, Awolad Hossain</p>
</li>
<li class="">
<p><strong>institution:</strong> Department of Computer Science &amp; Engineering, International Standard University, Dhaka, Bangladesh</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22205" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22205</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a custom CNN model for automated malaria diagnosis from blood cell images, achieving high accuracy (96%). 2. Compares the performance of the custom CNN with established deep learning architectures like ResNet50 and VGG16. 3. Enhances model interpretability for clinical trust by applying Explainable AI techniques, including SHAP, LIME, and Saliency Maps.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cafdabeab6baa067cead631acca4eba73f7c2812fd0d0e97d201544854ddd16b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cafdabeab6baa067cead631acca4eba73f7c2812fd0d0e97d201544854ddd16b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a deep learning-based system using a custom Convolutional Neural Network (CNN) to automatically diagnose malaria from blood cell images, achieving high accuracy. It compares this model against several established architectures and applies Explainable AI (XAI) techniques like SHAP and LIME to make the model&#x27;s decisions interpretable. The study concludes that this approach can provide a quick, accurate, and understandable diagnostic tool, particularly valuable in resource-limited settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [crowd counting], [weakly-supervised learning, vision transformer, density-guided aggregation, parameter efficiency, lightweight model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qiang Guo, Rubo Zhang, Bingbing Zhang, Junjie Liu, Jianqing Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Dalian Minzu University, Dalian University of Technology, Dalian Rijia Electronics Co., Ltd.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22203" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22203</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes TCFormer, an ultra-lightweight transformer-based framework with only 5 million parameters for weakly-supervised crowd counting. 2. Introduces a Learnable Density-Weighted Averaging module to dynamically re-weight local features based on predicted density, compensating for the lack of spatial annotations. 3. Designs a density-level classification loss to discretize crowd density into grades, regularizing training and enhancing performance across varying density levels.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67bff3bf5e0b02cbd2cc6071e68fd191f9adc37c49a6f5dd3f4b89fbc7205ca3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67bff3bf5e0b02cbd2cc6071e68fd191f9adc37c49a6f5dd3f4b89fbc7205ca3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes TCFormer, a tiny transformer-based model for weakly-supervised crowd counting that uses only image-level count labels. It introduces a density-guided feature aggregation module and a density-level classification loss to achieve accurate counting. Experiments show it achieves a superior trade-off between parameter efficiency and accuracy, making it suitable for edge devices.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [multimodal large language models], [Moxin, open-source LLM, vision-language-action, Model Openness Framework, multimodal models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Arash Akbari, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Northeastern University, Harvard University, Cornell University, Tulane University, University of Washington, Roboraction.ai, Futurewei, AIBAO LLC</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22208" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22208</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/moxin-org/Moxin-LLM" target="_blank" rel="noopener noreferrer" class="">https://github.com/moxin-org/Moxin-LLM</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Moxin 7B, a fully open-source LLM developed under the Model Openness Framework, promoting transparency in training, datasets, and implementation. 2. Develops three specialized variants of Moxin: Moxin-VLM for vision-language tasks, Moxin-VLA for vision-language-action tasks, and Moxin-Chinese for Chinese language capabilities. 3. Demonstrates superior performance of the proposed models in various evaluations using open-source frameworks and data, with all models, code, and data publicly released.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79873d0c3143fc2b06f1be1be9b8bc80555fa0aefd4a6f2b8d6bda39bce5f227_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79873d0c3143fc2b06f1be1be9b8bc80555fa0aefd4a6f2b8d6bda39bce5f227_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Moxin 7B, a fully transparent open-source large language model, and extends it into three multimodal variants for vision-language, vision-language-action, and Chinese tasks. The models are trained using open-source frameworks and data. The authors release the models, code, and data, reporting superior performance in evaluations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [pedestrian attribute recognition], [vision-language model, cross-attention fusion, class imbalance, domain generalization, SigLIP]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abdellah Zakaria Sellam, Salah Eddine Bekhouche, Fadi Dornaika, Cosimo Distante, Abdenour Hadid</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Salento, Institute of Applied Sciences and Intelligent Systems - CNR, University of the Basque Country UPV/EHU, IKERBASQUE, Sorbonne University Abu Dhabi</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22217" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22217</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes VLM-PAR, a modular vision-language framework built on frozen SigLIP 2 multilingual encoders for Pedestrian Attribute Recognition. 2. Introduces a compact cross-attention fusion mechanism to align image and prompt embeddings by refining visual features. 3. Demonstrates state-of-the-art performance on the imbalanced PA100K benchmark and significant gains on PETA and Market-1501, showing effectiveness against imbalance and domain shift.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44bf59cdfc87f0708e9f41a8899fbff5562f87b0b721fe79f7fcfc23fb5bb4d4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44bf59cdfc87f0708e9f41a8899fbff5562f87b0b721fe79f7fcfc23fb5bb4d4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes VLM-PAR, a vision-language model framework that uses a frozen SigLIP encoder and a cross-attention fusion module to refine visual features for Pedestrian Attribute Recognition. It achieves new state-of-the-art results on the PA100K benchmark and shows strong performance on other datasets, demonstrating the effectiveness of leveraging large-scale vision-language pretraining to address class imbalance and generalization challenges in PAR.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Signal-SGN++: Topology-Enhanced Time-Frequency Spiking Graph Network for Skeleton-Based Action Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [skeleton-based action recognition], [Spiking Neural Networks, Graph Convolutional Networks, Time-Frequency Learning, Topology-Aware Learning, Energy Efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Naichuan Zheng, Xiahai Lun, Weiyi Li, Yuchen Du</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing University of Posts and Telecommunications</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22214" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22214</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel spiking graph network backbone integrating 1D Spiking Graph Convolution (1D-SGC) and Frequency Spiking Convolution (FSC) for joint spatiotemporal and spectral feature extraction. 2. Introduces a Topology-Shift Self-Attention (TSSA) mechanism to adaptively route attention across learned skeletal topologies without increasing computational complexity. 3. Designs an auxiliary Multi-Scale Wavelet Transform Fusion (MWTF) branch with a Topology-Aware Time-Frequency Fusion (TATF) unit to preserve structural priors in multi-resolution spectral fusion.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6af8fb8807de54b37db40834a79908ad86cf7e159907b658e54986356c4494d4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6af8fb8807de54b37db40834a79908ad86cf7e159907b658e54986356c4494d4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Signal-SGN++, a novel spiking graph network for skeleton-based action recognition that integrates topology-aware learning with time-frequency spiking dynamics to capture motion dependencies. The method combines a spiking graph backbone with a topology-shift attention mechanism and a multi-scale wavelet fusion branch. Experiments show it achieves a superior accuracy-efficiency trade-off, outperforming other SNN methods and competing with state-of-the-art GCNs while using significantly less energy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On Extending Semantic Abstraction for Efficient Search of Hidden Objects</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [semantic abstraction, relevancy maps, 3D localization, hidden objects, unstructured search]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tasha Pais, Nikhilesh Belulkar</p>
</li>
<li class="">
<p><strong>institution:</strong> Columbia University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22220" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22220</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Extends the Semantic Abstraction framework to the novel domain of localizing hidden (occluded) objects. 2. Proposes using historical placement data to efficiently guide the unstructured search for hidden objects. 3. Demonstrates a model that can accurately identify a hidden object&#x27;s complete 3D location faster than a naive random search.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb1e039b06f845acfd3a644354907fc35eec78f060cf605799b7607c8a20ba8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb1e039b06f845acfd3a644354907fc35eec78f060cf605799b7607c8a20ba8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of efficiently finding hidden or lost objects by extending the Semantic Abstraction framework. The method uses 2D VLM relevancy maps as abstract object representations to learn 3D localization and leverages historical placement data to optimize the search. The result is a model that can locate hidden objects significantly faster than random search, aiming to improve the capabilities of household robots.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Towards Signboard-Oriented Visual Question Answering: ViSignVQA Dataset, Method and Benchmark</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [visual question answering], [signboard VQA, OCR-integrated VQA, multimodal dataset, Vietnamese, multi-agent framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hieu Minh Nguyen, Tam Le-Thanh Dang, Kiet Van Nguyen</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Information Technology, Vietnam National University, Ho Chi Minh City</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22218" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22218</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces ViSignVQA, the first large-scale Vietnamese dataset for signboard-oriented VQA, capturing diverse linguistic, cultural, and visual characteristics. 2. Benchmarks the task by adapting state-of-the-art VQA models with integrated Vietnamese OCR and language models, demonstrating significant performance gains from OCR-enhanced context. 3. Proposes a novel multi-agent VQA framework combining perception and reasoning agents with GPT-4, achieving high accuracy via majority voting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d16c0f62d737eb6a3e9a6e55cda2d721775e9d3be82e08e0e0cd03f4d648a93_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d16c0f62d737eb6a3e9a6e55cda2d721775e9d3be82e08e0e0cd03f4d648a93_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the under-explored task of understanding signboard text in natural scenes for Visual Question Answering (VQA), particularly in low-resource languages like Vietnamese. It introduces the ViSignVQA dataset and benchmarks adapted VQA models with OCR integration, showing substantial performance improvements, and proposes a multi-agent framework that achieves high accuracy. The work highlights the importance of domain-specific multimodal resources for enhancing text-based VQA in real-world applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video understanding], [streaming video, multimodal large language models, event segmentation, hierarchical representation, elastic-scale]</p>
</li>
<li class="">
<p><strong>authors:</strong> Naishan Zheng, Jie Huang, Qingpei Guo, Feng Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology of China, Ant Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22226" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22226</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/zheng980629/VideoScaffold" target="_blank" rel="noopener noreferrer" class="">https://github.com/zheng980629/VideoScaffold</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes VideoScaffold, a dynamic representation framework for streaming video understanding in MLLMs that adaptively adjusts event granularity. 2. Introduces Elastic-Scale Event Segmentation (EES) for prediction-guided, dynamic boundary refinement. 3. Introduces Hierarchical Event Consolidation (HEC) for progressively aggregating segments into multi-level abstractions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of understanding long, streaming videos with MLLMs by proposing VideoScaffold, a framework that dynamically segments and hierarchically consolidates video events to adapt granularity and preserve semantics. It achieves state-of-the-art performance on benchmarks and can extend image-based MLLMs to video comprehension.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] KAN-FPN-Stem<!-- -->:A<!-- --> KAN-Enhanced Feature Pyramid Stem for Boosting ViT-based Pose Estimation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [pose estimation], [KAN, Feature Pyramid Network, Vision Transformer, multi-scale fusion, convolutional layer]</p>
</li>
<li class="">
<p><strong>authors:</strong> HaoNan Tang</p>
</li>
<li class="">
<p><strong>institution:</strong> WuHan University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22228" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22228</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identified that the performance bottleneck in ViT front-ends for pose estimation lies in the post-fusion smoothing step, not in feature refinement via attention modules. 2. Proposed a novel KAN-enhanced FPN-Stem architecture that replaces the standard linear 3x3 convolution in the FPN with a KAN-based convolutional layer for superior non-linear smoothing. 3. Demonstrated a significant performance improvement of up to +2.0 AP on COCO dataset over the ViTPose-S baseline, providing a plug-and-play module and a new direction for improving feature fusion quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2245ed47610385d8ff0636f54ef0af43582616927b67e58ea212b9d6d195a902_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2245ed47610385d8ff0636f54ef0af43582616927b67e58ea212b9d6d195a902_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the performance bottleneck in Vision Transformer (ViT) based pose estimation models, which stems from simplistic front-end designs that cause information loss and poor multi-scale handling. The authors propose KAN-FPN-Stem, an architecture that enhances the Feature Pyramid Network by replacing its final linear smoothing convolution with a KAN-based layer to better rectify fusion artifacts. Experiments show a significant performance boost, revealing that improving feature fusion, not just refinement, is key to advancing ViT-based dense prediction tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image reconstruction], [diffusion model, cross-domain, meta-information, sinogram adapter, low-dose PET]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mengxiao Geng, Ran Hong, Xiaoling Xu, Bingxuan Li, Qiegen Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanchang University, Hefei Comprehensive National Science Center</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22237" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22237</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a meta-information guided cross-domain synergistic diffusion model (MiG-DM) that integrates cross-modal priors for PET reconstruction. 2. Introduces a meta-information encoding module that transforms clinical parameters into semantic prompts for cross-modal alignment. 3. Designs a cross-domain architecture with a specialized sinogram adapter to capture global physical structures in the projection domain.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/974872f0916ad96ffeb1ed9b169c4f627d5c534046b438f10fd015171720864a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/974872f0916ad96ffeb1ed9b169c4f627d5c534046b438f10fd015171720864a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of low-dose PET image reconstruction, which suffers from noise and loss of detail. It proposes a novel diffusion model called MiG-DM that guides the reconstruction using patient-specific meta-information and processes data across both the projection and image domains. Experiments show that MiG-DM outperforms existing methods in improving image quality and preserving physiological details.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical imaging], [algorithmic fairness, subgroup performance analysis, JustEFAB framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shaurya Gaur, Michel Vitale, Alessa Hering, Johan Kwisthout, Colin Jacobs, Lena Philipp, Fennie van der Graaf</p>
</li>
<li class="">
<p><strong>institution:</strong> Radboud University Medical Center, Radboud University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22242" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22242</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a fairness evaluation of three lung cancer risk estimation models (Sybil, Venkadesh21, PanCan2b) using the JustEFAB framework to assess ethically significant biases. 2. Identified and quantified statistically significant performance disparities across demographic subgroups (e.g., gender, race) that were not explained by available clinical confounders. 3. Highlighted the critical need for monitoring and improving model fairness in lung cancer screening AI to ensure equitable clinical application.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study evaluates the fairness of AI models for lung cancer risk estimation from CT scans. Using the JustEFAB framework, it assessed performance disparities across demographic groups and found significant, unexplained biases in two deep learning models. The findings underscore the importance of algorithmic fairness in medical AI to ensure equitable screening outcomes.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Masking Teacher and Reinforcing Student for Distilling Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [knowledge distillation, reinforcement learning, vision-language models, progressive masking, offline RL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Byung-Kwan Lee, Yu-Chiang Frank Wang, Ryo Hachiuma</p>
</li>
<li class="">
<p><strong>institution:</strong> NVIDIA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22238" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22238</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Masters, a mask-progressive RL distillation framework that first masks non-dominant teacher weights to reduce complexity and then progressively restores them for stable student learning. 2. Introduces an offline RL stage with complementary accuracy and distillation rewards, leveraging pre-generated responses from masked teachers for efficient guidance. 3. Demonstrates that progressive teacher scaling (e.g., from 14B to 38B) yields smoother convergence and stronger generalization than one-shot distillation, providing a scalable path to efficient VLMs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of distilling large vision-language models (VLMs) into compact ones by proposing Masters, a framework that uses progressive masking of the teacher model and offline reinforcement learning. This method enables stable knowledge transfer and efficient training, resulting in small VLMs that achieve strong performance, sometimes surpassing larger models, while being far more efficient for deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [model compression], [knowledge distillation, lightweight CNN, inverted residual blocks, dense connectivity, multi-objective learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Phi-Hung Hoang, Nam-Thuan Trinh, Van-Manh Tran, Thi-Thu-Hong Phan</p>
</li>
<li class="">
<p><strong>institution:</strong> FPT University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22239" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22239</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hybrid knowledge distillation framework integrating hard-label supervision, feature-level, response-level, and self-distillation for training efficient models. 2. Designs a customized student CNN architecture combining inverted residual blocks with dense connectivity to balance efficiency and accuracy. 3. Demonstrates strong generalization across multiple agricultural datasets (rice seeds and plant leaf diseases) with significant reductions in computational cost and model size while maintaining high accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccaf949c91086899f4aa9953236d8ee76957b0a5aaafcda22bf81f403ee1e0a5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccaf949c91086899f4aa9953236d8ee76957b0a5aaafcda22bf81f403ee1e0a5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a multi-objective hybrid knowledge distillation method to create a lightweight CNN for smart agriculture, combining inverted residual and dense blocks. The distilled model achieves near-teacher accuracy with drastically reduced computation and parameters, showing robust performance on rice seed and plant disease datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human motion segmentation], [temporal vision semantics, subspace clustering, large language model, temporal regularizer, feedback framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zheng Xing, Weibing Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Shenzhen University, Shenzhen MSU-BIT University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22249" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22249</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel method to learn Temporal Vision Semantics (TVS) from human motion sequences by querying a Large Language Model (LLM) to determine motion consistency between consecutive frames. 2. Develops a TVS-integrated subspace clustering framework that incorporates a temporal regularizer and constraint to enforce similarity among temporal neighbors in the subspace embedding and segmentation. 3. Introduces a feedback-enabled optimization framework that iteratively refines the subspace embedding based on the segmentation output to improve performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fce87220cc17109bbd9138b27d5aa353003bb134fb0924f68b7fc59fdfd3ee0d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fce87220cc17109bbd9138b27d5aa353003bb134fb0924f68b7fc59fdfd3ee0d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of unsupervised human motion segmentation by integrating temporal semantic information into subspace clustering. The proposed method uses a Large Language Model to extract textual motion descriptions from consecutive frames and incorporates this learned temporal neighbor information as constraints within the clustering framework. Experimental results show the method outperforms state-of-the-art approaches on four benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Human-Aligned Generative Perception: Bridging Psychophysics and Generative Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [generative models], [text-to-image diffusion, geometric control, human perception embedding, latent guidance, teacher model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Antara Titikhsha, Om Kulkarni, Dharun Muthaiah</p>
</li>
<li class="">
<p><strong>institution:</strong> Carnegie Mellon University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22272" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22272</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/omkul22/16824-Project-Human-Aligned-Generative-Perception" target="_blank" rel="noopener noreferrer" class="">https://github.com/omkul22/16824-Project-Human-Aligned-Generative-Perception</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposing a Human Perception Embedding (HPE) teacher trained on human similarity data to capture geometry-sensitive features. 2. Introducing a model-agnostic latent guidance framework for steering various generative architectures. 3. Demonstrating improved geometric control and semantic alignment in text-to-image synthesis without specialized training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da7d63c4f85d42c740b13906960a8e8749c9cb29eda218682db4a1db49adc38f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da7d63c4f85d42c740b13906960a8e8749c9cb29eda218682db4a1db49adc38f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem where text-to-image models prioritize style over geometric constraints. The proposed method uses a lightweight &quot;teacher&quot; model, trained on human perceptual data, to guide the diffusion process towards desired shapes. The results show that this approach significantly improves geometric control across different model architectures without requiring retraining.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GeCo: A Differentiable Geometric Consistency Metric for Video Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation evaluation], [geometric consistency, video generation, differentiable metric, depth prior, motion prior]</p>
</li>
<li class="">
<p><strong>authors:</strong> Leslie Gu, Junhwa Hur, Charles Herrmann, Fangneng Zhan, Todd Zickler, Deqing Sun, Hanspeter Pfister</p>
</li>
<li class="">
<p><strong>institution:</strong> Harvard University, Google DeepMind, Massachusetts Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22274" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22274</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GeCo, a novel differentiable metric that fuses motion and depth priors to detect geometric deformation and occlusion inconsistency in generated videos. 2. Introduces two synthetic datasets, WarpBench and OccluBench, to validate the metric&#x27;s performance on isolated artifacts. 3. Demonstrates GeCo&#x27;s dual utility for benchmarking state-of-the-art video generation models and as a training-free guidance loss to reduce geometric artifacts during generation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e4a287254d3ac04a1aa636a703d4f551fc0d8e920f903da7f174ef77ae6203f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e4a287254d3ac04a1aa636a703d4f551fc0d8e920f903da7f174ef77ae6203f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces GeCo, a differentiable metric for evaluating geometric consistency in generated videos by combining motion and depth cues. It validates the metric on new synthetic datasets and uses it to benchmark models and improve video generation by reducing artifacts as a guidance loss.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [RGB-LWIR fusion, multispectral imagery, YOLO, adaptive framework, illumination conditions]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aahan Sachdeva, Dhanvinkumar Ganeshkumar, James E. Gallagher, Tyler Treat, Edward J. Oughton</p>
</li>
<li class="">
<p><strong>institution:</strong> George Mason University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22263" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22263</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An adaptive framework that dynamically selects the optimal RGB-LWIR fusion ratio and detection model based on real-time illumination conditions. 2. Creation of a comprehensive dataset and model set, training 33 YOLO models on over 22,000 annotated images across three light levels with eleven fusion ratios. 3. Demonstrated significant performance improvements over RGB-only and thermal-only baselines, particularly in full-light and dim-light conditions, enhancing detection reliability for autonomous systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of RGB and thermal-only object detection in variable lighting by proposing an adaptive framework that fuses RGB and LWIR video streams at multiple ratios and selects the best model for the current illumination. The method, evaluated on a large dataset, showed that optimized fusion models significantly outperformed baseline models in full and dim light, improving detection confidence and reliability for autonomous robotic vision.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FETAL-GAUGE: A Benchmark for Assessing Vision-Language Models in Fetal Ultrasound</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical imaging], [vision-language models, fetal ultrasound, visual question answering, benchmark, multimodal learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hussain Alasmawi, Numan Saeed, Mohammad Yaqub</p>
</li>
<li class="">
<p><strong>institution:</strong> Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22278" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22278</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Fetal-Gauge, the first and largest visual question answering benchmark for evaluating Vision-Language Models (VLMs) in fetal ultrasound, comprising over 42,000 images and 93,000 question-answer pairs. 2. Systematically evaluates state-of-the-art general-purpose and medical-specific VLMs, revealing a substantial performance gap where the best model achieves only 55% accuracy. 3. Identifies critical limitations of current VLMs in this domain and establishes a foundation for advancing domain-adapted architectures and specialized training for prenatal care.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4890359c1dc1ee5f0dc6fae45f7bab8696f5048a1a22998019c7d37eab29b5da_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4890359c1dc1ee5f0dc6fae45f7bab8696f5048a1a22998019c7d37eab29b5da_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of a standardized benchmark for evaluating Vision-Language Models (VLMs) in fetal ultrasound by introducing Fetal-Gauge, a large-scale visual question answering dataset. The authors evaluate multiple VLMs and find their performance (max 55% accuracy) is far below clinical requirements, highlighting the urgent need for specialized models in this medical domain.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal reasoning], [clinical reasoning benchmark, vision-language models, multimodal integration, medical image interpretation, hallucination]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dingyu Wang, Zimu Yuan, Jiajun Liu, Shanggui Liu, Nan Zhou, Tianxing Xu, Di Huang, Dong Jiang</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University Third Hospital, Beihang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22275" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22275</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced the Bones and Joints (B&amp;J) Benchmark, a comprehensive evaluation framework with 1,245 questions derived from real-world patient cases to assess clinical reasoning. 2. Revealed a significant performance gap in VLMs, showing high accuracy on structured tasks but poor performance on open-ended, multimodal reasoning tasks, with severe text-driven hallucinations. 3. Demonstrated that medically fine-tuned models show no consistent advantage over general-purpose models, highlighting a fundamental limitation in current AI for clinical competency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43db8495c9ac46c5ea892f723394bd1e6c9b400003c2c67ace7ac9abe26f0bcf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43db8495c9ac46c5ea892f723394bd1e6c9b400003c2c67ace7ac9abe26f0bcf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces the Bones and Joints (B&amp;J) Benchmark to rigorously evaluate the clinical reasoning capabilities of vision-language and large language models. The results show that while models perform well on structured tasks, they struggle significantly with open-ended, multimodal reasoning essential for real-world patient care, indicating they are not yet clinically competent. The authors conclude that safe AI deployment should be limited to supportive roles until fundamental breakthroughs in multimodal integration are achieved.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [Masked Diffusion Models, Markov Decision Process, Group Relative Policy Optimization, inference schedule optimization, trajectory-level training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Renping Zhou, Zanlin Ni, Tianyi Chen, Zeyu Liu, Yang Yue, Yulin Wang, Yuxuan Wang, Jingshu Liu, Gao Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University (Leap Lab), Anyverse Dynamics</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22288" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22288</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://co-grpo.github.io" target="_blank" rel="noopener noreferrer" class="">https://co-grpo.github.io</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and addresses the discrepancy between the single-step training and multi-step inference of Masked Diffusion Models (MDMs). 2. Proposes Co-GRPO, a method that reformulates MDM generation as a unified Markov Decision Process to jointly optimize model parameters and inference schedule parameters. 3. Introduces a trajectory-level optimization using Group Relative Policy Optimization that avoids costly backpropagation through the multi-step generation process.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the misalignment between the training and inference procedures of Masked Diffusion Models (MDMs). It proposes Co-GRPO, a method that jointly optimizes the MDM and its inference schedule as a unified Markov Decision Process using Group Relative Policy Optimization. The approach improves generation quality across multiple benchmarks without requiring expensive backpropagation through the full generation trajectory.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal retrieval and generation], [3D retrieval, 4D generation, cross-modal alignment, multi-head attention, open-vocabulary]</p>
</li>
<li class="">
<p><strong>authors:</strong> Philip Xu, David Elizondo, Raouf Hamzaoui</p>
</li>
<li class="">
<p><strong>institution:</strong> De Montfort University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22294" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22294</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Uni4D, a unified framework for large-scale open-vocabulary 3D retrieval and controlled 4D generation. 2. Introduces a structured three-level alignment strategy across text, 3D models, and images to enhance semantic understanding. 3. Presents a 3D-Text Multi-head Attention and Search (ATMS) model to optimize text-to-3D retrieval efficiency and accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb826b31ecaac1f8358869614a5af4e6a0fe9eeceb1eb97ce8bbb0ff8afdcd6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb826b31ecaac1f8358869614a5af4e6a0fe9eeceb1eb97ce8bbb0ff8afdcd6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Uni4D, a framework that uses a three-level alignment strategy across text, 3D, and images to address the challenges of large-scale 3D retrieval and controlled 4D generation. The method employs a novel attention and search model to improve semantic alignment and retrieval efficiency. Experimental results demonstrate that Uni4D achieves high-quality 3D retrieval and controllable 4D generation, advancing dynamic multimodal understanding.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Learning Dynamic Scene Reconstruction with Sinusoidal Geometric Priors</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D scene reconstruction], [SirenPose, sinusoidal representation networks, geometric priors, spatiotemporal consistency, dynamic 3D reconstruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tian Guo, Hui Yuan, Philip Xu, David Elizondo</p>
</li>
<li class="">
<p><strong>institution:</strong> De Montfort University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22295" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22295</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel loss function &quot;SirenPose&quot; that combines periodic activation from SIREN networks with geometric priors from keypoint structures. 2. Introduces physics-inspired constraint mechanisms to enforce coherent keypoint predictions across spatial and temporal dimensions. 3. Expands the training dataset to 600,000 annotated instances to support robust learning and demonstrates significant improvements in spatiotemporal consistency metrics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09fea4b3f99fd0198624beda8099c963456afad518a9c74cf1532ea8667df82e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09fea4b3f99fd0198624beda8099c963456afad518a9c74cf1532ea8667df82e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of maintaining motion accuracy and spatiotemporal consistency in dynamic 3D scene reconstruction from monocular videos. It proposes a novel loss function called SirenPose, which integrates sinusoidal representation networks with geometric keypoint priors and physics-based constraints. Experiments show that models using SirenPose achieve superior performance in handling rapid motion and complex scene changes compared to prior methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human activity recognition], [driver monitoring systems, edge AI, quantization, temporal decision head, confounder-aware labeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vesal Ahsani, Babak Hossein Khalaj</p>
</li>
<li class="">
<p><strong>institution:</strong> Sharif University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22298" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22298</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A deployable single-camera driver behavior recognition pipeline optimized for low-cost edge hardware (Raspberry Pi 5 and Google Coral Edge TPU). 2. A confounder-aware label design to reduce false positives from visually similar actions. 3. A temporal decision head that generates stable alerts based on sustained, confident predictions rather than noisy per-frame outputs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bd52e06dc77080ab346fc3d6fe46b900bf06b5c1eaeb6ae89801ac125ee7c51_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bd52e06dc77080ab346fc3d6fe46b900bf06b5c1eaeb6ae89801ac125ee7c51_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a real-time driver behavior recognition system designed for low-cost edge hardware to address the challenges of compute, power, and cost constraints in vehicles. The method combines a compact vision model, confounder-aware labeling, and a temporal decision head to recognize 17 distraction and drowsiness-related behaviors. The optimized system achieves 16 FPS on a Raspberry Pi 5 and 25 FPS on a Coral Edge TPU, enabling practical deployment for in-cabin monitoring.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Attack-Aware Deepfake Detection under Counter-Forensic Manipulations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [deepfake detection], [counter-forensics, red-team training, test-time defense, two-stream architecture, tamper heatmaps]</p>
</li>
<li class="">
<p><strong>authors:</strong> Noor Fatima, Hasan Faraz Khan, Muzammil Behzad</p>
</li>
<li class="">
<p><strong>institution:</strong> King Fahd University of Petroleum and Minerals (KFUPM), SDAIA-KFUPM Joint Research Center for Artificial Intelligence</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22303" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22303</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an attack-aware deepfake detection method combining red-team training with randomized test-time defense for robustness against counter-forensic manipulations. 2. Introduces a two-stream architecture with a lightweight residual adapter for fusing semantic and forensic features, and a weakly supervised FPN-style head for generating tamper heatmaps. 3. Establishes a practical, modular, and data-efficient baseline with well-calibrated probabilities and actionable evidence, evaluated on standard and challenging surveillance-style datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45e4389b2e929ec3ecb92d5f6329f2086fd32a88abcf98391c61119cd497cc8f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45e4389b2e929ec3ecb92d5f6329f2086fd32a88abcf98391c61119cd497cc8f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of robust deepfake detection under realistic counter-forensic attacks. It proposes a two-stream model trained with worst-case adversarial manipulations and defended at test-time with random jitters, which achieves strong performance, reliable probability calibration, and useful localization heatmaps. The method provides a practical and data-efficient baseline for attack-aware detection in real-world conditions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PortionNet: Distilling 3D Geometric Knowledge for Food Nutrition Estimation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [food volume and nutrition estimation], [knowledge distillation, cross-modal learning, point cloud, RGB-to-Geometry Adapter, dual-mode training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Darrin Bright, Rakshith Raj, Kanchan Keisham</p>
</li>
<li class="">
<p><strong>institution:</strong> Vellore Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22304" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22304</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed PortionNet, a cross-modal knowledge distillation framework that enables RGB models to learn 3D geometric features from point clouds during training, eliminating the need for depth sensors at inference. 2. Introduced a dual-mode training strategy with a lightweight RGB-to-Geometry Adapter that learns to generate pseudo-3D features from standard RGB images. 3. Achieved state-of-the-art performance on MetaFood3D for volume and energy estimation and demonstrated strong generalization on SimpleFood45.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fecf7cb96cd72089359940c6705a9ff3efce7eba9810d635e1bbc23891d97d05_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fecf7cb96cd72089359940c6705a9ff3efce7eba9810d635e1bbc23891d97d05_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of estimating food nutrition from a single RGB image, which lacks 3D information. It proposes PortionNet, a framework that uses knowledge distillation to teach an RGB model geometric reasoning from point cloud data during training, requiring only RGB images at test time. The method achieves state-of-the-art accuracy on benchmark datasets, demonstrating effective 3D knowledge transfer without specialized hardware.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MoFu: Scale-Aware Modulation and Fourier Fusion for Multi-Subject Video Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [multi-subject video generation, scale-aware modulation, fourier fusion, permutation invariance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Run Ling, Ke Cao, Jian Lu, Ao Ma, Haowei Liu, Runze He, Changwei Wang, Rongtao Xu, Yihua Shao, Zhanjie Zhang, Peng Wu, Guibing Guo, Wei Feng, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law, Xingwei Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> JD.com, Inc., Northeastern University, University of Science and Technology of China, Chongqing University of Post and Telecommunications, University of Chinese Academy of Sciences, Northwestern Polytechnical University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22310" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22310</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Scale-Aware Modulation (SMO), an LLM-guided module to extract implicit scale cues from text prompts to address scale inconsistency. 2. Introduces a Fourier Fusion strategy using Fast Fourier Transform to process reference features for permutation-invariant generation. 3. Designs a dedicated benchmark and a Scale-Permutation Stability Loss to evaluate and jointly optimize for scale consistency and permutation invariance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb2e690ce0802d8b6f90a803d45abda742cd1de636984d46b6cf4117642746da_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb2e690ce0802d8b6f90a803d45abda742cd1de636984d46b6cf4117642746da_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes MoFu, a framework for multi-subject video generation that tackles scale inconsistency and permutation sensitivity. It introduces a Scale-Aware Modulation module and a Fourier Fusion strategy, validated by a new benchmark and a stability loss. Experiments show MoFu outperforms existing methods in preserving natural scale, subject fidelity, and visual quality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LangPrecip: Language-Aware Multimodal Precipitation Nowcasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [weather forecasting], [multimodal nowcasting, rectified flow, semantic motion constraint, latent space integration, large-scale dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xudong Ling, Tianxi Huang, Qian Dong, Tao He, Chaorong Li, Guiduo Duan</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China (UESTC), Chengdu Textile College, Yibin University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22317" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22317</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed LangPrecip, a language-aware multimodal nowcasting framework that uses meteorological text as a semantic motion constraint to guide precipitation evolution. 2. Introduced LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. 3. Formulated nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm for efficient and physically consistent multimodal integration.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes LangPrecip, a novel framework that integrates meteorological text descriptions with radar data to constrain precipitation nowcasting. By formulating the problem as semantically constrained trajectory generation using Rectified Flow, it achieves significant performance gains, especially for heavy rainfall at long lead times, as demonstrated on Swedish and MRMS datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video understanding], [agentic framework, temporal zoom, reinforcement learning, long video reasoning, multimodal large language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yang Ding, Yizhen Zhang, Xin Lai, Ruihang Chu, Yujiu Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, The Chinese University of Hong Kong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22315" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22315</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/zsgvivo/VideoZoomer" target="_blank" rel="noopener noreferrer" class="">https://github.com/zsgvivo/VideoZoomer</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control visual focus during reasoning for long videos. 2. Introduces a two-stage training strategy combining supervised fine-tuning on distilled trajectories with reinforcement learning to refine the agentic policy. 3. Demonstrates strong performance across long video benchmarks, surpassing open-source models and rivaling proprietary systems with superior efficiency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab9b4b645f4fff1b4f548256b858cb41da2b35db78b1083f110e983d60c3547e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab9b4b645f4fff1b4f548256b858cb41da2b35db78b1083f110e983d60c3547e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the limitation of Multimodal LLMs in understanding long videos due to context window constraints. It proposes VideoZoomer, an agentic framework that dynamically selects and zooms into key temporal moments for fine-grained evidence gathering, trained with a two-stage strategy. The resulting 7B model achieves state-of-the-art performance on long video reasoning benchmarks with high efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SpotEdit: Selective Region Editing in Diffusion Transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [Diffusion Transformers, selective region editing, training-free, perceptual similarity, dynamic fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhibin Qin, Zhenxiong Tan, Zeqing Wang, Songhua Liu, Xinchao Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Singapore, Shanghai Jiao Tong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22323" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22323</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://biangbiang0321.github.io/SpotEdit.github.io/" target="_blank" rel="noopener noreferrer" class="">https://biangbiang0321.github.io/SpotEdit.github.io/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a training-free framework (SpotEdit) for selective region editing in Diffusion Transformers, reducing redundant computation. 2. Introduces SpotSelector to identify stable, unmodified regions via perceptual similarity and skip their denoising. 3. Introduces SpotFusion to adaptively blend reused conditional features with edited tokens, preserving coherence and quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4788068dfc021eb102e739694d672f72a617b80ada2a17fbe2ccbc2780fc1287_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4788068dfc021eb102e739694d672f72a617b80ada2a17fbe2ccbc2780fc1287_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the inefficiency of full-image regeneration in diffusion-based editing when only small regions need modification. It proposes SpotEdit, a training-free framework that selectively updates only modified regions using a selector for stable areas and a fusion mechanism for coherence. This approach reduces computation and maintains fidelity in unedited areas for efficient, precise editing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [self-verifying agent, proactive evidence seeking, LLM-as-a-Judge, 3C Principles, agentic reinforcement learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University, Tencent</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22322" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22322</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://huggingface.co/collections/yolay/smartsnap" target="_blank" rel="noopener noreferrer" class="">https://huggingface.co/collections/yolay/smartsnap</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed SmartSnap, a paradigm shift from passive, post-hoc task verification to proactive, in-situ self-verification by the agent itself. 2. Introduced the Self-Verifying Agent, a new agent type with dual missions to complete tasks and prove accomplishment via curated snapshot evidences guided by 3C Principles (Completeness, Conciseness, Creativity). 3. Demonstrated that the SmartSnap paradigm enables scalable training of LLM-driven agents, achieving significant performance gains (up to 26.08% and 16.66%) on mobile tasks and competitive results against larger models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the scalability bottleneck in agentic RL caused by costly and unreliable post-hoc task verification. It proposes SmartSnap, a paradigm where agents proactively seek minimal, decisive snapshot evidence to prove task completion during execution, guided by 3C Principles. Experiments show this approach significantly improves agent performance and enables scalable training, achieving competitive results with much larger models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] DeMoGen: Towards Decompositional Human Motion Generation with Energy-Based Diffusion Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human motion generation], [energy-based diffusion model, motion decomposition, compositional training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jianrong Zhang, Hehe Fan, Yi Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Technology Sydney, Zhejiang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22324" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22324</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://jiro-zhang.github.io/DeMoGen/" target="_blank" rel="noopener noreferrer" class="">https://jiro-zhang.github.io/DeMoGen/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DeMoGen, a compositional training paradigm using an energy-based diffusion model to decompose holistic motions into semantic sub-components without needing ground-truth for individual concepts., 2. Introduces three training variants (DeMoGen-Exp, DeMoGen-OSS, DeMoGen-SC) to encourage decompositional understanding and disentangle reusable motion primitives., 3. Constructs a text-decomposed dataset to support compositional training and demonstrates that decomposed concepts can be recombined to generate novel motions beyond the training distribution.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3a4e4c44b030f7c77d17ff4917930237054b6fbd2ff7b40464abc4563cf33f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3a4e4c44b030f7c77d17ff4917930237054b6fbd2ff7b40464abc4563cf33f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of decomposing holistic human motions into simpler, reusable primitives. It proposes DeMoGen, an energy-based diffusion model training paradigm with three variants to learn this decomposition without individual concept supervision. The method successfully disentangles motion concepts, which can then be flexibly recombined to generate diverse and novel motions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [multi-view learning, variational autoencoder, latent representation learning, radiomics, glioblastoma]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mariya Miteva, Maria Nisheva-Pavlova</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in provided content.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22331" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22331</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a multi-view latent representation learning framework based on VAEs for integrating complementary MRI radiomic features. 2. Introduced independent probabilistic encoders for each modality to preserve modality-specific structure before fusion in a compact latent space. 3. Applied the learned latent embeddings for the non-invasive classification of MGMT promoter methylation status in glioblastoma.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bde275b3d90b700f19b613a2539cb5c16f7fb3637de09a820e24738011c2f8da_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bde275b3d90b700f19b613a2539cb5c16f7fb3637de09a820e24738011c2f8da_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of non-invasively predicting MGMT promoter methylation in glioblastoma from MRI scans. It proposes a multi-view framework using variational autoencoders to integrate features from T1Gd and FLAIR MRI sequences by fusing them in a latent space, aiming to better preserve modality-specific information. The resulting latent embeddings are used for classification, offering a potential improvement over conventional unimodal or early-fusion radiomics approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D scene understanding and manipulation], [Multimodal Large Language Models (MLLMs), 3D object arrangement, tool-augmented agents, MCP-based API, multi-agent framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhengfei Kuang, Rui Lin, Long Zhao, Gordon Wetzstein, Saining Xie, Sanghyun Woo</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University, Google, Google DeepMind, New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22351" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22351</a></p>
</li>
<li class="">
<p><strong>code:</strong> vulcan-3d.github.io</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced an MCP-based API to shift interaction from raw code to robust function-level updates, addressing MLLMs&#x27; weak visual grounding in 3D. 2. Augmented MLLMs with specialized visual tools for scene analysis, spatial information gathering, and action validation, creating a perceptual feedback loop. 3. Proposed a collaborative multi-agent framework with designated planning, execution, and verification roles to manage iterative, error-prone updates in complex tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79364171e71fa2e99be3aaae7f42a6f8bb502acdf59cc5033e98f9a1d424f8bb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79364171e71fa2e99be3aaae7f42a6f8bb502acdf59cc5033e98f9a1d424f8bb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the underexplored challenge of applying Multimodal Large Language Models (MLLMs) to complex 3D object arrangement. The proposed VULCAN system uses an MCP-based API, a suite of visual tools, and a multi-agent framework to enable robust, iterative 3D scene manipulation. The approach significantly outperforms baselines on a diverse set of 25 complex arrangement tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [vision transformer, whole slide image, HER2 scoring, multi-modality, tumor classification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Olaide N. Oyelade, Oliver Hoxey, Yulia Humrye</p>
</li>
<li class="">
<p><strong>institution:</strong> North Carolina A&amp;T State University, University of Chichester, Yale University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22335" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22335</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a novel mapping function to correlate malignant regions in H&amp;E whole slide images with corresponding regions in IHC images for joint analysis. 2. Developed an end-to-end pipeline using a multi-stage vision transformer system for automatic pixel-level annotation of 4-way HER2 status scoring (0, 1+, 2+, 3+). 3. Embedded a clinically inspired HER2 scoring mechanism that accurately classifies HER2-negative and HER2-positive cases from whole slide images.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03d448dc8fb7520382bb6ea1a3e317817181ebbce215c0d5c387ed2268b2f407_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03d448dc8fb7520382bb6ea1a3e317817181ebbce215c0d5c387ed2268b2f407_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an end-to-end pipeline using multi-stage vision transformers to jointly analyze H&amp;E and IHC whole slide images for HER2 status scoring and tumor classification. The method introduces a novel mapping function to align modalities and provides pixel-level HER2 scoring. The results demonstrate high accuracy (0.94) for HER2 status prediction, showing the method&#x27;s effectiveness comparable to human pathologists.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [pseudo-colouring, few-shot learning, prototypical networks, ResNet-18, explainability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alaa Alahmadi, Mohamed Hasan</p>
</li>
<li class="">
<p><strong>institution:</strong> Newcastle University, University of Leeds</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22349" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22349</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a perception-informed pseudo-colouring technique to encode clinically salient temporal ECG features (like QT-interval) into structured colour representations. 2. Demonstrates that this technique enables effective few-shot and one-shot learning for a complex physiological data task (drug-induced LQTS) using prototypical networks and ResNet-18. 3. Shows that the method improves model explainability by guiding attention to clinically meaningful features and that aggregating multiple cardiac cycles (mirroring human perceptual averaging) further boosts performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problems of data inefficiency and poor interpretability in deep learning models for physiological signal analysis. It proposes a human-inspired pseudo-colouring technique to encode ECG features, enabling effective few-shot learning and improving model explainability by focusing on clinically relevant signal components. The results demonstrate that incorporating human-like perceptual encoding can bridge data efficiency and interpretability in medical AI.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Self-Evaluation Unlocks Any-Step Text-to-Image Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [text-to-image generation, flow matching, self-evaluation, any-step inference, from-scratch training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xin Yu, Xiaojuan Qi, Zhengqi Li, Kai Zhang, Richard Zhang, Zhe Lin, Eli Shechtman, Tianyu Wang, Yotam Nitzan</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Hong Kong, Adobe Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22374" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22374</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Self-Evaluating Model (Self-E), a novel from-scratch training framework that combines local flow matching with a self-evaluation mechanism, eliminating the need for a pretrained teacher model. 2. Enables &quot;any-step&quot; inference, allowing the same model to perform both high-quality few-step and many-step generation, bridging the gap between local supervision and global matching paradigms. 3. Demonstrates competitive performance with state-of-the-art flow matching models at high step counts while excelling at very low step counts, offering a unified and scalable solution.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that traditional diffusion/flow models require many inference steps, and distillation methods need a pretrained teacher. It proposes Self-E, a model trained from scratch that uses self-evaluation as a dynamic teacher to enable high-quality generation at any number of steps. The results show Self-E excels at few-step generation and is competitive at many steps, providing a unified framework for efficient and scalable text-to-image synthesis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [few-shot learning], [exemplar selection, large language model, human activity recognition, facility-location optimization, PageRank]</p>
</li>
<li class="">
<p><strong>authors:</strong> Elsen Ronando, Sozo Inoue</p>
</li>
<li class="">
<p><strong>institution:</strong> Kyushu Institute of Technology, Universitas 17 Agustus 1945 Surabaya</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22385" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22385</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an LLM-Guided Exemplar Selection framework that incorporates semantic reasoning via LLM-generated knowledge priors (feature importance, inter-class confusability, budget multipliers) for HAR. 2. Integrates these semantic priors with multiple geometric and structural cues (margin-based validation, PageRank centrality, hubness penalization, facility-location optimization) for a unified exemplar scoring and selection process. 3. Demonstrates superior performance (88.78% macro F1-score on UCI-HAR) under strict few-shot conditions compared to classical selection methods like random sampling, herding, and k-center.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90a149428a6ff84d38e1ab6a741982394b05c9d5b98eaaa538dcd12183dbe7bf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90a149428a6ff84d38e1ab6a741982394b05c9d5b98eaaa538dcd12183dbe7bf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of relying on large labeled datasets and purely geometric exemplar selection in Human Activity Recognition (HAR) by proposing an LLM-Guided Exemplar Selection framework. The method uses an LLM to generate semantic knowledge priors, which are combined with structural and geometric cues to select a compact, informative set of exemplars for few-shot learning. Evaluated on the UCI-HAR dataset, the framework outperforms classical selection approaches, showing that integrating semantic reasoning improves representative exemplar selection for wearable-sensor HAR.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] iOSPointMapper: RealTime Pedestrian and Accessibility Mapping with Mobile AI</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [semantic segmentation], [on-device AI, LiDAR depth estimation, fused GPS/IMU, sidewalk mapping, accessibility]</p>
</li>
<li class="">
<p><strong>authors:</strong> Himanshu Naidu, Yuxiang Zhang, Sachin Mehta, Anat Caspi</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Washington</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22392" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22392</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces iOSPointMapper, a mobile app for real-time, privacy-conscious sidewalk mapping using iPhones/iPads. 2. Leverages on-device semantic segmentation, LiDAR depth, and fused GPS/IMU to detect and localize sidewalk features like signs and poles. 3. Incorporates a user-guided annotation interface for validation and integrates collected data with the Transportation Data Exchange Initiative (TDEI).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7ffc517e9052bb4ade596ae33e68c84c5942243e638f1f15055e1560eb0b6f0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7ffc517e9052bb4ade596ae33e68c84c5942243e638f1f15055e1560eb0b6f0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the costly and fragmented collection of sidewalk data by proposing iOSPointMapper, a mobile application that uses on-device AI, LiDAR, and sensor fusion for real-time detection and mapping of pedestrian infrastructure. The system includes a user validation interface and integrates data into a broader transportation dataset. Evaluations show its potential for scalable and enhanced pedestrian mapping.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] DeFloMat: Detection with Flow Matching for Stable and Efficient Generative Object Localization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [Conditional Flow Matching, Generative Object Detection, Rectified Flow, Ordinary Differential Equation, Magnetic Resonance Enterography]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hansang Lee, Chaelin Lee, Nieun Seo, Joon Seok Lim, Helen Hong</p>
</li>
<li class="">
<p><strong>institution:</strong> Seoul Women&#x27;s University, Severance Hospital, Yonsei University College of Medicine</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22406" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22406</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DeFloMat, a novel generative object detection framework that replaces the slow multi-step stochastic denoising of diffusion models with a fast, deterministic flow field based on Conditional Flow Matching and Rectified Flow. 2. Achieves state-of-the-art accuracy with only 3 inference steps, significantly outperforming prior diffusion-based detectors in speed and performance on a clinical MRE dataset. 3. Demonstrates superior localization stability and recall in the few-step regime, effectively resolving the trade-off between generative accuracy and inference efficiency for time-sensitive applications.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a84d309b1834782fd71a3208d1783483482053483ffa20811f690fd91c6683d9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a84d309b1834782fd71a3208d1783483482053483ffa20811f690fd91c6683d9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high latency of diffusion-based object detectors, which is impractical for clinical use. It proposes DeFloMat, a new framework that uses Conditional Flow Matching to create a deterministic flow for fast, stable object localization via an ODE solver. The method achieves superior accuracy with only 3 inference steps on a medical imaging dataset, setting a new standard for efficient generative detection.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [hyperspherical learning, native sparse attention, anisotropic patch embed]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amil Khan, Matheus Palhares Viana, Suraj Mishra, B.S. Manjunath</p>
</li>
<li class="">
<p><strong>institution:</strong> UC Santa Barbara, Allen Institute for Cell Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22423" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22423</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A 4B-parameter foundation model (Bright-4B) that learns on the unit hypersphere for segmenting subcellular structures directly from 3D brightfield volumes. 2. Novel architectural components: a hardware-aligned Native Sparse Attention mechanism, depth-width residual HyperConnections, and a soft Mixture-of-Experts for adaptive capacity. 3. A plug-and-play anisotropic patch embed that respects confocal point-spread and axial thinning for geometry-faithful 3D tokenization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces Bright-4B, a 4-billion parameter foundation model designed for volumetric segmentation of subcellular structures directly from label-free 3D brightfield microscopy images. It employs novel architectural components like hyperspherical learning, native sparse attention, and an anisotropic patch embed to handle 3D context and anisotropic sampling. The model outperforms contemporary baselines in preserving fine structural detail across depth and cell types without requiring fluorescence or heavy post-processing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [transformer, fluence map prediction, physics-informed loss, two-stage regression, Swin UNETR]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ujunwa Mgboh, Rafi Ibn Sultan, Joshua Kim, Kundan Thind, Dongxiao Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> Wayne State University, Henry Ford Health</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22425" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22425</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed FluenceFormer, a backbone-agnostic transformer framework for direct, geometry-aware fluence map regression. 2. Introduced a unified two-stage design (dose prior prediction followed by geometry-conditioned fluence regression) and the physics-informed Fluence-Aware Regression (FAR) loss. 3. Demonstrated the framework&#x27;s generality across multiple transformer backbones and achieved state-of-the-art performance, significantly reducing energy error.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d0e329d1bfd4c1f892f7333af6a3a4e76c5219cd192f715a25e502a35ef178_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d0e329d1bfd4c1f892f7333af6a3a4e76c5219cd192f715a25e502a35ef178_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces FluenceFormer, a transformer-based framework for automating radiotherapy planning by predicting multi-beam fluence maps. The method uses a two-stage, geometry-aware regression approach with a novel physics-informed loss function. The results show that FluenceFormer outperforms existing methods, achieving a low energy error and improved structural fidelity in fluence map prediction.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [point cloud processing], [Graph Attention Networks, LiDAR reconstruction, beam dropout, gated residual fusion, sparse point cloud]</p>
</li>
<li class="">
<p><strong>authors:</strong> Khalfalla Awedat, Mohamed Abidalrekab, Gurcan Comert, Mustafa Ayad</p>
</li>
<li class="">
<p><strong>institution:</strong> SUNY Morrisville College, Portland State University, North Carolina A&amp;T State University, SUNY Oswego</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22439" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22439</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SuperiorGAT, a novel graph attention-based framework for reconstructing missing elevation data in sparse LiDAR point clouds. 2. Introduces a beam-aware graph modeling approach for LiDAR scans combined with gated residual fusion and feed-forward refinement to achieve accurate reconstruction without increasing network depth. 3. Demonstrates superior performance in reconstruction error and geometric consistency across diverse environments compared to PointNet and deeper GAT baselines, validated through structured beam dropout simulation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68e019420f70e8da99107deedb1af90b7e95ec95b9487f8fc6c872f9994d15e1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68e019420f70e8da99107deedb1af90b7e95ec95b9487f8fc6c872f9994d15e1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of LiDAR beam dropout and sparse resolution in autonomous systems by proposing SuperiorGAT, a graph attention network framework that reconstructs missing elevation information. The method models LiDAR scans as beam-aware graphs and uses gated residual fusion for accurate reconstruction without deeper networks. The results show it achieves lower error and better geometric consistency than baselines, offering a computationally efficient way to improve LiDAR resolution.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] EmoCtrl: Controllable Emotional Image Content Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image generation], [controllable generation, emotion-aware generation, diffusion models, affective computing, multi-modal learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jingyuan Yang, Weibin Luo, Hui Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shenzhen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22437" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22437</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the novel task of Controllable Emotional Image Content Generation (C-EICG), which aims to generate images faithful to a content description while expressing a target emotion. 2. Proposes the EmoCtrl model, which incorporates textual and visual emotion enhancement modules to bridge abstract emotions to visual cues using learned emotion tokens. 3. Constructs a supporting dataset annotated with content, emotion, and affective prompts, and demonstrates through experiments and user studies that EmoCtrl outperforms existing methods in achieving both content faithfulness and expressive emotion control.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1083bd0b74d6006ca00b929998df65fb57106e86c8784a14b3c3bc1e8cc5ce7a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1083bd0b74d6006ca00b929998df65fb57106e86c8784a14b3c3bc1e8cc5ce7a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the gap between content-faithful and emotion-expressive image generation by proposing EmoCtrl, a model for Controllable Emotional Image Content Generation. EmoCtrl uses textual and visual modules to learn emotion tokens that enrich affective expression while maintaining semantic content. Experiments and user studies show it outperforms existing methods in balancing content accuracy and emotional tone.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LECalib: Line-Based Event Camera Calibration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [camera calibration], [event camera, line detection, geometric calibration, non-linear optimization, stereo calibration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zibin Liu, Banglei Guana, Yang Shanga, Zhenbao Yu, Yifei Bian, Qifeng Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Defense Technology, Wuhan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22441" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22441</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Zibin6/line_based_event_camera_calib" target="_blank" rel="noopener noreferrer" class="">https://github.com/Zibin6/line_based_event_camera_calib</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a line-based calibration framework for event cameras that uses geometric lines from common man-made objects, eliminating the need for dedicated flashing patterns or calibration boards. 2. Introduces a method to detect lines directly from raw event streams and leverages an event-line calibration model to generate an initial parameter guess suitable for both planar and non-planar lines. 3. Validates the method&#x27;s feasibility and accuracy through both simulation and real-world experiments on monocular and stereo event camera setups.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbbac4b87dc7bfa86714585a9bb2fe18a9dc39835aac2892d57f5396cbffbfc6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbbac4b87dc7bfa86714585a9bb2fe18a9dc39835aac2892d57f5396cbffbfc6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of time-consuming and manually intensive calibration for event cameras. It proposes LECalib, a framework that calibrates event cameras by detecting geometric lines directly from event streams and using them in a linear initialization and non-linear refinement process. The method is validated as feasible and accurate for both monocular and stereo setups.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [optical-SAR fusion, missing modality, quality-aware fusion, dynamic fusion, orthogonal constraint]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhicheng Zhao, Yuancheng Xu, Andong Lu, Chenglong Li, Jin Tang</p>
</li>
<li class="">
<p><strong>institution:</strong> Anhui University, China Electronics Technology Group Corporation (38th Research Institute)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22447" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22447</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection under missing or degraded modalities. 2. Designed a Dynamic Modality Quality Assessment (DMQA) module that uses learnable reference tokens to iteratively assess feature reliability and identify degraded regions. 3. Developed an Orthogonal Constraint Normalization Fusion (OCNF) module that uses orthogonal constraints to preserve modality independence and dynamically adjust fusion weights based on reliability scores to suppress unreliable features.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69b5db28a2b2a43b3d55d1592c7f26e5ce4421dd707ae3e19282c69c4e894e50_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69b5db28a2b2a43b3d55d1592c7f26e5ce4421dd707ae3e19282c69c4e894e50_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of robust object detection using optical and SAR images when one modality is missing or degraded. The proposed QDFNet method dynamically assesses feature quality and adaptively fuses information using learnable tokens and orthogonal constraints. Experiments show it outperforms other methods, especially when modalities are partially missing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SonoVision: A Computer Vision Approach for Helping Visually Challenged Individuals Locate Objects with the Help of Sound Cues</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [EfficientDet-D2, sound cues, on-device inference, assistive technology, Flutter]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Abu Obaida Zishan, Annajiat Alim Rasel</p>
</li>
<li class="">
<p><strong>institution:</strong> BRAC University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22449" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22449</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/MohammedZ666/SonoVision" target="_blank" rel="noopener noreferrer" class="">https://github.com/MohammedZ666/SonoVision</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed SonoVision, a smartphone application that uses real-time object detection and spatialized sound cues to help visually impaired individuals locate objects independently. 2. Implemented the system using the Flutter framework and the EfficientDet-D2 model, enabling it to function completely offline for safety and user-friendliness. 3. Designed an intuitive auditory interface where object location (left, center, right) is indicated by playing sinusoidal sounds in the corresponding ear(s) of the user&#x27;s headphones.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6b53eaa499e058f61b5d29ec05bfaf1db6d84a82c9c2ee9d18d6a68f4bad882_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6b53eaa499e058f61b5d29ec05bfaf1db6d84a82c9c2ee9d18d6a68f4bad882_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents SonoVision, a smartphone app that helps visually impaired people locate objects by using the EfficientDet-D2 model for real-time detection and providing directional sound cues through headphones. The application is built with Flutter and works offline, aiming to increase user independence and safety. The authors conclude that this approach can significantly assist users in a user-friendly manner.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SAM 3D for 3D Object Reconstruction from Remote Sensing Images</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D reconstruction], [SAM 3D, monocular reconstruction, urban modeling, remote sensing, foundation model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Junsheng Yao, Lichao Mou, Qingyu Li</p>
</li>
<li class="">
<p><strong>institution:</strong> The Chinese University of Hong Kong, Shenzhen; MedAI Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22452" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22452</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Presents the first systematic evaluation of the general-purpose foundation model SAM 3D for monocular remote sensing building reconstruction. 2. Benchmarks SAM 3D against TRELLIS on the NYC urban dataset using FID and CMMD metrics, showing superior roof geometry and boundary sharpness. 3. Extends SAM 3D to urban scene reconstruction via a novel segment-reconstruct-compose pipeline, demonstrating its potential for broader urban modeling.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9750167169c225301e2e37f4bf4b32716990121a9c6c455a260b102eb2835f5c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9750167169c225301e2e37f4bf4b32716990121a9c6c455a260b102eb2835f5c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper evaluates the SAM 3D foundation model for reconstructing 3D buildings from single remote sensing images. It benchmarks SAM 3D against TRELLIS, finding it produces more coherent geometry and sharper boundaries. The work also proposes a pipeline to extend the model for urban scene reconstruction, highlighting its potential and practical limitations for scalable urban modeling.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Comparing Object Detection Models for Electrical Substation Component Mapping</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [YOLOv8, YOLOv11, RF-DETR, substation mapping, computer vision]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haley Mody, Namish Bansal, Dennies Kiprono Bor, Edward J. Oughton</p>
</li>
<li class="">
<p><strong>institution:</strong> None (No affiliations or email domains provided in the given content)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22454" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22454</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Training and comparative evaluation of three state-of-the-art object detection models (YOLOv8, YOLOv11, RF-DETR) on a manually labeled dataset of US electrical substation images. 2. Analysis of model performance based on detection accuracy, precision, and efficiency to identify strengths and limitations for the specific application. 3. Demonstration of a practical use case by utilizing the best-performing model(s) to effectively map substation components across the United States, showcasing autonomous infrastructure assessment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/347825774ed597bd332295edbcb741eaa0167e22f948cf6387f16bda5dec5bd2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/347825774ed597bd332295edbcb741eaa0167e22f948cf6387f16bda5dec5bd2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the labor-intensive problem of manually mapping electrical substation components by training and comparing three computer vision models (YOLOv8, YOLOv11, RF-DETR) on a labeled dataset of US substation images. The models are evaluated on accuracy, precision, and efficiency to determine the most reliable solution for large-scale, autonomous substation component mapping. The research concludes by identifying the best model and demonstrating its application for mapping substation infrastructure across the United States.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [motion generation and editing], [residual vector quantization (RVQ), pose code, transformer, text-to-motion, motion editing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sukhyun Jeong, Yong-Hoon Choi</p>
</li>
<li class="">
<p><strong>institution:</strong> Kwangwoon University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22464" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22464</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/jayze3736/PGR2M" target="_blank" rel="noopener noreferrer" class="">https://github.com/jayze3736/PGR2M</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hybrid motion representation (PGR²M) that augments interpretable pose codes with residual codes learned via RVQ to capture both coarse structure and fine-grained details. 2. Introduces a pose-guided RVQ tokenizer and a two-stage Transformer architecture (base and refine) for generating and refining motion from text. 3. Demonstrates improved performance in generation and editing tasks over baselines through quantitative metrics and user studies, while preserving semantic alignment and editability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fc4d2cdfd610913fc29db703161a253780380e59619b49c5b160c01670eabac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fc4d2cdfd610913fc29db703161a253780380e59619b49c5b160c01670eabac_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of pose-code-based motion generation in capturing subtle temporal dynamics by introducing PGR²M, a hybrid representation combining interpretable pose codes with residual codes via RVQ. The method uses a two-stage Transformer to generate pose codes and then refine them with residual details, conditioned on text. Experiments show it outperforms baselines in both generation and editing while enabling intuitive, structure-preserving motion edits.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Event-based high temporal resolution measurement of shock wave motion field</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [event-based vision], [event cameras, shock wave measurement, motion field reconstruction, asymmetry estimation, polar coordinate encoding]</p>
</li>
<li class="">
<p><strong>authors:</strong> Taihang Lei, Banglei Guan, Minzu Liang, Pengju Sun, Jing Tao, Yang Shang, Qifeng Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Defense Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22474" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22474</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel framework using multiple event cameras to measure shock wave asymmetry with high spatiotemporal resolution. 2. A method involving polar coordinate event encoding and adaptive ROI extraction for revealing propagation patterns. 3. The derivation of a geometric model for event-based shock wave parameter estimation and 3D motion field reconstruction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c33eedd7cdaa2bde1d47f4aaa6a4e3d7b22bc50cf22f53cba8d989485748b0ee_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c33eedd7cdaa2bde1d47f4aaa6a4e3d7b22bc50cf22f53cba8d989485748b0ee_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a method for high-resolution shock wave measurement using multiple event cameras. It establishes a polar coordinate system to encode events, extracts shock fronts via iterative slope analysis, and derives models for parameter estimation and 3D reconstruction. The method achieves high-precision measurements with errors as low as 0.06% compared to pressure sensors, demonstrating significant progress in the field.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Scalpel-SAM: A Semi-Supervised Paradigm for Adapting SAM to Infrared Small Object Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [Segment Anything Model, semi-supervised learning, knowledge distillation, Mixture of Experts, infrared small object detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zihan Liu, Xiangning Ren, Dezhang Kong, Yipeng Zhang, Meng Han</p>
</li>
<li class="">
<p><strong>institution:</strong> Chengdu University, China University of Geosciences (Beijing), Zhejiang University, Zhejiang Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22483" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22483</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a Hierarchical Mixture of Experts (MoE) Adapter to adapt the Segment Anything Model (SAM) to the infrared domain and encode physical priors. 2. Introduced a novel two-stage semi-supervised paradigm (Scalpel-SAM) for knowledge distillation and transfer, requiring only 10% labeled data. 3. Demonstrated that the paradigm enables training lightweight downstream models with pseudo-labels, achieving performance comparable to or surpassing fully supervised models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c7412ec4948ed592cc4d01249cc14bc919c674a295820a0048ad8ccd18def86_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c7412ec4948ed592cc4d01249cc14bc919c674a295820a0048ad8ccd18def86_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the data scarcity and domain gap challenges in Infrared Small Object Detection (IR-SOT) by proposing Scalpel-SAM, a semi-supervised paradigm. It adapts the Segment Anything Model (SAM) using a Hierarchical MoE Adapter and a two-stage knowledge distillation/transfer process, enabling efficient downstream models to be trained with minimal annotations. Experiments show the method achieves performance on par with or better than fully supervised counterparts.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Tracking by Predicting 3-D Gaussians Over Time</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video representation learning], [self-supervised learning, Gaussian splatting, masked autoencoder, point tracking, video understanding]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tanish Baranwal, Himanshu Gaurav Singh, Jathushan Rajasegaran, Jitendra Malik</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Berkeley</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22489" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22489</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/tekotan/video-gmae" target="_blank" rel="noopener noreferrer" class="">https://github.com/tekotan/video-gmae</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Video-GMAE, a novel self-supervised method that learns video representations by encoding a sequence into a set of moving 3D Gaussians, enforcing temporal correspondence as an inductive bias. 2. Discovers that tracking emerges naturally from this pretraining, enabling zero-shot point tracking performance comparable to state-of-the-art methods. 3. Demonstrates superior performance after fine-tuning, achieving significant improvements on Kinetics and Kubric datasets over existing self-supervised video approaches.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f366f3c8e3f11bb196fa35702e462eaeb3b1cbb9de970853f5ab467656a2947c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f366f3c8e3f11bb196fa35702e462eaeb3b1cbb9de970853f5ab467656a2947c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Video-GMAE, a self-supervised learning method that represents a video as a set of 3D Gaussian primitives moving over time. This representation enforces temporal consistency, allowing the model to learn strong correspondences and enabling zero-shot point tracking. The method outperforms existing self-supervised approaches on video understanding and tracking benchmarks after fine-tuning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection via Latent Space Representation Learning and Alignment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [network intrusion detection], [concept drift, latent space alignment, graph neural network (GNN), IoT botnet detection, variational autoencoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hassan Wasswa, Timothy Lynar</p>
</li>
<li class="">
<p><strong>institution:</strong> University of New South Wales</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22488" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22488</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a scalable framework for adaptive IoT threat detection that eliminates the need for continuous classifier retraining, reducing computational overhead and catastrophic forgetting. 2. Introduces a method using latent space representation learning and an alignment model to map new traffic to a historical latent space, preserving knowledge of past attacks. 3. Transforms latent representations into a graph structure and uses a Graph Attention Network (GAT) to capture inter-instance relationships among attack samples for improved detection.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4c1a80d42c79bd3adbbc2c072359068d65253efabadc2bd6d3617f632439f72_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4c1a80d42c79bd3adbbc2c072359068d65253efabadc2bd6d3617f632439f72_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of concept drift in IoT botnet detection by proposing a framework that trains a classifier once on historical traffic representations. It uses an alignment model to map new traffic to this learned latent space and a graph neural network to classify the data, maintaining robust performance without retraining. Experimental results on real-world datasets show the framework&#x27;s effectiveness in dynamic IoT environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [multimodal fusion, BEV perception, coordinate attention, feature alignment, small object detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xin Chen, Kang Luo, Yangyi Xiao, Hesheng Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22503" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22503</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A Section-aware Coordinate Attention (SCA) module to enhance feature discrimination for small, irregular objects. 2. A parameter-efficient Cognitive Adapter for efficient camera backbone tuning. 3. A Contrastive Alignment Module (CAM) to enforce consistency between camera and LiDAR features.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8464280aa00fec5a6e285de4af1a67a7ca3564b134ff0090e0f402afdb80f4b9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8464280aa00fec5a6e285de4af1a67a7ca3564b134ff0090e0f402afdb80f4b9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes SCAFusion, a multimodal 3D object detection framework built upon BEVFusion to address the challenge of detecting small objects like rocks in lunar exploration. It introduces several modules including a Section-aware Coordinate Attention mechanism to improve small object detection, achieving significant performance gains over the baseline on both standard and simulated lunar datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] DreamOmni3: Scribble-based Editing and Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image editing and generation], [scribble-based editing, unified model, joint input scheme, data synthesis, multimodal instruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bin Xia, Bohao Peng, Jiyang Liu, Sitong Wu, Jingyao Li, Junjia Huang, Xu Zhao, Yitong Wang, Ruihang Chu, Bei Yu, Jiaya Jia</p>
</li>
<li class="">
<p><strong>institution:</strong> The Chinese University of Hong Kong (CUHK), ByteDance Inc, The Hong Kong University of Science and Technology (HKUST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22525" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22525</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/dvlab-research/DreamOmni3" target="_blank" rel="noopener noreferrer" class="">https://github.com/dvlab-research/DreamOmni3</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes two new tasks—scribble-based editing and generation—that combine text, images, and freehand sketches for more flexible and precise user control. 2. Introduces a comprehensive data synthesis pipeline to create training data for these tasks, including multiple sub-tasks like doodle editing and image fusion. 3. Designs a novel joint input scheme that feeds both original and scribbled images to the model, using color and shared encodings to precisely localize edit regions without relying on binary masks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7cc62335a6a71c25dc235748c19992c217446a470fba2f00837c04f3be83d92_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7cc62335a6a71c25dc235748c19992c217446a470fba2f00837c04f3be83d92_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces DreamOmni3, a unified model for scribble-based image editing and generation. It addresses the limitations of text-only instructions by allowing users to specify edits with freehand sketches, proposes a data creation pipeline and a joint input scheme for precise localization, and demonstrates outstanding performance on new benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [closed-loop framework, entity-level memory, vision-language verification, pacing-aware editing, multi-agent collaboration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qinglin Zeng, Kaitong Cai, Ruiqi Chen, Qinhan Lv, Keze Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Sun Yat-sen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22536" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22536</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CoAgent, a collaborative closed-loop framework that formulates video generation as a plan-synthesize-verify-edit process. 2. Introduces a Global Context Manager to maintain entity-level memory for cross-shot identity and appearance consistency. 3. Employs a Verifier Agent with vision-language reasoning to evaluate intermediate results and trigger selective regeneration for quality control.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfaf9ffbd76c531ee1d089b472175957646bd5b08a39cad1cce07b642bd237a4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfaf9ffbd76c531ee1d089b472175957646bd5b08a39cad1cce07b642bd237a4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of maintaining narrative coherence and visual consistency in long-form video generation. It proposes CoAgent, a collaborative multi-agent framework that uses a closed-loop plan-synthesize-verify-edit pipeline with entity memory and consistency verification. Experiments show that CoAgent significantly improves coherence, consistency, and narrative quality in generated videos.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [embodied ai / robot learning], [vision-language-action models, benchmark, generalization, robustness, structured task design]</p>
</li>
<li class="">
<p><strong>authors:</strong> Borong Zhang, Jiahao Li, Jiachen Shen, Yishuai Cai, Yuhao Zhang, Yuanpei Chen, Juntao Dai, Jiaming Ji, Yaodong Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University, State Key Laboratory of General Artificial Intelligence (Peking University), PKU-PsiBot Joint Lab, Beijing Academy of Artificial Intelligence</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22539" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22539</a></p>
</li>
<li class="">
<p><strong>code:</strong> this https URL (from abstract)</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces VLA-Arena, a comprehensive benchmark with a novel structured task design framework to quantify difficulty across three orthogonal axes (Task Structure, Language Command, Visual Observation). 2. Provides systematic robustness evaluation via decoupled language and visual perturbations, enabling precise analysis of model failure modes. 3. Releases a complete open-source framework including an end-to-end toolchain, datasets (VLA-Arena-S/M/L), and a leaderboard to foster reproducible research.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72fbe811b1b247e0cabad5619ed5d23d6155ddda1e84493fde30e54c1e9e1092_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72fbe811b1b247e0cabad5619ed5d23d6155ddda1e84493fde30e54c1e9e1092_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces VLA-Arena, an open-source benchmark and framework designed to systematically evaluate the capabilities and failure modes of Vision-Language-Action models. It proposes a structured task design with fine-grained difficulty levels across four dimensions and orthogonal perturbations to measure model robustness. The evaluation reveals critical limitations in current VLAs, such as memorization over generalization and poor safety consideration, and the released framework aims to address these challenges.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal reasoning], [self-rewarded learning, process alignment, multimodal large language models, reasoning coherence, visual grounding]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jesen Zhang, Ningyuan Liu, Kaitong Cai, Sidi Liu, Jing Yang, Ziliang Chen, Xiaofei Sun, Keze Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Sun Yat-sen University, Alibaba Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22545" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22545</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A lightweight, label-free framework (SR-MCR) that aligns multimodal reasoning by constructing a self-reward from intrinsic process signals (semantic alignment, lexical fidelity, non-redundancy, visual grounding, step consistency). 2. A normalized, reliability-weighted reward mechanism that adaptively combines multiple self-referential cues to provide fine-grained, process-level guidance. 3. A critic-free GRPO objective enhanced with a confidence-aware cooling mechanism to stabilize training and suppress trivial or overconfident generations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/629536dd38b71477a18bd2e7208023831047c11b4eafeff5c5c094c124751f98_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/629536dd38b71477a18bd2e7208023831047c11b4eafeff5c5c094c124751f98_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of multimodal LLMs producing fluent but unreliable reasoning with poor step coherence and visual grounding. It proposes SR-MCR, a self-rewarded framework that uses multiple intrinsic process signals from model outputs to create a fine-grained reward for alignment, achieving state-of-the-art accuracy and improved reasoning coherence on visual benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ReFRM3D: A Radiomics-enhanced Fused Residual Multiparametric 3D Network with Multi-Scale Feature Fusion for Glioma Characterization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [3D U-Net, Multi-scale Feature Fusion, Radiomics, Hybrid Upsampling, Residual Skip Mechanism]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Arefin Ittesafun Abian, Yan Zhang, Mirjam Jonkman, Sami Azam</p>
</li>
<li class="">
<p><strong>institution:</strong> United International University, Monash University, Charles Darwin University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22570" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22570</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ReFRM3D, a novel radiomics-enhanced fused residual multiparametric 3D network for brain tumor characterization, based on a 3D U-Net with multi-scale feature fusion, hybrid upsampling, and an extended residual skip mechanism. 2. Introduces a multi-feature tumor marker-based classifier that leverages radiomic features extracted from the segmented tumor regions. 3. Demonstrates state-of-the-art segmentation performance on multiple BraTS datasets (2019, 2020, 2021), achieving high Dice Similarity Coefficients for whole tumor, enhancing tumor, and tumor core.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5667074cd5b9a705381cd4273a457f043c3fd17da8d6bb9335a88a6f7f75338_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5667074cd5b9a705381cd4273a457f043c3fd17da8d6bb9335a88a6f7f75338_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses challenges in glioma segmentation and classification from multi-parametric MRI data by proposing ReFRM3D, a novel 3D network architecture enhanced with radiomics and multi-scale feature fusion. The method achieves superior segmentation performance on standard BraTS benchmarks, demonstrating its effectiveness for accurate brain tumor characterization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] KV-Tracker: Real-Time Pose Tracking with Transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [pose tracking and reconstruction], [key-value caching, real-time tracking, multi-view geometry, transformer, online reconstruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Marwan Taher, Ignacio Alzugaray, Kirill Mazur, Xin Kong, Andrew J. Davison</p>
</li>
<li class="">
<p><strong>institution:</strong> Imperial College London</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22581" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22581</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://marwan99.github.io/kv_tracker/" target="_blank" rel="noopener noreferrer" class="">https://marwan99.github.io/kv_tracker/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel method for adapting slow, multi-view 3D geometry networks for real-time online use by caching key-value pairs from the transformer&#x27;s self-attention block. 2. A model-agnostic caching strategy that can be applied to off-the-shelf multi-view networks without retraining, enabling significant inference speedup. 3. Demonstration of the system on challenging tasks like on-the-fly object tracking and reconstruction from monocular RGB video without depth or object priors.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba3e802fd09ff1999dbcceb5bbbd5d8a3b724730eaf05f3ff5d3d96e25a7c964_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba3e802fd09ff1999dbcceb5bbbd5d8a3b724730eaf05f3ff5d3d96e25a7c964_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that powerful multi-view 3D geometry networks are too slow for real-time applications. It proposes KV-Tracker, which adapts these networks for online use by selecting keyframes, running a full multi-view model on them, and then caching the transformer&#x27;s key-value pairs to serve as a compact scene representation for fast, real-time pose tracking. This approach achieves up to 15x speedup and high frame rates (e.g., ~27 FPS) on standard datasets while preventing drift and catastrophic forgetting.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PTalker: Personalized Speech-Driven 3D Talking Head Animation via Style Disentanglement and Modality Alignment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D facial animation], [style disentanglement, modality alignment, Graph Attention Networks, cross-attention, contrastive learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bin Wang, Yang Xu, Huan Zhao, Hao Zhang, Zixing Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Hunan University, Central South University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22602" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22602</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel framework for personalized 3D talking head animation that preserves individual speaking styles through style disentanglement from audio and motion sequences. 2. A three-level modality alignment mechanism (spatial, temporal, feature) to enhance lip-synchronization accuracy between speech and 3D mesh. 3. Extensive experiments demonstrating superior performance in generating realistic, stylized animations compared to state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b91879e2c31bbc71f6f461d0b3368225219946fef95227be54fd0978166a06f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b91879e2c31bbc71f6f461d0b3368225219946fef95227be54fd0978166a06f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes PTalker, a framework for personalized speech-driven 3D talking head animation. It addresses the lack of individual speaking style in existing methods by disentangling style and content from audio/motion and improves lip-sync via a three-level audio-mesh alignment mechanism. Experiments show PTalker outperforms state-of-the-art methods in generating realistic and stylized animations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [location-based recommendation], [multi-modal learning, spatial-temporal knowledge graph, cross-modal alignment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Junshu Dai, Yu Wang, Tongya Zheng, Wei Ji, Qinghong Guo, Ji Cao, Jie Song, Canghong Jin, Mingli Song</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Hangzhou City University, Nanjing University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22605" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22605</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://anonymous.4open.science/r/M3ob-62EF" target="_blank" rel="noopener noreferrer" class="">https://anonymous.4open.science/r/M3ob-62EF</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified spatial-temporal relational graph (STRG) for multi-modal representation, enhanced by LLMs. 2. Designs a gating mechanism to fuse spatial-temporal graph representations from different modalities. 3. Introduces an STKG-guided cross-modal alignment method to inject dynamic knowledge into static image representations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de0c70167dfab9ddb767e6d0d1161ce4f31f482e064a77521ffa2e25c598f1d2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de0c70167dfab9ddb767e6d0d1161ce4f31f482e064a77521ffa2e25c598f1d2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limited generalization of next location recommendation methods by proposing M³ob, a framework that leverages multi-modal spatial-temporal knowledge. It constructs a unified graph representation and uses a gating mechanism with cross-modal alignment to capture mobility dynamics. Experiments on six datasets show the method improves performance in both normal and abnormal scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Dream-VL &amp; Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [diffusion language model, vision-language-action, parallel generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiacheng Ye, Shansan Gong, Jiahui Gao, Junming Fan, Shuang Wu, Wei Bi, Haoli Bai, Lifeng Shang, Lingpeng Kong</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Hong Kong, Huawei Technologies</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22615" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22615</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Dream-VL, a state-of-the-art open diffusion-based Vision-Language Model (dVLM) that matches top AR-based VLMs on benchmarks and excels at visual planning. 2. Introduces Dream-VLA, a diffusion-based Vision-Language-Action model built upon Dream-VL, leveraging the bidirectional nature of diffusion for superior action chunking and faster fine-tuning convergence. 3. Demonstrates that diffusion-based VLMs/VLAs outperform autoregressive baselines on downstream tasks, achieving top-tier performance on robotic benchmarks like LIBERO, SimplerEnv-Bridge, and SimplerEnv-Fractal.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc3fa176585576be4f4bd1035cfa0a21e63722654274b464e83bb35c7ee5bc0c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc3fa176585576be4f4bd1035cfa0a21e63722654274b464e83bb35c7ee5bc0c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes building Vision-Language and Vision-Language-Action models on diffusion-based language models to overcome the limitations of autoregressive models in complex planning and control. The introduced models, Dream-VL and Dream-VLA, leverage the bidirectional, parallel generation nature of diffusion for superior performance in visual planning and robotic tasks, achieving state-of-the-art results on key benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Enhancing Noise Resilience in Face Clustering via Sparse Differential Transformer</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [face clustering], [Sparse Differential Transformer, Top-K Jaccard similarity, noise resilience]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dafeng Zhang, Yongqi Song, Shizhuo Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Samsung R&amp;D Institute China-Beijing (SRC-B)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22612" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22612</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a prediction-driven Top-K Jaccard similarity coefficient to enhance neighbor purity and similarity measurement reliability. 2. Developed a Transformer-based model to predict relationships near the Top-K boundary for more accurate similarity estimation. 3. Introduced a Sparse Differential Transformer (SDT) to eliminate noise from irrelevant feature relationships and improve the model&#x27;s anti-noise capability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f97a4b274ef0481f36b8e16c722fdbe08401dff5cc2fb6792723e930e0c3d67e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f97a4b274ef0481f36b8e16c722fdbe08401dff5cc2fb6792723e930e0c3d67e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of noise in face clustering caused by irrelevant nodes in Jaccard similarity measurements. The authors propose a Sparse Differential Transformer (SDT) to predict and refine Top-K Jaccard similarity, enhancing noise resilience. Experiments on datasets like MS-Celeb-1M show the method achieves state-of-the-art performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Rethinking Memory Design in SAM-Based Visual Object Tracking</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [visual object tracking], [Segment Anything Model, memory mechanism, hybrid memory, distractor-resolving, occlusion robustness]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mohamad Alansari, Muzammal Naseer, Hasan Al Marzouqi, Naoufel Werghi, Sajid Javed</p>
</li>
<li class="">
<p><strong>institution:</strong> Khalifa University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22624" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22624</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/HamadYA/SAM3_Tracking_Zoo" target="_blank" rel="noopener noreferrer" class="">https://github.com/HamadYA/SAM3_Tracking_Zoo</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a systematic analysis of memory design in SAM-based trackers, revealing that methods primarily differ in short-term memory frame selection. 2. Reimplemented and evaluated existing memory mechanisms within the SAM3 framework across ten benchmarks for a controlled analysis. 3. Proposed a unified hybrid memory framework that decomposes memory into short-term appearance and long-term distractor-resolving components, improving robustness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a51c2fc4ae4647cc17c52c77088a6dfc36a196eed861608fcdf64ee98185d453_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a51c2fc4ae4647cc17c52c77088a6dfc36a196eed861608fcdf64ee98185d453_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper systematically studies memory design in SAM-based visual object tracking. It analyzes existing methods, reimplements their memory mechanisms in SAM3, and proposes a unified hybrid memory framework. The framework improves tracking robustness in challenging scenarios like occlusion and distractors for both SAM2 and SAM3 backbones.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [embodied visual planning], [video diffusion, goal-conditioned generation, embodied agents, visual imagination, first-and-last-frame-conditioned model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuming Gu, Yizhi Wang, Yining Hong, Yipeng Gao, Hao Jiang, Angtian Wang, Bo Liu, Nathaniel S. Dennler, Zhengfei Kuang, Hao Li, Gordon Wetzstein, Chongyang Ma</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Southern California, ByteDance, Stanford University, Massachusetts Institute of Technology, MBZUAI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22626" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22626</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://envision-paper.github.io/" target="_blank" rel="noopener noreferrer" class="">https://envision-paper.github.io/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Envision, a two-stage diffusion framework for embodied visual planning that explicitly uses a goal image to constrain trajectory generation. 2. Introduces a Goal Imagery Model that synthesizes a coherent goal image by identifying task-relevant regions and performing region-aware cross attention. 3. Develops an Env-Goal Video Model based on a first-and-last-frame-conditioned video diffusion model (FL2V) to interpolate smooth, physically plausible video trajectories between start and goal states.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ca10604ad58b7959dc688e00e5d0eefe89420321b67b21d1c67cd72d8941c11_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ca10604ad58b7959dc688e00e5d0eefe89420321b67b21d1c67cd72d8941c11_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of spatial drift and goal misalignment in embodied visual planning by proposing Envision, a two-stage diffusion framework that first generates a goal image and then creates a video trajectory connecting the initial scene to that goal. This method enforces goal consistency and physical plausibility. Experiments show it outperforms baselines in goal alignment and spatial consistency, providing reliable visual plans for robotic control.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FinPercep-RM: A Fine-grained Reward Model and Co-evolutionary Curriculum for RL-based Real-world Super-Resolution</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image super-resolution], [reinforcement learning from human feedback (RLHF), reward hacking, perceptual quality, curriculum learning, fine-grained assessment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yidi Liu, Zihao Fan, Jie Huang, Jie Xiao, Dong Li, Wenlong Zhang, Lei Bai, Xueyang Fu, Zheng-Jun Zha</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology of China, Shanghai AI Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22647" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22647</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Fine-grained Perceptual Reward Model (FinPercep-RM) with an encoder-decoder architecture that outputs both a global quality score and a Perceptual Degradation Map to localize defects. 2. Introduces the FGR-30k dataset containing diverse and subtle distortions from real-world super-resolution models for training the reward model. 3. Designs a Co-evolutionary Curriculum Learning (CCL) mechanism that synchronizes the progressive training of the reward model and the ISR model to ensure stable training and suppress reward hacking.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1738fd28b1410f3f5c393bd5d70851ae8df2e1196df6379de62b5d7d48c736b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1738fd28b1410f3f5c393bd5d70851ae8df2e1196df6379de62b5d7d48c736b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of reward hacking in RLHF-based Image Super-Resolution, where traditional Image Quality Assessment models are insensitive to local distortions. The authors propose a fine-grained reward model (FinPercep-RM) and a co-evolutionary curriculum learning strategy to provide localized feedback and stabilize training. Experiments show the method improves both global quality and local realism in generated images.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Visual Autoregressive Modelling for Monocular Depth Estimation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [monocular depth estimation], [visual autoregressive modelling, classifier-free guidance, scale-wise conditional upsampling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amir El-Ghoussani, André Kaup, Nassir Navab, Gustavo Carneiro, Vasileios Belagiannis</p>
</li>
<li class="">
<p><strong>institution:</strong> Friedrich-Alexander University Erlangen-Nuremberg, Technical University of Munich, University of Surrey</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22653" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22653</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/AmirMaEl/VAR-Depth" target="_blank" rel="noopener noreferrer" class="">https://github.com/AmirMaEl/VAR-Depth</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a monocular depth estimation method based on visual autoregressive (VAR) priors as an alternative to diffusion models. 2. Introduces a scale-wise conditional upsampling mechanism with classifier-free guidance for the task. 3. Demonstrates the method&#x27;s efficiency (10-stage inference, 74K fine-tuning samples) and strong performance, achieving state-of-the-art results on indoor benchmarks under constrained training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/410796d30bc05e7bf6addcc414bee3b744b04c373092f698970e2770057a5f53_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/410796d30bc05e7bf6addcc414bee3b744b04c373092f698970e2770057a5f53_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a new method for monocular depth estimation that uses visual autoregressive (VAR) priors instead of diffusion models. The approach adapts a large text-to-image VAR model with a novel scale-wise upsampling mechanism and achieves competitive results with efficient fine-tuning. The work establishes autoregressive models as a complementary, data-scalable family of generative models for 3D vision tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival Melanocytic Intraepithelial Lesion Grading</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [multi-task learning, inter-task consistency, digital pathology, foundation models, combinatorial partial supervision]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mert Ikinci, Luna Toma, Karin U. Loeffler, Leticia Ussem, Daniela Süsskind, Julia M. Weller, Yousef Yeganeh, Martina C. Herwig-Carl, Shadi Albarqouni</p>
</li>
<li class="">
<p><strong>institution:</strong> University Hospital Bonn, Technical University of Munich</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22666" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22666</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces INTERACT-CMIL, a multi-head deep learning framework for jointly predicting five histopathological axes for CMIL grading. 2. Proposes a Shared Feature Learning with Combinatorial Partial Supervision and an Inter-Dependence Loss to enforce cross-task consistency. 3. Curates and evaluates on a new multi-center dataset, showing significant performance improvements over baselines and providing a reproducible benchmark.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ed4ee67160f114b6db4089d38a1fbeae9ea01e9231d6d4df14bea6d6144469c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ed4ee67160f114b6db4089d38a1fbeae9ea01e9231d6d4df14bea6d6144469c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the difficult problem of grading Conjunctival Melanocytic Intraepithelial Lesions (CMIL) by introducing INTERACT-CMIL, a multi-task deep learning framework that uses shared feature learning and an inter-dependence loss to jointly and consistently predict five diagnostic criteria. Evaluated on a new multi-center dataset, the method outperforms CNN and foundation model baselines, offering a coherent and interpretable computational tool for standardized diagnosis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [transfer learning / domain adaptation], [Cluster Attention Adapter, adapter tuning, data-limited domains, vision foundation models, adaptive transfer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qiankun Li, Feng He, Huabao Chen, Xin Ning, Kun Wang, Zengfu Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology of China, AnnLab (Institute of Semiconductors, Chinese Academy of Sciences), Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22664" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22664</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/qklee-lz/CLAdapter" target="_blank" rel="noopener noreferrer" class="">https://github.com/qklee-lz/CLAdapter</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Cluster Attention Adapter (CLAdapter) that refines and adapts pre-trained representations to data-limited downstream tasks using attention mechanisms and cluster centers. 2. Designs a unified interface for seamless integration with diverse model architectures (CNNs, Transformers) in both 2D and 3D contexts. 3. Demonstrates state-of-the-art performance through extensive experiments on 10 datasets spanning diverse scientific domains.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d174c6a31de34c34ee98111995fd6b43142db3342aa6c7a647cb2a8121615532_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d174c6a31de34c34ee98111995fd6b43142db3342aa6c7a647cb2a8121615532_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of adapting large-scale pre-trained vision foundation models to specialized, data-limited scientific domains. It proposes a novel Cluster Attention Adapter (CLAdapter) that personalizes feature enhancement for different downstream tasks, enabling effective adaptive transfer. Extensive experiments across 10 diverse datasets show that CLAdapter achieves state-of-the-art performance, demonstrating its effectiveness in unleashing the potential of foundation models for scientific applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [3D Inception, two-stream networks, CNN-RNN, EchoNet-Dynamic, video analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shravan Saranyan, Pramit Saha</p>
</li>
<li class="">
<p><strong>institution:</strong> Branham High School, University of Oxford</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22657" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22657</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A systematic investigation and comparison of multiple deep learning architectures (3D Inception, two-stream, CNN-RNN) for automated LVEF estimation from echocardiography videos. 2. Identification that modified 3D Inception architectures achieve the best performance (RMSE of 6.79%) on the EchoNet-Dynamic dataset. 3. Key insights on model design, including the tendency for smaller, simpler models to generalize better and the high sensitivity of performance to hyperparameters like kernel size and normalization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates deep learning models for automating the estimation of left ventricular ejection fraction (LVEF) from echocardiography videos to address the time-consuming and variable nature of manual assessment. The authors systematically evaluate and modify several architectures, including 3D Inception, two-stream, and CNN-RNN models, finding that a modified 3D Inception model performs best. The study concludes that while these models show promise, they are prone to overfitting and their performance is highly sensitive to specific hyperparameter choices.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] CritiFusion: Semantic Critique and Spectral Alignment for Faithful Text-to-Image Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [text-to-image generation], [inference-time refinement, semantic critique, spectral fusion, diffusion models, vision-language model]</p>
</li>
<li class="">
<p><strong>authors:</strong> ZhenQi Chen, TsaiChing Ni, YuanFu Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> National Yang Ming Chiao Tung University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22681" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22681</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces CritiCore, a multimodal semantic critique module using VLM and LLMs to provide high-level feedback for better prompt alignment. 2. Proposes SpecFusion, a frequency-domain method to merge intermediate generation states, preserving high-frequency details while injecting structure. 3. Presents CritiFusion as a plug-in, training-free framework compatible with existing diffusion backbones, improving both semantic correspondence and visual quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/270ae7b15be98cd6466e9deec0c96f336f351042ec08bab6084e768799a7b763_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/270ae7b15be98cd6466e9deec0c96f336f351042ec08bab6084e768799a7b763_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of text-to-image diffusion models struggling with semantic alignment to complex prompts. It proposes CritiFusion, an inference-time framework that uses a semantic critique module and spectral fusion to refine generated images, improving faithfulness and detail without requiring additional training. The method achieves results competitive with state-of-the-art reward optimization approaches on standard benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Autoregressive Flow Matching for Motion Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [motion prediction], [autoregressive flow matching, point tracks, probabilistic modeling, video conditioning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Johnathan Xie, Stefan Stojanov, Cristobal Eyzaguirre, Daniel L. K. Yamins, Jiajun Wu</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22688" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22688</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Johnathan-Xie/arfm-motion-prediction" target="_blank" rel="noopener noreferrer" class="">https://github.com/Johnathan-Xie/arfm-motion-prediction</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes autoregressive flow matching (ARFM), a new method for probabilistic modeling of sequential continuous data. 2. Develops benchmarks for evaluating motion prediction models on human and robot motion. 3. Demonstrates that conditioning downstream tasks (robot action prediction, human motion prediction) on predicted future tracks improves performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d8e5a909ec49727b601e446f309c509adc85c5184989119d290d0736109e261_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d8e5a909ec49727b601e446f309c509adc85c5184989119d290d0736109e261_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of predicting future motion from videos, which requires understanding agent behavior and physics. It proposes Autoregressive Flow Matching (ARFM), a method trained on diverse video datasets to probabilistically generate future point track locations. The model shows the ability to predict complex motions and its predictions can significantly improve downstream task performance in robotics and human motion analysis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image registration], [Neural ODEs, Structural Descriptors, Diffeomorphic Registration, Multimodal, Local Mutual Information]</p>
</li>
<li class="">
<p><strong>authors:</strong> Salvador Rodriguez-Sanz, Monica Hernandez</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Zaragoza, Aragon Institute for Engineering Research (I3A)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22689" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22689</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an instance-specific multimodal diffeomorphic registration framework using Neural ODEs, eliminating the need for large training datasets and avoiding performance drop on unseen modalities. 2. Introduces three method variants that integrate modality-agnostic structural descriptors (image-based or feature-based) with local mutual information for similarity measurement. 3. Demonstrates superior performance, robustness to varying regularization, suitability for different deformation scales, and computational efficiency compared to state-of-the-art baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/064c09ec95bfe97d740e2afb1b657324c426af97aabecb55dcfa4db080514f55_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/064c09ec95bfe97d740e2afb1b657324c426af97aabecb55dcfa4db080514f55_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of existing nonrigid registration methods, which often require intensity correlation and are limited to monomodal settings. It proposes a multimodal diffeomorphic registration method that combines Neural ODEs with structural descriptors and local mutual information. The method shows superior accuracy, robustness, and efficiency in aligning medical images from different modalities without requiring extensive training data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Mesquite MoCap: Democratizing Real-Time Motion Capture with Affordable, Bodyworn IoT Sensors and WebXR SLAM</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [motion capture, wearable computing, human-computer interaction], [IMU, WebXR, SLAM, IoT, edge computing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Poojan Vanani, Darsh Patel, Danyal Khorami, Siva Munaganuru, Pavan Reddy, Varun Reddy, Bhargav Raghunath, Ishrat Lallmamode, Romir Patel, Assegid Kidané, Tejaswi Gowda</p>
</li>
<li class="">
<p><strong>institution:</strong> Arizona State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22690" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22690</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An open-source, low-cost inertial motion capture system using 15 body-worn IMU sensors and a smartphone for SLAM-based position tracking. 2. A fully browser-based application built on modern web technologies (WebGL, WebXR, WebSerial) for cross-platform, real-time visualization and recording. 3. Demonstrated performance comparable to commercial optical systems (2-5° joint-angle error) at approximately 5% of the cost, with low latency and high reliability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/471e89e8fb0b1b1a76d2b789c6196559fd43adfdff5430691fa6cbaa29efc55b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/471e89e8fb0b1b1a76d2b789c6196559fd43adfdff5430691fa6cbaa29efc55b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents Mesquite, an affordable motion capture system that uses wearable IMU sensors and a smartphone with WebXR for SLAM. It operates entirely in a web browser using modern web technologies for real-time tracking. The system achieves accuracy close to expensive commercial systems at a fraction of the cost, aiming to democratize motion capture technology.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SCPainter: A Unified Framework for Realistic 3D Asset Insertion and Novel View Synthesis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [novel view synthesis], [3D Gaussian Splatting, diffusion models, autonomous driving simulation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Paul Dobre, Jackson Cooper, Xin Wang, Hongzhou Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Calgary</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22706" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22706</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A unified framework (SCPainter) that jointly handles realistic 3D asset insertion and novel view synthesis for autonomous driving simulation. 2. Integration of 3D Gaussian Splatting representations for assets with 3D scene point clouds and a diffusion model for high-quality image generation. 3. Demonstration of the framework&#x27;s capability to create diverse and realistic driving data on the Waymo Open Dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95f8be4c9d4efa079fef04457695c6277096cd0bb166f918ab1f0cd81c2c5a16_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95f8be4c9d4efa079fef04457695c6277096cd0bb166f918ab1f0cd81c2c5a16_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes SCPainter, a unified framework that combines 3D Gaussian Splatting for asset representation with diffusion models to jointly perform realistic 3D asset insertion and novel view synthesis for autonomous driving simulation. The method projects 3D assets and scene point clouds into novel views and uses them to condition a diffusion model to generate high-quality images. Evaluation shows the framework can create diverse and realistic driving scenarios for training data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Memento-II: Learning by Stateful Reflective Memory</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [stateful reflective decision process, episodic memory, policy iteration, continual learning, retrieval-augmented generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jun Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> University College London (UCL)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22716" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22716</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Stateful Reflective Decision Process (SRDP), a formal theoretical framework that models continual learning in LLM agents as a two-stage read-write interaction with episodic memory, linking it to policy evaluation and improvement. 2. Provides a theoretical analysis showing that the reflective learning process induces an equivalent Markov Decision Process, enabling the use of classical dynamic programming and RL tools, and establishes convergence guarantees when instantiated with entropy-regularised policy iteration. 3. Unifies heuristic approaches like case-based reasoning and retrieval-augmented generation with principled reinforcement learning, offering a rigorous mathematical foundation for building memory-augmented agents capable of online adaptation without parameter updates.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a theoretical framework for continual learning in LLM agents that uses episodic memory and reflection instead of back-propagation. The core method formalizes learning as a Stateful Reflective Decision Process, where writing to memory is policy evaluation and reading from it is policy improvement. The main conclusion is that this framework provides a principled, convergent foundation for agents to self-improve through interaction without fine-tuning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [self-supervised learning, masked autoencoding, Score-CAM, prenatal ultrasound, cystic hygroma]</p>
</li>
<li class="">
<p><strong>authors:</strong> Youssef Megahed, Robin Ducharme, Inok Lee, Inbal Willner, Olivier X. Miguel, Kevin Dick, Adrian D. C. Chan, Mark Walker, Steven Hawken</p>
</li>
<li class="">
<p><strong>institution:</strong> Carleton University, Ottawa Hospital Research Institute, University of Ottawa, Children&#x27;s Hospital of Eastern Ontario Research Institute</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22730" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22730</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Applied and fine-tuned a large-scale, ultrasound-specific self-supervised model (USF-MAE) for the task of cystic hygroma detection. 2. Demonstrated that this self-supervised pre-training approach significantly outperforms a supervised baseline (DenseNet-169) trained from scratch on a small labeled dataset. 3. Provided qualitative interpretability analysis using Score-CAM visualizations to show the model&#x27;s focus on clinically relevant anatomical regions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99e72cff0ee8a4f27fb9382b2b663077b0e2b4a00e62b6b2d32980df58b008c0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99e72cff0ee8a4f27fb9382b2b663077b0e2b4a00e62b6b2d32980df58b008c0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of automated cystic hygroma detection in prenatal ultrasound, where labeled data is scarce. The authors propose fine-tuning a self-supervised model (USF-MAE) pre-trained on a large corpus of unlabeled ultrasound images. Their method significantly outperforms a supervised baseline, demonstrating that ultrasound-specific self-supervised learning enables accurate, robust, and data-efficient detection for early prenatal screening.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Split4D: Decomposed 4D Scene Reconstruction Without Video Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [4D scene reconstruction], [Freetime FeatureGS, contrastive loss, streaming feature learning, Gaussian primitives, decomposed reconstruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yongzhen Hu, Yihui Yang, Haotong Lin, Yifan Wang, Junting Dong, Yifu Deng, Xinyu Zhu, Fan Jia, Hujun Bao, Xiaowei Zhou, Sida Peng</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Ant Group, Shanghai AI Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22745" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22745</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel decomposed 4D scene reconstruction method that eliminates the need for unstable video segmentation by relying only on per-image segmentation maps. 2. Introduces Freetime FeatureGS, a representation using Gaussian primitives with learnable features and linear motion, and a contrastive loss to enforce instance-level feature consistency. 3. Designs a temporally ordered streaming training strategy to propagate features over time, avoiding local minima and achieving accurate 4D segmentation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23d1f40ecd27989546bd142e33db7b3761940a690e2422a0d6fefac0e033e539_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23d1f40ecd27989546bd142e33db7b3761940a690e2422a0d6fefac0e033e539_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses decomposed 4D scene reconstruction from multi-view videos by proposing a method that avoids reliance on video segmentation. The core idea is to represent the scene with a novel Freetime FeatureGS model and train it with a contrastive loss and a streaming strategy using only per-frame segmentation. Experiments show the method outperforms recent state-of-the-art approaches in reconstruction quality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] TrimTokenator-LC: Towards Adaptive Visual Token Pruning for Large Multimodal Models with Long Contexts</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [visual token pruning, long context, adaptive budget allocation, intra-image diversity, inter-image variation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hao Zhang, Mengsi Lyu, Bo Huang, Yulong Ao, Yonghua Lin</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Academy of Artificial Intelligence (BAAI)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22748" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22748</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and analyzes the specific challenges of visual token pruning in long-context, multi-image scenarios. 2. Proposes a novel two-stage adaptive pruning method that decomposes redundancy into intra-image and inter-image components, guided by diversity and variation metrics. 3. Introduces a Pareto selection procedure in the inter-image stage to balance global token diversity with text alignment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0de6ebee8f5094e1c2e0115398dde4fca37120436baa74c8f17b1c0de33ca18_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0de6ebee8f5094e1c2e0115398dde4fca37120436baa74c8f17b1c0de33ca18_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high inference cost of Large Multimodal Models (LMMs) in long-context, multi-image settings by proposing TrimTokenator-LC, an adaptive visual token pruning method. The method uses a two-stage process to dynamically allocate token budgets based on intra-image diversity and inter-image variation, then selects tokens by balancing diversity and text relevance. Experiments show the approach significantly reduces the number of visual tokens while maintaining strong performance in long-context tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Neighbor-Aware Token Reduction via Hilbert Curve for Vision Transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [vision transformer efficiency], [token reduction, Hilbert curve, neighbor-aware pruning, token merging, spatial continuity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yunge Li, Lanyu Xu</p>
</li>
<li class="">
<p><strong>institution:</strong> Oakland University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22760" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22760</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Yunge6666/NAP-MAT" target="_blank" rel="noopener noreferrer" class="">https://github.com/Yunge6666/NAP-MAT</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel neighbor-aware token reduction framework for Vision Transformers that explicitly preserves spatial continuity and local context. 2. Introduces Neighbor-Aware Pruning (NAP), which incorporates the influence of neighboring tokens into importance scoring for selective retention. 3. Introduces Merging by Adjacent Token similarity (MAT), a localized token aggregation strategy that computes similarity and merges tokens only within adjacent regions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4ca51a47453a80fb378823f123202bcb915998b4809942ea05464a170da615b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4ca51a47453a80fb378823f123202bcb915998b4809942ea05464a170da615b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the computational inefficiency of Vision Transformers caused by redundant token representations. It proposes a novel token reduction method based on Hilbert curve reordering, which includes Neighbor-Aware Pruning (NAP) and Merging by Adjacent Token similarity (MAT) to preserve spatial neighbor structure. The approach achieves state-of-the-art accuracy-efficiency trade-offs, highlighting the importance of spatial continuity for ViT optimization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3d reconstruction], [3D Gaussian Splatting, Next Best View, Active Learning, Fisher Information, Dynamic Scene Modeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yiqian Li, Wen Jiang, Kostas Daniilidis</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Pennsylvania</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22771" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22771</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulates the next-best-view selection problem for dynamic and semantic 3D scenes as an active learning problem. 2. Proposes an active learning algorithm using Fisher Information to quantify view informativeness for both semantic Gaussian parameters and deformation networks. 3. Provides a unified framework that jointly handles semantic reasoning and dynamic scene modeling, outperforming heuristic and random baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/512528158b99bf9d8d5519331a5d1557baee5b3050273bff66df842767a73963_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/512528158b99bf9d8d5519331a5d1557baee5b3050273bff66df842767a73963_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of selecting the most informative camera views for training dynamic and semantic 3D Gaussian Splatting models. It proposes an active learning method based on Fisher Information to prioritize frames that maximize information gain for both geometry and semantics. The approach improves rendering quality and segmentation performance compared to random or uncertainty-based selection strategies.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Schrodinger AI: A Unified Spectral-Dynamical Framework for Classification, Reasoning, and Operator-Based Generalization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [quantum-inspired machine learning], [spectral decomposition, Hamiltonian learning, semantic wavefunctions, operator calculus, emergent manifolds]</p>
</li>
<li class="">
<p><strong>authors:</strong> Truong Son Nguyen</p>
</li>
<li class="">
<p><strong>institution:</strong> Arizona State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22774" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22774</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A unified spectral-dynamical framework for machine learning inspired by quantum mechanics, comprising a time-independent wave-energy solver, a time-dependent dynamical solver, and a low-rank operator calculus. 2. Demonstration of emergent semantic manifolds that reflect class relations without explicit supervision, dynamic reasoning that adapts to changing environments, and exact operator generalization on symbolic tasks. 3. Proposing a new foundational direction for ML where learning is cast as discovering and navigating an underlying semantic energy landscape, offering an alternative to cross-entropy training and transformer attention.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58825c1201e17641b4e19a581a742615913539c66fbf08e5c113620bf7eec6de_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58825c1201e17641b4e19a581a742615913539c66fbf08e5c113620bf7eec6de_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Schrödinger AI, a novel machine learning framework inspired by quantum mechanics that treats perception as spectral decomposition and reasoning as wavefunction dynamics. The method demonstrates robust generalization, interpretable semantics, and emergent topology on tasks like classification, maze navigation, and modular arithmetic. The results suggest a promising new paradigm for AI that learns by discovering an underlying semantic energy landscape.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Plug In, Grade Right: Psychology-Inspired AGIQA</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image quality assessment], [AGIQA, Graded Response Model, semantic drift, quality grading, plug-and-play]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhicheng Liao, Baoliang Chen, Hanwei Zhu, Lingyu Zhu, Shiqi Wang, Weisi Lin</p>
</li>
<li class="">
<p><strong>institution:</strong> South China Normal University, City University of Hong Kong, Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22780" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22780</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and defines the &quot;semantic drift&quot; problem in existing AGIQA models where image embeddings show inconsistent similarity to multi-grade text descriptions. 2. Proposes a psychology-inspired, improved Graded Response Model (GRM) for AGIQA, framing quality as an image&#x27;s &quot;ability&quot; to meet &quot;difficulty&quot; levels. 3. Designs a novel Arithmetic GRM based Quality Grading (AGQG) module that enforces a unimodal, interpretable quality distribution and demonstrates plug-and-play performance gains across various frameworks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e5502866af39224d2021cd83326b1c76e8d4a8dfa3a4f757b8f51a03362124c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e5502866af39224d2021cd83326b1c76e8d4a8dfa3a4f757b8f51a03362124c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the &quot;semantic drift&quot; issue in AI-generated image quality assessment (AGIQA), where inconsistent similarities between image and text embeddings degrade reliability. The authors propose a novel Arithmetic GRM based Quality Grading (AGQG) module, inspired by psychometrics, which models image quality as an ability to overcome graded difficulty levels. The plug-and-play module consistently improves state-of-the-art AGIQA models and generalizes to both natural and screen content images.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object tracking], [vision-language tracking, multimodal large language model, visual prompt, global search, location-aware]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jingchao Wang, Kaiwen Zhou, Zhijian Wu, Kunhua Ji, Dingjiang Huang, Yefeng Zheng</p>
</li>
<li class="">
<p><strong>institution:</strong> East China Normal University, Westlake University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22799" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22799</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/jcwang0602/VPTracker" target="_blank" rel="noopener noreferrer" class="">https://github.com/jcwang0602/VPTracker</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the first global vision-language tracking framework based on Multimodal Large Language Models (MLLMs)., 2. Proposes a location-aware visual prompting mechanism to incorporate spatial priors and suppress distractions., 3. Demonstrates enhanced tracking stability and target disambiguation in challenging scenarios, opening a new avenue for MLLM integration in visual tracking.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9617d5ed5f0e1ed9ff8bb7b07da944102f01b36dcfa1122d537a24b6a20916e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9617d5ed5f0e1ed9ff8bb7b07da944102f01b36dcfa1122d537a24b6a20916e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes VPTracker, a novel global vision-language tracking framework that leverages Multimodal Large Language Models (MLLMs) for robust target localization across the entire image. To address distractions from global search, it introduces a location-aware visual prompting mechanism that uses the target&#x27;s previous location as a spatial prior. Experiments show the method significantly improves tracking stability and disambiguation under challenging conditions like occlusions and rapid motion.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Medical Scene Reconstruction and Segmentation based on 3D Gaussian Representation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D reconstruction], [3D Gaussian Representation, tri-plane representation, sparse reconstruction, semantic segmentation, medical image analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bin Liu, Wenyan Tian, Huangxin Fu, Zizheng Li, Zhifen He, Bo Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanchang Hangkong University, Guilin University Of Electronic Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22800" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22800</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an efficient 3D reconstruction method for medical images by combining 3D Gaussian and tri-plane representations. 2. Enhances structural continuity and semantic consistency under sparse slice conditions, addressing a key limitation of traditional methods. 3. Demonstrates high-quality, anatomically coherent reconstruction on multimodal medical datasets (US, MRI) with improved efficiency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c719633132efd891759bf317060eb9e7deff2cc44fa35a7c33b3b080dba098fb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c719633132efd891759bf317060eb9e7deff2cc44fa35a7c33b3b080dba098fb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of computationally expensive and detail-losing 3D reconstruction from sparse medical image slices. It proposes a novel method that integrates 3D Gaussian and tri-plane representations to efficiently generate high-quality, anatomically coherent 3D visualizations. Experiments on US and MRI data confirm the method&#x27;s effectiveness in improving reconstruction quality and efficiency under sparse data conditions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ReDiF: Reinforced Distillation for Few Step Diffusion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [reinforcement learning, knowledge distillation, policy optimization, denoising paths, model agnostic]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amirhossein Tighkhorshid, Zahra Dehghanian, Gholamali Aminian, Chengchun Shi, Hamid R. Rabiee</p>
</li>
<li class="">
<p><strong>institution:</strong> Sharif University of Technology, Alan Turing Institute, London School of Economics</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22802" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22802</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel reinforcement learning framework for distilling diffusion models, treating distillation as a policy optimization problem. 2. Introduces a reward signal based on alignment with teacher outputs, allowing the student model to explore multiple denoising paths and take longer, optimized steps. 3. Demonstrates a model-agnostic framework that achieves superior performance with fewer inference steps and computational resources compared to existing distillation techniques.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73d47eb5443f9f8848454e65825da3569c70d954239d9794e6cff086fa5bc17a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73d47eb5443f9f8848454e65825da3569c70d954239d9794e6cff086fa5bc17a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the slow sampling problem in diffusion models by proposing ReDiF, a reinforcement learning-based distillation framework. Instead of using fixed losses, it treats distillation as policy optimization, using a reward signal to guide the student to take longer, optimized steps. The method achieves better performance with fewer steps and is applicable to various diffusion models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Evaluating the Performance of Open-Vocabulary Object Detection in Low-quality Image</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [open-vocabulary object detection, low-quality image, image degradation, benchmark dataset, OWLv2]</p>
</li>
<li class="">
<p><strong>authors:</strong> Po-Chih Wu</p>
</li>
<li class="">
<p><strong>institution:</strong> National Yang Ming Chiao Tung University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22801" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22801</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/gohakushi1118/Low-quality-image-dataset" target="_blank" rel="noopener noreferrer" class="">https://github.com/gohakushi1118/Low-quality-image-dataset</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Constructed a new benchmark dataset simulating real-world low-quality images with four types of degradation (lossy compression, image intensity, noise, blur). 2. Evaluated six state-of-the-art open-vocabulary object detection models on this benchmark to assess their robustness. 3. Provided analysis showing varying model sensitivity to degradation types and levels, with OWLv2 demonstrating more consistent performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3523d24298701084e07c062fb88e9e451f2e2a096e82a3cfc75f02cf63d866ab_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3523d24298701084e07c062fb88e9e451f2e2a096e82a3cfc75f02cf63d866ab_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper evaluates the robustness of open-vocabulary object detection models on low-quality images by creating a new benchmark dataset with various image degradations. The experiments show that while models are resilient to mild degradation, severe degradation causes significant performance drops, with OWLv2 models being the most robust.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Parallel Diffusion Solver via Residual Dirichlet Policy Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [ODE solver, parallel gradient evaluation, reinforcement learning fine-tuning, low-latency sampling, Dirichlet policy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ruoyu Wang, Ziyu Li, Beier Zhu, Liangyu Yuan, Hanwang Zhang, Xun Yang, Xiaojun Chang, Chi Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Westlake University, University of Illinois Urbana-Champaign, Nanyang Technological University, Shanghai Jiao Tong University, University of Science and Technology of China</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22796" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22796</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes EPD-Solver, a novel ODE solver that uses multiple parallel gradient evaluations per step to reduce truncation errors while maintaining low latency. 2. Introduces a two-stage optimization framework, including a parameter-efficient RL fine-tuning scheme that reformulates the solver as a stochastic Dirichlet policy to avoid reward hacking. 3. Demonstrates the method&#x27;s flexibility as a plugin (EPD-Plugin) to enhance existing ODE samplers and shows state-of-the-art performance in both unconditional and text-to-image generation benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2678a61b07c4f5b5cfdd2006673a05c8a4699c07dee6180c47750600496f796_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2678a61b07c4f5b5cfdd2006673a05c8a4699c07dee6180c47750600496f796_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high sampling latency of diffusion models by proposing EPD-Solver, a novel ODE solver that incorporates parallel gradient evaluations to reduce errors without increasing latency. The method uses a two-stage optimization, including RL fine-tuning with a Dirichlet policy, and can be used as a plugin. Experiments show it achieves superior image quality at low step counts and improves human preference scores in text-to-image generation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human motion generation], [egocentric video, 3D human reaction, autoregressive generation, VQ-VAE, GPT]</p>
</li>
<li class="">
<p><strong>authors:</strong> Libo Zhang, Zekun Li, Tianyu Li, Zeyu Cao, Rui Xu, Xiaoxiao Long, Wenjia Wang, Jingbo Wang, Yuan Liu, Wenping Wang, Daquan Zhou, Taku Komura, Zhiyang Dou</p>
</li>
<li class="">
<p><strong>institution:</strong> THU, Brown, Georgia Tech, Cambridge, HKU, NJU, CUHK, HKUST, TAMU, PKU, MIT</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22808" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22808</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Constructed the Human Reaction Dataset (HRD), a spatially aligned egocentric video-reaction dataset to address data scarcity and misalignment in existing resources. 2. Proposed EgoReAct, the first autoregressive framework for real-time, 3D-aligned human reaction motion generation from streaming egocentric video. 3. Incorporated 3D dynamic features (metric depth, head dynamics) into the generation pipeline to enhance spatial grounding and realism.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/157e088cb49368b1dbc239ff6380ef8897576c9c3fa92a5bf6737c3a5029e927_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/157e088cb49368b1dbc239ff6380ef8897576c9c3fa92a5bf6737c3a5029e927_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the challenge of generating realistic and spatially aligned 3D human reactions from egocentric video streams. The authors propose EgoReAct, an autoregressive framework that uses a VQ-VAE and a GPT to generate motions in real-time, enhanced by 3D features. Experiments show the method achieves superior realism, spatial consistency, and efficiency while maintaining strict causality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] 3D Scene Change Modeling With Consistent Multi-View Aggregation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D scene understanding / change detection], [3D Gaussian Splatting, signed-distance field, multi-view aggregation, continual reconstruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zirui Zhou, Junfeng Ni, Shujie Zhang, Yixin Chen, Siyuan Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, State Key Laboratory of General Artificial Intelligence (BIGAI)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22830" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22830</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://zr-zhou0o0.github.io/SCaR3D/" target="_blank" rel="noopener noreferrer" class="">https://zr-zhou0o0.github.io/SCaR3D/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SCaR-3D, a novel 3D scene change detection framework that uses a signed-distance-based 2D differencing module and multi-view aggregation with voting/pruning to robustly separate pre- and post-change states. 2. Develops a continual scene reconstruction strategy that selectively updates dynamic regions while preserving unchanged areas. 3. Contributes CCS3D, a challenging synthetic dataset for flexible and controlled evaluation of 3D change types.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78ed554a37f9d36201f10d441a5e94d0905658a4c25e8528463fcb824b31d7b3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78ed554a37f9d36201f10d441a5e94d0905658a4c25e8528463fcb824b31d7b3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of detecting object-level changes in 3D scenes from multi-view images, where existing methods suffer from spatial inconsistency and cannot separate pre- and post-change states. The proposed SCaR-3D framework leverages 3D Gaussian Splatting for consistent multi-view aggregation and includes a strategy for continual scene reconstruction. Experiments show the method outperforms existing approaches in both accuracy and efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] KANO: Kolmogorov-Arnold Neural Operator for Image Super-Resolution</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image super-resolution], [Kolmogorov-Arnold Neural Operator, B-spline functions, interpretability, spectral fitting, degradation modeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chenyu Li, Danfeng Hong, Bing Zhang, Zhaojie Pan, Jocelyn Chanussot</p>
</li>
<li class="">
<p><strong>institution:</strong> Southeast University, Aerospace Information Research Institute (Chinese Academy of Sciences), Univ. Grenoble Alpes</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22822" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22822</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the novel Kolmogorov-Arnold Neural Operator (KANO) for interpretable image super-resolution, inspired by the Kolmogorov-Arnold theorem. 2. Employs an additive structure of B-spline functions to model the degradation process transparently, capturing key spectral characteristics like local trends and peak-valley structures. 3. Provides a systematic comparative study between MLPs and KANs for complex sequence fitting, offering insights into interpretable SR model design.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96c01412f6132ddd4983bd966141d1e72338121a3cf338d50ea6ab033a26816c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96c01412f6132ddd4983bd966141d1e72338121a3cf338d50ea6ab033a26816c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of interpretability in single-image super-resolution by proposing a new model called KANO, which uses B-spline functions based on the Kolmogorov-Arnold theorem to transparently model the image degradation process. The method provides physically interpretable results by learning spectral characteristics. The authors demonstrate its effectiveness and compare it with other models across different image types.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Depth Anything in <span class="katex-error" title="ParseError: KaTeX parse error: Expected group after &#x27;^&#x27; at position 4: 360^̲" style="color:#cc0000">360^</span>: Towards Scale Invariance in the Wild</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [depth estimation], [panoramic depth estimation, scale invariance, zero-shot generalization, circular padding, ViT backbone]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hualie Jiang, Ziyang Song, Zhiqiang Lou, Rui Xu, Minglang Tan</p>
</li>
<li class="">
<p><strong>institution:</strong> Insta360 Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22819" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22819</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://insta360-research-team.github.io/DA360" target="_blank" rel="noopener noreferrer" class="">https://insta360-research-team.github.io/DA360</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DA360, a panoramic-adapted version of Depth Anything V2 that learns a shift parameter to transform scale- and shift-invariant output into scale-invariant disparity for direct 3D point cloud generation. 2. Integrates circular padding into the DPT decoder to eliminate seam artifacts and ensure spatially coherent depth maps respecting spherical continuity. 3. Introduces a new outdoor panoramic depth dataset, Metropolis, for evaluation and demonstrates state-of-the-art zero-shot performance on indoor and outdoor benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f48de8ab79cd62e59a71f803895b0da94c4bc7e241ab05b9ebd97bf7af12448_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f48de8ab79cd62e59a71f803895b0da94c4bc7e241ab05b9ebd97bf7af12448_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the gap in zero-shot generalization for panoramic depth estimation by proposing DA360, an adaptation of Depth Anything V2. The method learns a shift parameter for scale-invariant output and uses circular padding to prevent seam artifacts, enabling direct conversion to 3D point clouds. The results show significant error reduction compared to the base model and other panoramic methods, establishing new state-of-the-art performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Minimal Solver for Relative Pose Estimation with Unknown Focal Length from Two Affine Correspondences</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [relative pose estimation], [affine correspondences, polynomial eigenvalue method, minimal solver, inertial measurement unit, focal length estimation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhenbao Yu, Shirong Ye, Ronghe Jin, Shunkun Liang, Zibin Liu, Huiyun Zhang, Banglei Guan</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Defense Technology, Wuhan University, Henan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22833" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22833</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new minimal solver for estimating 3DOF relative pose and focal length from only two affine correspondences when the vertical direction is known from an IMU. 2. Derives a system of four constraint equations that reduce the problem to solving for only two parameters: focal length and relative rotation angle. 3. Utilizes the polynomial eigenvalue method to efficiently solve the derived equations, demonstrating superior performance over state-of-the-art solvers on synthetic and real-world datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd3f71922ab7368b3202c0df07c429238e34d3301688ac8ba9116c190b808c1b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd3f71922ab7368b3202c0df07c429238e34d3301688ac8ba9116c190b808c1b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a new minimal solver for estimating the relative pose and focal length between two camera views. The method uses only two affine correspondences and known vertical direction (from an IMU) to formulate constraint equations, which are then solved using a polynomial eigenvalue approach. Experimental results show the proposed solver outperforms existing state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [Human-Object Interaction, Diffusion Transformer, Relative Coordinate Maps, Progressive Curriculum Learning, Geometry Consistency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bangya Liu, Xinyu Gong, Zelin Zhao, Ziyang Song, Yulei Lu, Suhui Wu, Jun Zhang, Suman Banerjee, Hao Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Wisconsin-Madison, ByteDance, Georgia Institute of Technology, The Hong Kong Polytechnic University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22854" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22854</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://neutrinoliu.github.io/byteloom/" target="_blank" rel="noopener noreferrer" class="">https://neutrinoliu.github.io/byteloom/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ByteLoom, a Diffusion Transformer-based framework for generating realistic HOI videos with geometrically consistent objects. 2. Introduces the RCM-cache mechanism using Relative Coordinate Maps to maintain object geometry consistency and control 6-DoF transformations. 3. Designs a progressive training curriculum to compensate for HOI dataset scarcity and relax the need for fine-grained hand mesh annotations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9f5ff16308904b889be6695aafd24bfc28b798f080cc6c723a25066bcdc2bdf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9f5ff16308904b889be6695aafd24bfc28b798f080cc6c723a25066bcdc2bdf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of poor cross-view consistency and reliance on hand mesh annotations in Human-Object Interaction (HOI) video generation. It proposes ByteLoom, a framework that uses a novel RCM-cache mechanism for geometry consistency and a progressive curriculum learning strategy for training. The method effectively preserves human identity and object geometry while generating smooth motion.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MUSON: A Reasoning-oriented Multimodal Dataset for Socially Compliant Navigation in Urban Environments</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [embodied navigation], [socially compliant navigation, multimodal dataset, chain-of-thought, vision language models, benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhuonan Liu, Xinyu Zhang, Zishuo Wang, Tomohito Kawabata, Xuesu Xiao, Ling Xiao</p>
</li>
<li class="">
<p><strong>institution:</strong> Hokkaido University, George Mason University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22867" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22867</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MUSON, a new multimodal dataset for socially compliant navigation with structured five-step Chain-of-Thought annotations (perception, prediction, reasoning, action, explanation). 2. Addresses limitations of prior datasets by explicitly modeling static physical constraints and providing a rationally balanced discrete action space to overcome long-tailed action distributions. 3. Establishes MUSON as an effective benchmark, demonstrating its utility by benchmarking state-of-the-art Small Vision Language Models, with Qwen2.5-VL-3B achieving the highest decision accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5248e75c8017605d3d39791503fba58a4cce5e655f4d900abeee425ea863b824_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5248e75c8017605d3d39791503fba58a4cce5e655f4d900abeee425ea863b824_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces MUSON, a reasoning-oriented multimodal dataset designed to address the lack of explicit reasoning supervision and imbalanced action distributions in existing social navigation datasets. It features structured Chain-of-Thought annotations and a balanced action space. Benchmarking results show that MUSON serves as an effective benchmark, with Qwen2.5-VL-3B achieving the highest accuracy, demonstrating its utility for training and evaluating socially compliant navigation models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [self-supervised learning], [self-supervised learning, foundation model, chest radiographs, anatomical consistency, multi-perspective learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ziyu Zhou, Haozhe Luo, Mohammad Reza Hosseinzadeh Taher, Jiaxuan Pang, Xiaowei Ding, Michael B. Gotway, Jianming Liang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, Arizona State University, University of Bern, Mayo Clinic</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22872" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22872</a></p>
</li>
<li class="">
<p><strong>code:</strong> GitHub.com/JLiangLab/Lamps</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel self-supervised learning framework (Lamps) that explicitly leverages the consistency, coherence, and hierarchy of human anatomy as supervision signals. 2. Demonstrates superior performance and robustness across 10 chest X-ray datasets compared to 10 baseline models. 3. Releases code and pre-trained models to facilitate research in anatomy-aware foundation models for medical imaging.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f086b74ae968cdc69d3bc3132f42d25c77e16078cfd4f970417ab16219c8be5e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f086b74ae968cdc69d3bc3132f42d25c77e16078cfd4f970417ab16219c8be5e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the limitation of existing self-supervised learning methods in medical imaging, which often overlook anatomical structures. It proposes Lamps, a method that learns from multiple anatomical perspectives (consistency, coherence, hierarchy) using self-supervision on chest radiographs. The results show that Lamps achieves superior robustness and transferability, offering a promising foundation model aligned with human anatomy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Let Samples Speak: Mitigating Spurious Correlation by Exploiting the Clusterness of Samples</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [debiasing], [spurious correlation, feature transformation, worst group accuracy, bias-invariant representation, empirical risk minimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Weiwei Li, Junzhuo Liu, Yuanyuan Ren, Yuchen Zheng, Yahao Liu, Wen Li</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China, Shihezi University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22874" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22874</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/davelee-uestc/nsf_debiasing" target="_blank" rel="noopener noreferrer" class="">https://github.com/davelee-uestc/nsf_debiasing</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a data-oriented pipeline for mitigating spurious correlation without requiring prior annotation of bias attributes. 2. Introduces a method to identify spurious features by observing the dispersed distribution of biased samples in the feature space. 3. Develops a feature transformation learning process to align with a bias-invariant representation, leading to an unbiased classifier.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e28dca63104c78d5b10b26c867dca0089d6d3d5a4a9c58d62e325089a82e7214_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e28dca63104c78d5b10b26c867dca0089d6d3d5a4a9c58d62e325089a82e7214_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of deep learning models learning spurious correlations from biased training data. It proposes a method that identifies, neutralizes, and eliminates spurious features by exploiting the clusteredness of samples and aligning feature transformations with a bias-invariant representation. The approach significantly improves worst-group accuracy by over 20% compared to standard training on image and NLP benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Hash Grid Feature Pruning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [hash grid, Gaussian splatting, feature pruning, rate-distortion, implicit neural field]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yangzhi Ma, Bojun Liu, Jie Li, Li Li, Dong Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology of China</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22882" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22882</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies the problem of invalid features in hash grids due to the non-uniform distribution of Gaussian splats, leading to storage and transmission redundancy. 2. Proposes a hash grid feature pruning method that identifies and removes invalid features based on input Gaussian splat coordinates before encoding. 3. Demonstrates improved rate-distortion performance with an average 8% bitrate reduction in standardized tests, without compromising model reconstruction quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcac938c69d5ba62c79db746401a4102eb5d9b05e4f19058904cf65b316672f9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcac938c69d5ba62c79db746401a4102eb5d9b05e4f19058904cf65b316672f9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of redundant storage in hash grids used for Gaussian splatting compression, caused by invalid features in sparse 3D regions. It proposes a pruning method that removes these invalid features before encoding, reducing bitrate. The method achieves an average 8% bitrate reduction without affecting model performance, improving rate-distortion efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Guided Path Sampling: Steering Diffusion Models Back on Track with Principled Path Guidance</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [Guided Path Sampling, Classifier-Free Guidance, iterative refinement, off-manifold, path stability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haosen Li, Wenshuo Chen, Shaofeng Liang, Lei Wang, Haozhe Jia, Yutao Yue</p>
</li>
<li class="">
<p><strong>institution:</strong> The Hong Kong University of Science and Technology (Guangzhou), Griffith University, Data61/CSIRO</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22881" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22881</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies a fundamental limitation where Classifier-Free Guidance (CFG) causes iterative refinement methods to diverge by pushing the sampling path off the data manifold. 2. Proposes Guided Path Sampling (GPS), a new paradigm that replaces CFG&#x27;s extrapolation with a principled, manifold-constrained interpolation to ensure path stability. 3. Devises an optimal scheduling strategy to dynamically adjust guidance strength, aligning semantic injection with the model&#x27;s natural coarse-to-fine generation process.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c84aa59d1a3cb3e65a9467831b10684291f67a9acaa193d7066d5e5ed9a57831_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c84aa59d1a3cb3e65a9467831b10684291f67a9acaa193d7066d5e5ed9a57831_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies that standard Classifier-Free Guidance (CFG) causes instability in iterative refinement for diffusion models by pushing the sampling path off the data manifold. To solve this, the authors propose Guided Path Sampling (GPS), a method that uses manifold-constrained interpolation to keep the path stable and includes an optimal guidance schedule. Experiments show GPS improves image quality and prompt adherence, establishing path stability as key for effective refinement.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [concept erasure, multimodal evaluation, inference-time robustness, cross-attention, latent perturbation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ju-Hsuan Weng, Jia-Wei Liao, Cheng-Fu Chou, Jun-Cheng Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> National Taiwan University, Academia Sinica (Research Center for Information Technology Innovation)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22877" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22877</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces M-ErasureBench, the first comprehensive multimodal benchmark for evaluating concept erasure methods across text prompts, learned embeddings, and inverted latents under white-box and black-box settings. 2. Identifies a critical vulnerability where existing erasure methods fail against non-textual input modalities, with concept reproduction rates exceeding 90%. 3. Proposes IRECE, a plug-and-play inference-time module that enhances robustness by localizing concepts via cross-attention and perturbing associated latents, reducing CRR by up to 40% while preserving image quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/017a1c0b9ad339359323b5dc5a3742c3451a832f8f2f62b44559e5a1d6ae4a66_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/017a1c0b9ad339359323b5dc5a3742c3451a832f8f2f62b44559e5a1d6ae4a66_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies that existing concept erasure methods for diffusion models are vulnerable to attacks using non-textual inputs like learned embeddings or inverted latents. To address this, the authors propose a new multimodal benchmark (M-ErasureBench) and a plug-and-play defense module (IRECE) that perturbs latents during inference. Experiments show IRECE significantly reduces concept reproduction rates under challenging attacks while maintaining visual quality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [multimodal fusion, text-guided segmentation, transformer-based architecture, lightweight model, 3D segmentation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hasan Faraz Khan, Noor Fatima, Muzammil Behzad</p>
</li>
<li class="">
<p><strong>institution:</strong> King Fahd University of Petroleum and Minerals, SDAIA-KFUPM Joint Research Center for Artificial Intelligence</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22878" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22878</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SwinTF3D, a lightweight multimodal fusion model for text-guided 3D medical image segmentation, integrating visual and linguistic representations. 2. Introduces an efficient fusion mechanism to align semantic text prompts with spatial structures in volumetric medical images. 3. Demonstrates competitive performance and significant efficiency gains on the BTCV dataset, offering a practical and interpretable paradigm for interactive clinical segmentation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/991651742357d8ab55dc27a29fa78a0c7b0f65e13d3fe1d6d260f8acea2238d9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/991651742357d8ab55dc27a29fa78a0c7b0f65e13d3fe1d6d260f8acea2238d9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes SwinTF3D, a lightweight multimodal model that uses a transformer-based visual encoder and a text encoder to perform text-guided 3D medical image segmentation. It achieves competitive accuracy on the BTCV dataset with low computational overhead, establishing a practical paradigm for interactive, resource-efficient clinical imaging.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [benchmark evaluation], [scientific intelligence, hierarchical benchmark, multi-disciplinary evaluation, multimodal inputs, dependency-aware framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yaping Zhang, Qixuan Zhang, Xingquan Zhang, Zhiyuan Chen, Wenwen Zhuang, Yupu Liang, Lu Xiang, Yang Zhao, Jiajun Zhang, Yu Zhou, Chengqing Zong</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences; University of the Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22899" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22899</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces HiSciBench, a novel hierarchical benchmark spanning five levels (Scientific Literacy to Scientific Discovery) to evaluate the complete scientific workflow. 2. Provides a comprehensive, multi-disciplinary dataset of 8,735 instances across six scientific fields, supporting multimodal and cross-lingual inputs. 3. Establishes an integrated, dependency-aware evaluation framework that reveals significant performance gaps in foundation models, especially on higher-order discovery tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d6954386f880faf48b86cfbd5d72eb78413ee39a02fe0f600306713ecc9bda_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d6954386f880faf48b86cfbd5d72eb78413ee39a02fe0f600306713ecc9bda_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces HiSciBench, a hierarchical and multi-disciplinary benchmark designed to evaluate the full spectrum of scientific intelligence in foundation models, from basic literacy to creative discovery. It contains thousands of multimodal instances across six disciplines and uses a dependency-aware framework for evaluation. The evaluation of leading models shows a sharp performance decline on complex discovery tasks, highlighting a key capability gap and setting a new standard for assessing scientific AI.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal learning], [audio-video fusion, instruction tuning, diffusion transformer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kai Liu, Jungang Li, Yuchong Sun, Shengqiong Wu, Jianzhang Gao, Daoan Zhang, Wei Zhang, Sheng Jin, Sicheng Yu, Geng Zhan, Jiayi Ji, Fan Zhou, Liang Zheng, Shuicheng Yan, Hao Fei, Tat-Seng Chua</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Singapore (NUS), Zhejiang University (ZJU), Renmin University of China (RUC), University of Rochester (UR), Hong Kong University of Science and Technology (Guangzhou) (HKUST(GZ))</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22905" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22905</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://JavisVerse.github.io/JavisGPT-page" target="_blank" rel="noopener noreferrer" class="">https://JavisVerse.github.io/JavisGPT-page</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes JavisGPT, the first unified multimodal LLM for joint audio-video comprehension and generation. 2. Introduces a novel architecture with a SyncFusion module and synchrony-aware queries to bridge a pretrained JAV-DiT generator for coherent output. 3. Constructs a large-scale, high-quality instruction dataset (JavisInst-Omni) with over 200K GPT-4o-curated dialogues for training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf62ed65dd48eaa9ab99c17e036a7566d58f5dc20a79de9bbd8c2622972c5c14_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf62ed65dd48eaa9ab99c17e036a7566d58f5dc20a79de9bbd8c2622972c5c14_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces JavisGPT, a unified multimodal LLM designed for joint audio-video understanding and generation. It features a novel encoder-LLM-decoder architecture with a SyncFusion module and is trained using a three-stage pipeline on a newly created large-scale instruction dataset. Experiments show it outperforms existing models, especially in complex, temporally synchronized tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [autonomous driving], [vision-language-action, latent reasoning, hierarchical parallel planning, trajectory generation, cognitive reasoning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qihang Peng, Xuesong Chen, Chenye Yang, Shaoshuai Shi, Hongsheng Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, CUHK MMLab, Voyager Research (Didi Chuxing)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22939" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22939</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A unified vision-language-action framework that transfers reasoning from discrete text to a continuous latent space, bridging the gap between VLM reasoning and control. 2. A Cognitive Latent Reasoner that compresses scene understanding into decision-oriented meta-action embeddings efficiently using only two VLM forward passes. 3. A Hierarchical Parallel Planner that generates multi-scale, causality-consistent trajectories in a single forward pass, enabling real-time performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c251a1d307ff03613dd19778f6efc36d76e43f6a1b9c6003437556f7e294898_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c251a1d307ff03613dd19778f6efc36d76e43f6a1b9c6003437556f7e294898_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes ColaVLA, a framework for autonomous driving that uses cognitive latent reasoning to efficiently translate vision-language understanding into continuous control actions, coupled with a hierarchical parallel planner for trajectory generation. It addresses key challenges in VLM-based planning, such as latency and the text-control mismatch. Experiments on nuScenes show state-of-the-art performance in both open-loop and closed-loop settings with improved efficiency and robustness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [density map, dense tiny objects, remote sensing, cross-attention, feature fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhicheng Zhao, Xuanang Fan, Lingma Sun, Chenglong Li, Jin Tang</p>
</li>
<li class="">
<p><strong>institution:</strong> Anhui University, Hefei University, 38th Research Institute of China Electronics Technology Group Corporation</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22949" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22949</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a Density Generation Branch (DGB) to model object distribution and provide spatial priors for focusing on dense regions. 2. Designed a Dense Area Focusing Module (DAFM) that uses density maps to efficiently focus computational resources on dense areas for local-global feature interaction. 3. Introduced a Dual Filter Fusion Module (DFFM) that uses discrete cosine transform and density-guided cross-attention to disentangle and fuse multi-scale features, enhancing complementarity and suppressing background.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46c8b52e74aab3f56417eac731b52218ad491a572ba57a0c07a3ea636702a32d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46c8b52e74aab3f56417eac731b52218ad491a572ba57a0c07a3ea636702a32d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of detecting dense, tiny objects in remote sensing imagery by proposing DRMNet, a network that uses density maps to guide adaptive feature learning. The method focuses computational resources on dense regions and fuses multi-scale features effectively. Experiments on AI-TOD and DTOD datasets show it outperforms existing methods, especially in high-density and occluded scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D object detection], [4D radar-camera fusion, wavelet attention, geometry-guided progressive fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Runwei Guan, Jianan Liu, Shaofeng Liang, Fangqiang Ding, Shanliang Yao, Xiaokai Bai, Daizong Liu, Tao Huang, Guoqiang Mao, Hui Xiong</p>
</li>
<li class="">
<p><strong>institution:</strong> Hong Kong University of Science and Technology (Guangzhou)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22972" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22972</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes WRCFormer, a novel framework for fusing raw 4D radar tensors with camera data using multi-view representations. 2. Designs a Wavelet Attention Module within a wavelet-based Feature Pyramid Network to enhance sparse signal representation. 3. Introduces a two-stage query-based, modality-agnostic Geometry-guided Progressive Fusion mechanism for efficient multi-view feature integration.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f316c737148800a364d90dba1fd5c27af3b2bbf070f93951908aadb83414d6f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f316c737148800a364d90dba1fd5c27af3b2bbf070f93951908aadb83414d6f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of sparse data and high computational cost in 4D radar-camera fusion for 3D object detection by proposing WRCFormer. The method fuses raw radar tensors and camera inputs using a wavelet-based feature enhancement module and a geometry-guided progressive fusion mechanism. Experiments on the K-Radar benchmark show state-of-the-art performance, particularly highlighting robustness in adverse weather conditions like sleet.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] RealCamo: Boosting Real Camouflage Synthesis with Layout Controls and Textual-Visual Guidance</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [camouflaged object detection], [camouflaged image generation, out-painting, layout control, textual-visual guidance, distribution divergence metric]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chunyuan Chen, Yunuo Cai, Shujuan Li, Weiyun Liang, Bin Wang, Jing Xu</p>
</li>
<li class="">
<p><strong>institution:</strong> Nankai University, Fudan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22974" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22974</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified out-painting framework with explicit layout controls to improve semantic coherence between foreground objects and generated backgrounds. 2. Constructs a multi-modal textual-visual condition combining fine-grained textual descriptions with texture-oriented background retrieval to enhance visual fidelity. 3. Introduces a background-foreground distribution divergence metric for the quantitative assessment of camouflage quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d516b18958a8e5fdfa40b3447ee12adaeb3b8e9ebb310665a607e78e4f770c8c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d516b18958a8e5fdfa40b3447ee12adaeb3b8e9ebb310665a607e78e4f770c8c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the gap between generated and real camouflaged images by proposing RealCamo, a framework that uses layout controls and multi-modal textual-visual guidance for realistic out-painting. The method improves both visual similarity and semantic consistency in generated backgrounds. Experiments demonstrate its effectiveness in producing high-quality camouflaged images.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [CLIP, contrastive learning, joint training, learnable text embeddings, InfoNCE loss]</p>
</li>
<li class="">
<p><strong>authors:</strong> Behnam Raoufi, Hossein Sharify, Mohamad Mahdee Ramezanee, Khosrow Hajsadeghi, Saeed Bagheri Shouraki</p>
</li>
<li class="">
<p><strong>institution:</strong> Sharif University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22969" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22969</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CLIP-Joint-Detect, a detector-agnostic framework for end-to-end joint training of object detectors with CLIP-style contrastive vision-language supervision. 2. Introduces a lightweight parallel head that aligns visual features with learnable class-specific text embeddings using a combination of InfoNCE contrastive loss and an auxiliary cross-entropy term. 3. Demonstrates consistent performance improvements on standard benchmarks (Pascal VOC, MS COCO) with both two-stage (Faster R-CNN) and one-stage (YOLO) architectures while preserving real-time inference speed.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22774054f225017727182e405a72ff7328bc0b14e0b1cf60711f822efbf76eff_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22774054f225017727182e405a72ff7328bc0b14e0b1cf60711f822efbf76eff_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the vulnerability of conventional object detectors to class imbalance and label noise by proposing CLIP-Joint-Detect. This framework integrates contrastive vision-language supervision into object detectors via a lightweight parallel head and joint training, aligning visual features with learnable text embeddings. The method improves detection performance across different architectures and datasets without sacrificing inference speed.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Spatial-aware Symmetric Alignment for Text-guided Medical Image Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [symmetric optimal transport, spatial-aware guidance, multimodal alignment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Linglin Liao, Qichuan Geng, Yu Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Capital Normal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22981" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22981</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Spatial-aware Symmetric Alignment (SSA) framework for handling hybrid medical texts containing locational, descriptive, and diagnostic information. 2. Introduces a Dual-granularity Symmetric Optimal Transport (DSOT) alignment algorithm to establish bi-directional fine-grained multimodal correspondences. 3. Devises a composite directional guidance strategy that explicitly introduces spatial constraints by constructing region-level guidance masks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/948f766c4a0c8be35a0a1f60d4115b4d951d7647f2f0f9cad9ba51469c19d161_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/948f766c4a0c8be35a0a1f60d4115b4d951d7647f2f0f9cad9ba51469c19d161_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of existing text-guided medical image segmentation methods, which struggle with hybrid texts and spatial constraints. It proposes the Spatial-aware Symmetric Alignment (SSA) framework, which uses a symmetric optimal transport mechanism and spatial guidance strategy to improve alignment. Experiments show SSA achieves state-of-the-art performance, especially for lesions with spatial relational constraints.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PoseStreamer: A Multi-modal Framework for 6DoF Pose Estimation of Unseen Moving Objects</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [pose estimation], [6DoF pose estimation, multi-modal fusion, event camera, high-speed tracking, unseen objects]</p>
</li>
<li class="">
<p><strong>authors:</strong> Huiming Yang, Linglin Liao, Fei Ding, Sibo Wang, Zijian Zeng</p>
</li>
<li class="">
<p><strong>institution:</strong> Renmin University of China</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22979" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22979</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes PoseStreamer, a multi-modal framework integrating an Adaptive Pose Memory Queue, Object-centric 2D Tracker, and Ray Pose Filter for robust 6DoF pose estimation in high-speed scenarios. 2. Introduces a novel multi-modal dataset, MoCapCube6D, for benchmarking performance under rapid motion. 3. Demonstrates superior accuracy and generalizability as a template-free framework for unseen moving objects.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9f0bfab0dc1a30f5b4f232838a293d0e91da6ca620be227db20abf46e9dd7e3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9f0bfab0dc1a30f5b4f232838a293d0e91da6ca620be227db20abf46e9dd7e3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of 6DoF pose estimation for unseen objects in high-speed, low-light scenarios where RGB cameras suffer from motion blur. It proposes PoseStreamer, a multi-modal framework that fuses RGB and event camera data with three novel components for temporal consistency, 2D tracking priors, and geometric refinement. Experiments show the framework achieves superior accuracy in high-speed scenarios and strong generalizability for unseen objects.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] YOLO-IOD: Towards Real Time Incremental Object Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [incremental learning, catastrophic forgetting, knowledge distillation, YOLO, pseudo-labeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shizhou Zhang, Xueqiang Lv, Yinghui Xing, Qirui Wu, Di Xu, Chen Zhao, Yanning Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Northwestern Polytechnical University, Huawei</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22973" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22973</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1) Identifies three key knowledge conflicts (foreground-background confusion, parameter interference, misaligned knowledge distillation) causing forgetting in YOLO-based incremental detectors. 2) Proposes the YOLO-IOD framework with three novel components (CPR, IKS, CAKD) to address these conflicts. 3) Introduces a new benchmark, LoCo COCO, designed to prevent data leakage for more realistic incremental learning evaluation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47f65afece41f4627aa9576946c71bab7a7f85810eb8f6c061f75f4e70de8de8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47f65afece41f4627aa9576946c71bab7a7f85810eb8f6c061f75f4e70de8de8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of catastrophic forgetting in real-time incremental object detection (IOD) for YOLO detectors. It proposes YOLO-IOD, a framework built on YOLO-World that uses pseudo-label refinement, kernel selection, and asymmetric knowledge distillation to mitigate forgetting. The method shows superior performance with minimal forgetting on both conventional and a newly introduced, more realistic benchmark.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Reverse Personalization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [face anonymization], [diffusion inversion, identity-guided conditioning, attribute-controllable anonymization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Han-Wei Kung, Tuomas Varanka, Nicu Sebe</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Trento, University of Oulu</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22984" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22984</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/hanweikung/reverse-personalization" target="_blank" rel="noopener noreferrer" class="">https://github.com/hanweikung/reverse-personalization</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a reverse personalization framework for face anonymization using conditional diffusion inversion, eliminating the need for text prompts or subject-specific fine-tuning. 2. Incorporates an identity-guided conditioning branch to generalize anonymization to subjects not present in the model&#x27;s pre-training data. 3. Enables attribute-controllable anonymization, allowing users to preserve or modify specific facial attributes while removing identity, a capability lacking in prior methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc89346c2839fea97adb56c783375419bc47c5a059c9a550f5f7d39c86d33657_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc89346c2839fea97adb56c783375419bc47c5a059c9a550f5f7d39c86d33657_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of removing identity-specific features from facial images while preserving other attributes and scene context. It proposes a reverse personalization method based on conditional diffusion inversion and an identity-guided conditioning branch, which allows for direct image manipulation without text prompts. The method achieves state-of-the-art performance in balancing identity removal, attribute preservation, and image quality, while also offering user control over retained attributes.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Low-Cost UAV Deep Learning Pipeline for Integrated Apple Disease Diagnosis,Freshness Assessment, and Fruit Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [UAV, YOLOv8, ResNet50, VGG16, Edge Inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Soham Dutta, Soham Banerjee, Sneha Mahata, Anindya Sen, Sayantani Datta</p>
</li>
<li class="">
<p><strong>institution:</strong> Heritage Institute of Technology, Kolkata</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22990" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22990</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A unified UAV-based pipeline integrating leaf disease detection, apple freshness classification, and fruit localization. 2. A cost-effective hardware architecture using ESP32-CAM and Raspberry Pi for offline, on-site inference. 3. High-performance results using only RGB sensors as a low-cost alternative to multispectral solutions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7de7053032c557a59ed3742c120ae0bca3d789437f306b1fbbaefeaaa1ae6a22_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7de7053032c557a59ed3742c120ae0bca3d789437f306b1fbbaefeaaa1ae6a22_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a low-cost, integrated UAV pipeline using RGB cameras and deep learning models (ResNet50, VGG16, YOLOv8) to perform apple leaf disease diagnosis, fruit freshness assessment, and detection in orchards. The system runs offline on edge devices like Raspberry Pi and ESP32-CAM. Experiments show high accuracy, providing a practical and affordable alternative to expensive multispectral systems for precision agriculture.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object recognition], [Geo-Semantic Contextual Graph, graph-based classifier, contextual reasoning, panoptic segmentation, metric depth]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ciprian Constantinescu, Marius Leordeanu</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Science and Technology POLITEHNICA Bucharest</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23024" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23024</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel method for constructing a Geo-Semantic Contextual Graph (GSCG) from a monocular image by fusing metric depth with panoptic and material segmentation. 2. A specialized graph-based classifier that aggregates features from a target object, its neighbors, and the global scene context for classification. 3. Demonstrating that the explicit, structured, and interpretable context of the GSCG significantly outperforms context-agnostic and strong baseline models on object recognition.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4c46fbf20c738319733f8ca7df800fc791d3a2a749f28c23b0751806ad4bb4a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4c46fbf20c738319733f8ca7df800fc791d3a2a749f28c23b0751806ad4bb4a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a novel framework for contextual object classification by first constructing a Geo-Semantic Contextual Graph (GSCG) from a single image to represent objects and their relationships, and then using a graph-based classifier to leverage this context. The method achieves a classification accuracy of 73.4%, dramatically outperforming context-agnostic models and strong baselines like fine-tuned ResNets and a multimodal LLM, highlighting the power of structured, interpretable scene context.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D visual grounding], [open-world, zero-shot, active cognition-based reasoning, object lookup table, visual language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenyuan Huang, Zhao Wang, Zhou Wei, Ting Huang, Fang Zhao, Jian Yang, Zhenyu Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanjing University, China Mobile Zijin Innovation Institute</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23020" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23020</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes OpenGround, a novel zero-shot framework for open-world 3D visual grounding that overcomes the limitation of pre-defined object categories. 2. Introduces the Active Cognition-based Reasoning (ACR) module to progressively augment VLM cognition via a cognitive task chain and a dynamically updated Object Lookup Table (OLT). 3. Presents a new dataset named OpenTarget with over 7000 object-description pairs to evaluate open-world 3D grounding performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dca85890cab234049b1698ab9f6ade12ea02b16f297cec04cbcb907dcdffb7be_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dca85890cab234049b1698ab9f6ade12ea02b16f297cec04cbcb907dcdffb7be_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of existing 3D visual grounding methods that rely on a pre-defined object lookup table, which restricts their use in open-world scenarios. The authors propose OpenGround, a zero-shot framework featuring an Active Cognition-based Reasoning module that dynamically expands the model&#x27;s cognitive scope to handle undefined objects. The method achieves competitive or state-of-the-art results on standard benchmarks and shows a 17.6% improvement on their new OpenTarget dataset.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Interpretable Gallbladder Ultrasound Diagnosis: A Lightweight Web-Mobile Software Platform with Real-Time XAI</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [MobResTaNet, Explainable AI (XAI), lightweight model, ultrasound diagnosis, web-mobile platform]</p>
</li>
<li class="">
<p><strong>authors:</strong> Fuyad Hasan Bhoyan, Prashanta Sarker, Parsia Noor Ethila, Md. Emon Hossain, Md Kaviul Hossain, Md Humaion Kabir Mehedi</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Liberal Arts Bangladesh, BRAC University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23033" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23033</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a hybrid deep learning model (MobResTaNet) for classifying ten gallbladder conditions from ultrasound images with high accuracy (99.85%) and low parameter count (2.24M). 2. Developed an interpretable diagnostic system with real-time Explainable AI (XAI) visualizations to support transparent clinical decision-making. 3. Deployed the system as an efficient and accessible web-mobile software platform using technologies like HTML, CSS, JavaScript, Bootstrap, and Flutter for point-of-care use.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c6a6693ffd9f4375cd5a781c2544fba169bc30bc3cbb7590b1562ea78ba6678_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c6a6693ffd9f4375cd5a781c2544fba169bc30bc3cbb7590b1562ea78ba6678_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of interpreting gallbladder ultrasound images by developing an AI-driven diagnostic software. The core method is a lightweight hybrid deep learning model called MobResTaNet, which classifies diseases and provides real-time, interpretable predictions via XAI. The main conclusion is that the system achieves high accuracy with a small model size and is successfully deployed as accessible web and mobile applications for clinical support.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] An Architecture-Led Hybrid Report on Body Language Detection Project</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video understanding], [vision-language models, structured generation, bounding boxes, mixture-of-experts, video analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Thomson Tong, Diba Darooneh</p>
</li>
<li class="">
<p><strong>institution:</strong> None</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23028" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23028</a></p>
</li>
<li class="">
<p><strong>code:</strong> BodyLanguageDetection repository [1]</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides an architecture-led analysis of two modern VLMs (Qwen2.5-VL-7B-Instruct and Llama-4-Scout-17B-16E-Instruct) for a practical task. 2. Maps model architectural properties to a concrete video-to-artifact pipeline for person detection and attribute extraction. 3. Explicitly defines and analyzes critical system constraints and limitations arising from model behavior, such as semantic vs. syntactic correctness and frame-local identifiers.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a618c048bc336ad2ade96a7a97cf301fb10fee2c9c8e7bc16556348f1c0c4b9d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a618c048bc336ad2ade96a7a97cf301fb10fee2c9c8e7bc16556348f1c0c4b9d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This report analyzes two vision-language models (VLMs) and connects their architectures to a practical system for detecting people and their emotions in video frames. The system prompts VLMs to generate structured outputs like bounding boxes, validates the output structure, and can render annotated videos. The core conclusion is that understanding model architecture is crucial for designing robust interfaces and making defensible claims, as VLMs can produce syntactically correct but semantically incorrect outputs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [semantic segmentation], [semi-supervised learning, pseudo-label drift, vision-language model, self-supervised model, dual-student architecture]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yi Zhou, Xuechao Zou, Shun Zhang, Kai Li, Shiying Wang, Jingming Chen, Congyan Lang, Tengfei Cao, Pin Tao, Yuanchun Shi</p>
</li>
<li class="">
<p><strong>institution:</strong> Qinghai University, Beijing Jiaotong University, Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23035" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23035</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://xavierjiezou.github.io/Co2S/" target="_blank" rel="noopener noreferrer" class="">https://xavierjiezou.github.io/Co2S/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Co2S, a stable semi-supervised RS segmentation framework that fuses priors from vision-language and self-supervised models. 2. Introduced a heterogeneous dual-student architecture with ViT models initialized by CLIP and DINOv3 to mitigate error accumulation. 3. Developed an explicit-implicit semantic co-guidance mechanism and a global-local feature collaborative fusion strategy to enhance semantic consistency and segmentation precision.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c3e7bd7c766cb9388933edf8bea0a465af8034e2f1995e5edc4eaaa3062c87_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c3e7bd7c766cb9388933edf8bea0a465af8034e2f1995e5edc4eaaa3062c87_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of pseudo-label drift in semi-supervised remote sensing image segmentation. It proposes the Co2S framework, which uses a dual-student architecture combining CLIP and DINOv3 models, along with co-guidance and co-fusion mechanisms, to improve stability and accuracy. Experiments on six datasets show the method achieves leading performance across various scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] 3D sans 3D Scans: Scalable Pre-training from Video-Generated Point Clouds</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D scene understanding], [self-supervised learning, point clouds, video reconstruction, geometric regularization, indoor segmentation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ryousuke Yamada, Kohsuke Ide, Yoshihiro Fukuhara, Hirokatsu Kataoka, Gilles Puy, Andrei Bursuc, Yuki M. Asano</p>
</li>
<li class="">
<p><strong>institution:</strong> AIST, University of Technology Nuremberg, INRIA (Valeo.ai)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23042" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23042</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LAM3C, a self-supervised framework for learning 3D representations from video-generated point clouds without real 3D scans. 2. Introduces RoomTours, a large-scale dataset of 49,219 video-generated point cloud scenes from web videos. 3. Designs a noise-regularized loss to enforce local geometric smoothness and stabilize feature learning on noisy point clouds.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/29450a379b6659077d87333387a81a32e83ef28aa568a2d2618219886ad4f564_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/29450a379b6659077d87333387a81a32e83ef28aa568a2d2618219886ad4f564_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high cost of collecting 3D scans by proposing a method to learn 3D representations from unlabeled videos. The core method, LAM3C, learns from video-generated point clouds using a novel noise-regularized loss and a new dataset called RoomTours. The results show that this approach outperforms previous self-supervised methods on indoor segmentation tasks, demonstrating that videos are a viable and abundant source for 3D pre-training.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Video-BrowseComp: Benchmarking Agentic Video Research on Open Web</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video understanding], [agentic video research, open-web benchmark, temporal visual evidence, video browsing, multimodal reasoning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhengyang Liang, Yan Shu, Xiangrui Liu, Minghao Qin, Kaixin Liang, Paolo Rota, Nicu Sebe, Zheng Liu, Lizi Liao</p>
</li>
<li class="">
<p><strong>institution:</strong> Singapore Management University, University of Trento, Beijing Academy of Artificial Intelligence, Beijing University of Posts and Telecommunications, Hong Kong Polytechnic University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23044" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23044</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://liang-zhengyang.github.io/video-browsecomp/" target="_blank" rel="noopener noreferrer" class="">https://liang-zhengyang.github.io/video-browsecomp/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Video-BrowseComp, the first benchmark for open-web agentic video reasoning, comprising 210 questions that require active navigation of video timelines. 2. Enforces a mandatory dependency on temporal visual evidence, ensuring answers cannot be derived from text search alone, thus evaluating true video grounding. 3. Reveals a critical performance bottleneck in state-of-the-art models, showing they rely heavily on textual proxies and fail in metadata-sparse, dynamic video domains.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60d4510d9ebf0c25d20191be00b854d890c6942ebcae7fafd2869bb49e23acd9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60d4510d9ebf0c25d20191be00b854d890c6942ebcae7fafd2869bb49e23acd9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a gap in evaluating proactive, agentic video research on the open web. To address this, it introduces the Video-BrowseComp benchmark, which requires models to actively navigate and reason over video timelines to answer questions. The evaluation shows that even advanced models perform poorly, highlighting a critical reliance on text and a failure in visually-grounded video understanding.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [vision-language models], [Mask Fine-Tuning (MFT), Parameter Efficient Fine-Tuning (PEFT), Low-Rank Adaptation (LoRA), structural reparameterization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mingyuan Zhang, Yue Bai, Yifan Wang, Yiyang Huang, Yun Fu</p>
</li>
<li class="">
<p><strong>institution:</strong> Northeastern University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23073" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23073</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Ming-K9/MFT-VLM" target="_blank" rel="noopener noreferrer" class="">https://github.com/Ming-K9/MFT-VLM</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes applying Mask Fine-Tuning (MFT), a structural reparameterization method, to Vision-Language Models (VLMs) for adaptation., 2. Demonstrates that MFT, which learns gating scores for weights instead of updating them, consistently outperforms LoRA variants and full fine-tuning., 3. Reveals that effective adaptation can emerge from reestablishing connections within a model&#x27;s existing knowledge, not just from updating weights.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92b8916bd08492d9f9fbfd3b3a163d12c7de7a6a1fe0822bc6d711ca118dfb28_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92b8916bd08492d9f9fbfd3b3a163d12c7de7a6a1fe0822bc6d711ca118dfb28_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper rethinks fine-tuning for Vision-Language Models (VLMs) by applying Mask Fine-Tuning (MFT), a method that learns to gate existing weights instead of updating them. Experiments show MFT surpasses both Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA and full fine-tuning, demonstrating that reorganizing internal subnetworks is a powerful alternative to weight updates.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MedSAM-based lung masking for multi-label chest X-ray classification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [MedSAM, lung segmentation, multi-label classification, chest X-ray, spatial prior]</p>
</li>
<li class="">
<p><strong>authors:</strong> Brayden Miao, Zain Rehman, Xin Miao, Siming Liu, Jianjie Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Missouri State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23089" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23089</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a segmentation-guided CXR classification pipeline that integrates a fine-tuned MedSAM model for lung region extraction. 2. Empirically demonstrates that the effect of lung masking is task-dependent and architecture-dependent, revealing a trade-off between abnormality classification and normal case screening. 3. Suggests that lung masking should be treated as a controllable spatial prior tailored to the model backbone and clinical objective, rather than a uniform preprocessing step.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78073436289f27d236dc3f5c9f70b55eb6480c3a7ea02e8c30b8eb1b8e59faa9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78073436289f27d236dc3f5c9f70b55eb6480c3a7ea02e8c30b8eb1b8e59faa9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a method that uses a fine-tuned MedSAM model to extract lung masks from chest X-rays to guide multi-label abnormality classification. The study finds that the impact of masking depends on the task and model architecture, with loose masking improving normal case screening while tight masking aids training efficiency. The conclusion is that lung masking should be a tunable spatial prior aligned with the specific clinical goal and model, not a fixed step.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image synthesis], [diffusion model, disentangled representation, pathological residual, anatomical manifold, seam-aware fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jian Wang, Sixing Rong, Jiarui Xing, Yuling Xu, Weide Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Harvard Medical School, Northeastern University, Yale University, Nanchang University, Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23130" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23130</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified generative framework that reformulates MRI pathology synthesis as a disentangled additive deviation on a stable anatomical manifold. 2. Introduces a Deviation-Space Diffusion Model to learn the conditional distribution of pathological residuals, preserving global structure while modeling local variations. 3. Incorporates a seam-aware fusion strategy and an inference-time stabilization module to suppress boundary artifacts and ensure spatial coherence in synthesized lesions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b87a518aaabc4319ec5eba26d8ed0e23b50b1f611b88f2d8241ebecec605cc8c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b87a518aaabc4319ec5eba26d8ed0e23b50b1f611b88f2d8241ebecec605cc8c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes PathoSyn, a novel framework for synthesizing pathological MRI images by decomposing the task into deterministic anatomical reconstruction and stochastic modeling of pathological deviations using a diffusion model. This approach preserves anatomical integrity while generating realistic lesion heterogeneity. Evaluations show it outperforms existing baselines in perceptual realism and anatomical fidelity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Domain-Shift Immunity in Deep Deformable Registration via Local Feature Representations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image registration], [deformable registration, domain shift, local features, UniReg, cross-modal]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mingzhen Shao, Sarang Joshi</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Utah</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23142" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23142</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that domain-shift immunity is an inherent property of deep deformable registration models, stemming from their reliance on local feature representations. 2. Introduces UniReg, a universal registration framework that decouples feature extraction from deformation estimation to validate this mechanism. 3. Reveals that failures of conventional CNN-based models under modality shift originate from dataset-induced biases in early convolutional layers.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2267b3cdf4a5fdca597653c5a64d1db35cb34367c8cc7c7fcb14b4905e0d492b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2267b3cdf4a5fdca597653c5a64d1db35cb34367c8cc7c7fcb14b4905e0d492b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the robustness of deep learning models for deformable image registration under domain shift. It proposes UniReg, a framework using fixed feature extractors and a UNet, to show that reliance on local features provides inherent domain-shift immunity. The findings indicate that local feature consistency is key to robustness, and early CNN layers are the source of failure in conventional models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GeoTeacher: Geometry-Guided Semi-Supervised 3D Object Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D object detection], [semi-supervised learning, teacher-student framework, geometric relation supervision, voxel-wise augmentation, pseudo-labeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jingyu Li, Xiaolong Zhao, Zhe Liu, Wenxiao Wu, Li Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Fudan University, Shanghai Innovation Institute, Tongji University, The University of Hong Kong, Huazhong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23147" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23147</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/SII-Whaleice/GeoTeacher" target="_blank" rel="noopener noreferrer" class="">https://github.com/SII-Whaleice/GeoTeacher</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A keypoint-based geometric relation supervision module that transfers the teacher model&#x27;s knowledge of object geometry to the student model. 2. A voxel-wise data augmentation strategy with a distance-decay mechanism to increase the diversity of object geometries while preserving distant object integrity. 3. A flexible framework that can be combined with different semi-supervised 3D object detection methods to further improve their performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8bb5977bbadb9714d8d0a0863da6b57b54d85977aa07fe111b3071b1ec4bb55_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8bb5977bbadb9714d8d0a0863da6b57b54d85977aa07fe111b3071b1ec4bb55_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes GeoTeacher, a geometry-guided semi-supervised 3D object detection method. It introduces a geometric relation supervision module and a novel voxel-wise augmentation strategy to enhance the student model&#x27;s ability to capture object geometries when labeled data is limited. Experiments on ONCE and Waymo datasets show the method achieves state-of-the-art performance and demonstrates good generalization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robotic imitation learning], [world model, vision-language-action (VLA) model, inverse dynamics model, surgical robotics, synthetic data generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yufan He, Pengfei Guo, Mengya Xu, Zhaoshuo Li, Andriy Myronenko, Dillan Imans, Bingjie Liu, Dongren Yang, Mingxue Gu, Yongnan Ji, Yueming Jin, Ren Zhao, Baiyong Shen, Daguang Xu</p>
</li>
<li class="">
<p><strong>institution:</strong> NVIDIA, The Chinese University of Hong Kong, Sung Kyun Kwan University, Wenzhou Medical University, National University of Singapore, Ruijin Hospital</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23162" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23162</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Curated the Surgical Action-Text Alignment (SATA) dataset with detailed text descriptions for surgical robot actions. 2. Built SurgWorld, a generative world model capable of producing diverse and realistic synthetic surgical videos. 3. Pioneered the use of an inverse-dynamics model to infer pseudo-kinematics from synthetic videos, creating synthetic paired video-action data for training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd5e2ca51f8df31fc2fe20ced16d79b01d4a091736ac738cdfcd0c8fda793a16_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd5e2ca51f8df31fc2fe20ced16d79b01d4a091736ac738cdfcd0c8fda793a16_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the data scarcity problem in autonomous surgical robotics by proposing SurgWorld, a world model that generates realistic synthetic surgical videos. The method uses an inverse dynamics model to infer robot actions from these videos, creating a large-scale paired dataset to train a Vision-Language-Action policy. The resulting policy significantly outperforms models trained only on real demonstrations on a real surgical robot platform.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [text-image alignment evaluation], [reinforcement learning, multimodal large language models, visual reasoning, element-level grounding, group relative policy optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Fulin Shi, Wenyi Xiao, Bin Chen, Liang Din, Leilei Gan</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Alibaba Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23169" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23169</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes REVEALER, a unified framework for element-level text-image alignment evaluation based on a structured &quot;grounding-reasoning-conclusion&quot; visual reasoning paradigm. 2. Introduces a reinforcement learning optimization method using Group Relative Policy Optimization (GRPO) with a composite reward function to enhance judgment quality. 3. Demonstrates state-of-the-art performance and superior inference efficiency across multiple benchmarks compared to existing methods and proprietary models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/448c922a2b6a208469892f6edb23c470ac37ce4a9bbde51c3f92b2b86f87267e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/448c922a2b6a208469892f6edb23c470ac37ce4a9bbde51c3f92b2b86f87267e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of coarse-grained and non-interpretable evaluation of text-to-image model outputs. It proposes REVEALER, a framework that uses reinforcement-guided visual reasoning with MLLMs to perform fine-grained, element-level alignment assessment. Experiments show REVEALER achieves state-of-the-art performance and is more efficient than existing iterative reasoning methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Machine Learning-Assisted Vocal Cord Ultrasound Examination: Project VIPR</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [vocal cord ultrasound, image segmentation, VIPRnet, vocal cord paralysis, classification model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Will Sebelik-Lassiter, Evan Schubert, Muhammad Alliyu, Quentin Robbins, Excel Olatunji, Mustafa Barry</p>
</li>
<li class="">
<p><strong>institution:</strong> Milwaukee School of Engineering, Emory University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23177" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23177</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a machine learning pipeline for automated analysis of vocal cord ultrasound (VCUS) videos. 2. Created a segmentation model to automatically identify vocal cords in ultrasound images with 96% validation accuracy. 3. Proposed VIPRnet, a classification model to distinguish normal vocal cords from vocal cord paralysis (VCP) with 99% validation accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5abcb457a7fbefffa7d7836af553a8db8fa248fdd34a0459a027299b3ce2ce44_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5abcb457a7fbefffa7d7836af553a8db8fa248fdd34a0459a027299b3ce2ce44_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a machine learning-assisted system to automate the analysis of vocal cord ultrasound (VCUS) to reduce operator dependency. The method involves segmenting the vocal cords and classifying them as normal or paralyzed using models trained on frames from volunteer videos. The results show high validation accuracy (96% for segmentation, 99% for classification), indicating promise for improving diagnostic accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D object detection], [Gaussian Splatting, Voxel Representation, Multi-View Learning, Synergistic Learning, Geometry Extraction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yi Zhang, Yi Wang, Lei Yao, Lap-Pui Chau</p>
</li>
<li class="">
<p><strong>institution:</strong> The Hong Kong Polytechnic University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23176" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23176</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel synergistic framework (GVSynergy-Det) that integrates continuous Gaussian and discrete voxel representations for complementary geometric feature learning in 3D object detection. 2. Adapts generalizable Gaussian Splatting to extract geometric features for detection and develops a cross-representation enhancement mechanism to enrich voxel features with Gaussian-derived details. 3. Achieves state-of-the-art performance on major indoor benchmarks (ScanNetV2, ARKitScenes) without requiring dense 3D supervision (e.g., depth or point clouds).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1b5d4bbef1eb01585a47bc411dd2af3afb443450d925569c6ba83c498c225b6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1b5d4bbef1eb01585a47bc411dd2af3afb443450d925569c6ba83c498c225b6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces GVSynergy-Det, a novel image-based 3D object detection framework that synergistically combines Gaussian and voxel representations to capture complementary geometric information without dense 3D supervision. The method integrates features from both representations through a learnable mechanism, enabling more accurate object localization. Experiments show it achieves state-of-the-art results on indoor benchmarks, outperforming existing methods while maintaining a compact model size.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [autonomous driving, 3D scene understanding], [3D Gaussian Splatting, Driving World Model, Multi-Modal Generation, Vision-Language Model, Scene Representation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tianchen Deng, Xuefeng Chen, Yi Chen, Qu Chen, Yuyao Xu, Lijin Yang, Le Xu, Yu Zhang, Bo Zhang, Wuxiong Huang, Hesheng Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, Tsinghua University, MEGVII Technology, Mach Drive</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23180" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23180</a></p>
</li>
<li class="">
<p><strong>code:</strong> this https URL</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified Driving World Model framework based on 3D Gaussian scene representation for joint 3D scene understanding and multi-modal generation. 2. Introduces early modality alignment by embedding linguistic features into 3D Gaussian primitives and a task-aware language-guided sampling strategy to inject compact 3D tokens into an LLM. 3. Designs a dual-condition multi-modal generation model that uses high-level language conditions and low-level image conditions to guide the generation process.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/035656391cb2f9ea8e1b71368c7ca76202f290f6e67f8641414807e7be989eb8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/035656391cb2f9ea8e1b71368c7ca76202f290f6e67f8641414807e7be989eb8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes GaussianDWM, a novel Driving World Model framework that uses 3D Gaussians as a scene representation to unify 3D scene understanding and multi-modal generation for autonomous driving. It aligns text with the 3D scene via Gaussian primitives and uses a dual-condition model for generation. The method achieves state-of-the-art performance on nuScenes and NuInteract datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ForCM: Forest Cover Mapping from Multispectral Sentinel-2 Image by Integrating Deep Learning with Object-Based Image Analysis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [semantic segmentation], [Object-Based Image Analysis (OBIA), Deep Learning, Sentinel-2, Forest Cover Mapping, UNet]</p>
</li>
<li class="">
<p><strong>authors:</strong> Maisha Haque, Israt Jahan Ayshi, Sadaf M. Anis, Nahian Tasnim, Mithila Moontaha, Md. Sabbir Ahmed, Muhammad Iqbal Hossain, Mohammad Zavid Parvez, Subrata Chakraborty, Biswajeet Pradhan, Biswajit Banik</p>
</li>
<li class="">
<p><strong>institution:</strong> BRAC University, Charles Sturt University, University of Technology Sydney</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23196" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23196</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes &quot;ForCM&quot;, a novel method that integrates Object-Based Image Analysis (OBIA) with various Deep Learning models for forest cover mapping. 2. Evaluates and compares the performance of multiple DL models (UNet, UNet++, ResUNet, AttentionUNet, ResNet50-Segnet) combined with OBIA against traditional OBIA. 3. Demonstrates the practical application of free tools like QGIS for accurate environmental mapping, achieving improved accuracy (up to 95.64%) over traditional methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/477043abcfb8b4e851144d00c2ae33596765e1b2a27375709fcbcfc01f79e3e5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/477043abcfb8b4e851144d00c2ae33596765e1b2a27375709fcbcfc01f79e3e5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes ForCM, a method for forest cover mapping that combines Object-Based Image Analysis with Deep Learning models like ResUNet and AttentionUNet using Sentinel-2 imagery. The results show that this integration significantly improves mapping accuracy compared to traditional OBIA alone, demonstrating the potential of accessible tools for environmental monitoring.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Exploring Syn-to-Real Domain Adaptation for Military Target Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [domain adaptation, synthetic-to-real, Unreal Engine, military target detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jongoh Jeong, Youngjin Oh, Gyeongrae Nam, Jeongeun Lee, Kuk-Jin Yoon</p>
</li>
<li class="">
<p><strong>institution:</strong> Korea Advanced Institute of Science and Technology (KAIST), LIG Nex1</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23208" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23208</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed generating a synthetic RGB dataset for military target detection using Unreal Engine to address the lack of real-world data. 2. Conducted and benchmarked synthetic-to-real domain adaptation experiments on a new train-val dataset pair for military targets. 3. Found that domain adaptation methods using minimal supervision (e.g., object class hints) substantially outperform unsupervised or semi-supervised methods in this challenging cross-domain setting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b67711a2133943d8083642ed36e63614aae288f161e8fc258230c5d03bacb3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b67711a2133943d8083642ed36e63614aae288f161e8fc258230c5d03bacb3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of military target detection by generating synthetic RGB data using Unreal Engine to overcome the lack of real datasets and high costs of SAR data. It benchmarks state-of-the-art domain adaptation methods on this synthetic-to-real task and finds that methods using minimal supervision achieve the best performance, highlighting remaining challenges in this area.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [diffusion models], [diffusion timestep selection, few-shot learning, dense prediction, Taskonomy, parameter-efficient fine-tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Changgyoon Oh, Jongoh Jeong, Jegyeong Cho, Kuk-Jin Yoon</p>
</li>
<li class="">
<p><strong>institution:</strong> KAIST (Korea Advanced Institute of Science and Technology)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23210" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23210</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Task-aware Timestep Selection (TTS) module to adaptively select ideal diffusion timesteps for a given task based on losses and similarity scores. 2. Introduces a Timestep Feature Consolidation (TFC) module to consolidate the selected timestep features to improve dense prediction performance. 3. Presents a framework that, with a parameter-efficient fine-tuning adapter, achieves superior few-shot dense prediction performance on unseen tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a43fa8fc87c3dacfd338cae33c86a18032f1b16e9c26b532e45a4b6342b44c0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a43fa8fc87c3dacfd338cae33c86a18032f1b16e9c26b532e45a4b6342b44c0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the sub-optimal performance of diffusion models in few-shot dense prediction tasks due to heuristic timestep feature selection. It proposes a framework with learnable modules (TTS and TFC) to adaptively select and consolidate diffusion timestep features, which is validated on the Taskonomy dataset, showing superior performance in universal few-shot learning scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AVOID: The Adverse Visual Conditions Dataset with Obstacles for Driving Scene Understanding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [autonomous driving, adverse conditions, semantic segmentation, depth estimation, multi-task learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jongoh Jeong, Taek-Jin Song, Jong-Hwan Kim, Kuk-Jin Yoon</p>
</li>
<li class="">
<p><strong>institution:</strong> KAIST (Korea Advanced Institute of Science and Technology)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23215" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23215</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the AVOID dataset, a new simulated driving dataset focused on unexpected small road obstacles captured under diverse adverse weather and time conditions. 2. Provides rich multi-modal annotations per image, including semantic and depth maps, LiDAR data (raw and semantic), and waypoints to support various perception tasks. 3. Benchmarks real-time obstacle detection networks and proposes ablation studies using a comprehensive multi-task network for semantic segmentation, depth, and waypoint prediction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1278effa0d2d3d0566d9f768237cda657302c10c260f1b221c5739281af453cb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1278effa0d2d3d0566d9f768237cda657302c10c260f1b221c5739281af453cb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces the AVOID dataset to address the lack of driving datasets containing small, unexpected road obstacles under varied adverse conditions. The dataset, collected in simulation, provides rich multi-modal annotations and is used to benchmark real-time obstacle detection and multi-task perception models. The work aims to improve the robustness of visual perception for autonomous driving in challenging scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MM-UAVBench: How Well Do Multimodal Large Language Models See, Think, and Plan in Low-Altitude UAV Scenarios?</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal evaluation], [multimodal large language models, unmanned aerial vehicles, benchmark, low-altitude scenarios, spatial bias]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shiqi Dai, Zizhi Ma, Zhicong Luo, Xuesong Yang, Yibin Huang, Wanyue Zhang, Chi Chen, Zonghao Guo, Wang Xu, Yufei Sun, Maosong Sun</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Nankai University, Northwest Polytechnical University, Chinese Academy of Sciences, Harbin Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23219" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23219</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/MM-UAV/MM-UAVBench" target="_blank" rel="noopener noreferrer" class="">https://github.com/MM-UAV/MM-UAVBench</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MM-UAVBench, a comprehensive benchmark for evaluating MLLMs in low-altitude UAV scenarios across perception, cognition, and planning dimensions. 2. Constructs a dataset of over 5.7K manually annotated questions derived from real-world UAV data, covering 19 sub-tasks. 3. Through extensive experiments on 16 MLLMs, identifies critical bottlenecks like spatial bias and multi-view understanding that hinder model performance in these scenarios.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87d6840ebc06b3a6509f941a74651b1ac51e173bc32596cbc933f96b55e0db61_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87d6840ebc06b3a6509f941a74651b1ac51e173bc32596cbc933f96b55e0db61_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces MM-UAVBench, a new benchmark designed to evaluate the capabilities of Multimodal Large Language Models (MLLMs) in low-altitude Unmanned Aerial Vehicle (UAV) scenarios. The benchmark systematically tests models on perception, cognition, and planning using over 5.7K annotated questions from real UAV data. The evaluation reveals that current MLLMs struggle with the complex demands of low-altitude environments, highlighting key bottlenecks like spatial bias.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Holi-DETR: Holistic Fashion Item Detection Leveraging Contextual Information</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [Detection Transformer (DETR), Contextual Information, Holistic Detection, Fashion Item Detection, Co-occurrence Relationship]</p>
</li>
<li class="">
<p><strong>authors:</strong> Youngchae Kwon, Jinyoung Choi, Injung Kim</p>
</li>
<li class="">
<p><strong>institution:</strong> Handong Global University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23221" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23221</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Holi-DETR, a novel holistic detection framework for fashion items that leverages contextual information to reduce detection ambiguities., 2. Introduces a novel architecture that integrates three distinct types of contextual information (co-occurrence, inter-item spatial arrangements, and item-body keypoint relationships) into DETR-based models., 3. Demonstrates performance improvements over baseline models (vanilla DETR and Co-DETR) in terms of average precision (AP).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dda1eab87d833571a5feac46be0fc3ba879ccabde53c04f72e0d4fcae137cb6e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dda1eab87d833571a5feac46be0fc3ba879ccabde53c04f72e0d4fcae137cb6e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of fashion item detection, which is difficult due to diverse appearances and similar subcategories. The authors propose Holi-DETR, a holistic Detection Transformer that leverages three types of contextual information—co-occurrence, spatial arrangements, and body keypoints—to improve detection accuracy. The method shows improved performance over baseline DETR models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Anomaly Detection by Effectively Leveraging Synthetic Images</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [anomaly detection], [synthetic data, image-to-image translation, image retrieval, two-stage training, MVTec AD]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sungho Kang, Hyunkyu Park, Yeonho Lee, Hanbyul Lee, Mijoo Jeong, YeongHyeon Park, Injae Lee, Juneho Yi</p>
</li>
<li class="">
<p><strong>institution:</strong> Sungkyunkwan University, The University of Texas MD Anderson Cancer Center</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23227" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23227</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel framework that efficiently generates synthetic defect images by leveraging a pre-trained text-guided image-to-image translation model and an image retrieval model for filtering. 2. A two-stage training strategy that pre-trains on a large volume of rule-based synthetic images and then fine-tunes on a smaller set of high-quality generated images. 3. Demonstration of the approach&#x27;s effectiveness in reducing data collection costs while improving anomaly detection performance on the MVTec AD benchmark dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3546b17641c719167618772aa6e4da085e82a3b6d85cd2abc9940897d3e1f92_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3546b17641c719167618772aa6e4da085e82a3b6d85cd2abc9940897d3e1f92_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the trade-off in synthetic data generation for anomaly detection by proposing a framework that uses a pre-trained image-to-image translation model and an image retrieval filter to efficiently create realistic defect images. It also introduces a two-stage training strategy to leverage both cheap, low-quality and expensive, high-quality synthetic data effectively. Experiments on MVTec AD show this method reduces costs and improves detection performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SURE Guided Posterior Sampling: Trajectory Correction for Diffusion-Based Inverse Problems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [diffusion models], [diffusion-based inverse problems, Stein&#x27;s Unbiased Risk Estimate (SURE), posterior sampling, trajectory correction, PCA-based noise estimation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Minwoo Kim, Hongki Lim</p>
</li>
<li class="">
<p><strong>institution:</strong> Inha University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23232" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23232</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SURE Guided Posterior Sampling (SGPS), a method that corrects sampling trajectory deviations in diffusion-based inverse problem solvers. 2. Introduces the use of Stein&#x27;s Unbiased Risk Estimate (SURE) gradient updates and PCA-based noise estimation to mitigate noise-induced errors during early and middle sampling stages. 3. Demonstrates that SGPS maintains high reconstruction quality with fewer than 100 Neural Function Evaluations (NFEs), outperforming existing methods at low NFE counts.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/111900f78e63afd7898adde1bee4891035e48eb062b31baf3ef50535922421b6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/111900f78e63afd7898adde1bee4891035e48eb062b31baf3ef50535922421b6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of error accumulation in diffusion-based inverse problem solvers, which typically require many steps for high-quality reconstruction. The authors propose SURE Guided Posterior Sampling (SGPS), a method that corrects the sampling trajectory using SURE gradient updates and PCA-based noise estimation. The method reduces error accumulation, enabling high-quality reconstructions in fewer than 100 steps and outperforming existing approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Bridging Your Imagination with Audio-Video Generation via a Unified Director</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [unified director model, Mixture-of-Transformers, interleaved concept learning, disentangled expert learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiaxu Zhang, Tianshu Hu, Yuan Zhang, Zenan Li, Linjie Luo, Guosheng Lin, Xin Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> ByteDance Intelligent Creation, Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23222" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23222</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://kebii.github.io/UniMAGE" target="_blank" rel="noopener noreferrer" class="">https://kebii.github.io/UniMAGE</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes UniMAGE, a unified director model that integrates script drafting and key-shot design into a single framework. 2. Introduces a &quot;first interleaving, then disentangling&quot; training paradigm (Interleaved Concept Learning and Disentangled Expert Learning) to enhance narrative logic and keyframe consistency. 3. Employs a Mixture-of-Transformers architecture to unify text and image generation within one model.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66a6f11305dbb66573f2efcf30ff0c1abc82467fe78ca1d1328a5beb5932ebd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66a6f11305dbb66573f2efcf30ff0c1abc82467fe78ca1d1328a5beb5932ebd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the disjoint treatment of script drafting and key-shot design in AI-driven video creation by proposing UniMAGE, a unified director model. It uses a Mixture-of-Transformers architecture and a novel &quot;first interleaving, then disentangling&quot; training paradigm to generate coherent scripts and consistent keyframes. Experiments show UniMAGE achieves state-of-the-art performance among open-source models for long-context, multi-shot film generation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [physics-inspired modeling, edge detection, content-adaptive routing, multi-scale feature fusion, infrared gas leak detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dongsheng Li, Chaobo Chen, Siling Wang, Song Gao</p>
</li>
<li class="">
<p><strong>institution:</strong> (Inferred from author names and arXiv handle; specific institution not provided in the given text. Could be a Chinese research institution or university.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23234" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23234</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a physics-inspired Gas Block module that models gas transport using a diffusion-convection unit with local and large-kernel branches, fused via an edge-gated module to enhance weak plume features. 2. Introduced a novel Adaptive Gradient and Phase Edge Operator (AGPEO) and a Multi-Scale Edge Perception Module (MSEPM) to compute and integrate reliable hierarchical edge priors for boundary reinforcement. 3. Designed a Content-Adaptive Sparse Routing Path Aggregation Network (CASR-PAN) that uses adaptive modulation to selectively propagate informative features across scales based on content and edge cues, improving efficiency and discriminability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee948d9a0589e5467502d1d916e59d51137b1253413c1001ade729584fd4bf55_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee948d9a0589e5467502d1d916e59d51137b1253413c1001ade729584fd4bf55_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes PEG-DRNet, a physics-inspired and edge-guided network for detecting faint infrared gas leaks. The method combines a gas transport model, a novel edge detection operator, and a content-adaptive routing mechanism for multi-scale feature fusion. Experiments show PEG-DRNet achieves superior accuracy and computational efficiency on benchmark datasets compared to existing detectors.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] RS-Prune: Training-Free Data Pruning at High Ratios for Efficient Remote Sensing Diffusion Foundation Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [diffusion models], [data pruning, remote sensing, diffusion foundation models, entropy-based selection, scene-aware clustering]</p>
</li>
<li class="">
<p><strong>authors:</strong> Fan Wei, Runmin Dong, Yushan Lai, Yixiang Yang, Zhaoyang Luo, Jinxiao Zhang, Miao Yang, Shuai Yuan, Jiyao Zhao, Bin Luo, Haohuan Fu</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Sun Yat-sen University, National Supercomputing Center in Shenzhen, Tsinghua Shenzhen International Graduate School, The University of Hong Kong, Peking University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23239" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23239</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel, training-free, two-stage data pruning method for remote sensing diffusion models that jointly considers local information content and global scene-level diversity. 2. A reference-guided, scene-aware clustering strategy that leverages existing classification datasets to efficiently select a high-quality subset from large-scale unlabeled data. 3. Demonstrated effectiveness under high pruning ratios (e.g., 85%), significantly improving model convergence and generation quality, and achieving state-of-the-art performance on downstream tasks like super-resolution.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ceb6378a9dfeacd28f04cab82d2b54590992943b1b3cdc469d6c8c32434a9f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ceb6378a9dfeacd28f04cab82d2b54590992943b1b3cdc469d6c8c32434a9f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the inefficiency of training diffusion-based remote sensing foundation models on large, redundant datasets. It proposes RS-Prune, a training-free, two-stage data pruning method that first removes low-information samples and then performs scene-aware clustering for fine-grained selection. The method enables rapid model convergence even after pruning 85% of data and achieves superior performance on downstream generation tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Contour Information Aware 2D Gaussian Splatting for Image Representation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image representation], [2D Gaussian Splatting, contour awareness, segmentation priors, warm-up scheme]</p>
</li>
<li class="">
<p><strong>authors:</strong> Masaya Takabe, Hiroshi Watanabe, Sujun Hong, Tomohiro Ikai, Zheming Fan, Ryo Ishimoto, Kakeru Sugimoto, Ruri Imichi</p>
</li>
<li class="">
<p><strong>institution:</strong> Waseda University, Sharp Corporation</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23255" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23255</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A Contour Information-Aware 2D Gaussian Splatting framework that incorporates object segmentation priors to preserve edge structures. 2. A region-constrained rasterization method that prevents cross-boundary blending of Gaussians. 3. A warm-up training scheme to stabilize optimization and improve convergence.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1364e59f36054f08d771d6c6a564f9406990fd9cbe9926e23e664a2ebd5a3be4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1364e59f36054f08d771d6c6a564f9406990fd9cbe9926e23e664a2ebd5a3be4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of blurry object boundaries in highly compressed 2D Gaussian Splatting (2DGS) for image representation. The proposed method integrates segmentation priors to constrain Gaussians to specific object regions during rasterization, preventing blending across edges. Experiments show it achieves higher reconstruction quality around edges with very few Gaussians while maintaining fast rendering and low memory usage.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal fusion], [Dynamic Resolution Input Strategy (DRIS), Multi-scale Vision-language Alignment Mechanism (MS-VLAM), Vision-language Model (VLM), remote sensing image captioning, cross-modal retrieval]</p>
</li>
<li class="">
<p><strong>authors:</strong> Siyu Zhang, Ying Chen, Lianlei Shan, Runhe Qiu</p>
</li>
<li class="">
<p><strong>institution:</strong> Sanda University, Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23243" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23243</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Dynamic Resolution Input Strategy (DRIS) that adaptively allocates computational resources based on image content complexity to balance detail and efficiency. 2. Introduces a Multi-scale Vision-language Alignment Mechanism (MS-VLAM) with object, local-region, and global-level alignment to capture cross-modal semantic consistency. 3. Presents an integrated VLM framework that demonstrates superior performance in remote sensing image captioning and cross-modal retrieval tasks on the RS-GPT4V dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26c024de47fb99d79be55abc94c255a70b38cb2c16e81d298d21a49c4c05bd20_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26c024de47fb99d79be55abc94c255a70b38cb2c16e81d298d21a49c4c05bd20_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the inefficiency of fixed-resolution inputs and the lack of semantic hierarchy in single-scale alignment for multimodal remote sensing interpretation. It proposes a novel Vision-language Model framework featuring a Dynamic Resolution Input Strategy and a Multi-scale Vision-language Alignment Mechanism. The framework significantly improves both semantic understanding accuracy and computational efficiency in tasks like image captioning and cross-modal retrieval, as validated on the RS-GPT4V dataset.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ASemConsist: Adaptive Semantic Feature Control for Training-Free Identity-Consistent Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [text-to-image generation], [identity-consistent generation, text embedding modification, padding embeddings, adaptive feature-sharing, Consistency Quality Score (CQS)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shin seong Kim, Minjung Shin, Hyunin Cho, Youngjung Uh</p>
</li>
<li class="">
<p><strong>institution:</strong> Yonsei University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23245" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23245</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://minjung-s.github.io/asemconsist" target="_blank" rel="noopener noreferrer" class="">https://minjung-s.github.io/asemconsist</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel framework (ASemConsist) for identity-consistent generation using selective text embedding modification and a semantic control strategy that repurposes padding embeddings as semantic containers. 2. An adaptive feature-sharing strategy that automatically evaluates textual ambiguity and applies constraints only to ambiguous identity prompts. 3. A unified evaluation protocol, the Consistency Quality Score (CQS), which integrates identity preservation and prompt alignment into a single metric to capture performance imbalances.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d92693c5da5c617b2e5cb2836099c658b115dfb4a132adf0caeb861c3ab1f213_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d92693c5da5c617b2e5cb2836099c658b115dfb4a132adf0caeb861c3ab1f213_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of generating a sequence of images with consistent character identity while maintaining alignment with diverse per-image prompts. The proposed ASemConsist framework modifies text embeddings to control identity semantics, uses padding embeddings as semantic containers, and adaptively applies constraints based on prompt ambiguity. It achieves state-of-the-art performance by overcoming the trade-off between identity consistency and prompt alignment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [diffusion transformer, inference acceleration, caching, error minimization, dynamic programming]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tong Shao, Yusen Fu, Guoying Sun, Jingde Kong, Zhuotao Tian, Jingyong Su</p>
</li>
<li class="">
<p><strong>institution:</strong> Harbin Institute of Technology, Shenzhen</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23258" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23258</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CEM, a novel plugin for optimizing caching strategies in DiT acceleration via cumulative error minimization. 2. Introduces a dynamic programming algorithm guided by a predefined error prior to adaptively minimize caching error. 3. Demonstrates the method&#x27;s model-agnostic nature, seamless integration into existing frameworks, and significant fidelity improvements across multiple models and tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b2f0b193709fc539d32a8fa74b0405ea99491982cb3f280e0dd10ff89b6b0a3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b2f0b193709fc539d32a8fa74b0405ea99491982cb3f280e0dd10ff89b6b0a3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the slow inference of Diffusion Transformers (DiTs) by proposing CEM, a plug-and-play fidelity optimization plugin. CEM minimizes cumulative caching error via a dynamic programming algorithm, adapting to error variations during denoising. The method is training-free, model-agnostic, and significantly improves generation fidelity when integrated with existing acceleration techniques.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [change detection], [vision-language model, remote sensing, semantic change detection, supervised fine-tuning, reinforcement learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xingwei Ma, Shiyang Feng, Bo Zhang, Bin Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Fudan University, Shanghai Artificial Intelligence Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23244" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23244</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ViLaCD-R1, a novel two-stage vision-language framework for semantic change detection in remote sensing, comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). 2. Introduces a training strategy for the VLM using supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks to generate a coarse change mask. 3. Demonstrates that the framework significantly improves semantic change recognition and localization while suppressing non-semantic variations, achieving state-of-the-art performance on multiple benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5871856f6e1854dd58df304f783ce8ea57314887e0296e446d48afb30805307_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5871856f6e1854dd58df304f783ce8ea57314887e0296e446d48afb30805307_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of existing remote sensing change detection methods, such as poor semantic understanding and inaccurate localization, by proposing ViLaCD-R1. This two-stage vision-language framework first uses a fine-tuned VLM to generate a coarse change mask from dual-temporal images, then refines it with a decoder to produce a precise change map. The method shows superior performance in recognizing true semantic changes and suppressing irrelevant variations across several benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [Mixture-of-Experts, adaptive computation, real-time detection, dynamic routing, YOLO]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xu Lin, Jinlong Peng, Zhenye Gan, Jiawen Zhu, Jun Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Tencent Youtu Lab, Singapore Management University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23273" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23273</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/isLinXu/YOLO-Master" target="_blank" rel="noopener noreferrer" class="">https://github.com/isLinXu/YOLO-Master</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for real-time object detection. 2. Designs an Efficient Sparse Mixture-of-Experts (ES-MoE) block to dynamically allocate computational resources based on scene complexity. 3. Introduces a lightweight dynamic routing network with a diversity-enhancing objective to encourage complementary expert specialization and efficient inference.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7def7ba8e75e1cc89945c90cd74ab4aec5217397e89bc6d189d86c3260048636_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7def7ba8e75e1cc89945c90cd74ab4aec5217397e89bc6d189d86c3260048636_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that existing YOLO models use static computation, leading to inefficiency on simple scenes and poor performance on complex ones. To solve this, it proposes YOLO-Master, which uses a Mixture-of-Experts block and a dynamic router to adaptively allocate computation per input. Experiments show it achieves higher accuracy and faster speed than baselines, especially on dense scenes, while maintaining real-time performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multi-Track Multimodal Learning on iMiGUE: Micro-Gesture and Emotion Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal emotion recognition], [micro-gesture recognition, behavior-based emotion prediction, multimodal fusion, Cross-Modal Token Fusion, InterFusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Arman Martirosyan, Shahane Tigranyan, Maria Razzhivina, Artak Aslanyan, Nazgul Salikhova, Ilya Makarov, Andrey Savchenko, Aram Avetisyan</p>
</li>
<li class="">
<p><strong>institution:</strong> Russian-Armenian University, ISP RAS, HSE University, Innopolis University, AIRI, Sber AI Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23291" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23291</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a multimodal framework for micro-gesture classification that fuses RGB (MViTv2-S) and 3D pose (2s-AGCN) embeddings via a Cross-Modal Token Fusion module. 2. Developed a separate multimodal framework for behavior-based emotion prediction that fuses facial (SwinFace) and contextual (MViTv2-S) embeddings via an InterFusion module. 3. Demonstrated robust performance on the iMiGUE dataset, achieving 2nd place in the behavior-based emotion prediction task of the MiGA 2025 Challenge.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98bb53f61c281fed2a6cb4e682e56bc767703f6a640cfb8f4d681abb07a0e6ce_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98bb53f61c281fed2a6cb4e682e56bc767703f6a640cfb8f4d681abb07a0e6ce_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents two multimodal frameworks to tackle micro-gesture recognition and behavior-based emotion prediction on the iMiGUE dataset. The first framework fuses video and skeletal pose data, while the second fuses facial and contextual embeddings for emotion classification. The method demonstrated strong performance, securing 2nd place in the emotion prediction task of the MiGA 2025 Challenge.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [medical image classification], [MedGemma, GPT-4, LoRA, zero-shot classification, multimodal LLM]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md. Sazzadul Islam Prottasha, Nabil Walid Rafi</p>
</li>
<li class="">
<p><strong>institution:</strong> Bangladesh University of Professionals</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23304" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23304</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a critical comparison between the open-source MedGemma and proprietary GPT-4 for zero-shot medical disease classification from images. 2. Demonstrated that the LoRA-fine-tuned MedGemma model significantly outperformed the untuned GPT-4 in accuracy and sensitivity for high-stakes clinical tasks. 3. Highlighted the essential role of domain-specific fine-tuning in minimizing hallucinations and enabling complex, evidence-based medical reasoning for clinical implementation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58abac32272c08efbbe249ec33f3b4f4aa3a6f4d477b241718ddbde679cce96a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58abac32272c08efbbe249ec33f3b4f4aa3a6f4d477b241718ddbde679cce96a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study compares the performance of the open-source MedGemma model and the proprietary GPT-4 for zero-shot classification of six diseases from medical images. The MedGemma model, fine-tuned with LoRA, achieved higher mean accuracy and sensitivity than GPT-4. The results show that domain-specific fine-tuning is crucial for reliable clinical applications, positioning MedGemma as a sophisticated tool for medical diagnostics.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement Using Deep Learning-Based Dynamic Object Filtering</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [visual SLAM], [ORB-SLAM3, YOLOv8, dynamic object filtering, point cloud refinement, CUDA acceleration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sheng-Kai Chen, Jie-Yu Chao, Jr-Yu Chang, Po-Lien Wu, Po-Chiang Lin</p>
</li>
<li class="">
<p><strong>institution:</strong> Yuan Ze University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23318" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23318</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes PCR-ORB, an enhanced ORB-SLAM3 framework that integrates deep learning-based point cloud refinement to filter dynamic objects. 2. Implements a multi-stage filtering strategy combining semantic segmentation (YOLOv8), ground plane estimation, sky removal, edge filtering, and temporal consistency for robust dynamic object removal. 3. Achieves real-time performance through CUDA-accelerated processing and demonstrates significant accuracy improvements in specific dynamic sequences on the KITTI dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e72d53f96b383c73428b67d24fbf2d4163ac5820be1bfed6f370c529922f919_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e72d53f96b383c73428b67d24fbf2d4163ac5820be1bfed6f370c529922f919_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces PCR-ORB, an enhanced visual SLAM system that improves ORB-SLAM3&#x27;s robustness in dynamic environments by integrating YOLOv8 for semantic segmentation and a multi-stage point cloud refinement process to filter moving objects. The method achieves real-time performance with CUDA acceleration. Evaluation on KITTI shows scenario-dependent effectiveness, with notable accuracy improvements in some sequences but mixed results overall.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [agent evaluation], [spatial reasoning, long-horizon planning, partial observability, mental simulation, diagnostic benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Huan-ang Gao, Zikang Zhang, Tianwei Luo, Kaisen Yang, Xinzhe Juan, Jiahao Qiu, Tianxing Chen, Bingxiang He, Hao Zhao, Hao Zhou, Shilong Liu, Mengdi Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Princeton University, Shanghai Jiao Tong University &amp; University of Michigan, The University of Hong Kong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23328" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23328</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies three core cognitive challenges (spatial reasoning, long-horizon state tracking, active exploration under partial observation) hindering LLM agents in the physical world. 2. Introduces CubeBench, a novel generative benchmark based on the Rubik&#x27;s Cube with a three-tiered diagnostic framework to isolate and evaluate these capabilities. 3. Provides a diagnostic framework using external solver tools to analyze failure modes and reveals critical limitations of leading LLMs, including a 0.00% pass rate on long-horizon tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/210465a4bf9048c43ec900e17f922e63394d83664c6fe631fec0d54577fd9fb6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/210465a4bf9048c43ec900e17f922e63394d83664c6fe631fec0d54577fd9fb6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces CubeBench, a diagnostic benchmark using a Rubik&#x27;s Cube to evaluate LLM agents&#x27; spatial reasoning and long-horizon planning under partial observation. It employs a three-tiered framework from full symbolic to partial visual states. Experiments show leading LLMs fail completely on long-horizon tasks, highlighting a fundamental gap for physical-world deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] CME-CAD: Heterogeneous Collaborative Multi-Expert Reinforcement Learning for CAD Code Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [CAD code generation, multi-expert reinforcement learning, Chain-of-Thought, CADExpert benchmark, CADQuery]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ke Niu, Haiyang Yu, Zhuofan Chen, Zhengtao Yao, Weitao Jia, Xiaodong Ge, Jingqun Tang, Benlei Cui, Bin Li, Xiangyang Xue</p>
</li>
<li class="">
<p><strong>institution:</strong> Fudan University, ByteDance Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23333" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23333</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Heterogeneous Collaborative Multi-Expert Reinforcement Learning (CME-CAD) paradigm for generating precise and editable CAD models., 2. Introduces a two-stage training process: Multi-Expert Fine-Tuning (MEFT) and Multi-Expert Reinforcement Learning (MERL)., 3. Presents CADExpert, an open-source benchmark with 17,299 instances including orthographic projections, CoT processes, CADQuery code, and 3D models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/854c145ea4394c54526f3cfa5f5b5e6528680bb17418bfc2756a03817bea2de5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/854c145ea4394c54526f3cfa5f5b5e6528680bb17418bfc2756a03817bea2de5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of automating the generation of high-precision, editable CAD models from sketches, which existing methods struggle with. It proposes a new training paradigm called CME-CAD, which uses a two-stage process of Multi-Expert Fine-Tuning and Reinforcement Learning to collaboratively improve model performance. The approach aims to generate accurate, constraint-compatible CAD code and is supported by a new open-source benchmark called CADExpert.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Visual Language Hypothesis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [representation learning], [visual language hypothesis, fiber bundle, semantic quotient, expand-and-snap, topology change]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiu Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Bytedance</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23335" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23335</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the &quot;Visual Language Hypothesis,&quot; framing visual understanding as requiring a discrete semantic language, leading to a fiber-bundle-like structure for the observation space. 2. Derives a theoretical requirement for semantic invariance, arguing it necessitates a non-homeomorphic, discriminative target (e.g., supervised labels, multimodal alignment) rather than smooth deformation alone. 3. Identifies an architectural requirement for models, proposing an &quot;expand-and-snap&quot; process to achieve the necessary topology change for semantic abstraction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d70511c4d2f57ecb4e49170ed73c0a81de0fcfcdbdb84cc7bcbdca254140c3f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d70511c4d2f57ecb4e49170ed73c0a81de0fcfcdbdb84cc7bcbdca254140c3f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes the &quot;Visual Language Hypothesis,&quot; which posits that visual understanding requires a discrete semantic structure, implying the visual observation space is organized like a fiber bundle. From this, the authors theoretically argue that achieving semantic invariance demands discriminative supervision and that model architectures must support a specific &quot;expand-and-snap&quot; process to change topology. The framework provides a topological interpretation for empirical patterns in large-scale discriminative and multimodal models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [memory systems, cognitive neuroscience, LLM-driven agents, memory security, multimodal memory]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiafeng Liang, Hao Li, Chang Li, Jiaqi Zhou, Shixin Jiang, Zekun Wang, Changkai Ji, Zhihao Zhu, Runxuan Liu, Tao Ren, Jinlan Fu, See-Kiong Ng, Xia Liang, Ming Liu, Bing Qin</p>
</li>
<li class="">
<p><strong>institution:</strong> Harbin Institute of Technology, Fudan University, Peking University, National University of Singapore</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23343" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23343</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/AgentMemory/Huaman-Agent-Memory" target="_blank" rel="noopener noreferrer" class="">https://github.com/AgentMemory/Huaman-Agent-Memory</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a systematic synthesis and comparative analysis of memory systems from cognitive neuroscience to LLM-driven autonomous agents. 2. Reviews mainstream benchmarks for evaluating agent memory and explores memory security from attack and defense perspectives. 3. Envisions future research directions, focusing on multimodal memory systems and skill acquisition.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15773eb4c52c63f2641be869baf3af4b7f6bb74f6e36c67247957bfbd039e9b6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15773eb4c52c63f2641be869baf3af4b7f6bb74f6e36c67247957bfbd039e9b6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This survey paper bridges the interdisciplinary gap between cognitive neuroscience and AI by systematically analyzing memory systems for autonomous agents. It compares biological and artificial memory taxonomies, storage, and management, while also reviewing evaluation benchmarks and security issues. The work concludes by outlining future directions, including multimodal memory and skill learning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] CountGD++: Generalized Prompting for Open-World Counting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object counting], [open-world counting, negative prompting, pseudo-exemplars]</p>
</li>
<li class="">
<p><strong>authors:</strong> Niki Amini-Naieni, Andrew Zisserman</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Oxford (Visual Geometry Group)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23351" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23351</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/niki-amini-naieni/CountGDPlusPlus" target="_blank" rel="noopener noreferrer" class="">https://github.com/niki-amini-naieni/CountGDPlusPlus</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Extended the counting prompt to include negative specifications (what not to count) via text and/or visual examples. 2. Introduced &#x27;pseudo-exemplars&#x27; to automate the annotation of visual examples at inference time. 3. Enabled counting models to accept visual examples from both natural and synthetic external images, and integrated the model as a vision expert agent for an LLM.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1cf78a83ebaf07de6caf2a7ac0a9fc065838c62bc22493f5dab52e0be563c777_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1cf78a83ebaf07de6caf2a7ac0a9fc065838c62bc22493f5dab52e0be563c777_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses limitations in open-world object counting by introducing CountGD++, a model that significantly expands prompt flexibility. It allows specifying what not to count, automates visual example annotation via pseudo-exemplars, and accepts external visual examples, leading to improved accuracy and generalization across datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SpatialMosaic: A Multiview VLM Dataset for Partial Visibility</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multi-view vision-language reasoning], [spatial reasoning, multi-view images, partial visibility, instruction-tuning, 3D reconstruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kanghee Lee, Injae Lee, Minseok Kwak, Kwonyoung Ryu, Jungi Hong, Jaesik Park</p>
</li>
<li class="">
<p><strong>institution:</strong> Seoul National University, University College London, POSTECH</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23365" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23365</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A scalable multi-view data generation and annotation pipeline for realistic spatial reasoning QA pairs. 2. SpatialMosaic, a comprehensive 2M QA pair instruction-tuning dataset, and SpatialMosaic-Bench, a 1M QA pair benchmark for evaluating multi-view spatial reasoning under challenging conditions. 3. SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76b593a5c7594c6552e0421062301bf10006b9896666b9446c8fe3d5e0816795_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76b593a5c7594c6552e0421062301bf10006b9896666b9446c8fe3d5e0816795_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of enabling Vision-Language Models (VLMs) to perform 3D spatial reasoning directly from multi-view images under realistic conditions like partial visibility and occlusion. It proposes a data generation pipeline to create the SpatialMosaic dataset and benchmark, and introduces a hybrid VLM framework that incorporates 3D reconstruction models. Experiments show the approach effectively enhances spatial reasoning in challenging multi-view scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MGCA-Net: Multi-Graph Contextual Attention Network for Two-View Correspondence Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [two-view correspondence / outlier rejection], [geometric attention, graph neural network, cross-stage consensus, outlier rejection, camera pose estimation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuyuan Lin, Mengtin Lo, Haosheng Chen, Yanjie Liang, Qiangqiang Wu</p>
</li>
<li class="">
<p><strong>institution:</strong> Jinan University, Chongqing University of Posts and Telecommunications, Peng Cheng Laboratory, City University of Hong Kong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23369" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23369</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="http://www.linshuyuan.com" target="_blank" rel="noopener noreferrer" class="">http://www.linshuyuan.com</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a Contextual Geometric Attention (CGA) module that dynamically integrates spatial and feature information to capture local and global geometric relationships. 2. Introduced a Cross-Stage Multi-Graph Consensus (CSMGC) module to ensure geometric consistency across different network stages via a sparse graph network. 3. Demonstrated state-of-the-art performance on the YFCC100M and SUN3D datasets for outlier rejection and camera pose estimation tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37bf1bef9021d24530aae70ecb962a50ad8484fa1078db8fab9979ce3832164_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37bf1bef9021d24530aae70ecb962a50ad8484fa1078db8fab9979ce3832164_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of robust two-view correspondence learning for tasks like camera pose estimation. It proposes MGCA-Net, a novel network featuring a Contextual Geometric Attention module and a Cross-Stage Multi-Graph Consensus module to better model geometric constraints and ensure information consistency. Experiments show it outperforms existing methods on standard benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] NeXT-IMDL: Build Benchmark for NeXT-Generation Image Manipulation Detection &amp; Localization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image forensics], [image manipulation detection, generalization benchmark, cross-dimension evaluation, AIGC-based manipulation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yifei Li, Haoyuan He, Yu Zheng, Bingyao Yu, Wenzhao Zheng, Lei Chen, Jie Zhou, Jiwen Lu</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23374" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23374</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/JoeLeelyf/NeXT-IMDL" target="_blank" rel="noopener noreferrer" class="">https://github.com/JoeLeelyf/NeXT-IMDL</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes NeXT-IMDL, a large-scale diagnostic benchmark for Image Manipulation Detection and Localization (IMDL) designed to systematically probe the generalization boundaries of detectors. 2. Categorizes AI-generated content (AIGC) manipulations along four fundamental axes (editing models, manipulation types, content semantics, forgery granularity) and implements five rigorous cross-dimension evaluation protocols. 3. Through extensive experiments on 11 representative models, reveals that current models exhibit systemic failures and significant performance degradation under the proposed protocols, challenging the perceived progress in the field.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b0f52b5e0a9a84301b167e1fa374896f0b6eecedcf21edc2aff4e5102296cd6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b0f52b5e0a9a84301b167e1fa374896f0b6eecedcf21edc2aff4e5102296cd6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies a &quot;benchmark illusion&quot; in Image Manipulation Detection and Localization (IMDL), where current cross-dataset evaluations overestimate model robustness. To address this, the authors propose NeXT-IMDL, a diagnostic benchmark that systematically categorizes manipulations and introduces rigorous cross-dimension evaluation protocols. Experiments show that 11 state-of-the-art models suffer significant performance drops under these new protocols, highlighting their fragility and the need for more robust next-generation IMDL models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SoulX-LiveTalk Technical Report</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [Self-correcting Bidirectional Distillation, Multi-step Retrospective Self-Correction, hybrid sequence parallelism, Parallel VAE, kernel-level optimizations]</p>
</li>
<li class="">
<p><strong>authors:</strong> Le Shen, Qiao Qian, Tan Yu, Ke Zhou, Tianhang Yu, Yu Zhan, Zhenjie Wang, Ming Tao, Shunshun Yin, Siyuan Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Soul AI Lab, Donghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23379" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23379</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://soul-ailab.github.io/soulx-livetalk/" target="_blank" rel="noopener noreferrer" class="">https://soul-ailab.github.io/soulx-livetalk/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a Self-correcting Bidirectional Distillation strategy that retains bidirectional attention within video chunks to preserve spatiotemporal correlations and enhance visual fidelity. 2. Proposed a Multi-step Retrospective Self-Correction Mechanism to ensure stability during infinite generation by enabling autonomous recovery from accumulated errors. 3. Engineered a full-stack inference acceleration suite with hybrid sequence parallelism, Parallel VAE, and kernel-level optimizations to achieve real-time performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d6b1bb994b3c3273da207d2f19494d99c1ff6bf6663f41b1d85b2f0c69b83bb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d6b1bb994b3c3273da207d2f19494d99c1ff6bf6663f41b1d85b2f0c69b83bb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of deploying large diffusion models for real-time, audio-driven avatar generation by introducing SoulX-LiveTalk, a 14B-parameter framework. It employs a bidirectional distillation strategy and a self-correction mechanism to maintain high visual quality and stability, while a suite of inference optimizations enables sub-second latency and 32 FPS throughput, setting a new standard for interactive digital humans.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [log anomaly detection], [collaborative transformers, multi-head impressed attention, modality adaptation layer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mohammad Nasirzadeh, Jafar Tahmoresnezhad, Parviz Rashidi-Khazaee</p>
</li>
<li class="">
<p><strong>institution:</strong> Urmia University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23380" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23380</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/your-repo/CoLog" target="_blank" rel="noopener noreferrer" class="">https://github.com/your-repo/CoLog</a> (Note: The provided text states &quot;We also provide the implementation of CoLog atthis https URL.&quot; but the specific URL is cut off in the input. Based on the placeholder, the typical format is used. If the exact URL is required, it would be the one following &quot;atthis&quot; in the original text.)</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CoLog, a unified framework for detecting both point and collective anomalies in OS logs by applying multimodal sentiment analysis concepts. 2. Introduces collaborative transformers and multi-head impressed attention to learn interactions between different log data modalities. 3. Incorporates a modality adaptation layer to handle heterogeneity and adapt representations from different log modalities.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of log anomaly detection, where existing methods struggle with the multimodal nature of log data and the interactions between these modalities. It proposes CoLog, a framework that uses collaborative transformers and a modality adaptation layer to learn nuanced patterns across log modalities for comprehensive anomaly detection. Extensive experiments show CoLog achieves state-of-the-art performance, with mean precision, recall, and F1 scores over 99.5% across seven benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Bridging Cognitive Gap: Hierarchical Description Learning for Artistic Image Aesthetics Assessment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image aesthetics assessment], [aesthetic description, multimodal learning, large language model, hierarchical learning, entropy minimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Henglin Liu, Nisha Huang, Chang Liu, Jiangpeng Yan, Huijuan Huang, Jixuan Ying, Tong-Yee Lee, Pengfei Wan, Xiangyang Ji</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Kuaishou Technology, Pengcheng Laboratory, National Cheng Kung University, E Fund Management Co., Ltd.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23413" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23413</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Refined Aesthetic Description (RAD) dataset, a large-scale, multi-dimensional structured dataset generated via an iterative pipeline to address data scarcity and imbalance in artistic aesthetics. 2. Proposes ArtQuant, an aesthetics assessment framework that couples isolated aesthetic dimensions through joint description generation and utilizes LLM decoders to better model long-text semantics. 3. Provides a theoretical analysis showing that the semantic adequacy of the RAD dataset and the generation paradigm of ArtQuant collectively minimize prediction entropy, offering mathematical grounding for the framework.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d47a5d6d4a6ecaa69bca3187b7069584ea4c0191c7344484fef5b2419157e30d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d47a5d6d4a6ecaa69bca3187b7069584ea4c0191c7344484fef5b2419157e30d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges in artistic image aesthetics assessment by introducing a large-scale dataset (RAD) and a novel framework (ArtQuant) that uses hierarchical description learning with LLMs. The method achieves state-of-the-art performance on several benchmarks while requiring significantly fewer training epochs, effectively narrowing the cognitive gap between images and human aesthetic judgment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] DriveLaW<!-- -->:Unifying<!-- --> Planning and Video Generation in a Latent Driving World</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [autonomous driving], [world model, video generation, diffusion planner, latent representation, unified planning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tianze Xia, Yongkang Li, Lijun Zhou, Jingfeng Yao, Kaixin Xiong, Haiyang Sun, Bing Wang, Kun Ma, Hangjun Ye, Wenyu Liu, Xinggang Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Huazhong University of Science and Technology, Xiaomi EV</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23421" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23421</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DriveLaW, a novel paradigm that unifies video generation and motion planning for autonomous driving by directly injecting latent representations from the video generator into the planner. 2. Introduces a three-stage progressive training strategy to jointly optimize the video generation component (DriveLaW-Video) and the diffusion planning component (DriveLaW-Act). 3. Achieves state-of-the-art performance on both video prediction and motion planning benchmarks, significantly surpassing previous methods in metrics like FID, FVD, and NAVSIM.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ac4b7563a9d00eabc7ad0c94b379593c1add28414cfc8bc68e64145861ae65f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ac4b7563a9d00eabc7ad0c94b379593c1add28414cfc8bc68e64145861ae65f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the decoupling of world prediction and motion planning in current autonomous driving world models. It proposes DriveLaW, a unified framework that connects a video generator and a diffusion planner via latent representations, ensuring consistency between future scene generation and trajectory planning. The method achieves new state-of-the-art results on both video forecasting and planning benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SOFTooth: Semantics-Enhanced Order-Aware Fusion for Tooth Instance Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [2D-3D fusion, instance segmentation, order-aware matching, semantic feature injection, center-guided refinement]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiaolan Li, Wanquan Liu, Pengcheng Li, Pengyu Jie, Chenqiang Gao</p>
</li>
<li class="">
<p><strong>institution:</strong> Sun Yat-sen University, Hainan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23411" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23411</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A point-wise residual gating module that injects frozen 2D SAM embeddings into 3D point features to refine boundaries without 2D mask supervision. 2. A center-guided mask refinement mechanism that regularizes consistency between instance masks and geometric centroids to reduce center drift. 3. An order-aware Hungarian matching strategy that integrates anatomical tooth order and center distance for coherent instance labeling, especially for cases with missing or crowded teeth.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f08a5acde121d4a512791be08f24d9f39834e6660240bf65661d9f0bfb8c723_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f08a5acde121d4a512791be08f24d9f39834e6660240bf65661d9f0bfb8c723_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses challenges in 3D tooth instance segmentation, such as boundary leakage and inconsistent labeling, by proposing SOFTooth, a framework that fuses 2D semantic features from SAM with 3D geometric data. The method introduces modules for boundary refinement, center stabilization, and order-aware instance assignment. It achieves state-of-the-art performance on a benchmark dataset, demonstrating effective transfer of 2D semantics to 3D segmentation without fine-tuning the 2D model.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [preference optimization, diffusion models, score-space supervision, text-to-image synthesis, denoising trajectory]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dohyun Kim, Seungwoo Lyu, Seung Wook Kim, Paul Hongsuck Seo</p>
</li>
<li class="">
<p><strong>institution:</strong> Korea University, NVIDIA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23426" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23426</a></p>
</li>
<li class="">
<p><strong>code:</strong> this https URL</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DDSPO, a method for direct score-space preference optimization in diffusion models that provides per-timestep supervision from contrastive policy pairs. 2. Introduces a practical strategy to automatically generate preference signals using a pretrained model and semantically degraded prompts, avoiding costly human-labeled data. 3. Demonstrates improved text-image alignment and visual quality, outperforming or matching existing methods with significantly less supervision.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed32bc07f3693ab8cf568a880f7a3e3ae639d4fb54fb284f6ea438be183324d5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed32bc07f3693ab8cf568a880f7a3e3ae639d4fb54fb284f6ea438be183324d5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces Direct Diffusion Score Preference Optimization (DDSPO), a method that optimizes diffusion models by applying preference supervision directly in the score space at each denoising timestep, using automatically generated signals from a reference model. This approach provides dense, stepwise learning signals and reduces reliance on expensive human-labeled data. Empirical results show DDSPO improves text-image alignment and visual quality, matching or outperforming prior preference-based methods with less supervision.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Towards Integrating Uncertainty for Domain-Agnostic Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [semantic segmentation], [uncertainty quantification, domain-agnostic, Segment Anything Model (SAM), Laplace approximation, benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jesse Brouwers, Xiaoyan Xing, Alexander Timans</p>
</li>
<li class="">
<p><strong>institution:</strong> UvA-Bosch Delta Lab, University of Amsterdam</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23427" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23427</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/JesseBrouw/UncertSAM" target="_blank" rel="noopener noreferrer" class="">https://github.com/JesseBrouw/UncertSAM</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Curated UncertSAM, a benchmark of eight datasets to stress-test segmentation models under challenging conditions like shadows and camouflage. 2. Evaluated a suite of lightweight, post-hoc uncertainty estimation methods for segmentation foundation models. 3. Assessed a preliminary uncertainty-guided prediction refinement step, finding that last-layer Laplace approximation yields uncertainty estimates well-correlated with segmentation errors.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15865ed74ed61443aa69769e51585f4a7341748fc10c0250eef9243be675f215_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15865ed74ed61443aa69769e51585f4a7341748fc10c0250eef9243be675f215_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether uncertainty quantification can improve the robustness of foundation segmentation models like SAM in domain-agnostic settings. The authors propose a benchmark (UncertSAM) and evaluate several post-hoc uncertainty estimation methods, finding that a last-layer Laplace approximation provides meaningful uncertainty signals. The results indicate the potential of integrating uncertainty to enhance model generalizability, though refinement benefits are preliminary.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image classification], [convolutional neural networks, fuzzy logic, road surface classification, intelligent transport systems, data fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mustafa Demetgul, Sanja Lazarova Molnar</p>
</li>
<li class="">
<p><strong>institution:</strong> Karlsruhe Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23436" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23436</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a real-time system for road surface classification by fusing weather-conditional data and road condition data. 2. Compares the performance of multiple deep learning CNNs (AlexNet, LeNet, VGG, ResNet) on both image-based and acceleration-data-as-image classification tasks. 3. Introduces the use of fuzzy logic to classify road surfaces according to environmental factors like weather and time of day, using sensor data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d74437ab4a94c7ac8b2148bba4e1b6dd8d4706d55db8a2ae146a41ace94ef9f9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d74437ab4a94c7ac8b2148bba4e1b6dd8d4706d55db8a2ae146a41ace94ef9f9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a real-time system for road surface condition monitoring. It employs deep learning CNNs to classify road types from images and acceleration data, achieving over 95% accuracy, and suggests using fuzzy logic to incorporate weather and time-of-day factors. The work aims to enhance vehicle safety and autonomous driving systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D reconstruction], [benchmark, multi-view, physical degradation, neural radiance field, Gaussian splatting]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuhong Liu, Chenyu Bao, Ziteng Cui, Yun Liu, Xuangeng Chu, Lin Gu, Marcos V. Conde, Ryo Umagami, Tomohiro Hashimoto, Zijian Hu, Tianhan Xu, Yuan Gan, Yusuke Kurose, Tatsuya Harada</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Tokyo, NII, Tohoku University, University of Würzburg, RIKEN</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23437" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23437</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces RealX3D, a real-capture benchmark for evaluating multi-view visual restoration and 3D reconstruction under diverse physical degradations. 2. Proposes a unified acquisition protocol that captures four families of corruptions (illumination, scattering, occlusion, blurring) at multiple severity levels, providing pixel-aligned low-quality and ground-truth views, RAW images, and dense laser scans. 3. Demonstrates through extensive benchmarking that current state-of-the-art optimization-based and feed-forward reconstruction methods suffer substantial quality degradation when faced with these real-world corruptions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ccf1373873a24f24accd40b8329f93e9af6b32bea07b7e7c4b639214553f8a0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ccf1373873a24f24accd40b8329f93e9af6b32bea07b7e7c4b639214553f8a0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces RealX3D, a benchmark dataset for evaluating 3D reconstruction and novel view synthesis methods under real-world physical degradations like low light and blur. The benchmark provides aligned low-quality and high-quality multi-view data with ground-truth geometry. Experiments show current methods are fragile to these corruptions, highlighting a gap between lab performance and real-world deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Stochastic Siamese MAE Pretraining for Longitudinal Medical Images</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [masked autoencoder, siamese network, stochastic process, longitudinal data, variational inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Taha Emre, Arunava Chakravarty, Thomas Pinetz, Dmitrii Lachinov, Martin J. Menten, Hendrik Scholl, Sobha Sivaprasad, Daniel Rueckert, Andrew Lotery, Stefan Sacu, Ursula Schmidt-Erfurth, Hrvoje Bogunović</p>
</li>
<li class="">
<p><strong>institution:</strong> Medical University of Vienna, Imperial College London, Technical University of Munich, University College London, University of Southampton</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23441" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23441</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed STAMP, a novel Siamese MAE framework that incorporates temporal information by conditioning on the time difference between input volumes. 2. Introduced a stochastic learning approach by reframing the MAE reconstruction loss as a conditional variational inference objective to model the uncertainty in disease progression. 3. Demonstrated superior performance of STAMP-pretrained models over existing temporal MAE methods and foundation models on predicting progression of Age-Related Macular Degeneration and Alzheimer&#x27;s Disease.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c93396b0ded8fc65364d5714bd12e652a23ffc22eb276cbbca01426ecd0db791_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c93396b0ded8fc65364d5714bd12e652a23ffc22eb276cbbca01426ecd0db791_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the lack of temporal awareness in self-supervised learning methods like MAE for longitudinal medical images. It proposes STAMP, a stochastic Siamese MAE framework that learns temporal dynamics by conditioning on time differences and using a variational inference objective. The method outperformed existing approaches on disease progression prediction tasks for OCT and MRI datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [hallucination mitigation, coarse-to-fine conditioning, Wasserstein fusion, generative feedback, training-free decoding]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zongsheng Cao, Yangfan He, Anran Liu, Jun Xie, Feng Chen, Zepeng Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Lenovo (PCIE), University of Minnesota (UMN)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23453" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23453</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/AI-Researcher-Team/CoFi-Dec" target="_blank" rel="noopener noreferrer" class="">https://github.com/AI-Researcher-Team/CoFi-Dec</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CoFi-Dec, a training-free decoding framework that mitigates hallucinations in LVLMs by integrating generative self-feedback with coarse-to-fine visual conditioning. 2. Introduces a Wasserstein-based fusion mechanism to align predictive distributions from multiple visual conditions into a geometrically consistent decoding trajectory. 3. Demonstrates substantial reduction in both entity-level and semantic-level hallucinations across six benchmarks, showing the framework is model-agnostic and requires no additional training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7cab1d37f48692f41be898afb160456b94e821d8b6ea16ba282cb4ee3ac5046_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7cab1d37f48692f41be898afb160456b94e821d8b6ea16ba282cb4ee3ac5046_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of hallucinated content in Large Vision-Language Models (LVLMs). It proposes CoFi-Dec, a training-free decoding framework that uses coarse-to-fine visual conditioning and generative feedback to create multi-level visual hypotheses, which are then unified via a Wasserstein-based fusion mechanism. The method significantly reduces hallucinations across multiple benchmarks and can be applied to various LVLMs without retraining.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Automated river gauge plate reading using a hybrid object detection and generative AI framework in the Limpopo River Basin</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [YOLOv8-Pose, multimodal LLMs, waterline detection, scale gap estimation, geometric calibration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kayathri Vigneswaran, Hugo Retief, Jai Clifford Holmes, Mariangel Garcia Andarcia, Hansaka Tennakoon</p>
</li>
<li class="">
<p><strong>institution:</strong> International Water Management Institute (IWMI), Association for Water and Rural Development (AWARD)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23454" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23454</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel hybrid framework combining vision-based waterline detection, YOLOv8-Pose for scale extraction, and multimodal LLMs for automated river gauge reading. 2. Demonstrates that incorporating geometric metadata (scale gap) significantly improves the predictive accuracy of LLMs for water level estimation. 3. Provides a scalable and efficient solution for automated hydrological monitoring, highlighting its sensitivity to image quality and potential for real-time digitization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3eb2461b2254accb76162876e6a7ac7fa51fda24feaacba9e7280b9c3b490a4b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3eb2461b2254accb76162876e6a7ac7fa51fda24feaacba9e7280b9c3b490a4b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of automated river water level monitoring by proposing a hybrid framework that integrates computer vision (waterline detection and YOLOv8-Pose) with multimodal large language models (GPT-4o and Gemini 2.0 Flash) to read gauge plates. The method uses geometric calibration from scale gap detection to enhance LLM performance, achieving high accuracy under optimal conditions. The study concludes that combining geometric metadata with multimodal AI offers a robust, scalable solution for real-time hydrological monitoring.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Deterministic Image-to-Image Translation via Denoising Brownian Bridge Models with Dual Approximators</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image-to-image translation], [Brownian bridge, deterministic translation, denoising diffusion, dual approximators, super-resolution]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bohan Xiao, Peiyong Wang, Qisheng He, Ming Dong</p>
</li>
<li class="">
<p><strong>institution:</strong> Wayne State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23463" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23463</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/bohan95/dual-app-bridge" target="_blank" rel="noopener noreferrer" class="">https://github.com/bohan95/dual-app-bridge</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel denoising Brownian bridge model with dual neural network approximators for deterministic I2I translation., 2. Introduces a method that guarantees consistent, predictable outputs with high fidelity to ground truth, addressing the limitations of stochastic models., 3. Demonstrates superior performance in tasks like super-resolution compared to both stochastic and deterministic baseline models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/103bceca24bdd1f051193099bcdde9fa2cd561605d76eb8f9b4f42f608212956_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/103bceca24bdd1f051193099bcdde9fa2cd561605d76eb8f9b4f42f608212956_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Dual-approx Bridge, a novel generative model for deterministic image-to-image translation. It uses Brownian bridge dynamics with two neural approximators to produce high-fidelity, low-variance outputs. Experiments show it outperforms existing baselines in image quality and faithfulness to ground truth.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [motion generation], [flow matching, diffusion transformer (DiT), reinforcement learning from human feedback (RLHF)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuxin Wen, Qing Shuai, Di Kang, Jing Li, Cheng Wen, Yue Qian, Ningxin Jiao, Changhai Chen, Weijie Chen, Yiran Wang, Jinkun Guo, Dongyue An, Han Liu, Yanyu Tong, Chao Zhang, Qing Guo, Juan Chen, Qiao Zhang, Youyi Zhang, Zihao Yao, Cheng Zhang, Hong Duan, Xiaoping Wu, Qi Chen, Fei Cheng, Liang Dong, Peng He, Hao Zhang, Jiaxin Lin, Chao Zhang, Zhongyi Fan, Yifan Li, Zhichao Hu, Yuhong Liu, Linus, Jie Jiang, Xiaolong Li, Linchao Bao</p>
</li>
<li class="">
<p><strong>institution:</strong> Tencent Hunyuan</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23464" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23464</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Tencent-Hunyuan/HY-Motion-1.0" target="_blank" rel="noopener noreferrer" class="">https://github.com/Tencent-Hunyuan/HY-Motion-1.0</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. The first successful scaling of DiT-based flow matching models to billion parameters for motion generation. 2. A comprehensive full-stage training paradigm including large-scale pretraining, fine-tuning, and RLHF. 3. A meticulous data processing pipeline enabling extensive coverage of over 200 motion categories.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3019219aea0683c229d44ce63a0fed59b5ebb795811dc1b1638ae995c9a8156_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3019219aea0683c229d44ce63a0fed59b5ebb795811dc1b1638ae995c9a8156_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces HY-Motion 1.0, a large-scale model for generating 3D human motions from text. It scales up Diffusion Transformer-based flow matching and uses a full-stage training pipeline with pretraining, fine-tuning, and RLHF. The model achieves state-of-the-art performance and broad motion coverage, and is released open-source.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MCI-Net: A Robust Multi-Domain Context Integration Network for Point Cloud Registration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [point cloud registration], [graph neural network, multi-domain context, dynamic inlier selection, feature aggregation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuyuan Lin, Wenwu Peng, Junjie Huang, Qiang Qi, Miaohui Wang, Jian Weng</p>
</li>
<li class="">
<p><strong>institution:</strong> Jinan University, Qingdao University of Science and Technology, Shenzhen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23472" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23472</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="http://www.linshuyuan.com" target="_blank" rel="noopener noreferrer" class="">http://www.linshuyuan.com</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A graph neighborhood aggregation module that constructs a global graph to capture overall structural relationships in point clouds. 2. A progressive context interaction module that enhances feature discriminability through intra-domain decoupling and inter-domain interaction. 3. A dynamic inlier selection method that optimizes inlier weights using residual information from iterative pose estimation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2dc4a548abb0cfe84934aaf9ac3e41164df500ce5f41043542745cdb8b1984c4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2dc4a548abb0cfe84934aaf9ac3e41164df500ce5f41043542745cdb8b1984c4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes MCI-Net, a novel network for point cloud registration that improves feature learning by integrating contextual cues from multiple domains. The method introduces modules for graph-based neighborhood aggregation, progressive context interaction, and dynamic inlier selection. Experiments show it achieves state-of-the-art performance, including a 96.4% registration recall on the 3DMatch dataset.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SC-Net: Robust Correspondence Learning via Spatial and Cross-Channel Context</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [correspondence learning], [adaptive focused regularization, bilateral field adjustment, position-aware recovery]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuyuan Lin, Hailiang Liao, Qiang Qi, Junjie Huang, Taotao Lai, Jian Weng</p>
</li>
<li class="">
<p><strong>institution:</strong> Jinan University, Qingdao University of Science and Technology, Minjiang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23473" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23473</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="http://www.linshuyuan.com" target="_blank" rel="noopener noreferrer" class="">http://www.linshuyuan.com</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed the Adaptive Focused Regularization (AFR) module to enhance position-awareness and robustness against spurious motion samples. 2. Proposed the Bilateral Field Adjustment (BFA) module to refine motion fields by modeling long-range relationships across spatial and channel dimensions. 3. Proposed a Position-Aware Recovery (PAR) module to ensure consistent and precise recovery of motion vectors from the refined field.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6a1d4901b95443b2a2afa8148dfe229b7aaa60884a789ba90d1475aba259667_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6a1d4901b95443b2a2afa8148dfe229b7aaa60884a789ba90d1475aba259667_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of CNN-based two-view correspondence learning failing to aggregate global context and oversmoothing motion fields. It proposes SC-Net, a novel network that integrates spatial and cross-channel context using three key modules (AFR, BFA, PAR). Experiments show SC-Net outperforms state-of-the-art methods on pose estimation and outlier removal tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [temporal-aware retrieval, entropy-weighted sampling, training-free framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zongsheng Cao, Yangfan He, Anran Liu, Feng Chen, Zepeng Wang, Jun Xie</p>
</li>
<li class="">
<p><strong>institution:</strong> Lenovo (PCIE), University of Minnesota (UMN)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23483" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23483</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/AI-Researcher-Team/TV-RAG" target="_blank" rel="noopener noreferrer" class="">https://github.com/AI-Researcher-Team/TV-RAG</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A time-decay retrieval module that injects temporal offsets into similarity computation to rank queries by their true multimedia context. 2. An entropy-weighted key-frame sampler that selects information-dense frames to reduce redundancy while preserving representativeness. 3. A lightweight, training-free architecture that can be grafted onto any Large Video Language Model (LVLM) for improved long-video reasoning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a241b314650fa36c9f6d3a32e1dfd25ff643fa954ce0210d0462c2c6e84624ca_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a241b314650fa36c9f6d3a32e1dfd25ff643fa954ce0210d0462c2c6e84624ca_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes TV-RAG, a training-free framework to enhance long video understanding by Large Video Language Models (LVLMs). It introduces a temporal-aware retrieval module and an entropy-weighted frame sampler to better capture semantic shifts and temporal dependencies in long videos. The system outperforms leading baselines on multiple benchmarks without requiring model retraining.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multi-label Classification with Panoptic Context Aggregation Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multi-label classification], [context modeling, cross-scale aggregation, attention mechanism, geometric relationships, Hilbert space]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mingyuan Jiu, Hailong Zhu, Wenchuan Wei, Hichem Sahbi, Rongrong Ji, Mingliang Xu</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhengzhou University, Sorbonne University, Xiamen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23486" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23486</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Deep Panoptic Context Aggregation Network (PanCAN) that hierarchically integrates multi-order geometric contexts. 2. Introduces a method combining random walks with attention to learn multi-order neighborhood relationships in a high-dimensional Hilbert space. 3. Demonstrates effective cross-scale modeling by cascading modules and dynamically fusing salient anchor features, significantly improving complex scene understanding.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252d9e1fa32aca6156eb50c353f63b3e6265abd48d990c277a30d4dca3d5b654_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252d9e1fa32aca6156eb50c353f63b3e6265abd48d990c277a30d4dca3d5b654_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of existing multi-label classification methods in modeling cross-scale contextual interactions. It proposes the Panoptic Context Aggregation Network (PanCAN), which hierarchically aggregates multi-order geometric contexts using attention and random walks in a Hilbert space. Experiments on standard benchmarks show PanCAN outperforms state-of-the-art methods, substantially improving classification performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] IdentityStory: Taming Your Identity-Preserving Generator for Human-Centric Story Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image generation], [identity-preserving generation, iterative identity discovery, re-denoising identity injection, human-centric story generation, character consistency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Donghao Zhou, Jingyu Lin, Guibao Shen, Quande Liu, Jialin Gao, Lihao Liu, Lan Du, Cunjian Chen, Chi-Wing Fu, Xiaowei Hu, Pheng-Ann Heng</p>
</li>
<li class="">
<p><strong>institution:</strong> The Chinese University of Hong Kong, Monash University, The Hong Kong University of Science and Technology (Guangzhou), Kuaishou Technology, Amazon, South China University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23519" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23519</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://correr-zhou.github.io/IdentityStory" target="_blank" rel="noopener noreferrer" class="">https://correr-zhou.github.io/IdentityStory</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes IdentityStory, a framework for human-centric story generation that ensures consistent character identity across sequential images. 2. Introduces Iterative Identity Discovery to extract cohesive character identities. 3. Presents Re-denoising Identity Injection to inject identities into images while preserving the desired context.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48c67dbdd79287f3a90a67510ea2e86db0083c44b40ed86b753a884d135ab1b7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48c67dbdd79287f3a90a67510ea2e86db0083c44b40ed86b753a884d135ab1b7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the challenge of generating a series of images with consistent human characters from text prompts, a task known as human-centric story generation. It proposes the IdentityStory framework, which uses Iterative Identity Discovery and Re-denoising Identity Injection to tame identity-preserving generators. Experiments show it outperforms existing methods in maintaining face consistency and supports multi-character combinations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Iterative Inference-time Scaling with Adaptive Frequency Steering for Image Super-Resolution</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image super-resolution], [diffusion models, inference-time scaling, iterative refinement, frequency steering, training-free]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hexin Zhang, Dong Li, Jie Huang, Bingzhou Wang, Xueyang Fu, Zhengjun Zha</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology of China</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23532" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23532</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes IAFS, a training-free framework that combines iterative refinement and frequency-aware particle fusion for diffusion-based super-resolution. 2. Introduces adaptive frequency steering to balance high-frequency perceptual quality and low-frequency structural fidelity. 3. Demonstrates through extensive experiments that IAFS effectively resolves the perception-fidelity conflict and outperforms existing inference-time scaling methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba3e0aea28abd4e2c2b0742af634714a92519c3c67ebaa6960839e16e4a97152_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba3e0aea28abd4e2c2b0742af634714a92519c3c67ebaa6960839e16e4a97152_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of balancing perceptual quality and structural fidelity in diffusion-based image super-resolution. It proposes IAFS, a training-free framework that uses iterative inference-time scaling with adaptive frequency steering to progressively refine images. Experiments show IAFS outperforms existing methods in achieving better detail and accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [diffusion models], [multi-subject customization, layout guidance, attention decoupling, training-free, image adapter]</p>
</li>
<li class="">
<p><strong>authors:</strong> Binhe Yu, Zhen Wang, Kexin Li, Yuqian Yuan, Wenqiao Zhang, Long Chen, Juncheng Li, Jun Xiao, Yueting Zhuang</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, HKUST (The Hong Kong University of Science and Technology)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23537" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23537</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes AnyMS, a novel training-free framework for layout-guided multi-subject image customization. 2. Introduces a bottom-up dual-level attention decoupling mechanism (global and local) to balance text alignment, identity preservation, and layout control. 3. Employs pre-trained image adapters to extract subject features without requiring subject-specific training or adapter tuning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e5ac220ed9f71a26f20317e74d4ddc9cd5a957b5751dba85f593ddfeaf39640_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e5ac220ed9f71a26f20317e74d4ddc9cd5a957b5751dba85f593ddfeaf39640_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of generating coherent images containing multiple user-specified subjects while balancing text alignment, subject identity, and layout control. It proposes AnyMS, a training-free framework that uses a bottom-up attention decoupling mechanism and pre-trained adapters to integrate text, subject images, and layout constraints. The method achieves state-of-the-art performance, supporting complex compositions and scaling to many subjects.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [computational pathology], [agentic multimodal model, evidence-seeking inference, reinforcement learning, whole-slide images, vision-language model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shengyi Hua, Jianfeng Wu, Tianle Shen, Kangzhe Hu, Zhongzhen Huang, Shujuan Ni, Zhihong Zhang, Yuan Li, Zhe Wang, Xiaofan Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, Fourth Military Medical University, University of Science and Technology of China, Fudan University, Nanjing Medical University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23545" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23545</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed PathFound, an agentic multimodal model that introduces an evidence-seeking inference paradigm for pathological diagnosis, moving beyond static, single-pass analysis. 2. Integrated pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to enable proactive information acquisition and multi-stage diagnosis refinement. 3. Demonstrated that the evidence-seeking strategy consistently improves diagnostic accuracy across models and that PathFound achieves state-of-the-art performance, showing strong potential for discovering subtle pathological details.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db008475f544af0752cf146b5b6ba57eaf58c0e376cb94807dd3df594fa09037_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db008475f544af0752cf146b5b6ba57eaf58c0e376cb94807dd3df594fa09037_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes PathFound, an agentic multimodal model that mimics clinical workflows by actively seeking evidence for ambiguous pathological diagnoses through multi-turn interactions. It integrates visual foundation models, vision-language models, and reinforcement learning-based reasoning to refine its initial diagnosis. The method achieves state-of-the-art diagnostic accuracy and demonstrates the effectiveness of evidence-seeking workflows in computational pathology.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PurifyGen: A Risk-Discrimination and Semantic-Purification Model for Safe Text-to-Image Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [text-to-image generation, prompt purification, semantic distance, null space projection, training-free safety]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zongsheng Cao, Yangfan He, Anran Liu, Jun Xie, Feng Chen, Zepeng Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Minnesota, Lenovo PCIE</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23546" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23546</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/AI-Researcher-Team/PurifyGen" target="_blank" rel="noopener noreferrer" class="">https://github.com/AI-Researcher-Team/PurifyGen</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a novel, training-free, dual-stage strategy for safe text-to-image generation that retains the original model weights. 2. Proposes a fine-grained risk discrimination method using complementary semantic distance to classify prompt tokens without keyword matching. 3. Develops a dual-space transformation for semantic purification, projecting risky embeddings into the null space of toxic concepts and the range space of clean concepts.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce70223ecd8b36ca7561cfb1134bb73a6a3a71d9d1fd6f6e1f2cd067f9a77b4f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce70223ecd8b36ca7561cfb1134bb73a6a3a71d9d1fd6f6e1f2cd067f9a77b4f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes PurifyGen, a training-free method to make text-to-image generation safer. It works by first identifying risky tokens in a prompt using semantic distances and then purifying them by removing harmful semantic components while reinforcing safe ones. Experiments show it effectively reduces unsafe content across multiple datasets and competes with training-dependent approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal large language models], [chemical reaction understanding, multimodal benchmark, scientific literature, visual perception, cross-modal integration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hanzheng Li, Xi Fang, Yixuan Li, Chaozheng Huang, Junjie Wang, Xi Wang, Hongzhe Bai, Bojun Hao, Shenyu Lin, Huiqi Liang, Linfeng Zhang, Guolin Ke</p>
</li>
<li class="">
<p><strong>institution:</strong> DP Technology, Shanghai Jiao Tong University, Tsinghua University, New York University, Fudan University, Xiamen University, ShanghaiTech University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23565" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23565</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces RxnBench, a multi-tiered benchmark for evaluating MLLMs on chemical reaction understanding from scientific PDFs, featuring two tasks (SF-QA and FD-QA). 2. Provides a comprehensive evaluation revealing a critical capability gap in MLLMs, showing they struggle with deep chemical logic and precise structural recognition despite excelling at text extraction. 3. Highlights the importance of inference-time reasoning and underscores the urgent need for domain-specific visual encoders and stronger reasoning engines for autonomous AI chemists.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e7b73b049eb3232e03516a09f66b0fddafff9357b89527de70340faa28603c6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e7b73b049eb3232e03516a09f66b0fddafff9357b89527de70340faa28603c6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces RxnBench, a multimodal benchmark to evaluate Large Language Models on understanding chemical reactions from scientific literature. The evaluation reveals that while models are good at extracting text, they struggle with chemical logic and structural recognition, showing the need for better domain-specific visual and reasoning components.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Instruction-Following Evaluation of Large Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [instruction-following evaluation], [large vision-language models, visual instruction tuning, output format specification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Daiki Shiono, Shumpei Miyawaki, Ryota Tanaka, Jun Suzuki</p>
</li>
<li class="">
<p><strong>institution:</strong> Tohoku University, NTT Corporation</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23572" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23572</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Quantitatively demonstrates the decline in instruction-following ability of LVLMs after visual instruction fine-tuning. 2. Constructs new training datasets that highlight whether the output format is specified. 3. Shows that explicitly indicating the output format during fine-tuning helps LVLMs follow instructions more accurately.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9a9ab8218c47b2791fe28909d9e490dfaa5d680ecfb209ca796611f893b9430_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9a9ab8218c47b2791fe28909d9e490dfaa5d680ecfb209ca796611f893b9430_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies and quantifies a problem where Large Vision-Language Models (LVLMs) lose their instruction-following ability after visual instruction tuning. The authors propose constructing datasets that explicitly specify the output format and find that training with such data mitigates the performance decline. The main conclusion is that including instructions on output format during fine-tuning can help preserve LVLMs&#x27; instruction-following capabilities.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ThinkGen: Generalized Thinking for Visual Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [text-to-image generation], [Chain-of-Thought (CoT), Multimodal Large Language Model (MLLM), Diffusion Transformer (DiT), reinforcement learning, SepGRPO]</p>
</li>
<li class="">
<p><strong>authors:</strong> Siyu Jiao, Yiheng Lin, Yujie Zhong, Qi She, Wei Zhou, Xiaohan Lan, Zilong Huang, Fei Yu, Yingchen Yu, Yunqing Zhao, Yao Zhao, Yunchao Wei</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Jiaotong University, Bytedance</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23568" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23568</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/jiaosiyuu/ThinkGen" target="_blank" rel="noopener noreferrer" class="">https://github.com/jiaosiyuu/ThinkGen</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM&#x27;s CoT reasoning for various generation tasks. 2. Introduces a decoupled architecture using a pretrained MLLM to generate instructions and a DiT for image synthesis. 3. Proposes a separable GRPO-based training paradigm (SepGRPO) for alternating reinforcement learning between modules, enabling joint training across diverse datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f8377ac1537eac2a0f36b9ae8883a51e957cbbeb6e49b280cc20b5c5080e11f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f8377ac1537eac2a0f36b9ae8883a51e957cbbeb6e49b280cc20b5c5080e11f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces ThinkGen, a framework that integrates Chain-of-Thought reasoning from Multimodal LLMs with a Diffusion Transformer for visual generation. It uses a decoupled architecture and a novel separable reinforcement learning training method to generalize across diverse generation scenarios. Experiments show it achieves state-of-the-art performance on multiple benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ProGuard: Towards Proactive Multimodal Safeguard</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal safety], [proactive guard, out-of-distribution (OOD) detection, reinforcement learning (RL), multimodal safety taxonomy, synonym-bank reward]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shaohan Yu, Lijun Li, Chenyang Si, Lu Sheng, Jing Shao</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Artificial Intelligence Laboratory, Nanjing University, Beihang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23573" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23573</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://yushaohan.github.io/ProGuard" target="_blank" rel="noopener noreferrer" class="">https://yushaohan.github.io/ProGuard</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces ProGuard, a vision-language proactive guard model that identifies and describes out-of-distribution safety risks without requiring model adjustments. 2. Constructs a modality-balanced dataset of 87K samples with binary safety labels and hierarchical risk categories to mitigate modality bias. 3. Trains the model purely via reinforcement learning augmented with a synonym-bank-based similarity reward to enhance OOD risk inference and description.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ffca72668094b2c8095387a83d8b49cc465da7d2f333cac7db429f76193be61_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ffca72668094b2c8095387a83d8b49cc465da7d2f333cac7db429f76193be61_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes ProGuard, a proactive multimodal safeguard that uses reinforcement learning on a balanced dataset to detect and describe unseen safety risks. It achieves performance comparable to closed-source models on safety classification and significantly improves OOD risk detection and description by over 50%.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Image Denoising Using Global and Local Circulant Representation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image denoising], [circulant representation, tensor-SVD, Haar transform]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhaoming Kong, Xiaowei Yang, Jiahuan Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> South China University of Technology, Guangdong Provincial People&#x27;s Hospital, Southern Medical University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23569" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23569</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/ZhaomingKong/Haar-tSVD" target="_blank" rel="noopener noreferrer" class="">https://github.com/ZhaomingKong/Haar-tSVD</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Established a theoretical connection between PCA and the Haar transform under circulant representation. 2. Proposed a computationally simple, one-step plug-and-play denoiser (Haar-tSVD) that balances speed and performance by capturing global and local correlations. 3. Introduced an adaptive noise estimation scheme and integrated deep neural networks to enhance robustness under severe noise conditions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e8ca1da239d053e9277827699e78e6ac4dd3e3111e0069116ba0071aa405a82_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e8ca1da239d053e9277827699e78e6ac4dd3e3111e0069116ba0071aa405a82_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a new image denoising method called Haar-tSVD, which combines tensor singular value decomposition with the Haar transform to efficiently remove noise. It is designed as a fast, parallelizable algorithm that does not require learning and can be integrated with deep networks for better performance. Experiments show the method is effective and efficient for noise removal across various datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [on-policy distillation, real-time video diffusion, multimodal conditioning, Anchor-Heavy Identity Sinks, autoregressive generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ethan Chern, Zhulin Hu, Bohao Tang, Jiadi Su, Steffi Chern, Zhijie Deng, Pengfei Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> SII, SJTU, GAIR</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23576" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23576</a></p>
</li>
<li class="">
<p><strong>code:</strong> /githubCode</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an improved on-policy distillation recipe for real-time multimodal video diffusion, addressing issues like flickering and quality degradation in prior methods. 2. Develops LiveTalk, a real-time interactive avatar system integrating the distilled model with audio language models and long-form video inference techniques. 3. Demonstrates 20x reduction in inference cost/latency while matching baseline quality, and outperforms SOTA in multi-turn coherence and content quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/549213eb2c7dd73c1020df9057c0ed9979c11b408c183a2d3d0bc103f7356da9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/549213eb2c7dd73c1020df9057c0ed9979c11b408c183a2d3d0bc103f7356da9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the challenge of real-time interactive video generation by improving on-policy distillation for multimodal-conditioned diffusion models. The proposed method enhances training stability and output quality, enabling a 20x speedup. The resulting LiveTalk system achieves real-time, coherent multi-turn avatar interactions, significantly outperforming existing models in latency and quality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Same or Not? Enhancing Visual Perception in Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [vision-language models], [fine-grained visual understanding, dataset, benchmark, visual perception, image-pair queries]</p>
</li>
<li class="">
<p><strong>authors:</strong> Damiano Marsili, Aditya Mehta, Ryan Y. Lin, Georgia Gkioxari</p>
</li>
<li class="">
<p><strong>institution:</strong> California Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23592" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23592</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced TWIN, a large-scale dataset of 561,000 image-pair queries designed to train VLMs on fine-grained visual perception by determining if two similar images depict the same object. 2. Introduced FGVQA, a benchmark suite of 12,000 queries to evaluate fine-grained VQA capabilities across multiple domains. 3. Demonstrated that fine-tuning VLMs on TWIN significantly improves their fine-grained recognition on FGVQA (up to 19.3%) without harming general VQA performance, and showed the importance of dataset scale.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea8f098740ab2b0f35ff068b90bb9d954080b272f70a67a2bd48d7a7771756e0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea8f098740ab2b0f35ff068b90bb9d954080b272f70a67a2bd48d7a7771756e0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the limitation of Vision-Language Models (VLMs) in fine-grained visual perception. It proposes a new training dataset (TWIN) and a benchmark (FGVQA) to enhance VLMs&#x27; ability to notice subtle visual details. The results show that fine-tuning on TWIN significantly improves fine-grained recognition on unseen domains without compromising general performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [Scalable Residual Feature Aggregation (SRFA), Hybrid Metaheuristic Optimization (HHO-BA), Vision Transformer (ViT) with EfficientNet-B3]</p>
</li>
<li class="">
<p><strong>authors:</strong> Janani Annur Thiruvengadam, Kiran Mayee Nabigaru, Anusha Kovi</p>
</li>
<li class="">
<p><strong>institution:</strong> Amazon.com Services LLC</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23597" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23597</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Scalable Residual Feature Aggregation (SRFA) framework integrating MAGRes-UNet for segmentation and DenseNet-121 for hierarchical feature extraction. 2. Introduces a hybrid HHO-BA metaheuristic feature selection strategy to refine the optimal feature subset. 3. Develops a novel hybrid classifier combining Vision Transformer (ViT) and EfficientNet-B3, fine-tuned using a dual SSA-GWO optimization mechanism for robust classification.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/236616ac709d85c1958734582dc8a1182b385106ea696a4ffc79796016e70db9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/236616ac709d85c1958734582dc8a1182b385106ea696a4ffc79796016e70db9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of early pancreatic neoplasm detection in multimodal CT imaging by proposing a Scalable Residual Feature Aggregation (SRFA) framework. The method combines advanced segmentation, feature extraction with residual storage, hybrid metaheuristic feature selection, and a novel ViT-EfficientNet-B3 classifier optimized with SSA and GWO. The proposed system achieves high performance (96.23% accuracy), demonstrating significant improvement over traditional and contemporary models for robust early detection.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Detection Fire in Camera RGB-NIR</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [YOLOv11, EfficientNetV2, two-stage detection, NIR dataset, Patched-YOLO]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nguyen Truong Khai, Luong Duc Vinh</p>
</li>
<li class="">
<p><strong>institution:</strong> Viettel (inferred from email domain <a href="mailto:vinhld@viettel.com" target="_blank" rel="noopener noreferrer" class="">vinhld@viettel.com</a>)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23594" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23594</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An additional NIR dataset with various data augmentation strategies to address data scarcity. 2. A two-stage detection pipeline combining YOLOv11 and EfficientNetV2-B0 to improve night-time fire detection accuracy and reduce false positives from artificial lights. 3. Patched-YOLO, a patch-based processing method to enhance detection of small and distant fire objects in RGB images.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27f2ac726eb6c9db14e654d748210b2689604a2d35ed6affd7a7ea009d6094bd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27f2ac726eb6c9db14e654d748210b2689604a2d35ed6affd7a7ea009d6094bd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of improving fire detection accuracy, especially at night using NIR cameras and for small objects in RGB images. It proposes a two-stage model (YOLOv11 + EfficientNetV2-B0) for NIR detection and a Patched-YOLO method for RGB, alongside an augmented NIR dataset. The proposed approaches aim to achieve higher accuracy and reduce false positives compared to previous methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Memorization in 3D Shape Generation: An Empirical Study</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D shape generation], [memorization, diffusion models, latent vector-set, evaluation framework, data leakage]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shu Pu, Boya Zeng, Kaichen Zhou, Mengyu Wang, Zhuang Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Princeton University, Harvard University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23628" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23628</a></p>
</li>
<li class="">
<p><strong>code:</strong> github.com/zlab-princeton/3d_mem</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a novel evaluation framework to quantify memorization in 3D generative models. 2. Applied the framework to benchmark and quantify memorization in existing 3D generation methods. 3. Conducted controlled experiments to identify how data and modeling factors (e.g., modality, guidance scale, augmentation) influence memorization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca470d9a610f6e7172776f63b7bfed02850fdb4a843786976b699c63c87febc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca470d9a610f6e7172776f63b7bfed02850fdb4a843786976b699c63c87febc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether 3D shape generative models memorize training data. The authors design an evaluation framework to measure memorization and conduct experiments with a latent vector-set diffusion model. They find memorization depends on data modality and diversity, peaks at moderate guidance, and can be reduced with longer latent vectors and rotation augmentation without harming quality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [active perception, audio-guided, tool orchestration, coarse-to-fine perception, multimodal alignment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Keda Tao, Wenjie Du, Bohan Yu, Weiqiang Wang, Jian Liu, Huan Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Westlake University, Ant Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23646" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23646</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://kd-tao.github.io/OmniAgent" target="_blank" rel="noopener noreferrer" class="">https://kd-tao.github.io/OmniAgent</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces OmniAgent, an audio-guided active perception agent that shifts from passive response to active multimodal inquiry. 2. Proposes a novel coarse-to-fine audio-guided perception paradigm that uses audio cues to localize events and guide reasoning. 3. Demonstrates state-of-the-art performance on audio-video benchmarks, outperforming leading models by 10%-20% accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a339430d662ee6bf5ece3181e2cf4fba493ea56a973aed76a319a939348ef1e3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a339430d662ee6bf5ece3181e2cf4fba493ea56a973aed76a319a939348ef1e3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the lack of fine-grained cross-modal understanding in omnimodal LLMs by proposing OmniAgent, an active perception agent that dynamically orchestrates tools using audio cues to guide video analysis. It achieves superior performance on audio-video understanding benchmarks, significantly outperforming existing models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D object detection and tracking], [spatio-temporal alignment, multi-hypothesis decoding, motion modeling, end-to-end perception, autonomous driving]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiaoyu Li, Peidong Li, Xian Wu, Long Shi, Dedong Liu, Yitao Wu, Jiajia Fu, Dixiao Cui, Lijun Zhao, Lining Sun</p>
</li>
<li class="">
<p><strong>institution:</strong> Harbin Institute of Technology, IROOTECH</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23635" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23635</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/lixiaoyu2000/HAT" target="_blank" rel="noopener noreferrer" class="">https://github.com/lixiaoyu2000/HAT</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes HAT, a novel spatio-temporal alignment module that adaptively decodes optimal alignment from multiple motion hypotheses without direct supervision. 2. Integrates both explicit motion models and semantic cues to address suboptimal alignment caused by varying object motion states and features. 3. Demonstrates consistent improvements in 3D perception and tracking performance across diverse baselines and enhances robustness in end-to-end autonomous driving systems, especially under semantic corruption.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbd0387c83c53b2f59d9e00643e3c40d6bb651484df151a86d870b0e63fc6c53_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbd0387c83c53b2f59d9e00643e3c40d6bb651484df151a86d870b0e63fc6c53_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that existing spatio-temporal alignment methods in end-to-end 3D perception for autonomous driving are suboptimal due to simplified motion models. It proposes HAT, a module that generates multiple motion-aware feature proposals and adaptively selects the best alignment using semantic and motion cues. HAT improves detection and tracking performance on benchmarks and enhances the robustness of end-to-end autonomous driving systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robot learning], [video-to-locomotion, visual motion intent, diffusion policy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Academy of Artificial Intelligence (BAAI), University of Sydney, Hong Kong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23649" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23649</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes RoboMirror, the first retargeting-free framework that directly generates humanoid locomotion from raw videos by understanding visual motion intents. 2. Introduces a method that leverages Vision-Language Models (VLMs) to distill videos into semantic motion intents, which condition a diffusion-based policy, bypassing explicit pose estimation. 3. Demonstrates the framework&#x27;s effectiveness for both egocentric (telepresence) and third-person video control, significantly reducing control latency and improving task success rates.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb8087d8ec53ebe9a14dbea08d918cbe27838f7e4dc5d178ed65513c16703cd7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb8087d8ec53ebe9a14dbea08d918cbe27838f7e4dc5d178ed65513c16703cd7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the gap between visual understanding and control in humanoid locomotion by proposing RoboMirror, a framework that first understands visual motion intents from raw videos and then uses them to condition a diffusion policy for generating physically plausible actions. The method eliminates the need for explicit pose reconstruction and retargeting. Experiments show it enables effective telepresence, reduces control latency by 80%, and achieves higher task success than baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [intrinsic image decomposition], [multi-view consistency, transformer, physically grounded model, feed-forward inference, specular shading]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kang Du, Yirui Guan, Zeyu Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology, Tencent</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23667" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23667</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes IDT, a feed-forward transformer framework for multi-view intrinsic decomposition that directly outputs view-consistent factors without iterative sampling. 2. Introduces a physically grounded image formation model that explicitly decomposes images into diffuse reflectance, diffuse shading, and specular shading. 3. Demonstrates superior performance in producing cleaner decompositions and better multi-view consistency compared to prior methods on synthetic and real-world datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01e9c12974f4da9c82258f09668d7544f5ad1769d8b9ab02956676c6f1a49a81_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01e9c12974f4da9c82258f09668d7544f5ad1769d8b9ab02956676c6f1a49a81_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of view inconsistency in multi-view intrinsic image decomposition. It proposes IDT, a feed-forward transformer that jointly processes multiple views using a physically grounded model to decompose images into reflectance and shading components. Experiments show IDT achieves more consistent and interpretable decompositions than previous methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Web World Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [world model, language agent, web framework, structured latent state, deterministic generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jichen Feng, Yifan Zhang, Chenggong Zhang, Yifu Lu, Shilong Liu, Mengdi Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Princeton University, University of California, Los Angeles, University of Pennsylvania</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23676" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23676</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://princeton-ai2-lab.github.io/Web-World-Models/" target="_blank" rel="noopener noreferrer" class="">https://princeton-ai2-lab.github.io/Web-World-Models/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced the Web World Model (WWM), a hybrid architecture that uses ordinary web code to enforce logical consistency and LLMs to generate open-ended content. 2. Built a suite of practical WWM demonstrations across diverse domains (travel, fiction, encyclopedia, games) on a realistic web stack. 3. Identified key design principles for WWMs, such as separating code-defined rules from model-driven imagination and representing latent state as typed web interfaces.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed8017bbc1bd6722a0d7bd0f84c67f735c0f1b24747518e2aa15905d07d1b03c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed8017bbc1bd6722a0d7bd0f84c67f735c0f1b24747518e2aa15905d07d1b03c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Web World Models (WWMs), a framework that combines the reliability of web code for world &quot;physics&quot; with the generative power of LLMs for content and narratives. This hybrid approach aims to provide language agents with controllable, logically consistent, yet open-ended persistent environments. The work demonstrates that standard web stacks can serve as a scalable substrate for building such world models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [depth estimation], [video diffusion, transparent objects, LoRA, depth estimation, normal estimation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shaocong Xu, Songlin Wei, Qizhe Wei, Zheng Geng, Hong Li, Licheng Shen, Qianpu Sun, Shu Han, Bin Ma, Bohan Li, Chongjie Ye, Yuhang Zheng, Nan Wang, Saining Zhang, Hao Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Academy of Artificial Intelligence, Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23705" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23705</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://daniellli.github.io/projects/DKT/" target="_blank" rel="noopener noreferrer" class="">https://daniellli.github.io/projects/DKT/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces TransPhy3D, a large-scale synthetic video dataset for transparent/reflective scenes with RGB, depth, and normal maps. 2. Proposes DKT, a method that repurposes a pre-trained video diffusion model via lightweight LoRA adapters for temporally consistent depth and normal estimation from videos. 3. Demonstrates state-of-the-art zero-shot performance on real and synthetic benchmarks and shows practical improvement in robotic grasping tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4acfd062f7e9aa8f5d75a8134d26d8ef4e00ec2127c12cf0ecfcc6466b48290_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4acfd062f7e9aa8f5d75a8134d26d8ef4e00ec2127c12cf0ecfcc6466b48290_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenging problem of depth and normal estimation for transparent and reflective objects in videos. The proposed method, DKT, repurposes a pre-trained video diffusion model using LoRA adapters and a novel synthetic dataset (TransPhy3D) to achieve temporally consistent predictions. The results show state-of-the-art zero-shot performance on benchmarks and improved robotic manipulation, supporting the claim that generative video priors effectively capture the physics of transparency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video super-resolution], [diffusion models, online processing, low-latency, auto-regressive, temporal guidance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hau-Shiang Shiu, Chin-Yang Lin, Zhixiang Wang, Chi-Wei Hsiao, Po-Fan Yu, Yu-Chih Chen, Yu-Lun Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> National Yang Ming Chiao Tung University, Shanda AI Research Tokyo, MediaTek Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23709" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23709</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://jamichss.github.io/stream-diffvsr-project-page/" target="_blank" rel="noopener noreferrer" class="">https://jamichss.github.io/stream-diffvsr-project-page/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A causally conditioned diffusion framework for online VSR that operates strictly on past frames, enabling low-latency deployment. 2. An Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising to enhance temporal coherence. 3. A lightweight temporal-aware decoder with a Temporal Processor Module (TPM) and a four-step distilled denoiser, achieving fast inference (0.328s per 720p frame) while maintaining high perceptual quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f48d801921015ff458d12a1cb668a1d0e4be149fbb26d7ce6079b45ed8444d0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f48d801921015ff458d12a1cb668a1d0e4be149fbb26d7ce6079b45ed8444d0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the impracticality of diffusion-based video super-resolution (VSR) for low-latency applications by proposing Stream-DiffVSR, an online framework that uses causal conditioning, a distilled denoiser, and novel temporal modules. The method significantly reduces latency to 0.328 seconds per frame while improving perceptual quality, making it the first diffusion VSR approach suitable for real-time online deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AI-Enhanced Virtual Biopsies for Brain Tumor Diagnosis in Low Resource Settings</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [MobileNetV2, radiomics, Grad-CAM, RandomForest, feature fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Areeb Ehsan</p>
</li>
<li class="">
<p><strong>institution:</strong> Georgia State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22184" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22184</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hybrid &quot;virtual biopsy&quot; pipeline combining a lightweight CNN (MobileNetV2) with handcrafted radiomics features for brain tumor classification. 2. Employs a late fusion strategy using a RandomForest classifier on the concatenated CNN embeddings and radiomics features to improve performance. 3. Provides model explainability through Grad-CAM visualizations and radiomics feature importance analysis, and evaluates robustness under low-resolution and noisy imaging conditions relevant to low-resource settings.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b3ff899544d8a10dd2cc79899902ffd4236afc7cedc8e6e24bedb8927407cf2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b3ff899544d8a10dd2cc79899902ffd4236afc7cedc8e6e24bedb8927407cf2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of brain tumor diagnosis in low-resource settings by proposing a virtual biopsy pipeline. The method combines a lightweight CNN with interpretable radiomics features and fuses them using a RandomForest classifier. Experiments show the fusion approach improves classification performance and the analysis highlights its sensitivity to image quality issues common in resource-constrained environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Super-Resolution Enhancement of Medical Images Based on Diffusion Model: An Optimization Scheme for Low-Resolution Gastric Images</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image super-resolution], [diffusion models, SR3, DDPM, capsule endoscopy, HyperKvasir]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haozhe Jia</p>
</li>
<li class="">
<p><strong>institution:</strong> Boston University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22209" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22209</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Applied the SR3 diffusion model framework to the specific domain of capsule endoscopy image super-resolution, addressing hardware-imposed low-resolution constraints. 2. Demonstrated that the diffusion-based approach outperforms traditional interpolation and GAN-based methods (e.g., ESRGAN) in both quantitative metrics (PSNR, SSIM) and qualitative anatomical fidelity. 3. Showed that architectural enhancements like attention mechanisms further improve performance, achieving a PSNR of 29.3 dB and SSIM of 0.71.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70b95a1328af0d9ae7f16ab6cb15a216b2ad914761eed6496beb2ee934202e23_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70b95a1328af0d9ae7f16ab6cb15a216b2ad914761eed6496beb2ee934202e23_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes using a diffusion model (SR3/DDPM) for super-resolution enhancement of low-resolution capsule endoscopy images. The method learns a probabilistic mapping from low-resolution to high-resolution images and is evaluated on the HyperKvasir dataset. Results show it outperforms traditional and GAN-based methods, better preserving critical anatomical details for clinical diagnosis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Field strength-dependent performance variability in deep learning-based analysis of magnetic resonance imaging</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [nnU-Net, MRI field strength, radiomic analysis, UMAP clustering, model generalizability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Muhammad Ibtsaam Qadir, Duane Schonlau, Ulrike Dydak, Fiona R. Kolbinger</p>
</li>
<li class="">
<p><strong>institution:</strong> Purdue University, Indiana University School of Medicine, TUD Dresden University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22176" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22176</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A systematic quantitative evaluation framework to assess the impact of MRI scanner magnetic field strength (1.5T vs. 3.0T) on the performance and generalizability of deep learning segmentation models. 2. Empirical demonstration that training data field strength significantly influences model performance, especially for soft-tissue segmentation tasks, with models trained on 3.0T data often outperforming others. 3. The use of radiomic analysis and UMAP clustering to provide an interpretable, feature-based explanation for the observed performance differences, linking them to field-strength-dependent image characteristics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff8ab0f68e15ce620cdfd03bebfec56b322101c32b9cea5d7a2881045b311bc2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff8ab0f68e15ce620cdfd03bebfec56b322101c32b9cea5d7a2881045b311bc2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study investigates how MRI scanner magnetic field strength affects deep learning-based segmentation models. Using nnU-Net models trained on data from 1.5T, 3.0T, or combined field strengths across three anatomical datasets, the authors found that field strength in training data significantly impacts model performance, particularly for soft tissues. The conclusion is that magnetic field strength should be considered a confounding factor in AI studies for MRI analysis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Complex Swin Transformer for Accelerating Enhanced SMWI Reconstruction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image reconstruction], [Swin Transformer, complex-valued network, k-space undersampling, super-resolution, Parkinson&#x27;s disease]</p>
</li>
<li class="">
<p><strong>authors:</strong> Muhammad Usman, Sung-Min Gho</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University, DeepNoid Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22202" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22202</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a novel complex-valued Swin Transformer network for super-resolution reconstruction of multi-echo MRI data. 2. Demonstrated high-quality SMWI reconstruction from low-resolution/undersampled k-space data, significantly reducing required scan time. 3. Validated the method&#x27;s ability to preserve critical diagnostic features for Parkinson&#x27;s Disease, enhancing clinical applicability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f7b6d093d1496f97cdabeee1991bf51b7ca16e517f4f553ad6598a394e77948_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f7b6d093d1496f97cdabeee1991bf51b7ca16e517f4f553ad6598a394e77948_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the long scan time problem of Susceptibility Map Weighted Imaging (SMWI) for Parkinson&#x27;s disease diagnosis. The authors propose a complex-valued Swin Transformer network to reconstruct high-quality SMWI images from reduced k-space data. Experimental results show the method achieves high reconstruction quality (SSIM 0.9116) while preserving diagnostic details, enabling faster clinical scans.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MEGA-PCC: A Mamba-based Efficient Approach for Joint Geometry and Attribute Point Cloud Compression</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [point cloud compression], [Mamba, end-to-end learning, joint geometry-attribute compression, entropy model, rate-distortion optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kai-Hsiang Hsieh, Monyneath Yim, Wen-Hsiao Peng, Jui-Chiu Chiang</p>
</li>
<li class="">
<p><strong>institution:</strong> National Chung Cheng University, National Yang Ming Chiao Tung University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22463" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22463</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes MEGA-PCC, a fully end-to-end learning-based framework for joint point cloud geometry and attribute compression that eliminates the need for post-hoc recoloring and manual bitrate tuning. 2. Introduces a Mamba-based Entropy Model (MEM) that captures spatial and channel-wise correlations to improve probability estimation for entropy coding. 3. Employs a shared encoder with dual decoders built on the Mamba architecture to model long-range dependencies, enabling data-driven bitrate allocation and superior rate-distortion performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11cdb3cb29e92ab87ea1a3d602cc6e1c5d95edb121070a94d9bb554dc9f450b0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11cdb3cb29e92ab87ea1a3d602cc6e1c5d95edb121070a94d9bb554dc9f450b0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of existing point cloud compression methods, which rely on complex post-processing and manual bitrate allocation. The authors propose MEGA-PCC, an end-to-end framework using a Mamba-based shared encoder and a specialized entropy model for joint geometry and attribute compression. Experiments show the method outperforms traditional and learning-based baselines in both performance and efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Semantic contrastive learning for orthogonal X-ray computed tomography reconstruction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image reconstruction], [semantic contrastive learning, orthogonal CT, U-Net, GAN, sparse-view reconstruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiashu Dong, Jiabing Xiang, Lisheng Geng, Suqing Tian, Wei Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Beihang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22674" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22674</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel semantic feature contrastive learning loss function for CT reconstruction, 2. Introduces a three-stage U-Net-based architecture for coarse reconstruction, detail refinement, and semantic similarity measurement, 3. Demonstrates superior reconstruction quality and faster processing on a chest dataset with orthogonal projections.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/194dd7eaf0bdee678b87a34ec7828010e9d00abcbfbc039eed3a5c81ff6f3f72_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/194dd7eaf0bdee678b87a34ec7828010e9d00abcbfbc039eed3a5c81ff6f3f72_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of streak artifacts in sparse-view X-ray CT reconstruction by proposing a novel semantic contrastive learning loss and a three-stage U-Net architecture. The method improves image quality and processing speed compared to other algorithms, offering a practical solution for orthogonal CT reconstruction.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] JParc: Joint cortical surface parcellation with registration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [cortical parcellation, surface registration, atlas propagation, deep learning, geometric features]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jian Li, Karthik Gopinath, Brian L. Edlow, Adrian V. Dalca, Bruce Fischl</p>
</li>
<li class="">
<p><strong>institution:</strong> Athinoula A. Martinos Center for Biomedical Imaging (MGH &amp; HMS), MIT Computer Science and Artificial Intelligence Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22485" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22485</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes JParc, a novel joint framework that integrates cortical surface registration and parcellation into a single learning-based model. 2. Demonstrates that the performance improvement is primarily due to accurate registration and a learned parcellation atlas, providing an explanation for the success of learning-based methods. 3. Achieves state-of-the-art parcellation accuracy (Dice &gt;90%) on the Mindboggle dataset using only basic geometric features like sulcal depth and curvature.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fdaccb394b5688465ec87019fe53cfc2a72a02e29653a5e203b7187ce7f9515_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fdaccb394b5688465ec87019fe53cfc2a72a02e29653a5e203b7187ce7f9515_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces JParc, a joint framework for cortical surface registration and parcellation. It shows that combining these tasks and learning an atlas leads to superior performance, achieving over 90% Dice score on a standard dataset using simple geometric features. This accuracy can enhance brain mapping studies and clinical applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SwinCCIR: An end-to-end deep network for Compton camera imaging reconstruction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical imaging reconstruction], [Compton camera, Swin Transformer, end-to-end reconstruction, list-mode data, transposed convolution]</p>
</li>
<li class="">
<p><strong>authors:</strong> Minghao Dong, Xinyang Luo, Xujian Ouyang, Yongshun Xiao</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22766" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22766</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SwinCCIR, a novel end-to-end deep learning framework for Compton camera imaging that directly maps list-mode events to source distribution, bypassing traditional back-projection. 2. Introduces the use of Swin Transformer blocks to model the complex relationships in the data, combined with a transposed convolution-based image generation module. 3. Demonstrates the method&#x27;s effectiveness on both simulated and practical datasets, showing it overcomes artifacts and deformations inherent in conventional reconstruction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a15759f59069e025e4de49ef67e3dc4981218b18af8104c55acbdf36fd7cdf0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a15759f59069e025e4de49ef67e3dc4981218b18af8104c55acbdf36fd7cdf0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes SwinCCIR, an end-to-end deep learning model using Swin Transformer blocks and transposed convolutions to directly reconstruct images from Compton camera list-mode data. The method bypasses the problematic back-projection step of traditional approaches. Experiments on simulated and real data show it effectively reduces artifacts and deformations, improving image quality for practical applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Rapid GeoSAM-Based Workflow for Multi-Temporal Glacier Delineation: Case Study from Svalbard</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [remote sensing image segmentation], [GeoSAM, glacier delineation, multi-temporal mapping, Sentinel-2, spectral-index]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alexandru Hegyi</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Oslo</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22855" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22855</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a semi-automatic workflow combining GeoSAM with spectral-index pre-processing for rapid glacier mapping. 2. Demonstrates the method&#x27;s effectiveness for generating temporally consistent glacier outlines in a challenging Arctic environment (Svalbard). 3. Highlights the workflow&#x27;s flexibility and transferability to other optical datasets due to its reliance on derived RGB imagery.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c27fe1438ded3a4be984cfee619922eb0dc9f10dc786702d828f9a22a7355cd4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c27fe1438ded3a4be984cfee619922eb0dc9f10dc786702d828f9a22a7355cd4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This report presents a semi-automatic workflow for rapidly delineating glaciers from satellite imagery. The method uses GeoSAM guided by spectral-index prompts on Sentinel-2 data to create annual glacier outlines. The results show the approach is fast and produces consistent maps for major glaciers, though it requires some user inspection for refinement.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Deep Learning for Art Market Valuation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-modal learning], [multi-modal deep learning, visual embeddings, Grad-CAM, hedonic regression, repeated-sales dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jianping Mei, Michael Moses, Jan Waelty, Yucheng Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> Cheung Kong Graduate School of Business (CKGSB), University of Zurich, Swiss Finance Institute, Art Market Consultancy</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23078" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23078</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces and benchmarks multi-modal deep learning models that fuse tabular data (artist, history) with visual embeddings from artwork images for art market valuation. 2. Demonstrates that visual content provides a distinct and economically significant predictive contribution, especially for fresh-to-market works lacking prior transaction history. 3. Provides interpretability analyses using Grad-CAM and embedding visualizations to show models attend to compositional and stylistic visual cues.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes using multi-modal deep learning to improve art market valuation by incorporating visual content from artwork images alongside traditional tabular data. It finds that while artist identity and history are most predictive overall, visual features provide crucial value for first-time sales where historical data is absent. The results show deep learning offers new insights for valuation, particularly in the most challenging scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] EIR: Enhanced Image Representations for Medical Report Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image captioning], [cross-modal transformer, metadata fusion, domain-specific pre-training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qiang Sun, Zongcheng Ji, Yinlong Xiao, Peng Chang, Jun Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology of China, PAII Inc., Beijing University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23185" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23185</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Enhanced Image Representations (EIR) method for medical report generation. 2. Introduces cross-modal transformers to effectively fuse medical metadata with image features, addressing the information asymmetry problem. 3. Leverages medical domain pre-trained models to encode chest X-ray images, bridging the domain gap between general and medical images.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e359c65576903a65bf75a0600c08943744d6bd91d7b1f2e69d13330e289ce1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e359c65576903a65bf75a0600c08943744d6bd91d7b1f2e69d13330e289ce1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of generating medical reports from chest X-ray images. It proposes the EIR method, which uses cross-modal transformers to fuse metadata with visual features and employs medical domain pre-trained models for better image representation. Experiments on MIMIC and Open-I datasets demonstrate the method&#x27;s effectiveness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-30T13:08:39.000Z" itemprop="dateModified">Dec 30, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/cscv/20251222-20251228"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251222-20251228 (cs.CV)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/category/cscy"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">cs.CY</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-29" class="table-of-contents__link toc-highlight">2025-12-29</a></li><li><a href="#2025-12-30" class="table-of-contents__link toc-highlight">2025-12-30</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/cscv/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>