<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_CV/20251229-20260104" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251229-20260104 (cs.CV) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/cscv/20251229-20260104"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251229-20260104 (cs.CV) | AI头条"><meta data-rh="true" name="description" content="2025-12-29"><meta data-rh="true" property="og:description" content="2025-12-29"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/cscv/20251229-20260104"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cscv/20251229-20260104" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cscv/20251229-20260104" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.CV","item":"https://jokebear666.github.io/ai_toutiao/daily/cscv"},{"@type":"ListItem","position":3,"name":"20251229-20260104 (cs.CV)","item":"https://jokebear666.github.io/ai_toutiao/daily/cscv/20251229-20260104"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.9ae66a68.css">
<script src="/ai_toutiao/assets/js/runtime~main.58d97c7e.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.f7159ad0.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/daily">Daily</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/paper">Paper</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/arxiv-daily"><span title="Arxiv每日论文" class="linkLabel_WmDU">Arxiv每日论文</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Collapse sidebar category &#x27;cs.CV&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cs_CV/20251215-20251221"><span title="20251215-20251221 (cs.CV)" class="linkLabel_WmDU">20251215-20251221 (cs.CV)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cscv/20251222-20251228"><span title="20251222-20251228 (cs.CV)" class="linkLabel_WmDU">20251222-20251228 (cs.CV)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/cscv/20251229-20260104"><span title="20251229-20260104 (cs.CV)" class="linkLabel_WmDU">20251229-20260104 (cs.CV)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/daily/cscv"><span>cs.CV</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251229-20260104 (cs.CV)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251229-20260104 (cs.CV)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-29">2025-12-29<a href="#2025-12-29" class="hash-link" aria-label="Direct link to 2025-12-29" title="Direct link to 2025-12-29" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251229] Understanding Virality: A Rubric based Vision-Language Model Framework for Short-Form Edutainment Evaluation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video understanding], [Vision-Language Models, engagement prediction, multimodal features, YouTube Shorts, regression-based evaluator]</p>
</li>
<li class="">
<p><strong>authors:</strong> Arnav Gupta, Gurekas Singh Sahney, Hardik Rathi, Abhishek Chandwani, Ishaan Gupta, Pratik Narang, Dhruv Kumar</p>
</li>
<li class="">
<p><strong>institution:</strong> Birla Institute of Technology and Science, Pilani; GenimeLabs</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21402" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21402</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A large-scale curated dataset of 11,000 YouTube Shorts videos for edutainment engagement modeling. 2. An unsupervised multimodal framework using VLMs to extract audiovisual features without handcrafted selection. 3. An explainable, regression-based evaluator that predicts engagement with strong correlation and provides feature-level interpretability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bb4f5ae92f21f2e03de0476c0d49f5d4facf563d20f5bbb707cb9ed5960cd88_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bb4f5ae92f21f2e03de0476c0d49f5d4facf563d20f5bbb707cb9ed5960cd88_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of evaluating short-form edutainment videos by moving beyond traditional quality metrics to predict human engagement. The proposed method uses Vision-Language Models to extract unsupervised audiovisual features, clusters them, and trains a regression model to predict engagement. The results show strong correlation with actual engagement, offering a scalable and interpretable evaluation framework.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [tool-use framework, vision-language model, interpretability, data-efficiency, tool bottleneck model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Christina Liu, Alan Q. Wang, Joy Hsu, Jiajun Wu, Ehsan Adeli</p>
</li>
<li class="">
<p><strong>institution:</strong> California Institute of Technology, Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21414" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21414</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/christinaliu2020/tool-bottleneck-framework" target="_blank" rel="noopener noreferrer" class="">https://github.com/christinaliu2020/tool-bottleneck-framework</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed the Tool Bottleneck Framework (TBF) for medical image understanding, which uses a learned Tool Bottleneck Model (TBM) to compose tool outputs instead of text-based composition. 2. Introduced a strategy for the TBM to handle arbitrary VLM tool selections, enabling flexible and robust prediction. 3. Demonstrated that the framework improves performance, interpretability, and data-efficiency, matching or surpassing deep learning classifiers and other tool-use frameworks, especially in data-limited scenarios.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that existing tool-use frameworks for image understanding perform poorly on medical images due to their reliance on text to compose specialized tools. The authors propose the Tool Bottleneck Framework (TBF), which uses a vision-language model to select tools and a learned neural network (the Tool Bottleneck Model) to fuse their outputs. Evaluations on histopathology and dermatology tasks show TBF performs on par with or better than existing methods, offering gains in data-limited settings and more interpretable predictions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Scalable Deep Subspace Clustering Network</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [subspace clustering], [landmark-based approximation, self-expression, spectral clustering, linear complexity, convolutional auto-encoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nairouz Mrabah, Mohamed Bouguessa, Sihem Sami</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Quebec at Montreal</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21434" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21434</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a deep subspace clustering framework (SDSNet) that achieves linear O(n) computational complexity, breaking the traditional O(n^3) bottleneck. 2. Introduces a landmark-based approximation to avoid constructing the full n×n affinity matrix, combined with joint optimization of auto-encoder reconstruction and self-expression objectives. 3. Enables direct spectral clustering on factorized representations, integrating convolutional auto-encoders with subspace-preserving constraints for efficient and accurate clustering.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high computational cost (O(n^3)) of traditional subspace clustering methods. It proposes SDSNet, a scalable deep learning framework that uses landmark approximation and joint optimization to achieve linear O(n) complexity. Experiments show SDSNet maintains clustering quality comparable to state-of-the-art methods while being significantly more efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [multi-annotator segmentation, skin lesion segmentation, dermoscopy, consensus masks, annotator metadata]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kumar Abhishek, Jeremy Kawahara, Ghassan Hamarneh</p>
</li>
<li class="">
<p><strong>institution:</strong> Simon Fraser University, AIP Labs</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21472" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21472</a></p>
</li>
<li class="">
<p><strong>code:</strong> /githubsfu-mial/IMAplusplus</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces IMA++, the largest publicly available multi-annotator skin lesion segmentation dataset with 17,684 masks across 14,967 dermoscopic images. 2. Provides rich annotator metadata (e.g., skill level, tool) enabling research on annotator-specific modeling and metadata analysis. 3. Offers curated data partitions, consensus segmentation masks, and a detailed dataset analysis.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c892cf532aaf3e65ad92c26d2ef6701dc5d629cb5bc4b453893916992cd6089_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c892cf532aaf3e65ad92c26d2ef6701dc5d629cb5bc4b453893916992cd6089_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces IMA++, a large-scale public dataset to address the lack of multi-annotator data for dermoscopic skin lesion segmentation. The dataset contains thousands of images with multiple segmentation masks and annotator metadata, facilitating research into modeling annotator variability. It is presented as the largest publicly available resource of its kind for this medical imaging domain.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [Ground Penetrating Radar (GPR), Multi-modal Chain Feature Fusion (MCFF), Global Attention Mechanism (GAM), DCGAN, transfer learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haotian Lv, Yuhui Zhang, Jiangbo Dai, Hanli Wu, Jiaji Wang, Dawei Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Harbin Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21452" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21452</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a DCGAN-based data augmentation strategy to synthesize high-fidelity GPR images, mitigating data scarcity. 2. Designed a novel Multi-modal Chain and Global Attention Network (MCGA-Net) integrating Multi-modal Chain Feature Fusion (MCFF) and a Global Attention Mechanism (GAM) for enhanced defect representation. 3. Utilized MS COCO transfer learning to fine-tune the backbone network, accelerating convergence and improving model generalization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4debe9b33028e70ed06ab5d1f340e5cb76dcb8d09f7adf0d8195a2422c90668_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4debe9b33028e70ed06ab5d1f340e5cb76dcb8d09f7adf0d8195a2422c90668_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the subjective and inefficient interpretation of Ground Penetrating Radar (GPR) images for road defect detection by proposing a comprehensive framework. The method combines DCGAN-based data augmentation, a novel MCGA-Net architecture with feature fusion and attention mechanisms, and transfer learning. The proposed model achieves high precision, recall, and robustness, establishing a new paradigm for automated GPR-based defect detection.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] CCAD: Compressed Global Feature Conditioned Anomaly Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [anomaly detection], [global feature conditioning, adaptive compression, reconstruction-based anomaly detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiao Jin, Liang Diao, Qixin Xiao, Yifan Hu, Ziqi Zhang, Yuchen Liu, Haisong Gu</p>
</li>
<li class="">
<p><strong>institution:</strong> Columbia University, University of Michigan, University of California at Berkeley, Stevens Institute of Technology, VisionX LLC</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21459" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21459</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/chloeqxq/CCAD" target="_blank" rel="noopener noreferrer" class="">https://github.com/chloeqxq/CCAD</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CCAD, a novel method that synergizes reconstruction-based and representation-based anomaly detection by using compressed global features as a conditioning modality for the reconstruction model. 2. Introduces an adaptive compression mechanism to enhance model generalization and training efficiency. 3. Contributes a reorganized and re-annotated version of the DAGM 2007 dataset to validate the method&#x27;s effectiveness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of anomaly detection in industrial settings with limited anomalous data by proposing CCAD, a method that combines the strengths of reconstruction-based and representation-based approaches. CCAD conditions a reconstruction model on adaptively compressed global features, leading to improved generalization and training efficiency. Experiments show that CCAD outperforms state-of-the-art methods in AUC and achieves faster convergence.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image retrieval], [polyp re-identification, gated progressive fusion, multimodal feature fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Suncheng Xiang, Xiaoyang Wang, Junjie Jiang, Hejia Wang, Dahong Qian</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, Peking University, Shanghai Fifth People&#x27;s Hospital</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21476" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21476</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/JeremyXSC/GPF-Net" target="_blank" rel="noopener noreferrer" class="">https://github.com/JeremyXSC/GPF-Net</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1) Proposes a novel multimodal feature fusion framework named GPF-Net for polyp re-identification. 2) Introduces a gated progressive fusion strategy for layer-wise refinement of semantic information through multi-level feature interactions. 3) Demonstrates state-of-the-art performance on standard benchmarks, showing the benefit of multimodal fusion over unimodal methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75fe09ad1f753b61f2c30ab37c16d0deef5060ca95658846bc6dcaf6bb4c53f9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75fe09ad1f753b61f2c30ab37c16d0deef5060ca95658846bc6dcaf6bb4c53f9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of colonoscopic polyp re-identification, where coarse high-level features harm small object matching. The authors propose GPF-Net, a Gated Progressive Fusion network that selectively fuses multi-level features using gates. Experiments show this multimodal approach outperforms state-of-the-art unimodal ReID models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Generative Multi-Focus Image Fusion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image fusion], [multi-focus image fusion, latent diffusion models, generative restoration, StackMFF, IFControlNet]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xinzhe Xie, Buyu Guo, Bolin Li, Shuangyan He, Yanzhen Gu, Qingyan Jiang, Peiliang Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Donghai Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21495" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21495</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Xinzhe99/StackMFF-Series" target="_blank" rel="noopener noreferrer" class="">https://github.com/Xinzhe99/StackMFF-Series</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel two-stage generative framework (GMFF) for multi-focus image fusion, combining deterministic fusion with generative restoration. 2. Introduces a generative restoration stage using IFControlNet, a latent diffusion model, to reconstruct missing content and eliminate edge artifacts. 3. Demonstrates state-of-the-art performance, particularly in handling complex scenarios where not all scene areas are in focus in any input image.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32d197875e157fe73427be3804a8f00aec57f7112b871c69141003d4a9a92477_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32d197875e157fe73427be3804a8f00aec57f7112b871c69141003d4a9a92477_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes GMFF, a two-stage framework for multi-focus image fusion. It first uses StackMFF V4 for deterministic fusion and then employs a latent diffusion model (IFControlNet) for generative restoration to recover missing details and remove artifacts. Experiments show GMFF achieves state-of-the-art performance, especially for complex multi-focal content.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-view clustering], [incomplete multi-view clustering, missing pattern tree, decision ensemble, knowledge distillation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenyuan Yang, Jie Xu, Hongqing He, Jiangzhang Gan, Xiaofeng Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China, Hainan University, Singapore University of Technology and Design, Guangxi Normal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21510" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21510</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a missing-pattern tree model to group data into decision sets for fully utilizing available multi-view pairs, addressing the pair under-utilization issue. 2. Introduces a multi-view decision ensemble module that uses uncertainty-based weighting to aggregate robust clustering decisions from different sets. 3. Designs an ensemble-to-individual knowledge distillation module to transfer ensemble knowledge to view-specific models, promoting mutual optimization between ensemble and individual modules.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of incomplete multi-view clustering (IMVC) where inconsistent missing patterns hinder performance. It proposes a framework called TreeEIC that groups data by missing pattern, performs clustering within each group, ensembles the results with uncertainty weighting, and uses knowledge distillation to refine individual models. Experiments show TreeEIC achieves state-of-the-art performance and robustness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image forensics], [fixed-threshold evaluation, CNN-ViT hybrid, gated fusion, frequency-domain features, cross-domain detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Ashik Khan, Arafat Alam Jion</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology Kharagpur, Chittagong University of Engineering and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21512" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21512</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a fixed-threshold evaluation protocol for AI-generated image detection to provide deployment-relevant robustness estimates by preventing per-condition threshold retuning. 2. Proposed a lightweight CNN-ViT hybrid model with gated fusion and optional frequency enhancement for cross-domain detection across photos and art. 3. Conducted a systematic analysis revealing a forensic-semantic spectrum, showing CNNs are fragile to compression while ViTs are robust, and that semantic patterns in artistic content are more reliable detection cues.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca89224e09c82fcbaf5bf9db1a5d9fb5df38a1947dd8ae3ddcb0e9c385e126aa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca89224e09c82fcbaf5bf9db1a5d9fb5df38a1947dd8ae3ddcb0e9c385e126aa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of misleading robustness estimates in AI-generated image detection by proposing a fixed-threshold evaluation protocol. The authors develop a hybrid CNN-ViT model with gated fusion and evaluate it across photos and art, finding that ViTs are more robust to post-processing like compression due to semantic pattern recognition, and that detection is fundamentally easier on artistic content than photorealistic images.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] SVBench: Evaluation of Video Generation Models on Social Reasoning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [social reasoning, benchmark, agent-based pipeline, VLM judge, multi-agent interaction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenshuo Peng, Gongxuan Wang, Tianmeng Yang, Chuanhao Li, Xiaojie Xu, Hui He, Kaipeng Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Shanghai AI Laboratory, Peking University, Harbin Institute of Technology, The Hong Kong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21507" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21507</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Gloria2tt/SVBench-Evaluation" target="_blank" rel="noopener noreferrer" class="">https://github.com/Gloria2tt/SVBench-Evaluation</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the first benchmark (SVBench) for evaluating social reasoning in video generation, grounded in developmental and social psychology. 2. Develops a fully training-free, agent-based pipeline to synthesize and evaluate diverse social reasoning scenarios. 3. Conducts a large-scale evaluation of seven state-of-the-art video generation models, revealing their systematic failures in social cognition.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415eaf890354c8803ae31303b38b5679ebed73bd981f8860ef00ba636922568d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415eaf890354c8803ae31303b38b5679ebed73bd981f8860ef00ba636922568d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces SVBench, the first benchmark for evaluating social reasoning in video generation models. It uses an agent-based pipeline to create and assess videos based on psychological paradigms, finding that while current models are visually realistic, they fail at understanding intentions, beliefs, and social norms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [parameter-efficient training, frozen encoders, adapters, LoRA, BitFit]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Ashik Khan, Md Nahid Siddique</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology Kharagpur, Florida International University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21508" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21508</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrated that parameter-efficient training (PET) methods with frozen encoders achieve superior performance (AUROC 0.892-0.908) compared to full fine-tuning (AUROC 0.770) for multimodal chest X-Ray classification using only 2.51% of trainable parameters. 2. Conducted controlled, budget-matched experiments revealing that performance gains stem primarily from efficient parameter allocation rather than cross-modal synergy, as vision-only models outperformed multimodal models under the same parameter budget. 3. Identified and quantified a tractable limitation of PET methods—degraded model calibration (ECE: 0.29-0.34)—and suggested post-hoc calibration as a solution for clinical deployment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3511bf13e05579029e8458f9b63afdf7f5fb3ed3a02050a524e53a4fcb570084_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3511bf13e05579029e8458f9b63afdf7f5fb3ed3a02050a524e53a4fcb570084_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates parameter-efficient training (PET) strategies for multimodal chest X-Ray classification to reduce computational costs. By freezing pre-trained encoders and training only small modules like adapters or LoRA under a fixed parameter budget, the methods significantly outperform full fine-tuning. The main conclusion is that frozen encoder PET provides superior discrimination at a fraction of the cost, though calibration correction is needed for clinical use.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [diffusion models], [GRPO, mode collapse, diversity-aware reward, spectral clustering, structure-aware regularization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Henglin Liu, Huijuan Huang, Jing Wang, Chang Liu, Xiu Li, Xiangyang Ji</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Kuaishou Technology (Kling Team), Sun Yat-sen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21514" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21514</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and analyzes the mode collapse problem in GRPO-based image generation from both reward modeling and generation dynamics perspectives. 2. Proposes a distributional creativity bonus reward based on semantic grouping via spectral clustering to encourage novel visual modes. 3. Introduces a structure-aware regularization that applies stronger constraints during early-stage denoising to preserve diversity without sacrificing quality optimization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862e58c2beed3d5383235af01560a2dc06bc384250083e4027ddff5a3aa32368_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862e58c2beed3d5383235af01560a2dc06bc384250083e4027ddff5a3aa32368_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the mode collapse problem in GRPO-based image generation, where models produce homogenized outputs. The proposed DiverseGRPO method introduces a diversity-aware reward based on semantic clustering and a structure-aware regularization to preserve generation diversity. Experiments show the method significantly improves semantic diversity while maintaining image quality, establishing a better quality-diversity trade-off.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] MuS-Polar3D: A Benchmark Dataset for Computational Polarimetric 3D Imaging under Multi-Scattering Conditions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D reconstruction], [polarization imaging, computational imaging, underwater imaging, benchmark dataset, multi-scattering]</p>
</li>
<li class="">
<p><strong>authors:</strong> Puyun Wang, Kaimin Yu, Huayang He, Xianyu Wu</p>
</li>
<li class="">
<p><strong>institution:</strong> Fuzhou University, Research Institute of Highway, Ministry of Transport</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21513" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21513</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/WangPuyun/MuS-Polar3D" target="_blank" rel="noopener noreferrer" class="">https://github.com/WangPuyun/MuS-Polar3D</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MuS-Polar3D, the first publicly available benchmark dataset for quantitative turbidity underwater polarization-based 3D imaging, featuring 42 objects captured under seven controlled scattering conditions and five viewpoints. 2. Proposes a two-stage computational imaging pipeline that decouples underwater 3D reconstruction into descattering followed by 3D reconstruction. 3. Provides extensive evaluations using multiple baseline methods, demonstrating the dataset&#x27;s effectiveness for fair algorithm comparison under complex scattering conditions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c66fd636f62e6b85894934c19824730a7d5f2125cecd8c2495b556ef0c4c42_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c66fd636f62e6b85894934c19824730a7d5f2125cecd8c2495b556ef0c4c42_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of diverse public datasets for polarization-based underwater 3D imaging by introducing MuS-Polar3D, a benchmark dataset captured under controlled multi-scattering conditions. The authors also propose a two-stage computational imaging pipeline (descattering then 3D reconstruction) for this task. The dataset enables accurate reconstruction and fair evaluation, with experiments achieving a best mean angular error of 15.49 degrees.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-view clustering], [contrastive learning, incomplete multi-view data, noise-robust clustering, graph-guided learning, imputation-free]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hongqing He, Jie Xu, Wenyuan Yang, Yonghua Zhu, Guoqiu Wen, Xiaofeng Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> Guangxi Normal University, University of Electronic Science and Technology of China, Singapore University of Technology and Design, Hainan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21516" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21516</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a global-graph guided contrastive learning strategy to address the rare-paired sample issue in incomplete multi-view data by constructing a global affinity graph to form new sample pairs. 2. Introduces a local-graph weighted contrastive learning mechanism to mitigate the mis-paired sample issue caused by noise, using local neighbors to generate adaptive weights for pair-wise contrast. 3. Integrates these strategies into a unified, imputation-free framework for effective clustering on both incomplete and noisy multi-view data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of rare-paired and mis-paired samples in contrastive learning for multi-view clustering on incomplete and noisy data. It proposes a unified framework combining global-graph guided contrastive learning to explore complementary information and local-graph weighted contrastive learning to adaptively handle noisy pairs. Experiments show the method outperforms state-of-the-art approaches on both incomplete and noisy data settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Hierarchy-Aware Fine-Tuning of Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal learning], [hierarchical classification, vision-language models, efficient fine-tuning, LoRA, Tree-Path KL Divergence]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiayu Li, Rajesh Gangireddy, Samet Akcay, Wei Cheng, Juhua Hu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Washington, Intel</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21529" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21529</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an efficient hierarchy-aware fine-tuning framework for Vision-Language Models (VLMs) that updates only a few parameters. 2. Introduces two novel loss functions: Tree-Path KL Divergence (TP-KL) for vertical consistency along label paths and Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) for horizontal consistency among sibling classes. 3. Demonstrates consistent improvements in Full-Path Accuracy and reduced Tree-based Inconsistency Error across multiple hierarchical benchmarks with minimal parameter overhead.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b60d2fc3dd0ad92b5ac8857f844fed2dbe51e280dfb702c26028d91c14fd92_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b60d2fc3dd0ad92b5ac8857f844fed2dbe51e280dfb702c26028d91c14fd92_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of adapting large Vision-Language Models (VLMs) to hierarchical classification tasks efficiently. The proposed method combines two novel hierarchy-aware loss functions (TP-KL and HiSCE) with lightweight LoRA adaptation to enforce structural consistency in predictions. Experiments show the approach improves accuracy and reduces inconsistency across taxonomy levels with minimal computational cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Vision Transformers are Circulant Attention Learners</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [vision transformers], [circulant attention, block circulant matrix with circulant blocks (BCCB), computational complexity, vision transformers, fast Fourier transform (FFT)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dongchen Han, Tianyu Li, Ziyi Wang, Gao Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21542" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21542</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/LeapLabTHU/Circulant-Attention" target="_blank" rel="noopener noreferrer" class="">https://github.com/LeapLabTHU/Circulant-Attention</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies that self-attention matrices in vision Transformers often approximate Block Circulant matrices with Circulant Blocks (BCCB). 2. Proposes a novel Circulant Attention paradigm that explicitly models the attention map as its nearest BCCB matrix. 3. Introduces an efficient computation algorithm using 2D Discrete Fourier Transform (DFT) to achieve O(N log N) complexity while largely preserving the modeling capacity of standard self-attention.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14a3b6c919de65b8d25da885c52023b90262e94f46956f6aed612d5e62b5f19b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14a3b6c919de65b8d25da885c52023b90262e94f46956f6aed612d5e62b5f19b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the quadratic computational complexity of self-attention in vision Transformers by proposing Circulant Attention. The method models the attention matrix as a Block Circulant matrix with Circulant Blocks (BCCB) and leverages Fast Fourier Transform for efficient O(N log N) computation. Experiments show it effectively reduces computation cost while maintaining model performance, offering a promising alternative to standard self-attention.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Toward Intelligent Scene Augmentation for Context-Aware Object Placement and Sponsor-Logo Integration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image editing], [context-aware object insertion, sponsor-product logo augmentation, vision-language models, diffusion models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Unnati Saraswat, Tarun Rao, Namah Gupta, Shweta Swami, Shikhar Sharma, Prateek Narang, Dhruv Kumar</p>
</li>
<li class="">
<p><strong>institution:</strong> Birla Institute of Technology and Science, Pilani</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21560" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21560</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces two new tasks for advertising: context-aware object insertion and sponsor-product logo augmentation. 2. Builds two new datasets with annotations for category, placement, and sponsor-product labels to support these tasks. 3. Identifies and addresses a research gap in jointly performing category reasoning, placement prediction, object generation, and seamless compositing for scene augmentation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0959c405e8f65b70f4fa16189d8d2817513788b4aa2715490a9136daff2672b7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0959c405e8f65b70f4fa16189d8d2817513788b4aa2715490a9136daff2672b7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces two new tasks for intelligent image editing in advertising: context-aware object insertion and sponsor-product logo augmentation. It proposes to address these tasks using vision-language models and diffusion models, and builds new datasets to support them. The main conclusion is that a significant research gap exists in systems that can jointly reason about context, placement, generation, and compositing for realistic scene augmentation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image inpainting], [object removal, dataset-free, test-time adaptation, multimodal large-language model, background-aware reasoning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sanghyun Jo, Donghwan Lee, Eunji Jung, Seong Je Oh, Kyungsu Kim</p>
</li>
<li class="">
<p><strong>institution:</strong> Seoul National University, OGQ</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21545" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21545</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel dataset-free framework, EraseLoRA, that replaces attention manipulation with background-aware reasoning and test-time adaptation for object removal. 2. Introduces Background-aware Foreground Exclusion (BFE), which uses an MLLM to separate target foreground, non-target foregrounds, and clean background from a single image-mask pair without supervision. 3. Introduces Background-aware Reconstruction with Subtype Aggregation (BRSA), a test-time optimization that treats background subtypes as complementary pieces and enforces their consistent integration through reconstruction and alignment objectives.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f86516075c3f56dc02ca42051c7afa7154583c986a395447b42dd8dc4bf354_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f86516075c3f56dc02ca42051c7afa7154583c986a395447b42dd8dc4bf354_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of object removal in inpainting, where existing dataset-free methods often regenerate unwanted objects or disrupt details. It proposes EraseLoRA, a framework that uses an MLLM to separate image components and performs test-time optimization to reconstruct the background coherently. The method shows consistent improvements over dataset-free baselines and competitive results against dataset-driven approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Exploration of Reproducible Generated Image Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image forensics], [AIGC detection, reproducibility, generalizability, diffusion models, binary classification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yihang Duan</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in the provided content. (Author name only, no affiliation or email domain provided)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21562" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21562</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and analyzes the root causes of poor reproducibility in AIGC image detection research, citing omitted experimental details and overfitting to generator-specific features. 2. Provides empirical evidence for the reproducibility issue by constructing a test dataset and reproducing a representative detection method, demonstrating performance drops under cross-generator testing. 3. Proposes reference directions for the research community to improve reproducibility and generalizability, such as more comprehensive disclosure of experimental details.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c9df3d6873df2ea90e378adf52625583637b76319eb9a55ebf11b8f17abf1fc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c9df3d6873df2ea90e378adf52625583637b76319eb9a55ebf11b8f17abf1fc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the reproducibility and generalizability challenges in AI-Generated Content (AIGC) image detection. By reviewing key literature, building a test dataset, and reproducing a detection method, it identifies causes like omitted experimental details and model overfitting. The study concludes that while basic performance can be reproduced, detection fails when preprocessing disrupts key features or when testing across different generators, highlighting the need for better methodological disclosure and robustness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Towards Long-window Anchoring in Vision-Language Model Distillation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [knowledge distillation, long-context, rotary position embeddings (RoPE), attention mechanism, vision-language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haoyi Zhou, Shuo Li, Tianyu Chen, Qi Song, Chonghan Gao, Jianxin Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Beihang University, Zhongguancun Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21576" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21576</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies the problem of limited effective context windows in small, distilled vision-language models despite using identical positional embeddings and architectures as their larger counterparts. 2. Proposes LAid, a novel distillation method featuring progressive distance-weighted attention matching and learnable RoPE response gain modulation to transfer long-range attention mechanisms. 3. Demonstrates that LAid-distilled models achieve significantly longer effective context windows (up to 3.2x) while maintaining performance on standard benchmarks, and provides spectral analysis showing successful transfer of low-frequency attention components.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4663870a97f8c352e6cd352d0f9f9be365648a5545642796d256fa99c7ddcd4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4663870a97f8c352e6cd352d0f9f9be365648a5545642796d256fa99c7ddcd4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that small, distilled vision-language models have much shorter effective context windows than their large teacher models. The authors propose LAid, a new distillation method that transfers long-range attention capabilities via progressive attention matching and learnable RoPE modulation. Their method successfully extends the context window of small models by up to 3.2 times while preserving performance on standard benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] LLM-Free Image Captioning Evaluation in Reference-Flexible Settings</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image captioning evaluation], [LLM-free evaluation, reference-flexible, supervised metric, image-caption similarity, human-annotated dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shinnosuke Hirano, Yuiga Wada, Kazuki Matsuda, Seitaro Otsuki, Komei Sugiura</p>
</li>
<li class="">
<p><strong>institution:</strong> Keio University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21582" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21582</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Pearl, a novel LLM-free supervised metric for image captioning that works in both reference-based and reference-free settings. 2. Introduces a novel mechanism to learn representations of image-caption and caption-caption similarities. 3. Constructs a large-scale human-annotated dataset for metric evaluation, comprising ~333k judgments from over 2k annotators across 75k+ images.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad4cf942f6b913d144878df5c7f41a2b4ea396044f5286c9aeed1071cfa693a1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad4cf942f6b913d144878df5c7f41a2b4ea396044f5286c9aeed1071cfa693a1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the issues of bias in LLM-based and the limited performance of existing LLM-free metrics for evaluating image captions. It proposes Pearl, a fast, LLM-free supervised metric that learns joint image-caption and caption-caption similarity representations and is applicable to both reference-based and reference-free settings. Pearl outperforms other LLM-free metrics on multiple benchmarks, demonstrating higher alignment with human judgment without the neutrality and speed concerns of LLM-based approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] UltraLBM-UNet: Ultralight Bidirectional Mamba-based Model for Skin Lesion Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [Mamba, state-space model, knowledge distillation, lightweight model, U-Net]</p>
</li>
<li class="">
<p><strong>authors:</strong> Linxuan Fan, Juntao Jiang, Weixuan Liu, Zhucun Xue, Jiajun Lv, Jiangning Zhang, Yong Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in the provided content. Could be inferred from author names but no affiliations/domains are given.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21584" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21584</a></p>
</li>
<li class="">
<p><strong>code:</strong> this https URL</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a lightweight Global–Local Multi-branch Perception Module integrating global context and local features. 2. Demonstrates a bidirectional Mamba with shared weights for enhanced information flow without extra parameters. 3. Introduces a hybrid knowledge distillation strategy to create an ultra-compact, high-performance variant.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089b376dfef7caeeae8830bbdef8bf95a5c6ff909a643481b728550d7ff3d937_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089b376dfef7caeeae8830bbdef8bf95a5c6ff909a643481b728550d7ff3d937_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes UltraLBM-UNet, a lightweight model for skin lesion segmentation that combines a bidirectional Mamba for global modeling with multi-branch local feature perception. It achieves state-of-the-art accuracy on benchmark datasets with minimal parameters and computational cost, and a distilled variant further compresses the model, making it suitable for point-of-care deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal learning], [harmful meme detection, large multimodal model, agent self-improvement, label-free adaptation, contrastive learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jian Lang, Rongpei Hong, Ting Zhong, Leiting Chen, Qiang Gao, Fan Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China, Southwestern University of Finance and Economics</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21598" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21598</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ALARM, the first label-free harmful meme detection framework using LMM agent self-improvement. 2. Introduces a Confidence-based Explicit Meme Identification mechanism to isolate and pseudo-label explicit memes. 3. Introduces a Pairwise Learning Guided Agent Self-Improvement paradigm that uses contrastive pairs to refine the agent&#x27;s ability to handle complex memes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c54d55d69468e93b3a2275f2a3f89c24499aa76dbe87423ea8069fb10f1df9f1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c54d55d69468e93b3a2275f2a3f89c24499aa76dbe87423ea8069fb10f1df9f1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes ALARM, a label-free framework for detecting harmful memes by using a Large Multimodal Model agent that self-improves by learning from easy, explicit examples to handle complex, subtle ones. The method outperforms label-driven approaches on three datasets, demonstrating strong adaptability to evolving online content.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] GaussianEM: Model compositional and conformational heterogeneity using 3D Gaussians</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [cryo-EM image analysis], [3D Gaussians, conformational heterogeneity, two-encoder-one-decoder, pseudo-atomic model, cryo-EM]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bintao He, Yiran Cheng, Hongjia Li, Xiang Gao, Xin Gao, Fa Zhang, Renmin Han</p>
</li>
<li class="">
<p><strong>institution:</strong> Shandong University, Ningxia Medical University, Beijing Institute of Technology, King Abdullah University of Science and Technology (KAUST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21599" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21599</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GaussianEM, a novel Gaussian pseudo-atomic framework for modeling compositional and conformational heterogeneity from cryo-EM images. 2. Introduces a two-encoder-one-decoder architecture to map images to individual Gaussian components and represent variability through Gaussian parameter changes. 3. Bridges the gap between density-based models and atomic models, providing an intuitive and interpretable description of conformational changes while preserving local structural consistency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b95f9f3fcbc3c6429b2ee462e30233f1b388c03440a44273a62612612e5e7244_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b95f9f3fcbc3c6429b2ee462e30233f1b388c03440a44273a62612612e5e7244_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces GaussianEM, a method that uses 3D Gaussians to model both compositional and conformational heterogeneity in proteins from cryo-EM images. It employs a two-encoder-one-decoder architecture to map images to Gaussian components, representing structural changes through parameter variations. The method is shown to be effective on both simulated and experimental datasets, offering an interpretable bridge between density maps and atomic models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data in Emergency and Critical Care</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [imbalanced classification], [XGBoost, TabNet, TabResNet, Bayesian hyperparameter search, class imbalance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yusuf Brima, Marcellin Atemkeng</p>
</li>
<li class="">
<p><strong>institution:</strong> Osnabrück University, Rhodes University, National Institute for Theoretical and Computational Sciences (NITheCS)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21602" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21602</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Systematic evaluation of robustness and scalability for classical ML and deep learning models on imbalanced clinical tabular data. 2. Introduction of TabResNet, a custom lightweight residual network designed as a computationally efficient alternative to TabNet for real-time clinical use. 3. Evidence-based finding that tree-based ensemble methods (especially XGBoost) offer the most stable and scalable performance for high-stakes, time-sensitive clinical environments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper evaluates the robustness and scalability of machine learning models on imbalanced clinical data from emergency and critical care settings. It proposes TabResNet, a lightweight deep learning alternative to TabNet, and compares it against tree-based methods like XGBoost. The main conclusion is that tree-based ensemble models, particularly XGBoost, provide the most stable performance and computational scalability for practical clinical deployment, outperforming more complex deep learning architectures in these environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] TAMEing Long Contexts in Personalization: Towards Training-Free and State-Aware MLLM Personalized Assistant</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [MLLM personalization, long-context, training-free, state-aware, retrieval-augmented generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rongpei Hong, Jian Lang, Ting Zhong, Yong Wang, Fan Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China, Aiwen Technology Co., Ltd.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21616" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21616</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LCMP, the first Long-Context MLLM Personalization benchmark for evaluating personalized dialogues. 2. Introduces TAME, a novel training-free and state-aware framework using double memories to manage concept variations. 3. Presents the RA2G (Retrieve-then-Align Augmented Generation) paradigm to align retrieved knowledge with current queries for better interaction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0916d89499cb78545669c049322f7cc673c2b69f516444a870dd8869bb13521_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0916d89499cb78545669c049322f7cc673c2b69f516444a870dd8869bb13521_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of existing MLLM personalization methods in handling long-context dialogues. It proposes a training-free, state-aware framework called TAME, which uses double memories and a novel retrieval-augmentation paradigm (RA2G) to manage personalized concepts over time. Experiments on the new LCMP benchmark show TAME achieves the best performance, enabling evolving interactions in long-context scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [novel view synthesis], [diffusion models, 3D Gaussian Splatting, auto-regressive restoration, context-aware inpainting, view synthesis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhiyuan Liu, Daocheng Fu, Pinlong Cai, Lening Wang, Ying Liu, Yilong Ren, Botian Shi, Jianqiang Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Shanghai Artificial Intelligence Laboratory, Beihang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21618" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21618</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SymDrive, a unified diffusion-based framework for joint high-quality rendering and scene editing in autonomous driving simulation. 2. Introduces a Symmetric Auto-regressive Online Restoration paradigm for recovering fine-grained details and generating consistent lateral views. 3. Leverages the restoration capability for a training-free harmonization mechanism, treating vehicle insertion as context-aware inpainting to ensure lighting and shadow consistency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> SymDrive addresses the challenge of creating high-fidelity and controllable 3D driving simulators by proposing a unified diffusion-based framework. It uses a symmetric auto-regressive online restoration method to enhance novel-view synthesis and a training-free harmonization mechanism for realistic vehicle insertion. The method achieves state-of-the-art performance in both rendering quality and scene editing realism.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] CausalFSFG: Rethinking Few-Shot Fine-Grained Visual Categorization from Causal Perspective</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [fine-grained visual categorization], [causal intervention, structural causal model, few-shot learning, interventional multi-scale encoder, interventional masked feature reconstruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhiwen Yang, Jinglin Xu, Yuxin Pen</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University, University of Science and Technology Beijing</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21617" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21617</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/PKU-ICST-MIPL/CausalFSFG_TMM" target="_blank" rel="noopener noreferrer" class="">https://github.com/PKU-ICST-MIPL/CausalFSFG_TMM</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a causal perspective to address the confounding effect of support samples in FS-FGVC, framing the task as a causal inference problem. 2. Introduces a novel CausalFSFG method with two key components: an Interventional Multi-Scale Encoder (IMSE) for sample-level intervention and an Interventional Masked Feature Reconstruction (IMFR) module for feature-level intervention. 3. Demonstrates state-of-the-art performance on standard FS-FGVC datasets (CUB-200-2011, Stanford Dogs, Stanford Cars) through extensive experiments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdf9390290fc2d16f99be55ee74ab9d2d794c0c3343cf9ea92ae99e679f1d456_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdf9390290fc2d16f99be55ee74ab9d2d794c0c3343cf9ea92ae99e679f1d456_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies that the support sample set acts as a confounding variable in Few-Shot Fine-Grained Visual Categorization (FS-FGVC), introducing bias and spurious correlations. To address this, the authors propose CausalFSFG, a method that uses causal intervention via a sample-level and a feature-level module to reveal true causal relationships. The approach achieves new state-of-the-art results on multiple benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image editing], [StyleGAN2, CLIP, L1 regularization, latent space manipulation, attribute disentanglement]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mutiara Shabrina, Nova Kurnia Putri, Jefri Satria Ferdiansyah, Sabita Khansa Dewi, Novanto Yudistira</p>
</li>
<li class="">
<p><strong>institution:</strong> Universitas Brawijaya</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21637" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21637</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An empirical analysis identifying that L2-based regularization in the PPE framework leads to dense latent updates and attribute leakage. 2. The introduction of a sparsity-based constraint using L1 regularization (referred to as an ultra-strict layer masking strategy) for latent space manipulation. 3. Demonstration that the proposed approach enforces more focused edits, reducing unintended changes and better preserving facial identity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d3d2b6fa78e88b5a6e668468f53e16660057860c14bd7c81ab11e3e9b86d455_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d3d2b6fa78e88b5a6e668468f53e16660057860c14bd7c81ab11e3e9b86d455_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of attribute entanglement in text-guided image editing. It analyzes the PPE framework and proposes a new method using L1 regularization to enforce sparsity in latent space updates. The results show this approach achieves more controlled edits with less unintended semantic leakage.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal weather modeling], [multimodal foundation model, weather generation, weather understanding, Chain-of-Thought, self-attention]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhiwang Zhou, Yuandong Pu, Xuming He, Yidi Liu, Yixin Chen, Junchao Gong, Xiang Zhuang, Wanghan Xu, Qinglong Cao, Shixiang Tang, Yihao Liu, Wenlong Zhang, Lei Bai</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai AI Laboratory, Tongji University, Shanghai Jiao Tong University, Zhejiang University, University of Science and Technology of China, UCLA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21643" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21643</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Zhouzone/OmniWeather" target="_blank" rel="noopener noreferrer" class="">https://github.com/Zhouzone/OmniWeather</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Omni-Weather, the first unified multimodal foundation model that integrates weather generation and understanding in a single architecture. 2. Introduces a Chain-of-Thought dataset for causal reasoning in weather generation to improve interpretability and perceptual quality. 3. Demonstrates through experiments that generative and understanding tasks in weather can mutually enhance each other, achieving state-of-the-art performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df6816c446631f92d0b9ba66f4d2f4ddda08200edc161b65555c257b6a7b7a85_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df6816c446631f92d0b9ba66f4d2f4ddda08200edc161b65555c257b6a7b7a85_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the gap between weather prediction and interpretation by proposing Omni-Weather, a unified multimodal foundation model that combines generation and understanding using a shared self-attention mechanism and a novel Chain-of-Thought dataset. The model achieves state-of-the-art results in both tasks, showing that they can mutually benefit each other.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D object grounding], [temporal multimodal grounding, LiDAR-image fusion, language-conditioned decoding, UniScene representation, NuPrompt benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Fudan University, Huawei Technologies Ltd.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21641" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21641</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes TrackTeller, a unified temporal multimodal framework for 3D grounding that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning. 2. Introduces a shared UniScene representation aligned with textual semantics to generate language-aware 3D proposals. 3. Demonstrates significant performance improvements on the NuPrompt benchmark, including a 70% relative gain in Average Multi-Object Tracking Accuracy and a 3.15-3.4x reduction in False Alarm Frequency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc60054c622874cc83217afc715702b65eb56126f722620ffb7004f46ebe296d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc60054c622874cc83217afc715702b65eb56126f722620ffb7004f46ebe296d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of grounding natural language references to objects in dynamic 3D driving scenes, which often depend on recent motion or behavior. The authors propose TrackTeller, a framework that fuses LiDAR and camera data with language, builds a unified scene representation, and uses temporal reasoning to refine object identification. Experiments show that TrackTeller significantly outperforms existing baselines in language-grounded tracking accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [deepfake detection], [mechanistic interpretability, sparse autoencoder, forensic manifold analysis, feature selectivity, vision-language model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Subramanyam Sahoo, Jared Junkin</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Berkeley, Johns Hopkins University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21670" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21670</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/SubramanyamSahoo/The-Deepfake-Detective" target="_blank" rel="noopener noreferrer" class="">https://github.com/SubramanyamSahoo/The-Deepfake-Detective</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a novel mechanistic interpretability framework specifically for deepfake detection, combining sparse autoencoder analysis with forensic manifold analysis. 2. Demonstrates that only a small fraction of latent features are actively used in each layer of the model for detection. 3. Shows that geometric properties of the model&#x27;s feature manifold (e.g., intrinsic dimensionality, curvature) vary systematically with different types of deepfake artifacts, linking learned features to specific forensic cues.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the &quot;black box&quot; nature of deepfake detectors by proposing a mechanistic interpretability framework that analyzes a vision-language model&#x27;s internal representations. The method uses sparse autoencoders and a novel forensic manifold analysis to probe how the model&#x27;s features respond to forensic artifacts. The key findings are that detection relies on a sparse set of features and that the geometry of the feature manifold correlates with specific artifact types, providing a step towards more interpretable and robust detectors.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [YOLO-NAS, YOLOv8, perception, autonomous vehicles, custom dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jalal Khan</p>
</li>
<li class="">
<p><strong>institution:</strong> United Arab Emirates University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21673" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21673</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal understanding], [perceptual-level image understanding, multimodal large language models, domain-adaptive pre-training, task-aligned reinforcement learning, unified benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuo Cao, Jiayang Li, Xiaohui Li, Yuandong Pu, Kaiwen Zhu, Yuanting Gao, Siqi Luo, Yi Xin, Qi Qin, Yu Zhou, Xiangyu Chen, Wenlong Zhang, Bin Fu, Yu Qiao, Yihao Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai AI Laboratory, University of Science and Technology of China, Peking University, Shanghai Jiao Tong University, Tsinghua University, Nanjing University, Sun Yat-sen University, Tele-AI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21675</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/thunderbolt215/UniPercept" target="_blank" rel="noopener noreferrer" class="">https://github.com/thunderbolt215/UniPercept</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes UniPercept-Bench, a unified benchmark for perceptual-level image understanding across aesthetics, quality, structure, and texture with hierarchical definitions and large-scale datasets. 2. Develops a strong baseline model, UniPercept, trained via Domain-Adaptive Pre-Training and Task-Aligned Reinforcement Learning for robust generalization. 3. Demonstrates that UniPercept outperforms existing MLLMs on perceptual tasks and can serve as a plug-and-play reward model for text-to-image generation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e93f4678ef30567f8d2c7b2034256f3795d2cf297d9c6c013681b974accd77a7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e93f4678ef30567f8d2c7b2034256f3795d2cf297d9c6c013681b974accd77a7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limited ability of Multimodal Large Language Models (MLLMs) to perceive perceptual-level image features. It introduces UniPercept, a unified framework and benchmark for perceptual-level image understanding across aesthetics, quality, structure, and texture, trained with Domain-Adaptive Pre-Training and Task-Aligned RL. The proposed model outperforms existing MLLMs and can be used as a reward model for image generation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [blockchain provenance, vision-language models, semantic extraction, reproducibility, educational AI]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Motaleb Hossen Manik, Md Zabirul Islam, Ge Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Rensselaer Polytechnic Institute</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21684" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21684</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces SlideChain, a blockchain-backed provenance framework for verifiable integrity in multimodal semantic extraction from educational content. 2. Presents the SlideChain Slides Dataset, a curated corpus of 1,117 medical imaging lecture slides, and conducts the first systematic analysis of semantic disagreement across four state-of-the-art VLMs. 3. Demonstrates practical scalability, perfect tamper detection, and deterministic reproducibility through evaluation of gas usage, throughput, and auditability on a local EVM-compatible blockchain.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369d8533474f19127055b5c4bf7fad048caff38e6ab3e9aca56aa3a6039a79a4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369d8533474f19127055b5c4bf7fad048caff38e6ab3e9aca56aa3a6039a79a4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces SlideChain, a framework that uses blockchain to provide verifiable provenance for semantic outputs from vision-language models (VLMs) on educational slides. It analyzes discrepancies across VLMs and shows that SlideChain ensures tamper-evident auditability and reproducibility for AI-assisted instructional systems. The results indicate it is a practical step toward trustworthy multimodal educational pipelines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [graph neural network, contrastive learning, few-shot learning, cross-domain, structural consistency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuntian Bo, Tao Zhou, Zechao Li, Haofeng Zhang, Ling Shao</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanjing University of Science and Technology, University of Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21683" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21683</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/primebo1/C-Graph" target="_blank" rel="noopener noreferrer" class="">https://github.com/primebo1/C-Graph</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Structural Prior Graph (SPG) layer to capture and transfer target-category node dependencies for global structure modeling. 2. Introduces a Subgraph Matching Decoding (SMD) mechanism that leverages semantic node relations to guide prediction. 3. Designs a Confusion-minimizing Node Contrast (CNC) loss to reduce node ambiguity and subgraph heterogeneity via contrastive learning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9571022ab0a47d7b3509a2620b8081c3b3ecf53a3d4371337ce5b1b8a68a57c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9571022ab0a47d7b3509a2620b8081c3b3ecf53a3d4371337ce5b1b8a68a57c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of cross-domain few-shot medical image segmentation, where existing methods degrade performance by filtering out domain-specific information. The proposed C-Graph framework leverages the structural consistency of medical images, modeling features as graphs with novel SPG layers, SMD decoding, and a CNC loss. It achieves state-of-the-art cross-domain performance while preserving strong source-domain accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Analyzing the Mechanism of Attention Collapse in VGGT from a Dynamics Perspective</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D reconstruction], [attention collapse, degenerate diffusion, token-merging, mean-field PDE, VGGT]</p>
</li>
<li class="">
<p><strong>authors:</strong> Huan Li, Longjun Luo, Yuling Shi, Xiaodong Gu</p>
</li>
<li class="">
<p><strong>institution:</strong> Huazhong University of Science and Technology, Guangdong University of Technology, Shanghai Jiao Tong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21691" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21691</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a rigorous mathematical explanation for the attention collapse phenomenon in VGGT by modeling it as a degenerate diffusion process. 2. Derives a closed-form mean-field partial differential equation that quantitatively predicts the observed rank profile and convergence rate of token features. 3. Explains why the token-merging remedy delays the collapse by slowing the effective diffusion coefficient, offering a principled lens for future scalable 3D-vision transformers.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81a116424b9dbb97e00548c11b9b6393d53b336e80df62d4caf4add021d7d599_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81a116424b9dbb97e00548c11b9b6393d53b336e80df62d4caf4add021d7d599_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes the attention collapse problem in Visual Geometry Grounded Transformers (VGGT) for 3D reconstruction. It models the global self-attention iteration as a degenerate diffusion process, proving that token features converge to a Dirac measure and deriving a mean-field PDE to predict the collapse. The theory explains the empirical observations and shows how token-merging mitigates the issue by reducing the diffusion rate.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] ShinyNeRF: Digitizing Anisotropic Appearance in Neural Radiance Fields</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [neural rendering], [Neural Radiance Fields, anisotropic specular reflections, Anisotropic Spherical Gaussian, von Mises-Fisher distribution, material editing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Albert Barreiro, Roger Marí, Rafael Redondo, Gloria Haro, Carles Bosch</p>
</li>
<li class="">
<p><strong>institution:</strong> Eurecat, Centre Tecnològic de Catalunya; Universitat Pompeu Fabra; Universitat de Vic - UCC</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21692" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21692</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces ShinyNeRF, a novel NeRF framework capable of modeling both isotropic and anisotropic specular reflections. 2. Proposes a method to jointly estimate physical surface properties (normals, tangents, specular concentration, anisotropy) by approximating outgoing radiance with a mixture of isotropic von Mises-Fisher distributions. 3. Achieves state-of-the-art performance in digitizing anisotropic materials and enables interpretable material property editing.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad12a4dd31b4ec9b89dafef4a6d4d851fe0aa09b5e3d8c6f918d085ee2acda50_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad12a4dd31b4ec9b89dafef4a6d4d851fe0aa09b5e3d8c6f918d085ee2acda50_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces ShinyNeRF, a novel Neural Radiance Fields framework designed to accurately model anisotropic specular reflections, such as those on brushed metals, which previous methods struggled with. The method learns an approximation of outgoing radiance using a mixture of isotropic von Mises-Fisher distributions to jointly estimate physical surface properties. Experimental results show it achieves state-of-the-art performance and enables plausible material editing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Prior-AttUNet: Retinal OCT Fluid Segmentation Based on Normal Anatomical Priors and Attention Gating</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [anatomical prior, attention mechanism, variational autoencoder, densely connected blocks, spatial pyramid pooling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Li Yang, Yuting Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Wannan Medical College</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21693" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21693</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hybrid dual-path architecture integrating a generative prior pathway (using a VAE) with a segmentation network to provide multi-scale normative anatomical priors. 2. Introduces an anatomical prior-guided triple-attention mechanism to dynamically modulate feature importance across decoding stages for improved boundary delineation. 3. Embeds densely connected blocks and spatial pyramid pooling modules in the segmentation backbone to enhance contextual feature extraction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85dc59e9646562af5e8c3595b08a9f5064375d7976961a2ab7b04fe287a243d1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85dc59e9646562af5e8c3595b08a9f5064375d7976961a2ab7b04fe287a243d1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Prior-AttUNet, a model for segmenting macular edema fluid in OCT images. It uses a dual-path architecture with generative anatomical priors and a novel triple-attention mechanism to improve accuracy, especially at boundaries, and achieves high Dice scores across multiple imaging devices while maintaining low computational cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] BeHGAN: Bengali Handwritten Word Generation from Plain Text Using Generative Adversarial Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [handwritten text generation], [Generative Adversarial Networks, Handwritten Text Generation, Bengali, Dataset, Pre-processing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md. Rakibul Islam, Md. Kamrozzaman Bhuiyan, Safwan Muntasir, Arifur Rahman Jawad, Most. Sharmin Sultana Samu</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in provided text. Affiliation/domain cannot be reliably inferred.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21694" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21694</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a method for generating Bengali handwritten words from plain text using GANs, addressing a significant research gap. 2. Developed and used a novel, self-collected dataset of Bengali handwriting from approximately 500 diverse individuals. 3. Demonstrated the ability to produce diverse and realistic handwritten outputs through the described approach.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8b36b4afe805283b88409b54815e1bd251762075786246ae741c57cb65c191a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8b36b4afe805283b88409b54815e1bd251762075786246ae741c57cb65c191a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of research on Bengali handwritten text generation by proposing a GAN-based method to generate words from plain text. The authors created a new dataset of Bengali handwriting samples from hundreds of contributors. The work successfully generates diverse handwritten outputs and contributes to advancing research in this area for the Bengali language.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [ai-generated image detection], [Fast Fourier Transform, CLIP, hybrid system, spectral features, semantic features]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md. Zahid Hossain, Most. Sharmin Sultana Samu, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman, Md. Rakibul Islam</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in provided content. Affiliation/email domain not present.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21695" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21695</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FUSE, a novel hybrid detection system that fuses spectral (FFT-based) and semantic (CLIP-based) features into a joint representation. 2. Introduces a progressive two-stage training strategy to effectively learn the fused representation. 3. Demonstrates state-of-the-art robustness and generalization across multiple high-fidelity image generators and diverse benchmarks, particularly on challenging datasets like Chameleon.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9517071d67c92839f8173192ae8096327651e4417f7ed69aefae08dc78c34838_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9517071d67c92839f8173192ae8096327651e4417f7ed69aefae08dc78c34838_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of robustly detecting AI-generated images by proposing FUSE, a hybrid method that combines spectral features from Fast Fourier Transform with semantic features from CLIP&#x27;s vision encoder. The fused features are trained progressively in two stages. The approach achieves strong generalization and state-of-the-art performance across multiple datasets and generators, highlighting the benefit of integrating spectral and semantic cues.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human motion prediction], [Mixture of Experts, Mamba, spatiotemporal dependencies, computational efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zheng Yin, Chengjian Li, Xiangbo Shu, Meiqi Cao, Rui Yan, Jinhui Tang</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanjing University of Science and Technology, Nanjing Forestry University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21707" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21707</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/alanyz106/ST-MoE" target="_blank" rel="noopener noreferrer" class="">https://github.com/alanyz106/ST-MoE</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Spatiotemporal-Untrammelled Mixture of Experts (ST-MoE) model to flexibly capture complex spatio-temporal dependencies in multi-person motion. 2. Introduces four distinct spatiotemporal experts based on bidirectional spatiotemporal Mamba to achieve parameter sharing and efficiency. 3. Demonstrates superior accuracy, a 41.38% reduction in model parameters, and a 3.6x training speedup on benchmark datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfad49ae18e732f3537bb8bddb97086ffd94c3c545ca0a735b6cfe245b14f6f0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfad49ae18e732f3537bb8bddb97086ffd94c3c545ca0a735b6cfe245b14f6f0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of existing multi-person motion prediction methods, which suffer from inflexible spatiotemporal representations and high computational costs. The authors propose the ST-MoE model, which uses a mixture of Mamba-based experts to adaptively capture spatiotemporal patterns. The method achieves higher accuracy while being more parameter-efficient and faster to train than state-of-the-art approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] RAPTOR: Real-Time High-Resolution UAV Video Prediction with Efficient Video Attention</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video prediction], [Efficient Video Attention (EVA), spatiotemporal factorization, real-time inference, on-device AI, training curriculum]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhan Chen, Zile Guo, Enze Zhu, Peirong Zhang, Xiaoxuan Liu, Lei Wang, Yidan Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Aerospace Information Research Institute, Chinese Academy of Sciences; University of Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21710" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21710</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Thelegendzz/RAPTOR" target="_blank" rel="noopener noreferrer" class="">https://github.com/Thelegendzz/RAPTOR</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces RAPTOR, a single-pass video prediction architecture that avoids the latency and error accumulation of iterative models like diffusion., 2. Proposes Efficient Video Attention (EVA), a novel module that factorizes spatiotemporal modeling to achieve linear time and memory complexity, enabling high-resolution (512^2) processing., 3. Presents a 3-stage training curriculum that progressively refines predictions from coarse structure to sharp, temporally coherent details.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca32784d3d86e53d15479ff35e2982c66656147309d47e2856d7c2334de35f97_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca32784d3d86e53d15479ff35e2982c66656147309d47e2856d7c2334de35f97_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the trilemma in video prediction between speed, resolution, and quality by introducing RAPTOR, a model featuring a novel Efficient Video Attention (EVA) module for linear-complexity spatiotemporal modeling. RAPTOR achieves real-time, high-resolution prediction on edge hardware, setting new state-of-the-art results on benchmark datasets and significantly improving UAV navigation success rates.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] AstraNav-World: World Model for Foresight Control and Consistency</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [world models], [world model, diffusion model, embodied navigation, foresight control, vision-language policy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Junjun Hu, Jintao Chen, Haochen Bai, Minghua Luo, Shichao Xie, Ziyi Chen, Fei Liu, Zedong Chu, Xinda Xue, Botao Ren, Xiaolong Wu, Mu Xu, Shanghang Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Amap Alibaba, Peking University (PKU), Tsinghua University (THU)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21714" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21714</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://astra-amap.github.io/AstraNav-World.github.io/" target="_blank" rel="noopener noreferrer" class="">https://astra-amap.github.io/AstraNav-World.github.io/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. 2. Introduces a training paradigm with bidirectional constraints, optimizing for both action-conditioned visual prediction and visual-conditioned trajectory derivation to ensure executability and physical consistency. 3. Demonstrates improved navigation performance and exceptional zero-shot generalization in real-world testing, showing the model captures transferable spatial and dynamic understanding.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15f89f809a2e6654a0c9645caeda1ab33306e62b8276228d05e7467ef63fc5b8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15f89f809a2e6654a0c9645caeda1ab33306e62b8276228d05e7467ef63fc5b8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes AstraNav-World, a unified world model that integrates a diffusion-based video generator with a vision-language policy to jointly predict future visual scenes and plan action sequences. This bidirectional, synchronized approach mitigates cumulative errors from decoupled pipelines. Experiments show it improves navigation accuracy and success rates, and exhibits strong zero-shot generalization to real-world scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [autoregressive video generation, KV caching, sliding window attention, temporal knot, chunk-wise generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Steven Xiao, XIndi Zhang, Dechao Meng, Qi Wang, Peng Zhang, Bang Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tongyi Lab, Alibaba Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21734" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21734</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://humanaigc.github.io/knot_forcing_demo_page/" target="_blank" rel="noopener noreferrer" class="">https://humanaigc.github.io/knot_forcing_demo_page/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A chunk-wise generation strategy with global identity preservation via cached KV states and local temporal modeling using sliding window attention. 2. A temporal knot module that overlaps adjacent chunks and uses image-to-video conditioning to smooth inter-chunk motion transitions. 3. A &quot;running ahead&quot; mechanism that dynamically updates the reference frame&#x27;s temporal coordinate to maintain long-term coherence.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3db0cbfa541f28563a8f65a9f7c0b4cb0b909e1c62fbec822e091b0ad44811c7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3db0cbfa541f28563a8f65a9f7c0b4cb0b909e1c62fbec822e091b0ad44811c7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Knot Forcing, a streaming framework for real-time, infinite portrait animation. It addresses error accumulation and motion discontinuities in autoregressive video diffusion models through chunk-wise generation, a temporal knot module, and a running-ahead mechanism. The method achieves high-fidelity, temporally consistent animation with real-time performance on consumer GPUs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] SyncAnyone: Implicit Disentanglement via Progressive Self-Correction for Lip-Syncing in the wild</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [lip-syncing, diffusion transformer, two-stage learning, inpainting, self-correction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xindi Zhang, Dechao Meng, Steven Xiao, Qi Wang, Peng Zhang, Bang Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tongyi Lab, Alibaba Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21736" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21736</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://humanaigc.github.io/sync_anyone_demo_page/" target="_blank" rel="noopener noreferrer" class="">https://humanaigc.github.io/sync_anyone_demo_page/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel two-stage learning framework (SyncAnyone) for lip-syncing that decouples accurate motion modeling from high-fidelity generation. 2. A Stage 2 mask-free tuning pipeline that uses a self-generated pseudo-dataset to correct artifacts from the initial mask-based training. 3. Demonstrates state-of-the-art performance in visual quality, temporal coherence, and identity preservation for in-the-wild scenarios.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/825b387e09612033170e3bd5c2f7bedd0f32c2a644514ee397fd09b64088653e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/825b387e09612033170e3bd5c2f7bedd0f32c2a644514ee397fd09b64088653e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of generating high-quality, audio-synchronized lip movements in videos without disrupting the spatiotemporal context or background. The proposed method, SyncAnyone, uses a two-stage framework: first training a diffusion-based video transformer for masked mouth inpainting, and then fine-tuning it mask-free on self-generated data to correct artifacts. Experiments show it achieves state-of-the-art results in lip-syncing for challenging, real-world videos.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [continual learning], [entropy scaling, catastrophic forgetting, stability-plasticity dilemma, dynamic feedback, layer-wise control]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hengyi Wu, Zhenyi Wang, Heng Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Maryland, College Park, University of Central Florida</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21743" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21743</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel entropy-aware continual learning method that uses a dynamic feedback mechanism to regulate each layer based on its entropy, addressing underfitting and overfitting. 2. Introduces Self-Adaptive Entropy Scaling, which adaptively adjusts regularization strength per layer to encourage convergence to wider local minima for better generalization. 3. Presents a complementary adaptive training mechanism that modulates layer plasticity based on performance, preserving knowledge in high-performing layers while amplifying updates in underperforming ones.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses catastrophic forgetting in continual learning by proposing a dynamic feedback mechanism that regulates each neural network layer based on its entropy. This entropy-aware method reduces entropy in high-entropy layers to prevent underfitting and increases it in low-entropy layers to prevent overfitting, promoting convergence to wider minima for improved generalization. The approach is general and integrates with existing replay- and regularization-based methods, showing substantial performance gains over state-of-the-art baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [brain-computer interface (BCI)], [EEG, TSception, Adaptive Average Pooling, spatiotemporal features, drowsiness detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gourav Siddhad, Anurag Singh, Rajkumar Saini, Partha Pratim Roy</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology Roorkee, OP Jindal University, Luleå University of Technology, Indian Institute of Technology Dhanbad</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21747" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21747</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a Modified TSception architecture with a five-layer temporal refinement strategy to capture multi-scale brain dynamics. 2. Introduced Adaptive Average Pooling for structural flexibility to handle varying EEG input dimensions and a two-stage fusion mechanism for optimized spatiotemporal feature integration. 3. Demonstrated improved performance stability (reduced confidence interval) on the SEED-VIG dataset and state-of-the-art generalizability on the STEW mental workload dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43d71afc9fcee5064adfe94f1fb1d9f8a1c55ccd8d1c759dcd1b5801b9d23f46_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43d71afc9fcee5064adfe94f1fb1d9f8a1c55ccd8d1c759dcd1b5801b9d23f46_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a Modified TSception deep learning model for robust EEG-based detection of driver drowsiness and mental workload. The key modifications include a multi-layer temporal refinement strategy and Adaptive Average Pooling, which improve the model&#x27;s stability and ability to handle varying input sizes. The model achieves comparable accuracy with significantly better stability on a drowsiness dataset and state-of-the-art results on a mental workload dataset, demonstrating its effectiveness for reliable cognitive state monitoring.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network for Multimodal Liver Tumor Segmentation from Unpaired Datasets</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [Quaternion Neural Networks, Cross-Attention, Unpaired Data, Multimodal Learning, Explainable AI]</p>
</li>
<li class="">
<p><strong>authors:</strong> Arunkumar V, Firos V M, Senthilkumar S, Gangadharan G R</p>
</li>
<li class="">
<p><strong>institution:</strong> Anna University, National Institute of Technology Tiruchirappalli</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21760" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21760</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an Adaptive Quaternion Cross-Fusion (A-QCF) block for bidirectional knowledge transfer between unpaired CT and MRI data streams., 2. Introduces a unified segmentation model (A-QCF-Net) that leverages Quaternion Neural Networks to build a shared feature space from separate datasets., 3. Demonstrates significant performance gains over strong unimodal baselines and validates clinical relevance through explainability analysis.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebec19949c3fad65f84d0acee71e271ec2d8caca288cc4ecf24768e9533dbbe8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebec19949c3fad65f84d0acee71e271ec2d8caca288cc4ecf24768e9533dbbe8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of training multimodal segmentation models with unpaired datasets. It proposes A-QCF-Net, which uses Quaternion Neural Networks and an adaptive cross-fusion block to enable knowledge transfer between separate CT and MRI data. The method significantly outperforms unimodal baselines and provides a viable paradigm for leveraging large, unpaired medical image archives.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3d medical image analysis], [Masked Autoencoder, Swin Transformer, Self-Supervised Learning, 3D Vision Transformer, Structural Priority Loss]</p>
</li>
<li class="">
<p><strong>authors:</strong> Evgeny Alves Limarenko, Anastasiia Studenikina</p>
</li>
<li class="">
<p><strong>institution:</strong> Moscow Institute of Physics and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21769" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21769</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed BertsWin, a hybrid architecture combining full BERT-style token masking with Swin Transformer windows to preserve 3D spatial topology during SSL pre-training. 2. Introduced a structural priority loss function to enhance learning. 3. Demonstrated significant acceleration in semantic convergence (5.8x) and a 15-fold reduction in training epochs to reach SOTA fidelity when combined with the GradientConductor optimizer, without increasing computational FLOPs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the difficulty of applying standard Masked Autoencoders to 3D medical images, which lose spatial context. It proposes BertsWin, a hybrid architecture that maintains a full 3D token grid using Swin Transformer windows and a structural loss. The method achieves much faster convergence and state-of-the-art reconstruction fidelity for 3D CBCT scans without extra computational cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Inference-based GAN Video Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [VAE-GAN, Markov chain, long video generation, temporal consistency, encoder-decoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jingbo Yang, Adrian G. Bors</p>
</li>
<li class="">
<p><strong>institution:</strong> University of York</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21776" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21776</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new video generator, Encoder GAN3 (EncGAN3), which is a VAE-GAN hybrid structure that incorporates inference capabilities into an adversarial-based unconditional video generator. 2. Introduces a novel, memory-efficient approach to generate long videos (hundreds/thousands of frames) by extending the base VAE-GAN model. 3. Leverages a Markov chain framework with a recall mechanism, where each state is a short VAE-GAN generator, to sequentially connect video sub-sequences and ensure temporal continuity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5d6cd59a07e4478b6b21e586a92b15f90e237037b758a29738bf31e46f9843c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5d6cd59a07e4478b6b21e586a92b15f90e237037b758a29738bf31e46f9843c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of generating long, high-quality videos, a task where existing models suffer from quality degradation. The proposed method first introduces a VAE-GAN hybrid video generator (EncGAN3) and then extends it using a Markov chain framework to sequentially generate short video clips, enabling the creation of temporally consistent long videos. The main conclusion is that this approach overcomes the temporal scaling limitation and allows for memory-efficient generation of long video sequences.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video scene segmentation], [vision-language model, multimodal reasoning, context-focus window, confidence score extraction, explainable AI]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nimrod Berman, Adam Botach, Emanuel Ben-Baruch, Shunit Haviv Hakimi, Asaf Gendler, Ilan Naiman, Erez Yosef, Igor Kviatkovsky</p>
</li>
<li class="">
<p><strong>institution:</strong> Ben-Gurion University, Amazon Prime Video, Tel-Aviv University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21778" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21778</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Scene-VLM, the first fine-tuned vision-language model framework for video scene segmentation, enabling multimodal reasoning across visual and textual cues. 2. Proposes a method to extract confidence scores from VLM token-level logits, enabling controllable precision-recall trade-offs. 3. Demonstrates the model can be aligned to generate coherent natural-language rationales for its boundary decisions with minimal supervision.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d88d54ab6607412e3eb29cafcea4e88a9b83dcabc3bd75f5b4eca365e6ada2b5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d88d54ab6607412e3eb29cafcea4e88a9b83dcabc3bd75f5b4eca365e6ada2b5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Scene-VLM, a novel vision-language model framework for segmenting long videos into coherent scenes by jointly processing visual frames, dialogue, and metadata. It predicts boundaries sequentially with a context-focus window and can provide confidence scores and explanations. The method achieves state-of-the-art performance on benchmarks like MovieNet, significantly outperforming previous methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [image captioning], [scientific figure captioning, large-scale dataset, domain-specific training, human evaluation, large language models (LLMs)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ting-Hao K.Huang, Ryan A. Rossi, Sungchul Kim, Tong Yu, Ting-Yao E. Hsu, Ho Yin, C. Lee Giles</p>
</li>
<li class="">
<p><strong>institution:</strong> The Pennsylvania State University, Adobe Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21789" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21789</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Creation and continuous updating of a large-scale, real-world dataset of scientific figure-caption pairs from arXiv papers. 2. Conducting extensive evaluations, both automatic and human, on generated and author-written captions to assess quality. 3. Developing interactive systems and launching annual challenges to advance the field and help scientists write better captions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper reviews the SciCap project&#x27;s first five years, which focused on generating and evaluating captions for scientific figures. The core method involved building a large-scale dataset from arXiv and exploring domain-specific training, similar to models like SciBERT, for captioning. The conclusion outlines key lessons learned and proposes future research directions to address unsolved challenges in the field.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [diffusion models], [Parameter-Efficient Fine-Tuning, Mixture of Low-rank Experts, Instruction-Guided Routing, Multi-Conditional Generation, Diffusion Transformers]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jinqi Xiao, Qing Yan, Liming Jiang, Zichuan Liu, Hao Kang, Shen Sang, Tiancheng Zhi, Jing Liu, Cheng Yang, Xin Lu, Bo Yuan</p>
</li>
<li class="">
<p><strong>institution:</strong> ByteDance Inc., Rutgers University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21788" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21788</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/yanq095/InstructMoLE" target="_blank" rel="noopener noreferrer" class="">https://github.com/yanq095/InstructMoLE</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces InstructMoLE, a framework using an Instruction-Guided Mixture of Low-Rank Experts for multi-conditional image generation., 2. Proposes Instruction-Guided Routing (IGR), a global routing signal derived from user instructions to select a coherent expert council for all tokens, addressing spatial fragmentation and semantic drift., 3. Introduces an output-space orthogonality loss to promote expert functional diversity and prevent representational collapse.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11bd8639cb632e86b35367843cf453bbc6a02fb6882f88ff7441c78b42b653ce_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11bd8639cb632e86b35367843cf453bbc6a02fb6882f88ff7441c78b42b653ce_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of task interference in multi-conditional image generation when using monolithic PEFT adapters like LoRA. It proposes InstructMoLE, a novel framework that uses global instruction-guided routing to select a consistent mixture of low-rank experts, combined with an orthogonality loss for diversity, which outperforms existing methods on benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] AI for Mycetoma Diagnosis in Histopathological Images: The MICCAI 2024 Challenge</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [histopathological images, image segmentation, grain detection, deep learning, mycetoma classification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hyam Omar Ali, Sahar Alhesseen, Lamis Elkhair, Adrian Galdran, Ming Feng, Zhixiang Xiong, Zengming Lin, Kele Xu, Liang Hu, Benjamin Keel, Oliver Mills, James Battye, Akshay Kumar, Asra Aslam, Prasad Dutande, Ujjwal Baid, Bhakti Baheti, Suhas Gajre, Aravind Shrenivas Murali, Eung-Joo Lee, Ahmed Fahal, Rachid Jennane</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Khartoum, Orleans University, Tongji University, National University of Defense Technology, University of Leeds</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21792" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21792</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Organized the mAIcetoma challenge to advance AI solutions for mycetoma diagnosis. 2. Provided the standardized Mycetoma database (MyData) for model development. 3. Demonstrated the effectiveness of automated deep learning models for segmenting grains and classifying mycetoma types from histopathological images.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2608e467b615afdd887a03b7ca003d5d2989f1640e4dc84304a9ee0f5a6e979_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2608e467b615afdd887a03b7ca003d5d2989f1640e4dc84304a9ee0f5a6e979_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents the mAIcetoma challenge, which aimed to develop AI models for automating the diagnosis of mycetoma by segmenting grains and classifying disease types from histopathological images. Multiple teams proposed various deep learning architectures, which were evaluated on a provided standardized dataset. The results showed that the models achieved high segmentation accuracy and significant performance in classification, highlighting the potential of AI to assist in diagnosing this neglected tropical disease.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Diffusion Posterior Sampling for Super-Resolution under Gaussian Measurement Noise</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image super-resolution], [diffusion posterior sampling, single-image super-resolution, inverse problems, measurement consistency, unconditional diffusion prior]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abu Hanif Muhammad Syarubany</p>
</li>
<li class="">
<p><strong>institution:</strong> Korea Advanced Institute of Science &amp; Technology (KAIST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21797" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21797</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Implementation and evaluation of diffusion posterior sampling (DPS) for single-image super-resolution under a known degradation model. 2. An ablation study analyzing the impact of guidance scale and noise level on reconstruction quality, identifying an optimal configuration. 3. Demonstration that balancing the diffusion prior and measurement-gradient strength enables stable, high-quality reconstructions without model retraining.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc3596dd62ab28bf4cee2aec80e4147a24811ace57a7d9d76802c0ea0767bbdd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc3596dd62ab28bf4cee2aec80e4147a24811ace57a7d9d76802c0ea0767bbdd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the use of diffusion posterior sampling (DPS) for single-image super-resolution under Gaussian noise. The method combines an unconditional diffusion prior with a likelihood-guided update to enforce measurement consistency during sampling. The main finding is that moderate guidance, at a specific scale and noise level, yields the best reconstruction quality, highlighting the importance of balancing prior and data fidelity for stable results.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] CellMamba: Adaptive Mamba for Accurate and Efficient Cell Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [Mamba, Triple-Mapping Adaptive Coupling (TMAC), Adaptive Mamba Head, biomedical instance detection, VSSD backbone]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ruochen Liu, Yi Tian, Jiahao Wang, Hongbin Liu, Xianxu Hou, Jingxin Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Liverpool, National University of Singapore, Xi&#x27;an Jiaotong-Liverpool University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21803" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21803</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Triple-Mapping Adaptive Coupling (TMAC) module that splits channels into parallel branches with dual idiosyncratic and one consensus attention map for enhanced spatial discriminability. 2. Designs an Adaptive Mamba Head that fuses multi-scale features via learnable weights to handle varying object sizes robustly. 3. Introduces CellMamba, a lightweight one-stage detector built on a VSSD backbone with CellMamba Blocks, achieving superior accuracy and efficiency on biomedical cell detection datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3fe9b8372a27eedc5a8e2e1150bcdf6cf461c195f9ed154d8890662de191da_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3fe9b8372a27eedc5a8e2e1150bcdf6cf461c195f9ed154d8890662de191da_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes CellMamba, a lightweight one-stage detector for cell detection in pathological images. It introduces a novel Triple-Mapping Adaptive Coupling (TMAC) module and an Adaptive Mamba Head to improve spatial discriminability and multi-scale feature fusion. Experiments show CellMamba outperforms CNN, Transformer, and Mamba baselines in accuracy while being more efficient in model size and inference speed.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] S&amp;P 500 Stock&#x27;s Movement Prediction using CNN</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [financial time series forecasting], [Convolutional Neural Network (CNN), multivariate raw data, stock movement prediction, historical data matrices, S&amp;P 500]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rahul Gupta</p>
</li>
<li class="">
<p><strong>institution:</strong> None (No affiliation or email domain provided in the given content)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21804" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21804</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the application of Convolutional Neural Networks (CNNs), typically used for image classification, to the problem of stock movement prediction by treating multivariate historical stock data as image-like matrices. 2. Utilizes raw, unprocessed market data including events like stock splits and dividends, instead of relying on pre-engineered financial features. 3. Demonstrates a flexible prediction framework that can be applied at different levels: individual stocks, sectors, or entire portfolios.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d6197655292ac8e55d8c7606c8c3cfe730f7f8dad4004b93d4a9b3a8d8f457_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d6197655292ac8e55d8c7606c8c3cfe730f7f8dad4004b93d4a9b3a8d8f457_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the problem of predicting stock price movements for the S&amp;P 500 index. The core method involves using a Convolutional Neural Network (CNN) to analyze multivariate historical stock data, which is structured as image-like matrices, without extensive feature engineering. The approach shows promising results and offers a flexible model for stock, sector, or portfolio-level predictions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [adversarial attacks], [entropy-guided attacks, vision-language models, adversarial robustness, harmful content generation, transferability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mengqi He, Xinyu Tian, Xin Shen, Jinhong Ni, Shu Zou, Zhaoyuan Yang, Jing Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Australian National University, The University of Queensland, GE Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21815" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21815</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies that only a small fraction (~20%) of high-entropy tokens (critical decision points) disproportionately govern output trajectories in autoregressive VLMs, challenging the prior assumption of equal token importance. 2. Proposes a selective attack strategy that concentrates adversarial perturbations on these high-entropy positions, achieving strong semantic degradation with a smaller perturbation budget and inducing 35-49% of benign outputs to become harmful. 3. Demonstrates that vulnerable high-entropy forks recur across diverse VLMs, enabling feasible transferable attacks (17-26% harmful rates), and proposes the Entropy-bank Guided Adversarial attack (EGA) method which achieves high attack success rates (93-95%) while exposing new safety weaknesses.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper reveals that adversarial attacks on Vision-Language Models (VLMs) can be made more efficient and dangerous by targeting only the critical high-entropy tokens during autoregressive generation, rather than all tokens. The proposed Entropy-bank Guided Adversarial attack (EGA) concentrates perturbations on these positions, achieving high attack success and converting many benign outputs into harmful ones, while also showing significant transferability across models. This exposes a critical and previously overlooked vulnerability in current VLM safety mechanisms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [autonomous driving perception], [multimodal fusion, multi-view cooperative perception, spatiotemporal modeling, V2X communication, deformable attention]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhenwei Yang, Yibo Ai, Weidong Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology Beijing, National Center for Materials Service Safety</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21831" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21831</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes XET-V2X, an end-to-end tracking framework that unifies multi-view multimodal sensing within a shared spatiotemporal representation for V2X collaboration. 2. Introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention to efficiently align heterogeneous viewpoints and modalities. 3. Demonstrates robust performance on real-world and simulated V2X datasets, showing consistent improvements in detection and tracking under varying communication delays.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2257ca524cd2e350a5c2abf7059415ea35045bbf6cb45991c73d32410116cb9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2257ca524cd2e350a5c2abf7059415ea35045bbf6cb45991c73d32410116cb9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of reliable 3D perception in autonomous driving under occlusions and communication delays by proposing XET-V2X, an end-to-end framework that fuses multi-view images and point clouds using a novel dual-layer spatial cross-attention module. The method achieves effective cross-modal interaction and reduces computational overhead. Experiments show it delivers robust and temporally stable detection and tracking performance in complex V2X scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Scalable Class-Incremental Learning Based on Parametric Neural Collapse</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [class-incremental learning], [parametric neural collapse, equiangular tight frame, knowledge distillation, adaptive expansion, feature alignment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chuangxin Zhang, Guangfeng Lin, Enhui Zhao, Kaiyang Liao, Yajun Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in the provided content. Affiliation information is not included.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21845" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21845</a></p>
</li>
<li class="">
<p><strong>code:</strong> this https URLETF2</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a scalable class-incremental learning method (SCL-PNC) based on parametric neural collapse, enabling demand-driven, minimal-cost backbone expansion via an adapt-layer. 2. Introduces a dynamic parametric Equiangular Tight Frame (ETF) classifier to address class misalignment caused by evolving class distributions. 3. Presents a parallel expansion framework with a knowledge distillation algorithm to counteract feature drift and ensure feature consistency across expansion modules.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses challenges in class-incremental learning, such as overfitting and catastrophic forgetting, by proposing SCL-PNC. This method uses a parametric neural collapse framework with an adaptive backbone expansion and a dynamic ETF classifier to efficiently handle growing categories while maintaining feature consistency via distillation. Experiments show the method is effective and efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Breaking Alignment Barriers: TPS-Driven Semantic Correlation Learning for Alignment-Free RGB-T Salient Object Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [salient object detection], [RGB-T, unaligned images, Thin-Plate Spline, MobileViT, Mamba]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lupiao Hu, Fasheng Wang, Fangmei Chen, Fuming Sun, Haojie Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Dalian Minzu University, Shandong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21856" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21856</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/HTUTU2/TPS-SCL" target="_blank" rel="noopener noreferrer" class="">https://github.com/HTUTU2/TPS-SCL</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Thin-Plate Spline Alignment Module (TPSAM) to mitigate spatial discrepancies between unaligned RGB-T modalities, addressing local deformations better than homography estimation. 2. Designs a Semantic Correlation Constraint Module (SCCM) to hierarchically constrain salient features and suppress background interference during alignment. 3. Introduces a Cross-Modal Correlation Module (CMCM) to fully explore and integrate inter-modal dependencies, enhancing detection performance in a lightweight architecture using MobileViT and Mamba.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcfc4f61262c973564dfeb3e9591537a78f5165f01a7fece4325e5961bd3fe89_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcfc4f61262c973564dfeb3e9591537a78f5165f01a7fece4325e5961bd3fe89_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the performance degradation of RGB-T salient object detection (SOD) methods on real-world, unaligned image pairs. It proposes TPS-SCL, a lightweight network that uses a Thin-Plate Spline module for alignment, semantic correlation constraints, and cross-modal fusion. Experiments show the method achieves state-of-the-art performance among lightweight SOD methods and outperforms mainstream RGB-T SOD approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image embedding], [conditional image embedding, large vision-language model, training-free, image similarity, hidden state]</p>
</li>
<li class="">
<p><strong>authors:</strong> Masayuki Kawarada, Kosuke Yamada, Antonio Tejero-de-Pablos, Naoto Inoue</p>
</li>
<li class="">
<p><strong>institution:</strong> CyberAgent</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21860" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21860</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/CyberAgentAILab/DIOR_conditional_image_embeddings" target="_blank" rel="noopener noreferrer" class="">https://github.com/CyberAgentAILab/DIOR_conditional_image_embeddings</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DIOR, a novel training-free framework for generating conditional image embeddings using Large Vision-Language Models (LVLMs)., 2. Introduces a method that prompts an LVLM to describe an image with a single word based on a given condition and uses the last token&#x27;s hidden state as the embedding., 3. Demonstrates superior performance over existing training-free baselines (e.g., CLIP) and even methods requiring additional training on conditional similarity tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dc3046fafcb5e9e27821f45ec139d58a5fbb5098fe33ea6a51f89167ca9df74_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dc3046fafcb5e9e27821f45ec139d58a5fbb5098fe33ea6a51f89167ca9df74_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of generating image embeddings that focus on specific textual conditions, which standard models like CLIP cannot do. It proposes DIOR, a training-free method that uses a Large Vision-Language Model to produce a one-word description based on a condition and extracts its hidden state as the conditional embedding. Experiments show DIOR outperforms both training-free and training-required baselines on conditional image similarity tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Fast Inference of Visual Autoregressive Model with Adjacency-Adaptive Dynamical Draft Trees</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [speculative decoding, draft tree, inference acceleration, autoregressive image generation, dynamic tree structure]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haodong Lei, Hongsong Wang, Xin Geng, Liang Wang, Pan Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> Southeast University, Institute of Automation Chinese Academy of Sciences, Singapore Management University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21857" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21857</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Haodong-Lei-Ray/ADT-Tree" target="_blank" rel="noopener noreferrer" class="">https://github.com/Haodong-Lei-Ray/ADT-Tree</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identified the key obstacle of applying speculative decoding to visual AR models: inconsistent acceptance rates across draft trees due to spatially varying token prediction difficulty. 2. Proposed ADT-Tree, an adjacency-adaptive dynamic draft tree that dynamically adjusts tree depth and width based on adjacent token states and prior acceptance rates. 3. Demonstrated significant speedups (over 3x) on benchmarks and seamless integration with relaxed sampling methods for further acceleration.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b9ff13149fa2d6733868b2125e7af2ae06239a529152a057b80d0d6f357ccf3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b9ff13149fa2d6733868b2125e7af2ae06239a529152a057b80d0d6f357ccf3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the slow inference of visual autoregressive models by proposing ADT-Tree, a dynamic draft tree method that adapts its structure to image region complexity. It achieves over 3x speedup on standard benchmarks and can be combined with other sampling techniques for additional gains.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [feature-level fusion, convolutional neural networks, diabetic retinopathy screening, EfficientNet, DenseNet]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan</p>
</li>
<li class="">
<p><strong>institution:</strong> North South University, Korea Institute of Oriental Medicine, American International University–Bangladesh</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21861" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21861</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] EasyOmnimatte: Taming Pretrained Inpainting Diffusion Models for End-to-End Video Layered Decomposition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video matting], [video omnimatte, diffusion models, LoRA, DiT blocks, dual-expert]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yihan Hu, Xuelin Chen, Xiaodong Cun</p>
</li>
<li class="">
<p><strong>institution:</strong> Great Bay University, Adobe Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21865" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21865</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/GVCLab/EasyOmnimatte" target="_blank" rel="noopener noreferrer" class="">https://github.com/GVCLab/EasyOmnimatte</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the first unified, end-to-end video omnimatte method by finetuning a pretrained video inpainting diffusion model. 2. Introduces a Dual-Expert strategy (Effect Expert and Quality Expert) that selectively applies LoRA to specific DiT blocks to capture foreground-associated effects and refine alpha mattes. 3. Achieves state-of-the-art performance in quality and efficiency by using experts at different denoising steps, eliminating the need for two full diffusion passes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/200afe63944dc4d9793ba3dadc45f6ca120880dcbf65f86982757bb158120f60_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/200afe63944dc4d9793ba3dadc45f6ca120880dcbf65f86982757bb158120f60_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces EasyOmnimatte, a method for video layered decomposition that finetunes a pretrained video inpainting diffusion model with a novel Dual-Expert strategy to efficiently produce high-quality alpha mattes with associated effects. It significantly outperforms existing methods in both speed and quality, enabling various downstream video editing tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [autoregressive image generation, dynamic tokenization, next-token prediction entropy, patch merging, training efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Divyansh Srivastava, Akshay Mehra, Pranav Maneriker, Debopam Sanyal, Vishnu Raj, Vijay Kamarshi, Fan Du, Joshua Kimball</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, San Diego, Dolby Laboratories</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21867" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21867</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces DPAR, a novel decoder-only autoregressive model that dynamically aggregates image tokens into variable-sized patches for efficient generation, using next-token prediction entropy as a merging criterion. 2. Demonstrates that training with dynamic patches yields patch-boundary-robust representations, enabling inference with larger patches for further efficiency. 3. Achieves significant reductions in token count (up to 2.06x) and training FLOPs (up to 40%) while improving image quality (FID) by up to 27.1% compared to fixed-tokenization baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dac1942675af4bd3638ebec23734e3fed0239b066b673633b094708fa3d2d26f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dac1942675af4bd3638ebec23734e3fed0239b066b673633b094708fa3d2d26f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the computational inefficiency of fixed tokenization in autoregressive image generation, where token count grows quadratically with resolution. It proposes DPAR, a method that dynamically merges image tokens into larger patches based on the information content predicted by a lightweight model&#x27;s entropy, allocating more compute to complex regions. This approach reduces training costs by up to 40% and improves generated image quality (FID) by up to 27.1% compared to standard models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [visual localization], [early-fusion, sparse mask attention, pose tokenizer, VGGT backbone, multi-view geometry]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tianchen Deng, Wenhua Wu, Kunzhen Wu, Guangming Wang, Siting Zhu, Shenghai Yuan, Xun Chen, Guole Shen, Zhe Liu, Hesheng Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, Nanyang Technological University, Cambridge University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21883" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21883</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/dtc111111/Reloc-VGGT" target="_blank" rel="noopener noreferrer" class="">https://github.com/dtc111111/Reloc-VGGT</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the first visual localization framework with an early-fusion mechanism for multi-view spatial integration. 2. Proposes a novel sparse mask attention strategy to reduce computational complexity and enable real-time performance. 3. Presents a pose tokenizer and projection module to better exploit spatial relationships from multiple database views.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a9b7693fa823e25dd55dc558d5c57f7d31f7adebc941d9ff45f7ef2ad7c6508_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a9b7693fa823e25dd55dc558d5c57f7d31f7adebc941d9ff45f7ef2ad7c6508_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Reloc-VGGT, a visual re-localization framework that uses an early-fusion mechanism and a VGGT backbone to integrate multi-view 3D geometry for robust pose estimation. It introduces a sparse mask attention strategy for efficiency and is trained on millions of image pairs. The method achieves strong accuracy, generalization, and real-time performance across diverse datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [fMRI, foundation model, data-efficient, training-efficient, hierarchical encoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mo Wang, Junfeng Xia, Wenhao Ye, Enyu Liu, Kaining Peng, Jianfeng Feng, Quanying Liu, Hongkai Wen</p>
</li>
<li class="">
<p><strong>institution:</strong> Southern University of Science and Technology, University of Warwick, Fudan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21881" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21881</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SLIM-Brain, a novel atlas-free foundation model for fMRI analysis that addresses the dual bottlenecks of data- and training-efficiency. 2. Introduces a two-stage adaptive design featuring a lightweight temporal extractor for saliency ranking and a 4D hierarchical encoder (Hiera-JEPA) that learns from selected windows and uses aggressive masking. 3. Demonstrates state-of-the-art performance across seven benchmarks while requiring significantly less pre-training data (4k sessions) and GPU memory (~30% of traditional methods).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03335d41a7bea8c473f46251d91847fc35908f4f15a206682290034deaf6ef92_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03335d41a7bea8c473f46251d91847fc35908f4f15a206682290034deaf6ef92_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the data- and training-efficiency bottlenecks in fMRI foundation models. It proposes SLIM-Brain, a two-stage model that uses a temporal extractor to select salient data windows and a hierarchical encoder to learn from them efficiently. The method achieves state-of-the-art results on multiple tasks using far less pre-training data and computational resources than traditional approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] CrownGen: Patient-customized Crown Generation via Point Diffusion Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D shape generation], [diffusion model, point cloud, dental crown, generative framework, boundary prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Juyoung Bae, Moo Hyun Son, Jiale Peng, Wanting Qu, Wener Chen, Zelin Qiu, Kaixin Li, Xiaojuan Chen, Yifan Lin, Hao Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Hong Kong University of Science and Technology, University of Hong Kong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21890" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21890</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel generative framework (CrownGen) that automates patient-customized dental crown design. 2. A method using a denoising diffusion model on a tooth-level point cloud representation with a boundary prediction module for spatial priors. 3. Validation through a large-scale quantitative benchmark and a clinical study showing the generated crowns are non-inferior to manual designs and reduce design time.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2baf8cabd0677f151bf313fb7831de809fe9ecc0a70533ba9d2cffabe95b064b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2baf8cabd0677f151bf313fb7831de809fe9ecc0a70533ba9d2cffabe95b064b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents CrownGen, a framework that automates dental crown design using a point diffusion model. It uses a boundary prediction module and a generative module to create patient-customized crowns in a single pass. Clinical validation shows the generated crowns match manual quality while significantly reducing design time.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] High-Fidelity and Long-Duration Human Image Animation with Diffusion Transformer</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human image animation], [diffusion transformer, hybrid implicit guidance, position shift adaptive module, skeleton alignment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shen Zheng, Jiaran Cai, Yuansheng Guan, Shenneng Huang, Xingpei Ma, Junjie Cao, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang, Xiao-Ping Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Guangzhou Quwan Network Technology, Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21905" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21905</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Designed a set of hybrid implicit guidance signals and a sharpness guidance factor to incorporate detailed facial and hand features for high-fidelity generation. 2. Proposed a Position Shift Adaptive Module (time-aware position shift fusion) to enable video generation of arbitrary length. 3. Introduced a novel data augmentation strategy and a skeleton alignment model to mitigate the impact of human shape variations across identities.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dcd7ed4538aa6140dc40b848f4f018648b9407793f582523c563f79c97253f1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dcd7ed4538aa6140dc40b848f4f018648b9407793f582523c563f79c97253f1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of generating high-fidelity and long-duration human animation videos by proposing a Diffusion Transformer (DiT)-based framework. The method introduces novel guidance mechanisms for facial/hand details, a module for arbitrary-length generation, and techniques to handle identity shape variations. Experimental results show it outperforms existing state-of-the-art approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal action recognition], [human-centric graph representation learning, attention-based post calibration, spatiotemporal graph, multimodal fusion, skeleton-guided sampling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zeyu Liang, Hailun Xia, Naichuan Zheng</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing University of Posts and Telecommunications</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21916" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21916</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes PAN, the first human-centric graph representation learning framework for multimodal action recognition, which models RGB patches containing human joints as spatiotemporal graphs to align with skeleton data and suppress redundancy. 2. Introduces an attention-based post calibration module to reduce the framework&#x27;s dependency on high-quality skeletal data with minimal performance cost. 3. Presents two model variants, PAN-Ensemble (dual-path with late fusion) and PAN-Unified (single-network unified learning), both achieving state-of-the-art performance on three benchmark datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ada463935a334d688b0ec740d70f3f4578fc9000b1c19263667ad6c53ea2b18_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ada463935a334d688b0ec740d70f3f4578fc9000b1c19263667ad6c53ea2b18_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of effectively fusing heterogeneous RGB and skeleton data for action recognition by proposing PAN, a human-centric graph learning framework. PAN represents RGB patches containing human joints as spatiotemporal graphs, enabling semantically coherent multimodal fusion and reducing RGB redundancy. The proposed method achieves state-of-the-art performance on three datasets through its two variants, PAN-Ensemble and PAN-Unified.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [unsupervised anomaly detection, disentangled representation, pseudo-healthy image reconstruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tao Yang, Xiuying Wang, Hao Liu, Guanzhong Gong, Lian-Ming Wu, Yu-Ping Wang, Lisheng Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21924" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21924</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a disentangled representation module to decouple brain MRI into imaging-invariant anatomical information and imaging-specific information, improving model generalizability across multi-modality and multi-center data. 2. Designed an edge-to-image restoration module that reconstructs high-quality pseudo-healthy images from edge information, suppressing the propagation of abnormal residuals. 3. Introduced brain anatomical priors and a differentiable one-hot encoding operator to constrain and stabilize the disentanglement learning process.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/785ff0154b41353c2fdfb9a61f66c2f97de1b49c0e9dbba451d335e3a8460e71_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/785ff0154b41353c2fdfb9a61f66c2f97de1b49c0e9dbba451d335e3a8460e71_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of generalizability and performance in unsupervised anomaly detection for brain MRI. It proposes a new framework that disentangles anatomical from imaging information and reconstructs pseudo-healthy images from edges, achieving state-of-the-art results on multi-center datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] AutoPP: Towards Automated Product Poster Generation and Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image generation], [product poster generation, click-through rate optimization, isolated direct preference optimization, AutoPP1M dataset, unified design module]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiahao Fan, Yuxin Qin, Wei Feng, Yanyin Chen, Yaoyu Li, Ao Ma, Yixiu Li, Li Zhuang, Haoyi Bian, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law</p>
</li>
<li class="">
<p><strong>institution:</strong> JD.COM</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21921" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21921</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/JD-GenX/AutoPP" target="_blank" rel="noopener noreferrer" class="">https://github.com/JD-GenX/AutoPP</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An automated pipeline (AutoPP) for end-to-end product poster generation and optimization, requiring only basic product information as input. 2. A novel optimization method that uses systematic element replacement and Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to specific poster elements. 3. The creation and release of AutoPP1M, the largest dataset for product poster generation and optimization, containing one million posters and user feedback.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces AutoPP, an automated pipeline that generates product posters from basic product information and then optimizes them for higher Click-Through Rate (CTR) using online feedback and a novel Isolated Direct Preference Optimization technique. It is supported by a large-scale dataset, AutoPP1M. Experiments show that AutoPP achieves state-of-the-art performance in both offline and online evaluations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Data relativistic uncertainty framework for low-illumination anime scenery image enhancement</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [low-light image enhancement], [Data Relativistic Uncertainty, Unsupervised Learning, EnlightenGAN, Anime Scenery, Domain Gap]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yiquan Gao, John See</p>
</li>
<li class="">
<p><strong>institution:</strong> Heriot-Watt University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21944" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21944</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Automated Discovery of Parsimonious Spectral Indices via Normalized Difference Polynomials</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [remote sensing, vegetation classification], [normalized difference polynomials, spectral indices, feature selection, Sentinel-2, illumination invariance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ali Lotfi, Adam Carter, Thuan Ha, Mohammad Meysami, Kwabena Nketia, Steve Shirtliffe</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Saskatchewan, New Mexico State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21948" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21948</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces an automated framework for generating a structured search space of spectral indices using pairwise normalized differences and polynomial combinations up to a fixed degree. 2. Employs feature selection methods (ANOVA filtering, recursive elimination, L1-regularized SVM) to select small, interpretable sets of indices that maintain high classification accuracy. 3. Demonstrates the effectiveness of the approach on a real-world task (Kochia detection), showing that simple degree-2 product indices achieve high accuracy and are deployable on platforms like Google Earth Engine.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/511f8067b02514f2cb415c41eb77aefa341affb7b0beba8fc78dcc37ebff9508_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/511f8067b02514f2cb415c41eb77aefa341affb7b0beba8fc78dcc37ebff9508_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces an automated method to discover simple spectral indices for vegetation classification by generating polynomial combinations of pairwise normalized differences and applying feature selection. The method was tested on detecting Kochia with Sentinel-2 imagery, where a single degree-2 index achieved over 96% accuracy. The results show that discriminative signals often come from spectral interactions, and the resulting indices are simple enough for direct deployment in cloud platforms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-modal robustness], [multi-modal large language model, input perturbation, training-free calibration, denoising, benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dunyuan XU, Xikai Yang, Yaoqian Li, Juzheng Miao, Jinpeng Li, Pheng-Ann Heng</p>
</li>
<li class="">
<p><strong>institution:</strong> The Chinese University of Hong Kong (CUHK)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21964" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21964</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A systematic analysis of the impact of various visual and textual perturbations on medical MLLMs, revealing their sensitivity. 2. A novel training-free Inherent-enhanced Multi-modal Calibration (IMC) framework that leverages MLLMs&#x27; own capabilities for robustness enhancement. 3. The introduction of two specific methods within IMC: Perturbation-aware Denoising Calibration (PDC) for visual noise and a Self-instantiated Multi-agent System (SMS) for textual noise.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252aad7806c9998b636d3335e0a4e90e437f57a51e2a942eb6d7b871a984ef2b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252aad7806c9998b636d3335e0a4e90e437f57a51e2a942eb6d7b871a984ef2b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the vulnerability of Medical Multi-modal Large Language Models (MLLMs) to input noise like imaging artifacts and text errors. It proposes a training-free framework called Inherent-enhanced Multi-modal Calibration (IMC) that uses the model&#x27;s own vision encoder and self-assessment capabilities to denoise inputs. Experiments on a new benchmark show the method achieves state-of-the-art robustness, enhancing clinical applicability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] LVLM-Aided Alignment of Task-Specific Vision Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [model alignment and interpretability], [LVLM-VA, spurious correlations, explainable AI (XAI), vision-language model, human-in-the-loop]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alexander Koebler, Lukas Kuhn, Ingo Thon, Florian Buettner</p>
</li>
<li class="">
<p><strong>institution:</strong> Goethe University Frankfurt, Siemens AG, German Cancer Research Center (DKFZ)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21985" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21985</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces LVLM-Aided Visual Alignment (LVLM-VA), a novel method for aligning small task-specific vision models with human domain knowledge using a Large Vision Language Model (LVLM). 2. Proposes a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling efficient expert-model interaction. 3. Demonstrates that the method effectively reduces the model&#x27;s dependence on spurious features and group-specific biases without requiring fine-grained, instance-level feedback.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c85ee676094853ba26d7711ac1d50d0ab994498bc2f91f2aaab4f1104d3222_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c85ee676094853ba26d7711ac1d50d0ab994498bc2f91f2aaab4f1104d3222_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of small task-specific vision models relying on spurious correlations, which leads to brittle real-world performance. The authors propose LVLM-Aided Visual Alignment (LVLM-VA), a method that uses a Large Vision Language Model to create a bidirectional interface between human domain knowledge and the model, translating explanations and critiques. The method is shown to significantly improve model alignment with human specifications and reduce dependence on spurious features across synthetic and real-world datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Lightweight Multi-Scale Attention Framework for Real-Time Spinal Endoscopic Instance Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [instance segmentation], [re-parameterized convolution, efficient multi-scale attention, lightweight multi-task head]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qi Lai, JunYan Li, Qiang Cai, Lei Wang, Tao Yan, XiaoKun Liang</p>
</li>
<li class="">
<p><strong>institution:</strong> The affiliations include IEEE members, suggesting an academic or research institution, but specific names are not fully listed in the provided text. Based on the GitHub URL pattern, a likely institution is not explicitly stated in the given content.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21984" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21984</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/hhwmortal/PELD-Instance-segmentation" target="_blank" rel="noopener noreferrer" class="">https://github.com/hhwmortal/PELD-Instance-segmentation</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed LMSF-A, a lightweight multi-scale attention framework co-designed across backbone, neck, and head for real-time instance segmentation. 2. Introduced the C2f-Pro backbone module combining RepViT-style re-parameterized convolution with efficient multi-scale attention for multi-branch training and fast inference. 3. Released a new clinically reviewed PELD dataset for spinal endoscopic instance segmentation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ddb0a22c2e72d9e15e574413f3c78c60ca63b7152d6320583b76bc00a64110f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ddb0a22c2e72d9e15e574413f3c78c60ca63b7152d6320583b76bc00a64110f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of real-time instance segmentation in spinal endoscopy, where factors like a narrow field of view and hardware constraints make deployment difficult. The authors propose LMSF-A, a lightweight framework featuring a re-parameterized backbone, a feature fusion neck, and a shared head, which achieves competitive accuracy with only 1.8M parameters. The method is validated on a new clinical dataset and shows good generalization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [hallucination mitigation, adversarial parametric editing, parameter clustering, visual-language models, activation dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji, Zhongshi He</p>
</li>
<li class="">
<p><strong>institution:</strong> Chongqing University, Xinjiang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21999" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21999</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/hujiayu1223/ALEAHallu" target="_blank" rel="noopener noreferrer" class="">https://github.com/hujiayu1223/ALEAHallu</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel adversarial parametric editing framework (ALEAHallu) following an Activate-Locate-Edit Adversarially paradigm for mitigating hallucinations in VLMs. 2. Introduces a method to construct an activation dataset with grounded and hallucinatory response pairs and identifies critical hallucination-prone parameter clusters via differential hidden state analysis. 3. Demonstrates the framework&#x27;s effectiveness by fine-tuning identified parameter clusters with adversarially tuned prefixes to force the model to prioritize visual evidence over linguistic priors.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the hallucination problem in Vision-Language Models (VLMs), where models generate content misaligned with visual inputs due to over-reliance on linguistic priors. The authors propose ALEAHallu, an adversarial parametric editing framework that identifies and fine-tunes hallucination-prone parameter clusters using adversarially optimized prompts to enhance visual grounding. Evaluations show the method significantly reduces hallucinations in both generative and discriminative VLM tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [multimodal large language models (MLLMs), slow-fast inference, adaptive perception, visual grounding, lightweight agent]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sarthak Mehrotra, Sairam V C Rebbapragada, Mani Hemanth Reddy Bonthu, Vineeth N Balasubramanian</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology, Bombay, Indian Institute of Technology, Hyderabad</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22009" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22009</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces iSHIFT, a lightweight (2.5B parameter) agent framework that integrates implicit chain-of-thought reasoning with a perception control module. 2. Proposes a novel slow-fast hybrid inference mechanism, allowing the model to adaptively switch between a detailed, high-precision &quot;slow mode&quot; and an efficient &quot;fast mode&quot; based on task demands. 3. Employs special perception tokens to dynamically guide the model&#x27;s visual attention to relevant screen regions, enabling precise visual grounding for GUI interaction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6067c294acdf08cb7927ad962f2d0d2c4f3efd938837b5cdb9ca1bdad4019422_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6067c294acdf08cb7927ad962f2d0d2c4f3efd938837b5cdb9ca1bdad4019422_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of building efficient yet precise GUI agents by proposing iSHIFT, a lightweight MLLM agent. iSHIFT uses an adaptive slow-fast inference mechanism and perception tokens to dynamically balance efficiency and accuracy for GUI tasks. Despite its small 2.5B size, it achieves state-of-the-art performance on multiple benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [vision-and-language navigation], [spatiotemporal context modeling, slot-based compression, prompt-guided multimodal integration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wen Jiang, Li Wang, Kangyao Huang, Wei Fan, Jinyuan Liu, Shaoyu Liu, Hongwei Duan, Bin Xu, Xiangyang Ji</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Institute of Technology, Tsinghua University, Dalian University of Technology, Xidian University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22010" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22010</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A slot-based historical image compression module to distill multi-view historical observations into fixed-length contextual representations. 2. A spatiotemporal trajectory encoding module to capture the temporal dynamics and spatial structure of UAV trajectories. 3. A prompt-guided multimodal integration module to fuse spatiotemporal context with current observations for robust waypoint prediction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c00484796a9df425f1884ea8ae22314b0592466ebe305303cf7f1f0c467b528_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c00484796a9df425f1884ea8ae22314b0592466ebe305303cf7f1f0c467b528_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes LongFly, a framework for long-horizon UAV vision-and-language navigation that addresses the challenge of modeling spatiotemporal context. The method integrates a history-aware modeling strategy with modules for compressing past observations, encoding trajectories, and fusing multimodal information. Experimental results show it outperforms state-of-the-art baselines in navigation success metrics across seen and unseen environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [Human-Computer Interaction (HCI)], [Gestural Interaction, Physics Simulation, Air-drawn Sketches, VR Content Creation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiangwen Zhang, Xiaowei Dai, Runnan Chen, Xiaoming Chen, Zeke Zexi Hu</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Technology and Business University, The University of Sydney</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22016" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22016</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel VR interaction framework (SketchPlay) that combines air-drawn sketches and gestures to create dynamic, physically realistic scenes. 2. A method that uses sketches to capture object/scene structure and gestures to convey physical cues (velocity, force) for defining motion and behavior. 3. Enables the generation of complex physical phenomena (rigid body motion, elastic deformation, cloth dynamics) through an intuitive, controller-free creation process.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79eefb65bc566df3d83196a3500892e4d09f26b4aa064a68193142a9ec59b0c0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79eefb65bc566df3d83196a3500892e4d09f26b4aa064a68193142a9ec59b0c0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes SketchPlay, a VR framework that allows users to create physically realistic dynamic scenes by sketching objects in the air and using gestures to define their motion. This method combines structural and dynamic intent to simulate phenomena like rigid body and cloth dynamics. The approach is shown to be more expressive and offer a better user experience than text-driven methods, lowering the barrier for non-expert creators.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Patch-Discontinuity Mining for Generalized Deepfake Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [deepfake detection], [patch-discontinuity, feature space redistribution, classification-invariant feature augmentation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Huanhuan Yuan, Yang Ping, Zhengqin Xu, Junyi Cao, Shuai Jia, Chao Ma</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, Chinese Academy of Military Science</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22027" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22027</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://gendf.github.io/" target="_blank" rel="noopener noreferrer" class="">https://gendf.github.io/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a deepfake-specific representation learning (DSRL) scheme to capture patch-discontinuity patterns in fake images and continuity in real ones. 2. Introduced a feature space redistribution (FSR) scheme to separately optimize the distributions of real and fake features, mitigating domain mismatch. 3. Designed a classification-invariant feature augmentation (CIFAug) strategy to enhance generalization by expanding feature scopes orthogonally to the classification direction without adding trainable parameters.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f93245fe55a81a22e8997db22f045beb4494e864df08fd145bf36088517c580_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f93245fe55a81a22e8997db22f045beb4494e864df08fd145bf36088517c580_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes GenDF, a generalized deepfake detection framework that transfers a large-scale vision model to detect fake faces by mining patch-discontinuity patterns. It introduces three key components—deepfake-specific representation learning, feature space redistribution, and a parameter-free feature augmentation strategy—to improve generalization to unseen forgery methods. The method achieves state-of-the-art cross-domain performance with only 0.28M trainable parameters, demonstrating high efficiency and effectiveness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [backdoor attacks], [video segmentation foundation models, backdoor attack, two-stage training, gradient analysis, attention shift]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zongmin Zhang, Zhen Sun, Yifan Liao, Wenhan Dong, Xinlei He, Xingshuo Han, Shengmin Xu, Xinyi Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Hong Kong University of Science and Technology (Guangzhou), Nanjing University of Aeronautics and Astronautics, Fujian Normal University, Jinan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22046" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22046</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies the ineffectiveness of classic backdoor attacks on prompt-driven Video Segmentation Foundation Models (VSFMs) and provides analysis via gradient and attention maps. 2. Proposes BadVSFM, the first dedicated backdoor attack framework for VSFMs, using a novel two-stage training strategy to separate clean and triggered representations. 3. Demonstrates strong, controllable attack performance across multiple models and datasets while preserving clean-task accuracy, and shows the vulnerability persists against existing defenses.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a7e9ff0ffc63f2085d6ca65db8e87f14fed435be5d8a51fd3d1265b22b668b1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a7e9ff0ffc63f2085d6ca65db8e87f14fed435be5d8a51fd3d1265b22b668b1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies that standard backdoor attacks fail on prompt-driven Video Segmentation Foundation Models (VSFMs) like SAM2. To solve this, the authors propose BadVSFM, a two-stage backdoor attack framework that successfully implants a controllable backdoor by steering the encoder and decoder separately. Experiments show the attack is effective and evades current defenses, revealing a significant security vulnerability in VSFMs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [autoregressive distillation, adversarial refinement, real-time streaming, reference-anchored positional re-encoding, consistency-aware discriminator]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhiyao Sun, Ziqiao Peng, Yifeng Ma, Yi Chen, Zhengguang Zhou, Zixiang Zhou, Guozhen Zhang, Youliang Zhang, Yuan Zhou, Qinglin Lu, Yong-Jin Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Renmin University of China, Tencent Hunyuan, Nanjing University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22065" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22065</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://streamavatar.github.io" target="_blank" rel="noopener noreferrer" class="">https://streamavatar.github.io</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A two-stage autoregressive adaptation and acceleration framework (autoregressive distillation + adversarial refinement) to adapt a non-causal human video diffusion model for real-time, interactive streaming. 2. Three novel components to ensure long-term stability and consistency: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. 3. A one-shot, interactive human avatar model capable of generating both natural talking and listening behaviors with coherent full-body gestures, surpassing existing methods in quality, efficiency, and interaction naturalness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of making diffusion-based human avatar generation suitable for real-time, interactive streaming. It proposes StreamAvatar, a two-stage framework that adapts a high-fidelity human video diffusion model using autoregressive distillation and adversarial refinement, incorporating novel components for long-term consistency. The method achieves state-of-the-art performance in generating high-resolution, full-body interactive avatars with natural talking/listening behaviors in real-time.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] MAI-UI Technical Report: Real-World Centric Foundation GUI Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [GUI agent, device-cloud collaboration, online reinforcement learning, self-evolving data pipeline, foundation model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hanzhang Zhou, Xu Zhang, Panrong Tong, Jianan Zhang, Liangyu Chen, Quyu Kong, Chenglin Cai, Chen Liu, Yue Wang, Jingren Zhou, Steven Hoi</p>
</li>
<li class="">
<p><strong>institution:</strong> Tongyi Lab, Alibaba Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22047" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22047</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Tongyi-MAI/MAI-UI" target="_blank" rel="noopener noreferrer" class="">https://github.com/Tongyi-MAI/MAI-UI</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A self-evolving data pipeline that expands GUI navigation data to include user interaction and tool calls. 2. A native device-cloud collaboration system that routes task execution based on state to improve efficiency and privacy. 3. An online RL framework with optimizations for scaling parallel environments and context length.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/819d67f31c44f0847dff819fbdf82773fea54c76d001f429e9e731b2ba253b85_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/819d67f31c44f0847dff819fbdf82773fea54c76d001f429e9e731b2ba253b85_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents MAI-UI, a family of foundation GUI agents designed to address challenges in real-world deployment, such as limited interaction and dynamic environments. Its unified methodology includes a self-evolving data pipeline, a device-cloud collaboration system, and an online RL framework. MAI-UI achieves state-of-the-art performance on multiple GUI grounding and mobile navigation benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Yume-1.5: A Text-Controlled Interactive World Generation Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [interactive world generation, long-video generation, attention distillation, context compression, text-controlled generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiaofeng Mao, Zhen Li, Chuanhao Li, Xiaojie Xu, Kaining Ying, Tong He, Jiangmiao Pang, Yu Qiao, Kaipeng Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai AI Laboratory, Fudan University, Shanghai Innovation Institute</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22096" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22096</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/stdstu12/YUME" target="_blank" rel="noopener noreferrer" class="">https://github.com/stdstu12/YUME</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A long-video generation framework integrating unified context compression with linear attention. 2. A real-time streaming acceleration strategy using bidirectional attention distillation and an enhanced text embedding scheme. 3. A text-controlled method for generating world events.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f7ffd3f0a90ba67551ade4e28abf8e27d5d08c106e463f85f9447011008416b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f7ffd3f0a90ba67551ade4e28abf8e27d5d08c106e463f85f9447011008416b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Yume-1.5, a framework to address challenges in generating interactive, explorable worlds using diffusion models, such as large model size and slow inference. The method introduces a novel architecture combining context compression, attention distillation, and text-based event control to enable real-time, keyboard-controlled world generation from text or images. The work concludes with a public codebase demonstrating the feasibility of text-controlled interactive world creation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Learning Association via Track-Detection Matching for Multi-Object Tracking</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multi-object tracking], [tracking-by-detection, link prediction, association learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Momir Adžemović</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Belgrade</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22105" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22105</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Robotmurlock/TDLP" target="_blank" rel="noopener noreferrer" class="">https://github.com/Robotmurlock/TDLP</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Track-Detection Link Prediction (TDLP), a tracking-by-detection method that learns data association via link prediction between tracks and detections, eliminating handcrafted heuristics. 2. Demonstrates that TDLP achieves state-of-the-art performance across multiple benchmarks, outperforming both traditional tracking-by-detection and end-to-end methods. 3. Provides analysis showing link prediction is more effective than metric learning for association, especially when handling heterogeneous features like bounding boxes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d61776d860da3a903f769a1427363e91df9be50d56b83666c73ac36008099062_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d61776d860da3a903f769a1427363e91df9be50d56b83666c73ac36008099062_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes TDLP, a tracking-by-detection method that learns to associate object detections across video frames by predicting links between existing tracks and new detections. This approach avoids handcrafted rules while remaining computationally efficient. Experiments show it outperforms state-of-the-art methods, and analysis indicates link prediction is superior to metric learning for this association task.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [visual question answering], [perceptual shaping, KL-consistency, KL-separation, evidence-preserving view, evidence-ablated view]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Yujiu Yang, Rui Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Microsoft Research, Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22120" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22120</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/zss02/BiPS" target="_blank" rel="noopener noreferrer" class="">https://github.com/zss02/BiPS</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Bi-directional Perceptual Shaping (BiPS), a novel training paradigm that uses question-conditioned masked views to generate bidirectional &quot;where-to-look&quot; signals. 2. Introduces a dual KL constraint mechanism: a KL-consistency constraint to encourage complete visual evidence coverage and a KL-separation constraint to discourage text-only shortcuts and enforce fine-grained visual reliance. 3. Demonstrates strong performance improvements and out-of-domain generalization across multiple benchmarks without adding inference-time cost.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74efcdb66ae5cc2a16fd7b59382dce3e78d24b03e998520275db9a96986fa158_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74efcdb66ae5cc2a16fd7b59382dce3e78d24b03e998520275db9a96986fa158_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that large vision-language models often overlook fine-grained visual evidence and rely on text shortcuts. It proposes Bi-directional Perceptual Shaping (BiPS), a training method that uses KL constraints on evidence-preserving and evidence-ablated image views to shape the model&#x27;s perception. The method significantly boosts the performance of a base VLM model and shows strong generalization to unseen data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] ProEdit: Inversion-based Editing From Prompts Done Right</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image editing], [inversion-based editing, KV-mix, Latents-Shift, plug-and-play, flow inversion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhi Ouyang, Dian Zheng, Xiao-Ming Wu, Jian-Jian Jiang, Kun-Yu Lin, Jingke Meng, Wei-Shi Zheng</p>
</li>
<li class="">
<p><strong>institution:</strong> Sun Yat-sen University, CUHK MMLab, Nanyang Technological University, The University of Hong Kong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22118" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22118</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://isee-laboratory.github.io/ProEdit/" target="_blank" rel="noopener noreferrer" class="">https://isee-laboratory.github.io/ProEdit/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes KV-mix to mix source and target KV features in the attention mechanism, reducing source influence in edited regions while preserving background. 2. Introduces Latents-Shift to perturb the source latent in the edited region, eliminating the influence of the inverted latent during sampling. 3. Presents a plug-and-play design that can be integrated into existing inversion-based editing methods like RF-Solver, FireFlow, and UniEdit.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/878121cf01366a9dcec34198113a6d864c4ea37e9b41c39e221a71b26e72039f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/878121cf01366a9dcec34198113a6d864c4ea37e9b41c39e221a71b26e72039f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem in inversion-based visual editing where over-reliance on source image information hinders accurate attribute changes. The proposed method, ProEdit, introduces two techniques: KV-mix in the attention aspect and Latents-Shift in the latent aspect, to mitigate source influence. It achieves state-of-the-art performance on image and video editing benchmarks and is designed as a plug-and-play module compatible with existing methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Graph-Augmented knowledge Distillation based Dual-Stream Vision Transformer with Region-Aware Attention for Gastrointestinal Disease Classification with Explainable AI</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [Knowledge Distillation, Vision Transformer, Swin Transformer, Explainable AI, Wireless Capsule Endoscopy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Assaduzzaman, Nushrat Jahan Oyshi, Eram Mahamud</p>
</li>
<li class="">
<p><strong>institution:</strong> Daffodil International University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21372" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21372</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a hybrid dual-stream teacher model combining Swin Transformer for global context and Vision Transformer for local features. 2. Developed a compact Tiny-ViT student model via knowledge distillation to balance accuracy and efficiency for clinical deployment. 3. Validated the model&#x27;s clinical relevance through extensive interpretability analysis using Grad-CAM, LIME, and Score-CAM.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cd724a13af61d1181b015dc7b5fe86e10cc834beac172261c5bebe54e371bc3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cd724a13af61d1181b015dc7b5fe86e10cc834beac172261c5bebe54e371bc3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of classifying gastrointestinal diseases from endoscopic images by proposing a knowledge distillation framework. A high-capacity dual-stream teacher model guides a compact Tiny-ViT student, achieving near-perfect accuracy on WCE datasets while providing explainable predictions. The work offers an efficient and interpretable solution suitable for resource-constrained clinical environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [diffusion models, generative modeling, evidence lower bound, residual learning, two-stage framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Takuro Kutsuna</p>
</li>
<li class="">
<p><strong>institution:</strong> Toyota Central R&amp;D Labs., Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21593" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21593</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image deblurring], [lightweight network, real-time inference, edge deployment, U-shaped architecture, motion blur]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhuoyu Wu, Wenhui Ou, Qiawei Zheng, Jiayan Yang, Quanjun Wang, Wenqi Fang, Zheng Wang, Yongkui Yang, Heshan Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Unifyware Co., Ltd.; Monash University; Hong Kong University of Science and Technology; Shenzhen Infynova Co., Ltd.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21975" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21975</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/ReaganWu/RT-Focuser" target="_blank" rel="noopener noreferrer" class="">https://github.com/ReaganWu/RT-Focuser</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a lightweight U-shaped network (RT-Focuser) with a Lightweight Deblurring Block (LD) for edge-aware feature extraction. 2. Introduces a Multi-Level Integrated Aggregation module (MLIA) for encoder feature integration. 3. Designs a Cross-source Fusion Block (X-Fuse) for progressive decoder refinement.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d65834719412d7909823764ddbf757cadad16b1b273fd3e67cf177cad638b9d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d65834719412d7909823764ddbf757cadad16b1b273fd3e67cf177cad638b9d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes RT-Focuser, a lightweight U-shaped neural network designed for real-time image deblurring on edge devices. It achieves a balance between speed and accuracy with only 5.85M parameters, running at over 140 FPS on both GPU and mobile platforms. The model demonstrates strong potential for deployment in real-time applications like autonomous driving and UAV perception.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] The Color-Clinical Decoupling: Why Perceptual Calibration Fails Clinical Biomarkers in Smartphone Dermatology</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical imaging], [colorimetric calibration, clinical biomarkers, Individual Typology Angle (ITA), Melanin Index, intraclass correlation coefficient (ICC)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sungwoo Kang</p>
</li>
<li class="">
<p><strong>institution:</strong> Korea University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21988" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21988</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and defines the &quot;color-clinical decoupling&quot; phenomenon, where perceptual color accuracy does not guarantee reliability of clinical biomarkers. 2. Demonstrates that anatomical variance (facial region) is a dominant source of color variance, significantly outweighing device effects, challenging single-patch calibration. 3. Provides a mathematical explanation for the decoupling by showing the ITA biomarker&#x27;s disproportionate sensitivity to noise in the b* color channel.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fbc8b90f041b184cdc9f22e1063fa065b2e1f8b098825058b11e381c89ffee7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fbc8b90f041b184cdc9f22e1063fa065b2e1f8b098825058b11e381c89ffee7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the assumption that colorimetric calibration ensures reliable clinical biomarker extraction in smartphone dermatology. Using a large dataset and standard calibration, it finds that while color error is reduced, biomarker reliability remains poor, a phenomenon termed &quot;color-clinical decoupling,&quot; primarily due to anatomical variance and formula sensitivity. The conclusion is that current calibration standards are insufficient for clinical use and require region-aware protocols.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-29T12:06:29.000Z" itemprop="dateModified">Dec 29, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/cscv/20251222-20251228"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251222-20251228 (cs.CV)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/category/cscy"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">cs.CY</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-29" class="table-of-contents__link toc-highlight">2025-12-29</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/cscv/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>