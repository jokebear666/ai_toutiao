<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_CV/20260105-20260111" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20260105-20260111 (cs.CV) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/cscv/20260105-20260111"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20260105-20260111 (cs.CV) | AI头条"><meta data-rh="true" name="description" content="2026-01-05"><meta data-rh="true" property="og:description" content="2026-01-05"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/cscv/20260105-20260111"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cscv/20260105-20260111" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cscv/20260105-20260111" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.CV","item":"https://jokebear666.github.io/ai_toutiao/daily/cscv"},{"@type":"ListItem","position":3,"name":"20260105-20260111 (cs.CV)","item":"https://jokebear666.github.io/ai_toutiao/daily/cscv/20260105-20260111"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.647bd1ff.css">
<script src="/ai_toutiao/assets/js/runtime~main.a5a9d6f2.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.8703b74f.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/arxiv-daily"><span title="Arxiv每日论文" class="linkLabel_WmDU">Arxiv每日论文</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Collapse sidebar category &#x27;cs.CV&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cs_CV/20251215-20251221"><span title="20251215-20251221 (cs.CV)" class="linkLabel_WmDU">20251215-20251221 (cs.CV)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cscv/20251222-20251228"><span title="20251222-20251228 (cs.CV)" class="linkLabel_WmDU">20251222-20251228 (cs.CV)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cscv/20251229-20260104"><span title="20251229-20260104 (cs.CV)" class="linkLabel_WmDU">20251229-20260104 (cs.CV)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/cscv/20260105-20260111"><span title="20260105-20260111 (cs.CV)" class="linkLabel_WmDU">20260105-20260111 (cs.CV)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csoh"><span title="cs.OH" class="categoryLinkLabel_W154">cs.OH</span></a><button aria-label="Expand sidebar category &#x27;cs.OH&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20260105-20260111"><span title="20260105-20260111" class="linkLabel_WmDU">20260105-20260111</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/daily/cscv"><span>cs.CV</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20260105-20260111 (cs.CV)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20260105-20260111 (cs.CV)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2026-01-05">2026-01-05<a href="#2026-01-05" class="hash-link" aria-label="Direct link to 2026-01-05" title="Direct link to 2026-01-05" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv260105] From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [generative AI, diffusion models, architectural intelligence, computational reasoning, vernacular architecture]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abolhassan Pishahang, Maryam Badiei</p>
</li>
<li class="">
<p><strong>institution:</strong> Florida Atlantic University, North Carolina State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00029" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00029</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a three-stage prompting methodology (referential, adaptive, speculative) to evaluate generative AI&#x27;s interpretation of vernacular architecture. 2. Develops a five-criteria evaluation framework (typology, materiality, environment, realism, cultural specificity) to assess AI-generated architectural outputs. 3. Identifies a boundary between visual resemblance and architectural reasoning in AI, introducing the concept of &quot;computational vernacular reasoning&quot; as an analytical framework.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d658a502216dc69f4f977616d5c09ec4155b48e6d602df132e1eac107fa169f4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d658a502216dc69f4f977616d5c09ec4155b48e6d602df132e1eac107fa169f4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study investigates how generative AI interprets the architectural intelligence of vernacular forms, using Iranian pigeon towers as a case study. It tests three diffusion models (Midjourney, DALL-E 3, Stable Diffusion XL) across different prompt stages and evaluates outputs using a custom framework. The results show that AI reliably reproduces geometric patterns but fails to grasp underlying material and climatic reasoning, highlighting a gap between visual generation and true architectural understanding.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] It&#x27;s Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [diffusion models], [mode collapse, noise optimization, frequency characteristics, text-to-image generation, inference-time scaling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Anne Harrington, A. Sophia Koepke, Shyamgopal Karthik, Trevor Darrell, Alexei A. Efros</p>
</li>
<li class="">
<p><strong>institution:</strong> UC Berkeley, University of Tübingen (Tübingen AI Center), Technical University of Munich (MCML)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00090" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00090</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a simple noise optimization objective to mitigate mode collapse in trained diffusion models while preserving model fidelity. 2. Analyzes the frequency characteristics of noise and demonstrates that alternative noise initializations with different frequency profiles can improve optimization and search. 3. Empirically shows that the proposed noise optimization method yields superior results in generation quality and variety compared to existing approaches.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90471f45dfeeb47a677f1a1f9518845473c93d7756e36367a947d6ebb4031c24_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90471f45dfeeb47a677f1a1f9518845473c93d7756e36367a947d6ebb4031c24_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of mode collapse in text-to-image diffusion models, where repeated sampling with the same prompt yields nearly identical images. The proposed solution is to optimize the initial noise input to the model to encourage diverse outputs, while also analyzing and leveraging the frequency characteristics of the noise for better performance. The experiments demonstrate that this noise optimization approach effectively recovers diversity without compromising the quality of the generated images.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [4D world model, autoregressive diffusion, Macro-from-Micro Planning (MMPL), Distribution Matching Distillation (DMD), generation-reconstruction-guidance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yabo Chen, Yuanzhi Liang, Jiepeng Wang, Tingxi Chen, Junfei Cheng, Zixiao Gu, Yuyang Huang, Zicheng Jiang, Wei Li, Tian Li, Weichen Li, Zuoxin Li, Guangce Liu, Jialun Liu, Junqi Liu, Haoyuan Wang, Qizhen Weng, Xuan&#x27;er Wu, Xunzhi Xiang, Xiaoyan Yang, Xin Zhang, Shiwen Zhang, Junyu Zhou, Chengcheng Zhou, Haibin Huang, Chi Zhang, Xuelong Li</p>
</li>
<li class="">
<p><strong>institution:</strong> TeleWorld Team (Institution inferred from corresponding author&#x27;s email domain: ieee.org, but specific institution not explicitly stated in provided content. Likely an academic or research lab.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00051" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00051</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term memory in a closed-loop system. 2. Introduces a novel generation-reconstruction-guidance paradigm where a reconstructed 4D spatio-temporal representation guides subsequent generation for consistency. 3. Employs an autoregressive diffusion video model enhanced with Macro-from-Micro Planning (MMPL) and Distribution Matching Distillation (DMD) for efficient, long-horizon, real-time synthesis.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd22588d6a4323504eff6af53ad388e6f896282e336d6c9dc1169ff7797cfd75_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd22588d6a4323504eff6af53ad388e6f896282e336d6c9dc1169ff7797cfd75_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents TeleWorld, a framework for building practical world models by integrating video generation and dynamic scene reconstruction into a closed-loop 4D system. It uses a novel generation-reconstruction-guidance paradigm and efficient planning/distillation techniques to achieve long-term consistency and real-time performance. The results demonstrate strong performance in world understanding and generation, advancing towards interactive, memory-enabled AI systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal reasoning], [spatial intelligence, multimodal large language models, benchmark, spatiotemporal reasoning, evaluation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Pan Wang, Yang Liu, Guile Wu, Eduardo R. Corral-Soto, Chengjie Huang, Binbin Xu, Dongfeng Bai, Xu Yan, Yuan Ren, Xingxin Chen, Yizhe Wu, Tao Huang, Wenjun Wan, Xin Wu, Pei Zhou, Xuyang Dai, Kangbo Lv, Hongbo Zhang, Yosef Fried, Aixue Ye, Bailan Feng, Zhenyu Chen, Zhen Li, Yingcong Chen, Yiyi Liao, Bingbing Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Huawei Technologies, CUHK-Shenzhen, HKUST-GZ, Zhejiang University, Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00092" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00092</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://spatial4d-bench.github.io/spatial4d/" target="_blank" rel="noopener noreferrer" class="">https://spatial4d-bench.github.io/spatial4d/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Spatial4D-Bench, a large-scale benchmark with ~40,000 QA pairs for evaluating 4D spatial intelligence in MLLMs. 2. Systematically organizes 18 tasks into six cognitive categories for structured and comprehensive assessment. 3. Benchmarks state-of-the-art MLLMs, revealing their substantial limitations in 4D spatial reasoning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0097cea07eeada4f8c5abb88f65e48c3017620922cc4628bd067545dd73a10f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0097cea07eeada4f8c5abb88f65e48c3017620922cc4628bd067545dd73a10f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Spatial4D-Bench, a large-scale benchmark designed to comprehensively evaluate the 4D spatial reasoning abilities of Multimodal Large Language Models (MLLMs). The benchmark covers 18 tasks across six cognitive categories. The evaluation reveals that current MLLMs have significant limitations in achieving human-level 4D spatial intelligence.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal image segmentation], [Synthetic Aperture Radar (SAR), Multispectral Imaging (MSI), Spatially Masked Adaptive Gated Network (SMAGNet), feature fusion, missing data robustness]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hyunho Lee, Wenwen Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Arizona State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00123" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00123</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SMAGNet, a novel multimodal deep learning model for post-flood water extent mapping that uses SAR as the primary input and adaptively integrates MSI data. 2. Introduces a method for handling incomplete or partially available MSI data, enhancing model applicability in real-world scenarios. 3. Demonstrates superior performance and robustness compared to other multimodal models, maintaining performance even when MSI data is completely missing.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/301db27cf6a0b833c578daa334eb3b7d3ec33f0947279912344e4d78cc3853d5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/301db27cf6a0b833c578daa334eb3b7d3ec33f0947279912344e4d78cc3853d5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of integrating incomplete multispectral data with SAR imagery for post-flood water mapping. It proposes SMAGNet, a multimodal deep learning model that adaptively fuses features from both data sources. The model shows improved accuracy and robustness to missing data, making it more practical for real-world disaster response.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Explicit Abstention Knobs for Predictable Reliability in Video Question Answering</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [selective prediction], [selective prediction, confidence-based abstention, risk-coverage tradeoff, distribution shift, video question answering]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jorge Ortiz</p>
</li>
<li class="">
<p><strong>institution:</strong> Rutgers University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00138" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00138</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that confidence thresholding provides smooth, mechanistic control over error rates in-distribution for video QA. 2. Shows that this confidence-based control is not epistemic and fails under distribution shift (evidence degradation), as confidence does not decrease with reduced visual information. 3. Proposes the need for warrant-based selective prediction, where confidence is explicitly bounded by the supporting evidence.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c77afc466868547720556984ed3dee075c707916f0173e73e7904851983d320b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c77afc466868547720556984ed3dee075c707916f0173e73e7904851983d320b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the reliability of confidence-based abstention for controlling error rates in video question answering using VLMs. It finds that while confidence thresholding works well in-distribution, it fails under distribution shift because the model&#x27;s confidence does not properly reflect reduced evidence quality. The results motivate moving towards warrant-based selective prediction for more predictable reliability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Compressed Map Priors for 3D Perception</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D object detection], [spatial priors, binarized hashmap, map compression, nuScenes dataset, end-to-end training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Brady Zhou, Philipp Krähenbühl</p>
</li>
<li class="">
<p><strong>institution:</strong> UT Austin</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00139" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00139</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Compressed Map Priors (CMP), a framework to learn spatial priors from historic traversals for 3D perception. 2. Introduces a highly efficient binarized hashmap representation requiring only 32KB/km², achieving a 20x memory reduction. 3. Demonstrates seamless integration into existing 3D perception architectures with minimal computational overhead, leading to consistent performance improvements on nuScenes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32160e88a58c9af3e298b04ccdb16e8b3661883dc08a9dacc052f6a0dca3ed36_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32160e88a58c9af3e298b04ccdb16e8b3661883dc08a9dacc052f6a0dca3ed36_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that autonomous vehicle perception systems ignore prior knowledge from repeated traversals of the same area. It proposes Compressed Map Priors (CMP), a framework that learns and stores spatial priors in a highly compressed binarized hashmap. The method integrates easily into existing systems with little computational cost and significantly improves 3D object detection performance on the nuScenes dataset.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image forensics / ai-generated content detection], [GLASS, stratified sampling, global-local attention, high-resolution image detection, vision transformer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lawrence Han</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent researcher (inferred from personal email domain)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00141" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00141</a></p>
</li>
<li class="">
<p><strong>code:</strong> GitHub Project Code</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GLASS, a novel architecture that combines a globally resized view with multiple original-resolution local crops for AI-generated image detection. 2. Introduces a spatially stratified sampling method to efficiently select diverse, non-overlapping local regions from high-resolution images. 3. Demonstrates the integration and effectiveness of GLASS with various backbone models (ViT, ResNet, ConvNeXt), showing superior performance over standard transfer learning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c2089b3b64b3853b86afa92498a4cd117de1fac49f0b90abfb199ea7602ca9d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c2089b3b64b3853b86afa92498a4cd117de1fac49f0b90abfb199ea7602ca9d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of losing fine-grained details when detecting AI-generated images due to standard downsampling. It proposes the GLASS architecture, which processes both a global resized view and multiple original-resolution local crops, aggregated via attention. Experiments show GLASS improves detection performance across different model backbones within practical computational limits.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [facial attribute analysis], [multi-attribute description, facial action units, vision-language model, fine-tuning, region-focal analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kaiwen Zheng, Junchen Fu, Songpei Xu, Yaoqing He, Joemon M.Jose, Han Hu, Xuri Ge</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Glasgow, Shandong University, Institute of Computing Technology, Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00156" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00156</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the novel problem (FaceFocalDesc) of generating multi-attribute natural language descriptions for arbitrarily selected face regions. 2. Constructs a new dataset with region-level annotations and natural language descriptions for this task. 3. Proposes the Focal-RegionFace model, a fine-tuned vision-language model that incrementally refines focus on localized features for interpretable multi-attribute analysis.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3349e43a2ecc55b86da3398a05bd86f3d3f6b4908e47c1d46e4b116fd1bd901_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3349e43a2ecc55b86da3398a05bd86f3d3f6b4908e47c1d46e4b116fd1bd901_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a new task of generating fine-grained, multi-attribute descriptions for specific face regions. To address it, the authors create a new dataset and propose Focal-RegionFace, a model fine-tuned from Qwen2.5-VL that progressively focuses on local features. Experiments show the model achieves state-of-the-art performance on the new benchmark, demonstrating its effectiveness for region-focal facial analysis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal benchmark], [financial credit, multimodal AI, robustness evaluation, vision-language models, privacy-compliant dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yehui Yang, Dalu Yang, Wenshuo Zhou, Fangxin Shang, Yifan Liu, Jie Ren, Haojun Fei, Qing Yang, Tao Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Qifu Technology, Fudan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00150" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00150</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces FCMBench-V1.0, a large-scale, privacy-compliant multimodal benchmark specifically for financial credit applications, featuring 18 document types, 4,043 images, and 8,446 QA samples. 2. Proposes a novel three-dimensional evaluation framework (Perception, Reasoning, Robustness) with credit-specific reasoning tasks and real-world artifact stress testing. 3. Designs a closed synthesis-capture pipeline to construct realistic yet privacy-safe samples, mitigating data leakage and enabling effective performance discrimination across 23 state-of-the-art VLMs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a51bae803814f634858ded96030e47cb55c5330c7e8901ac175d3536cf4b4a9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a51bae803814f634858ded96030e47cb55c5330c7e8901ac175d3536cf4b4a9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces FCMBench, a new multimodal benchmark for evaluating AI models on financial credit document review tasks. The benchmark is built using a privacy-compliant synthesis-capture pipeline and tests models on perception, reasoning, and robustness. Experiments show that even top models like Gemini 3 Pro struggle with robustness, while the authors&#x27; domain-specific model, Qfin-VL-Instruct, achieves the best overall performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [hybrid feature engineering, wavelet decomposition, graph-theoretic descriptors, linear separability, model compression]</p>
</li>
<li class="">
<p><strong>authors:</strong> Moirangthem Tiken Singh, Manibhushan Yaikhom</p>
</li>
<li class="">
<p><strong>institution:</strong> Dibrugarh University Institute of Engineering and Technology (DUIET), Dibrugarh University; Regional Institute of Medical Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00192" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00192</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A resource-efficient, data-centric framework that makes high-dimensional ECG data linearly separable through hybrid feature engineering. 2. A novel feature space integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors like PageRank centrality. 3. An ultra-lightweight model achieving state-of-the-art efficiency (8.54 KB, 0.46 µs latency) for real-time arrhythmia detection on edge devices.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a662793cb6f13133cb7159786a9b30cdc33ffe10a9ca51a20ba5d501aa94a04_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a662793cb6f13133cb7159786a9b30cdc33ffe10a9ca51a20ba5d501aa94a04_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a resource-efficient framework for arrhythmia detection on edge devices by using hybrid feature engineering (wavelet and graph descriptors) to make ECG data linearly separable, enabling the use of ultra-lightweight linear classifiers. The resulting model achieves high accuracy (98.44%) with a very small footprint (8.54 KB) and low latency, offering significant efficiency gains over compressed deep learning models for IoMT applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [underwater image restoration], [conditional generative adversarial network, hyperspectral imagery, underwater image formation equation, satellite imagery, PRISMA]</p>
</li>
<li class="">
<p><strong>authors:</strong> Salma Gonzalez-Sabbagh, Antonio Robles-Kelly, Shang Gao</p>
</li>
<li class="">
<p><strong>institution:</strong> Deakin University, The University of Adelaide</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00194" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00194</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DichroGAN, a novel cGAN architecture for restoring in-air seafloor colours from satellite imagery. 2. Introduces a two-step simultaneous training process with four generators to model atmospheric radiance and underwater light transmission based on the image formation equation. 3. Demonstrates competitive performance on satellite and underwater datasets using a compact dataset derived from PRISMA imagery.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be68e0cd098733ddad1ebb98756053749a127b9805026bd451900ff62bd75701_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be68e0cd098733ddad1ebb98756053749a127b9805026bd451900ff62bd75701_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of restoring the true colours of the seafloor from satellite images, which are degraded by light absorption and scattering in water. The authors propose DichroGAN, a conditional GAN that uses a multi-generator architecture to estimate and remove underwater effects based on hyperspectral data. Experiments show the method achieves competitive results compared to state-of-the-art underwater restoration techniques.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D instance segmentation], [Neural Radiance Field (NeRF), 3D instance segmentation, crop counting, mask consistency, view synthesis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Ahmed Al Muzaddid, William J. Beksi</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Texas at Arlington</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00207" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00207</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel framework for exact crop enumeration via 3D instance segmentation using multi-view images and NeRF. 2. Introduction of crop visibility and mask consistency scores to effectively segment instances in 3D. 3. Demonstration of consistent performance across diverse crops (cotton, apples, pears) without crop-specific parameter tuning and release of a new cotton plant dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47e945ab54ccf5cb00069822c1c6264667ea9ffbd12ce279cdcb2f58a39c38ec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47e945ab54ccf5cb00069822c1c6264667ea9ffbd12ce279cdcb2f58a39c38ec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces CropNeRF, a framework for accurate crop counting in agriculture. It uses multi-view 2D images and instance masks to train a Neural Radiance Field (NeRF), incorporating novel visibility and consistency scores to perform 3D instance segmentation and count crops. The method shows superior counting performance across different crop types and releases a new dataset to advance research.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [domain adaptation], [style synthesis, contrastive learning, unpaired image translation, disentanglement]</p>
</li>
<li class="">
<p><strong>authors:</strong> Han Liu, Yubo Fan, Hao Li, Dewei Hu, Daniel Moyer, Zhoubing Xu, Benoit M. Dawant, Ipek Oguz</p>
</li>
<li class="">
<p><strong>institution:</strong> Vanderbilt University, Siemens Healthineers, Johnson &amp; Johnson Innovative Medicine</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00212" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00212</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/han-liu/IntraStyler" target="_blank" rel="noopener noreferrer" class="">https://github.com/han-liu/IntraStyler</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes IntraStyler, an exemplar-based style synthesis method for cross-modality domain adaptation that can capture diverse intra-domain styles without requiring prior knowledge of the variations. 2. Introduces a style encoder based on contrastive learning to discriminatively extract style-only features for guiding the synthesis. 3. Demonstrates the method&#x27;s efficacy in controllable style synthesis and the benefits of diverse synthetic data for improving downstream segmentation performance on the CrossMoDA 2023 dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a0b3a9f7f09f7deb3ee5eff095c1a8b11767f7eec62f9f366a11c0a0e7557f1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a0b3a9f7f09f7deb3ee5eff095c1a8b11767f7eec62f9f366a11c0a0e7557f1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the under-explored issue of intra-domain variability in unsupervised domain adaptation by proposing IntraStyler, an exemplar-based method for synthesizing diverse target domain styles without prior knowledge. It uses a contrastive learning-based style encoder to extract style features and guide synthesis to match an exemplar&#x27;s style. The method shows effective controllable style synthesis and improves downstream segmentation on a cross-modality medical imaging dataset.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D generation and morphing], [Structured Latent (SLAT), Morphing Cross-Attention (MCA), Temporal-Fused Self-Attention (TFSA), training-free, 3D morphing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiaokun Sun, Zeyu Cai, Hao Tang, Ying Tai, Jian Yang, Zhenyu Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanjing University, Peking University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00204" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00204</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A training-free framework (MorphAny3D) that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. 2. The introduction of Morphing Cross-Attention (MCA) for structural coherence and Temporal-Fused Self-Attention (TFSA) for temporal consistency. 3. An orientation correction strategy to mitigate pose ambiguity, enabling state-of-the-art morphing even for challenging cross-category cases.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3f44aeae5b55667e0a256e60be3bfcd2021c03662cb86eb300de39c194e2ad1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3f44aeae5b55667e0a256e60be3bfcd2021c03662cb86eb300de39c194e2ad1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of generating semantically consistent and temporally smooth 3D morphing sequences, especially across categories. It proposes MorphAny3D, a training-free framework that intelligently blends Structured Latent (SLAT) features within a 3D generator&#x27;s attention mechanisms using novel MCA and TFSA modules. The method achieves state-of-the-art results and supports advanced applications like decoupled morphing and 3D style transfer.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [reinforcement learning, multimodal large language models, visual reasoning, group relative policy optimization, reward functions]</p>
</li>
<li class="">
<p><strong>authors:</strong> Omar Sharif, Eftekhar Hossain, Patrick Ng</p>
</li>
<li class="">
<p><strong>institution:</strong> Dartmouth College, University of Central Florida</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00215" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00215</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identified visual perception as the primary bottleneck for MLLMs in visual puzzle tasks, empirically validated by showing significant performance gains when images are converted to text. 2. Proposed a reward-driven RL framework using GRPO to incentivize longer, structured visual reasoning in open-source MLLMs without costly supervision. 3. Designed and evaluated six novel reward functions targeting different reasoning aspects like image understanding, thinking steps, and answer accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa78fbb9d1620ad3674739f2e8cf9cfb6929637fa0f209ae3ee4c439ab5205c8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa78fbb9d1620ad3674739f2e8cf9cfb6929637fa0f209ae3ee4c439ab5205c8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that multimodal large language models (MLLMs) generate reasoning chains that lack integration of visual information, limiting their performance on visual puzzles. The authors propose using reinforcement learning with specifically designed reward functions and Group Relative Policy Optimization (GRPO) to incentivize longer, visually-grounded reasoning. Their method improves the performance of the Qwen-2.5-VL-7B model by 5.56%, demonstrating consistent gains across different settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [model compression (quantization/pruning)], [vector quantization, compositional codebook, extrapolation-by-interpolation, codebook collapse, low-dimensional codevectors]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jie Li, Kwan-Yee K. Wong, Kai Han</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Hong Kong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00222" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00222</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a parameter-efficient, low-dimensional compositional codebook that treats codevectors as compositional units, expanding the solution space and enabling a more compact representation. 2. Incorporates a parameter-free extrapolation-by-interpolation mechanism to enhance feature smoothing and detail preservation during quantization. 3. Functions as a plug-and-play module for existing VQ-based methods, achieving state-of-the-art performance with full codebook usage and avoidance of collapse.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ceb99c2a3db9f368cc5ad30d8a605f970e9d0b2b86eaa1b9baba790fc3ef514_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ceb99c2a3db9f368cc5ad30d8a605f970e9d0b2b86eaa1b9baba790fc3ef514_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes LooC, a new vector quantization method that uses a low-dimensional, compositional codebook to achieve high capacity with a compact size. It introduces a novel way to combine codevectors and a feature smoothing mechanism, leading to better performance and full codebook utilization. Extensive evaluations show LooC outperforms existing VQ methods with a significantly smaller codebook.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image quality assessment], [synthetic data distribution, generalization error, distribution reshaping, upsampling, downsampling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aobo Li, Jinjian Wu, Yongxu Liu, Leida Li, Weisheng Dong</p>
</li>
<li class="">
<p><strong>institution:</strong> Xidian University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00225" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00225</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Li-aobo/SynDR-IQA" target="_blank" rel="noopener noreferrer" class="">https://github.com/Li-aobo/SynDR-IQA</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identified a key issue where models trained on synthetic IQA data learn discrete, clustered feature representations that hinder regression performance. 2. Proposed a novel framework, SynDR-IQA, which reshapes synthetic data distribution based on theoretical analysis of sample diversity and redundancy&#x27;s impact on generalization. 3. Introduced two core strategies: distribution-aware diverse content upsampling to enhance visual diversity, and density-aware redundant cluster downsampling to balance sample density.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87b5234a13da45ce4c7f0208fb30b09fd593e69954422ef5f2b30a452dda5177_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87b5234a13da45ce4c7f0208fb30b09fd593e69954422ef5f2b30a452dda5177_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limited generalization of Blind Image Quality Assessment (BIQA) models trained on synthetic data by identifying that the problem stems from the clustered distribution of synthetic data features. The authors propose the SynDR-IQA framework, which reshapes the data distribution through strategic upsampling and downsampling to improve sample diversity and balance. Extensive experiments across multiple cross-dataset settings demonstrate the effectiveness of their method in enhancing model generalization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [CycleGAN, YOLOv8, infrared image generation, data augmentation, PCB defect detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chao Yang, Haoyuan Zheng, Yue Ma</p>
</li>
<li class="">
<p><strong>institution:</strong> Xi’an Jiaotong Liverpool University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00237" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00237</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a cross-modal data augmentation framework using CycleGAN for unpaired translation from visible-light to infrared images to address IR data scarcity. 2. Introduces a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a YOLOv8 detector. 3. Demonstrates that the method significantly improves detection performance under low-data conditions, approaching fully supervised benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6328838143840b03119bcacf495d6f90fd2cfbf75f09c866a7d8dbf7d351c32_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6328838143840b03119bcacf495d6f90fd2cfbf75f09c866a7d8dbf7d351c32_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the problem of scarce infrared data for PCB defect detection by using CycleGAN to generate synthetic infrared images from abundant visible-light images and training a YOLOv8 detector on this augmented dataset. The proposed method enhances feature learning with limited real data, significantly outperforming models trained only on real data and nearly matching the performance of fully supervised training.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [few-shot object detection], [lightweight CNN, prototypical meta-learning, decision support system, precision agriculture, pest recognition]</p>
</li>
<li class="">
<p><strong>authors:</strong> Anirudha Ghosh, Ritam Sarkar, Debaditya Barman</p>
</li>
<li class="">
<p><strong>institution:</strong> Visva-Bharati, Uttar Banga Krishi Viswavidyalaya</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00243" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00243</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A lightweight framework for pest detection and pesticide recommendation designed for low-resource devices like smartphones and drones. 2. A Pest Detection Module using a compact CNN with prototypical meta-learning for accurate identification with few training samples. 3. A Pesticide Recommendation Module that integrates environmental factors (crop type, growth stage) to suggest safe and eco-friendly pesticides.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b158d9a6a0af4814e3e06857f269146ab38d6c48de45aa34ede928a60f203123_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b158d9a6a0af4814e3e06857f269146ab38d6c48de45aa34ede928a60f203123_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a lightweight framework for precision agriculture that combines a few-shot pest detection model with a context-aware pesticide recommendation system. The method uses a compact CNN with prototypical learning for detection and a rule-based system incorporating environmental factors for recommendations. The framework achieves high accuracy with low computational cost, demonstrating potential for real-time use on resource-constrained devices.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [wireless networking], [O-RAN, UAV, trajectory optimization, RIC, low-altitude economy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aly Sabri Abdalla, Vuk Marojevic</p>
</li>
<li class="">
<p><strong>institution:</strong> Mississippi State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00257" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00257</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an O-RAN-enabled framework for intelligent orchestration of Low-Altitude Economy (LAE) operations, integrating disaggregated RAN architecture with AI-driven RAN Intelligent Controllers (RICs). 2. Presents a semantic-aware rApp and a reinforcement learning-enabled xApp for closed-loop, context-aware trajectory planning of UAV swarms. 3. Surveys available UAV testbeds for LAE research and identifies critical research challenges and standardization needs for future deployments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cae00fb3b22c64bae2b8ae14c12c3a5262a87f4a6954a5d6578c0a24172848_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cae00fb3b22c64bae2b8ae14c12c3a5262a87f4a6954a5d6578c0a24172848_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of orchestrating UAV missions in complex, signal-constrained environments for the Low-Altitude Economy (LAE). It proposes a novel framework that leverages the Open Radio Access Network (O-RAN) architecture, using AI-driven controllers (RICs) and specialized apps (rApp/xApp) for semantic-aware, real-time trajectory planning. The work concludes by evaluating the framework&#x27;s feasibility and outlining future research and standardization directions for scalable LAE deployments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [3D-CT, vision-language model, organ separation, contrastive learning, zero-shot classification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kohei Yamamoto, Tomohiro Kikuchi</p>
</li>
<li class="">
<p><strong>institution:</strong> Jichi Medical University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00260" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00260</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes TotalFM, a 3D-CT vision foundation model based on an organ-separated learning framework to balance computational efficiency and representation capability. 2. Introduces an automated pipeline for creating organ volume and finding-sentence pairs using segmentation and LLM-based report processing. 3. Demonstrates superior zero-shot performance in organ-wise and finding-wise lesion classification compared to baselines like CT-CLIP and Merlin.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63f8af2a1f2287af4b6bfea2831333ab4c9aa61e972f8e6a82db66ceefc12ccc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63f8af2a1f2287af4b6bfea2831333ab4c9aa61e972f8e6a82db66ceefc12ccc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes TotalFM, a 3D-CT vision foundation model that uses an organ-separated framework and combines self-supervised pre-training with contrastive learning to efficiently align volumetric images with text. It shows strong zero-shot classification performance on clinical tasks, outperforming existing models, and demonstrates the framework&#x27;s effectiveness for practical 3D-CT model implementation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal dataset], [multimodal learning, image-text alignment, semantic enhancement, Qwen-VL, CLIP score]</p>
</li>
<li class="">
<p><strong>authors:</strong> He Wang, Longteng Guo, Pengkang Huo, Xuanxu Lin, Yichen Yuan, Jie Jiang, Jing Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00264" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00264</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign" target="_blank" rel="noopener noreferrer" class="">https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces S1-MMAlign, a large-scale, multi-disciplinary dataset with over 15.5 million scientific image-text pairs from 2.5 million papers. 2. Proposes an AI-ready semantic enhancement pipeline using the Qwen-VL model to recaption images by synthesizing context from abstracts and citations, improving alignment. 3. Demonstrates significant data quality improvement via technical validation, showing reduced semantic ambiguity and an 18.21% increase in CLIP scores for image-text alignment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a274c2d0052fe8c7fd0017adcdc91d037137c89f27fc6fe6181e5428d3a20036_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a274c2d0052fe8c7fd0017adcdc91d037137c89f27fc6fe6181e5428d3a20036_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces S1-MMAlign, a large-scale multimodal dataset for scientific figure-text understanding, addressing the semantic gap between complex scientific imagery and sparse textual descriptions. It proposes a semantic enhancement pipeline using the Qwen-VL model to recaption images by integrating context from paper abstracts and citations, which significantly improves image-text alignment as validated by CLIP scores. The dataset serves as a foundational resource for advancing scientific reasoning and cross-modal understanding in AI for Science.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [concept erasure, activation patching, training-free, text-to-image, adversarial robustness]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yi Sun, Xinhao Zhong, Hongyan Li, Yimin Zhou, Junhao Li, Bin Chen, Xuan Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Harbin Institute of Technology, Shenzhen; Tsinghua Shenzhen International Graduate School, Tsinghua University; Peng Cheng Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00267" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00267</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel training-free paradigm for concept erasure in diffusion models, eliminating the need for data-intensive fine-tuning. 2. Introduces a method that identifies and patches activation differences via prompt-pair analysis to precisely remove target concepts. 3. Demonstrates state-of-the-art performance across multiple erasure tasks while preserving model capability and showing robustness against adversarial attacks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1b9f0e8dbea10b4d4a0767c89d80cc2225788088b0a6b57818b98fc389dd914_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1b9f0e8dbea10b4d4a0767c89d80cc2225788088b0a6b57818b98fc389dd914_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes ActErase, a training-free method for erasing sensitive concepts from text-to-image diffusion models. It works by identifying and patching activation differences during inference, avoiding costly fine-tuning. The method achieves strong erasure performance, maintains general generation quality, and is robust to attacks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [hallucination detection, vision-language models, uncertainty estimation, model-driven learning, LLM-as-a-Judge]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chaodong Tong, Qi Zhang, Chen Li, Lei Jiang, Yanbing Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Information Engineering, Chinese Academy of Sciences (CAS); School of Cyber Security, University of CAS; China Industrial Control Systems Cyber Emergency Response Team; China Electronics Standardization Institute</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00269" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00269</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FaithSCAN, a lightweight network for VQA hallucination detection that fuses rich internal signals from VLMs (token-level uncertainty, visual representations, cross-modal alignment) using branch-wise evidence encoding and uncertainty-aware attention. 2. Extends the LLM-as-a-Judge paradigm to VQA to automatically generate low-cost, model-dependent supervision signals for training, eliminating the need for expensive human annotation. 3. Provides an in-depth analysis showing hallucinations stem from systematic variations in internal states across visual perception, cross-modal reasoning, and language decoding, offering new insights into multimodal hallucination causes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0bbbea5cd6fb55e2cc155e1b226cd59e138d25b8ec98a141d833613135b7cd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0bbbea5cd6fb55e2cc155e1b226cd59e138d25b8ec98a141d833613135b7cd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of detecting faithfulness hallucinations in Visual Question Answering (VQA), where models give fluent but visually ungrounded answers. It proposes FaithSCAN, a model-driven method that detects hallucinations in a single pass by exploiting and fusing internal signals from the vision-language model, and uses an automated strategy based on LLM-as-a-Judge for low-cost supervision. Experiments show FaithSCAN outperforms existing methods in both effectiveness and efficiency, and the analysis provides new insights into the internal causes of hallucinations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Disentangling Hardness from Noise: An Uncertainty-Driven Model-Agnostic Framework for Long-Tailed Remote Sensing Classification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [long-tailed classification], [uncertainty estimation, evidential deep learning, remote sensing, long-tailed distribution, adaptive label smoothing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chi Ding, Junxiao Xue, Xinyi Yin, Shi Chen, Yunyun Shi, Yiduo Wang, Fengjian Xue, Xuecheng Wu</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang Lab, Zhengzhou University, Xi&#x27;an Jiaotong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00278" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00278</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a model-agnostic framework (DUAL) that disentangles prediction uncertainty into epistemic and aleatoric types to address long-tailed classification. 2. Introduces epistemic uncertainty to guide a reweighting strategy for hard-to-learn tail samples. 3. Leverages aleatoric uncertainty to quantify data ambiguity and employs an adaptive label smoothing mechanism to suppress noise.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88fd0f3a7e430d011bd0f213783194e9b67472a37625e97a47a9d515f2c442af_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88fd0f3a7e430d011bd0f213783194e9b67472a37625e97a47a9d515f2c442af_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of distinguishing hard-to-learn samples from noisy ones in long-tailed remote sensing image classification. The authors propose DUAL, an uncertainty-aware framework that uses epistemic uncertainty to reweight tail samples and aleatoric uncertainty to apply adaptive label smoothing for noise suppression. Experiments show the framework outperforms strong baselines and generalizes across different datasets and backbones.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [4D reconstruction], [Gaussian Splatting, Sparse View, Skeleton-Driven, Deformation Field, Motion Interpolation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jun-Jee Chao, Volkan Isler</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Minnesota, The University of Texas at Austin</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00285" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00285</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A framework (SV-GS) for 4D reconstruction from sparse observations in both time and viewpoint, using a skeleton-driven Gaussian Splatting approach. 2. A novel skeleton-driven deformation field with a time-dependent joint pose estimator and a separate fine-grained deformation module, enabling smooth motion interpolation. 3. Demonstrating that the method&#x27;s initial static reconstruction input can be replaced by a diffusion-based generative prior, enhancing practical applicability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa4c8f4c46b1bfef8ae3be7aac4ec172ccaf47a8d9e0c9d1fd9369f8155ae0d8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa4c8f4c46b1bfef8ae3be7aac4ec172ccaf47a8d9e0c9d1fd9369f8155ae0d8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the challenging problem of 4D reconstruction from sparse observations in time and viewpoint. It proposes SV-GS, a method that uses a skeleton-driven deformation field with Gaussian Splatting to simultaneously estimate motion and geometry. The approach outperforms existing methods under sparse settings and shows practical potential by relaxing input requirements with generative priors.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [Swin Transformer, BatchFormer, Focal Loss, ReduceLROnPlateau, ISIC2019]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ali Anaissi, Ali Braytee, Weidong Huang, Junaid Akram, Alaa Farhat, Jie Hua</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Technology Sydney, University of Sydney, Shaoyang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00286" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00286</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a deep learning model based on the Swin Transformer architecture for automated differential diagnosis of skin diseases. 2. Applied targeted data augmentation and imbalance-aware strategies (e.g., BatchFormer, Focal Loss) to handle class imbalance in medical image datasets. 3. Achieved a high prediction accuracy of 87.71% on the ISIC2019 dataset, demonstrating the model&#x27;s potential as a clinical support tool.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b53f6f4ea6d1249b026a76d397de3fca0db67dcfee5584653cd543f2ac0bacf9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b53f6f4ea6d1249b026a76d397de3fca0db67dcfee5584653cd543f2ac0bacf9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limited access to dermatologists by developing a deep learning model for automated skin disease diagnosis. The method uses a Swin Transformer architecture pretrained on public datasets and employs imbalance-aware strategies like BatchFormer and Focal Loss to improve classification on the ISIC2019 dataset. The model achieved 87.71% accuracy, showing promise as a diagnostic aid for clinicians and patients.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] TimeColor: Flexible Reference Colorization via Temporal Concatenation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video colorization], [diffusion models, temporal concatenation, spatiotemporal correspondence-masked attention, modality-disjoint RoPE indexing, sketch-based colorization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bryan Constantine Sadihin, Yihao Meng, Michael Hua Wang, Matteo Jiahao Chen, Hang Su</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, HKUST</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00296" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00296</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://bconstantine.github.io/TimeColor/" target="_blank" rel="noopener noreferrer" class="">https://bconstantine.github.io/TimeColor/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a method to support heterogeneous, variable-count references for video colorization via temporal concatenation of reference latents, keeping model parameters fixed. 2. Introduces spatiotemporal correspondence-masked attention and modality-disjoint RoPE indexing to enforce subject-reference binding and prevent shortcutting and palette leakage. 3. Demonstrates improved color fidelity, identity consistency, and temporal stability over prior baselines on the SAKUGA-42M dataset under single- and multi-reference protocols.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f086badbc2b843aa255965a7a32f6586803b7dd5e735b89850a7948f9825d28c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f086badbc2b843aa255965a7a32f6586803b7dd5e735b89850a7948f9825d28c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the limitation of existing video colorization models that rely on a single reference frame, which restricts the use of diverse references like character sheets. It proposes TimeColor, a sketch-based video colorization model that uses temporal concatenation to incorporate multiple references and novel attention mechanisms to bind subjects to references, improving color consistency and stability. Experiments show TimeColor outperforms prior methods in color fidelity and temporal coherence.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [person re-identification], [feature fusion, alpha-divergence loss, dynamic multi-task learning, semantic clustering, computational efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Anns Ijaz, Muhammad Azeem Javed</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Management and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00307" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00307</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A multi-scale feature fusion method with automatic attention that fuses ResNet50 stages without parallel paths. 2. A semantic clustering technique using rule-based pseudo-labeling for anatomical body partitioning. 3. A dynamic weight averaging technique and the use of the FIDI loss function for balanced multi-task learning and improved metric learning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a75221926fa101494a89575b6ea3a1c780209910ea62ecf484286140685d3de7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a75221926fa101494a89575b6ea3a1c780209910ea62ecf484286140685d3de7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes VisNet, an efficient person re-identification model that combines multi-scale feature fusion, semantic clustering, and dynamic multi-task learning with an alpha-divergence loss to achieve a good balance between accuracy and computational cost. It achieves 87.05% Rank-1 and 77.65% mAP on Market-1501 with only 32.41M parameters and 4.601 GFLOPs. The work demonstrates a practical approach for real-time deployment in resource-constrained environments like surveillance and mobile applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] ReMA: A Training-Free Plug-and-Play Mixing Augmentation for Video Behavior Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video behavior recognition], [data augmentation, representation-aware mixing, spatiotemporal coherence, plug-and-play, motion-aware masking]</p>
</li>
<li class="">
<p><strong>authors:</strong> Feng-Qi Cui, Jinyang Huang, Sirui Zhao, Jinglong Guo, Qifan Cai, Xin Yan, Zhi Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology of China, Hefei University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00311" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00311</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel plug-and-play, training-free augmentation strategy (ReMA) that formulates video mixing as a controlled replacement process to expand representations while preserving class-conditional stability. 2. Introduces a Representation Alignment Mechanism (RAM) to perform structured intra-class mixing under distributional constraints, suppressing irrelevant intra-class drift. 3. Introduces a Dynamic Selection Mechanism (DSM) to generate motion-aware spatiotemporal masks, localizing perturbations away from discrimination-sensitive regions to promote temporal coherence.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5aff87765b4798f78236528165f9ca3ca4ce0f7dc233c968a5c9d457b62260e0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5aff87765b4798f78236528165f9ca3ca4ce0f7dc233c968a5c9d457b62260e0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that current perturbation-driven video data augmentation methods introduce uncontrolled variations that harm representation stability. To solve this, it proposes ReMA, a training-free plug-and-play method that uses representation alignment and dynamic spatiotemporal masking to control how and where mixing is applied. Experiments show ReMA consistently improves model generalization and robustness across various video behavior benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Depth-Synergized Mamba Meets Memory Experts for All-Day Image Reflection Separation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image reflection separation], [Mamba, state-space model, depth-aware, memory expert, nighttime dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Siyan Fang, Long Peng, Yuntao Wang, Ruonan Wei, Yuehuan Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Huazhong University of Science and Technology, University of Science and Technology of China</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00322" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00322</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/fashyon/DMDNet" target="_blank" rel="noopener noreferrer" class="">https://github.com/fashyon/DMDNet</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed the Depth-Memory Decoupling Network (DMDNet) with Depth-Aware Scanning (DAScan) and a Depth-Synergized State-Space Model (DS-SSM) to guide Mamba for better layer disentanglement. 2. Introduced a Memory Expert Compensation Module (MECM) that leverages historical knowledge to provide layer-specific compensation. 3. Constructed a new Nighttime Image Reflection Separation (NightIRS) dataset to address the lack of data for nighttime scenes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de4a78d4e76dde376001e66b330ec905d0b5a81cc699f37a8614a613788ea9e0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de4a78d4e76dde376001e66b330ec905d0b5a81cc699f37a8614a613788ea9e0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenging problem of separating reflection and transmission layers from a single blended image, especially in low-contrast nighttime scenes. The authors propose DMDNet, a novel network that integrates depth-aware guidance into a Mamba-based state-space model and utilizes a memory expert module for compensation. The method, evaluated on a newly created nighttime dataset, is shown to outperform existing state-of-the-art approaches for both daytime and nighttime reflection separation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [anomaly detection], [frequency-guided learning, structural attention, semantic consistency, dual-branch framework, CLIP]</p>
</li>
<li class="">
<p><strong>authors:</strong> Naiqi Zhang, Chuancheng Shi, Jingtong Dou, Wenhua Wu, Fei Shen, Jianhua Cao</p>
</li>
<li class="">
<p><strong>institution:</strong> Tianjin University of Science and Technology, The University of Sydney, National University of Singapore</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00327" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00327</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed HarmoniAD, a frequency-guided dual-branch framework that decouples features into high- and low-frequency paths to balance structural detail and semantic context. 2. Introduced two novel modules: a Fine-grained Structural Attention Module (FSAM) for enhancing textures/edges in the high-frequency branch, and a Global Structural Context Module (GSCM) for capturing long-range dependencies in the low-frequency branch. 3. Adopted a multi-class joint training strategy and demonstrated state-of-the-art performance on multiple benchmark datasets (MVTec-AD, VisA, BTAD).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e849537a51e67abdf003473c95f5885e48c7364ea926f9e5a9d19c230dd572ec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e849537a51e67abdf003473c95f5885e48c7364ea926f9e5a9d19c230dd572ec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the trade-off between structural sensitivity and semantic consistency in anomaly detection. It proposes HarmoniAD, a framework that uses a CLIP encoder and frequency-domain decoupling into dual branches with specialized attention modules to model fine details and global context. Experiments show the method achieves state-of-the-art performance with improved sensitivity and robustness on industrial inspection datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Joint Geometry-Appearance Human Reconstruction in a Unified Latent Space via Bridge Diffusion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D human reconstruction], [bridge diffusion, latent representation, 3D Gaussian splatting, variational autoencoder, unified modeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yingzhi Tang, Qijian Zhang, Junhui Hou</p>
</li>
<li class="">
<p><strong>institution:</strong> City University of Hong Kong, Tencent Games</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00328" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00328</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/haiantyz/JGA-LBD" target="_blank" rel="noopener noreferrer" class="">https://github.com/haiantyz/JGA-LBD</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified framework (JGA-LBD) that jointly models human geometry and appearance in a single latent space, addressing inconsistency issues in decoupled pipelines. 2. Introduces a method to unify heterogeneous input conditions (e.g., depth, SMPL) into 3D Gaussian representations and compresses them into a shared latent space via a sparse VAE. 3. Formulates the generation process using a specialized bridge diffusion model that infers missing components from a partially observed latent code, enabling high-fidelity reconstruction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/119bb1243f3a69d2681ae4bd998269231fc8133680dd32c7363d04c1c7d67b82_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/119bb1243f3a69d2681ae4bd998269231fc8133680dd32c7363d04c1c7d67b82_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the challenging problem of reconstructing consistent 3D digital humans from a single RGB image. It proposes JGA-LBD, a novel framework that unifies geometry and appearance modeling into a joint latent representation and uses bridge diffusion for generation. Experiments show the method outperforms state-of-the-art approaches in both geometry and appearance quality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Intelligent Traffic Surveillance for Real-Time Vehicle Detection, License Plate Recognition, and Speed Estimation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [YOLOv8, CNN, Transformer, License Plate Recognition, Speed Estimation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bruce Mugizi, Sudi Murindanyi, Olivia Nakacwa, Andrew Katumba</p>
</li>
<li class="">
<p><strong>institution:</strong> Makerere University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00344" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00344</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a real-time intelligent traffic surveillance system specifically tailored for resource-constrained environments like Uganda, integrating vehicle detection, license plate recognition, and speed estimation. 2. Achieved high performance with a 97.9% mAP for license plate detection using YOLOv8 and a low 1.79% character error rate for recognition using a Transformer model, significantly improving over a CNN baseline. 3. Implemented a practical enforcement pipeline by creating a database to correlate vehicle data with user information and enabling automated ticket issuance via SMS using the Africa&#x27;s Talking API.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e06adb8e70928bbdb99a1135f4b52eac80ef595baa23279552a5a3e3f811f32_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e06adb8e70928bbdb99a1135f4b52eac80ef595baa23279552a5a3e3f811f32_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a real-time computer vision system to tackle speeding in developing countries by detecting vehicles, recognizing license plates, and estimating speed. The method uses YOLOv8 for detection and a Transformer for character recognition, achieving high accuracy and integrating with an SMS-based ticketing system. The work demonstrates a practical, automated solution for traffic enforcement in resource-limited settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal learning], [visual-tactile learning, domain generalization, fractional Fourier transform, multimodal fusion, hierarchical tree structure]</p>
</li>
<li class="">
<p><strong>authors:</strong> Liuxiang Qiu, Hui Da, Yuzhen Niu, Tiesong Zhao, Yang Cao, Zheng-Jun Zha</p>
</li>
<li class="">
<p><strong>institution:</strong> Fuzhou University, University of Science and Technology of China</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00352" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00352</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulates a new task, Single Domain Generalization for Multimodal Visual-Tactile Learning (SDG-VTL), to address modality and domain gaps in VTL. 2. Proposes the OmniVaT framework, which introduces a Multimodal Fractional Fourier Adapter (MFFA) to map visual and tactile embeddings into a unified embedding-frequency space, mitigating the modality gap. 3. Incorporates a Discrete Tree Generation (DTG) module to obtain diverse and reliable multimodal fractional representations via a hierarchical tree structure, enhancing adaptability to unseen domain shifts.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/953f6158cc5edbde2cdc2e32bb128da6cc19e6d671bc9af71f082f73051f598e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/953f6158cc5edbde2cdc2e32bb128da6cc19e6d671bc9af71f082f73051f598e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of modality discrepancies and domain gaps in visual-tactile learning by proposing a new task (SDG-VTL) and a framework called OmniVaT. OmniVaT uses a Multimodal Fractional Fourier Adapter to unify visual and tactile features and a Discrete Tree Generation module to enhance generalization to unseen domains. Experiments show that OmniVaT achieves superior cross-domain generalization performance on the SDG-VTL task.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [semantic segmentation], [dense visual embeddings, knowledge distillation, RGB-D transformer, real-time inference, Alpha-CLIP]</p>
</li>
<li class="">
<p><strong>authors:</strong> Söhnke Benedikt Fischedick, Daniel Seichter, Benedict Stephan, Robin Schmidt, Horst-Michael Gross</p>
</li>
<li class="">
<p><strong>institution:</strong> Technische Universität Ilmenau</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00359" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00359</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DVEFormer, an efficient RGB-D Transformer-based model for predicting dense, text-aligned visual embeddings via knowledge distillation from Alpha-CLIP. 2. Enables flexible, open-vocabulary scene understanding (e.g., text-based querying) beyond fixed-class semantic segmentation while maintaining the ability to perform classical segmentation. 3. Demonstrates real-time performance on embedded hardware (NVIDIA Jetson AGX Orin), making it suitable for mobile robotics applications like 3D mapping.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5eb4801e4f56b1c232cf2b5828f41733ec3576e1fe32d825febbfdde2e108d6c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5eb4801e4f56b1c232cf2b5828f41733ec3576e1fe32d825febbfdde2e108d6c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the need for robots to have a detailed, open-vocabulary understanding of indoor environments. It proposes DVEFormer, an efficient model that uses an RGB-D Transformer and knowledge distillation from Alpha-CLIP to predict dense visual embeddings, enabling both classical segmentation and flexible text-based querying. The method achieves competitive performance and real-time inference speeds, making it a practical drop-in replacement for traditional segmentation in mobile robotics.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Mask-Conditioned Voxel Diffusion for Joint Geometry and Color Inpainting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D reconstruction and inpainting], [voxel diffusion, mask-conditioned inpainting, joint geometry-color completion, 3D U-Net, cultural heritage]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aarya Sumuk</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00368" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00368</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A lightweight two-stage framework for joint geometry and color inpainting of 3D objects, separating damage localization from reconstruction. 2. A mask-conditioned volumetric diffusion model (3D U-Net) that directly inpaints voxel grids to reconstruct occupancy and color while preserving intact regions. 3. A composite training objective combining occupancy reconstruction, masked color reconstruction, and perceptual regularization for joint prediction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e164a07c4c12a1e90fe17c8219fb61313f5381a2f2f4e0a77f870006403576f8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e164a07c4c12a1e90fe17c8219fb61313f5381a2f2f4e0a77f870006403576f8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a two-stage method for digitally restoring damaged 3D cultural heritage artifacts. It first predicts a 3D damage mask from 2D RGB slices, then uses a mask-conditioned voxel diffusion model to jointly inpaint the missing geometry and color. The results show this approach produces more complete and coherent reconstructions than symmetry-based baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] BHaRNet: Reliability-Aware Body-Hand Modality Expertized Networks for Fine-grained Skeleton Action Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [skeleton-based action recognition], [probabilistic fusion, multi-modal integration, reliability-aware learning, noisy-or, cross-modal ensemble]</p>
</li>
<li class="">
<p><strong>authors:</strong> Seungyeon Cho, Tae-kyun Kim</p>
</li>
<li class="">
<p><strong>institution:</strong> Imperial College London (Inferred from author Tae-Kyun Kim&#x27;s affiliation)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00369" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00369</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A calibration-free preprocessing pipeline that learns directly from native skeleton coordinates, removing canonical-space transformations. 2. A probabilistic Noisy-OR fusion mechanism for reliability-aware dual-stream learning without explicit confidence supervision. 3. An intra- to cross-modal ensemble that couples four skeleton modalities with RGB representations in a unified framework.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceb23f64a9170fda547d5e2c6b35431a9988bf0c41f827b0bb8299b07dce413b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceb23f64a9170fda547d5e2c6b35431a9988bf0c41f827b0bb8299b07dce413b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of body-centric skeleton action recognition by proposing BHaRNet, a probabilistic dual-stream framework that integrates body and hand modalities for fine-grained recognition. The method introduces reliability-aware learning and a unified cross-modal ensemble of skeleton and RGB data. It demonstrates improved performance and robustness across multiple benchmarks, including a new hand-centric dataset.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [small-object detection, dashcam dataset, roadside litter, long-tail distribution, transformer detectors]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tao Wu, Qing Xu, Xiangjian He, Oakleigh Weekes, James Brown, Wenting Duan</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Nottingham Ningbo China, University of Lincoln</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00398" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00398</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/xq141839/RoLID-11K" target="_blank" rel="noopener noreferrer" class="">https://github.com/xq141839/RoLID-11K</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces RoLID-11K, the first large-scale dataset for roadside litter detection from dashcam footage, featuring over 11k annotated images with extreme small-object and long-tail characteristics. 2. Provides a comprehensive benchmark of modern object detectors, including transformer and YOLO models, on this challenging task. 3. Establishes a new benchmark for extreme small-object detection in dynamic driving scenes to support scalable, low-cost litter monitoring systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07af957156a6b1fe042100c136a3a47ff653381303e2f5f88a5c4dfabde172c3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07af957156a6b1fe042100c136a3a47ff653381303e2f5f88a5c4dfabde172c3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces RoLID-11K, a new dataset for detecting small roadside litter from dashcam videos, and benchmarks various modern object detectors on it. The results show that transformer-based models like CO-DETR achieve the best localization accuracy, while real-time models are limited by coarse feature hierarchies. The dataset aims to facilitate the development of scalable systems for automated roadside litter monitoring.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [4D reconstruction and generation], [4D Gaussian Splatting, monocular video, feed-forward reconstruction, novel view synthesis, world model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuxue Yang, Lue Fan, Ziqi Shi, Junran Peng, Feng Wang, Zhaoxiang Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> NLPR &amp; MAIS, CASIA; CreateAI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00393" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00393</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://neoverse-4d.github.io" target="_blank" rel="noopener noreferrer" class="">https://neoverse-4d.github.io</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A scalable 4D world model (NeoVerse) built from diverse in-the-wild monocular videos, eliminating the need for expensive multi-view data or cumbersome pre-processing. 2. A pose-free, feed-forward 4D reconstruction method using 4D Gaussian Splatting (4DGS). 3. An online monocular degradation pattern simulation technique to enable high-quality, coherent novel-trajectory video generation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e49e322200509b4252a5a149b2bdd18f2f3e7138a6cc52255fbd8ace6f79697b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e49e322200509b4252a5a149b2bdd18f2f3e7138a6cc52255fbd8ace6f79697b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes NeoVerse, a scalable 4D world model that performs feed-forward 4D reconstruction from monocular videos and generates novel-viewpoint videos. It addresses scalability limitations of prior methods by not requiring multi-view data or complex pre-processing. The model achieves state-of-the-art performance on standard benchmarks for reconstruction and generation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] ABFR-KAN: Kolmogorov-Arnold Networks for Functional Brain Analysis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [Kolmogorov-Arnold Network (KAN), Functional Connectivity (FC), Transformer, Autism Spectrum Disorder (ASD), ABIDE I]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tyler Ward, Abdullah Imran</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Kentucky</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00416" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00416</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/tbwa233/ABFR-KAN" target="_blank" rel="noopener noreferrer" class="">https://github.com/tbwa233/ABFR-KAN</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ABFR-KAN, a novel transformer-based classification network that integrates advanced brain function representation components with Kolmogorov-Arnold Networks (KANs) to address limitations of atlas-based functional connectivity analysis. 2. Introduces a method designed to mitigate structural bias, improve anatomical conformity, and enhance the reliability of functional connectivity estimation for brain disorder diagnosis. 3. Demonstrates through extensive experiments, including cross-site evaluation and ablation studies on the ABIDE I dataset, that the proposed model consistently outperforms state-of-the-art baselines in autism spectrum disorder (ASD) classification.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51e3b468fa0a409dc2ba718a14a375622d6eec61468b3a97ab00e67542b75241_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51e3b468fa0a409dc2ba718a14a375622d6eec61468b3a97ab00e67542b75241_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the issues of selection bias and lack of subject specificity in traditional atlas-based functional connectivity (FC) analysis for brain disorders. It proposes ABFR-KAN, a transformer-based classification network that incorporates novel brain function representation components and Kolmogorov-Arnold Networks (KANs) to improve FC estimation. Experiments on the ABIDE I dataset show that ABFR-KAN outperforms state-of-the-art methods in classifying autism spectrum disorder (ASD).</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Robust Assembly Progress Estimation via Deep Metric Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [metric learning], [assembly progress estimation, deep metric learning, quadruplet loss, anomaly detection, small-scale dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kazuma Miura, Sarthak Pathak, Kazunori Umeda</p>
</li>
<li class="">
<p><strong>institution:</strong> Chuo University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00422" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00422</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a robust system for assembly progress estimation that handles occlusion and minimal visual changes using a small-scale dataset. 2. Introduced a Quadruplet Loss-based learning approach specifically designed for anomaly images. 3. Developed a custom data loader that strategically selects training samples to enhance estimation accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b36c1f1235c837f4744e7bc1a15ac0006eccde3fb6d1ab8e95980b3b73e5027_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b36c1f1235c837f4744e7bc1a15ac0006eccde3fb6d1ab8e95980b3b73e5027_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of misclassification in assembly progress estimation when visual changes between tasks are subtle. It proposes Anomaly Quadruplet-Net, which uses a Quadruplet Loss and a custom data loader for robust learning. The method outperforms prior work, improving accuracy by 1.3% and reducing adjacent task misclassification by 1.9% on a desktop PC assembly dataset.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Deep Delta Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [neural network architecture], [residual networks, geometric transformation, spectral analysis, rank-1 perturbation, dynamic gating]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yifan Zhang, Yifeng Liu, Mengdi Wang, Quanquan Gu</p>
</li>
<li class="">
<p><strong>institution:</strong> Princeton University, University of California, Los Angeles</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00417" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00417</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/yifanzhang-pro/deep-delta-learning" target="_blank" rel="noopener noreferrer" class="">https://github.com/yifanzhang-pro/deep-delta-learning</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Deep Delta Learning (DDL), a novel architecture that generalizes residual connections with a learnable, data-dependent geometric transformation called the Delta Operator. 2. Provides a spectral analysis of the Delta Operator, showing it can dynamically interpolate between identity mapping, orthogonal projection, and geometric reflection via a gating scalar. 3. Restructures the residual update as a synchronous rank-1 injection, unifying feature erasure and writing under a dynamic step size to enable complex, non-monotonic dynamics while preserving stable training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e9151cd8b5ec94dcb21276489c4319c73f4c1a7295a6715a6c2a36d36f9a9b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e9151cd8b5ec94dcb21276489c4319c73f4c1a7295a6715a6c2a36d36f9a9b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies that the strictly additive inductive bias of standard residual networks limits their capacity to model complex state transitions. To address this, it proposes Deep Delta Learning (DDL), which modulates the identity shortcut with a learnable, data-dependent geometric transformation (the Delta Operator). This allows the network to explicitly control its layer-wise transition spectrum, enabling the modeling of complex dynamics like oscillations while maintaining stable training.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning for human feedback], [reinforcement learning from human feedback (RLHF), flow matching, stochastic differential equations (SDE), group relative policy optimization (GRPO), entropy-aware sampling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shengjun Zhang, Zhang Zhang, Chensheng Dai, Yueqi Duan</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00423" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00423</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/shengjun-zhang/VisualGRPO" target="_blank" rel="noopener noreferrer" class="">https://github.com/shengjun-zhang/VisualGRPO</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identified that high-entropy denoising steps are crucial for effective exploration in RL for flow models, while low-entropy steps lead to ambiguous rewards. 2. Proposed E-GRPO, an entropy-aware method that consolidates consecutive low-entropy steps into a single high-entropy step for SDE sampling and uses ODE sampling elsewhere. 3. Introduced a multi-step group normalized advantage calculation that computes advantages relative to samples sharing the same consolidated SDE step.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/517610c9d7d0b5f72ab6ea2e2c36dda14fb3879dc1ff5c4a8ae66c97dd3a6457_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/517610c9d7d0b5f72ab6ea2e2c36dda14fb3879dc1ff5c4a8ae66c97dd3a6457_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of sparse and ambiguous reward signals when applying reinforcement learning to flow models over multiple denoising steps. It proposes E-GRPO, an entropy-aware method that strategically uses SDE sampling on high-entropy steps and ODE sampling on others, along with a group-relative advantage calculation. Experiments show this approach is more effective for aligning flow models with human preferences.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] CPPO: Contrastive Perception for Vision Language Policy Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [contrastive perception loss, entropy shift, vision-language models, policy optimization, multimodal reasoning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ahmad Rezaei, Mohsen Gholami, Saeed Ranjbar Alvar, Kevin Cannons, Mohammad Asiful Hossain, Zhou Weimin, Shunbo Zhou, Yong Zhang, Mohammad Akbari</p>
</li>
<li class="">
<p><strong>institution:</strong> Huawei Technologies Canada Co. Ltd., Huawei Cloud</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00501" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00501</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a method to detect perception tokens via entropy shifts under perturbed input images, avoiding reliance on extra LLMs or ground-truth data. 2. Proposes a Contrastive Perception Loss (CPL) that enforces output consistency for information-preserving perturbations and sensitivity for information-removing ones. 3. Demonstrates improved performance over prior perception-rewarding methods while enhancing training efficiency and scalability by eliminating the need for auxiliary models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa7f947cc56b98ca75000ef69e1ebe5ac183e118802862066844909b5763f7e9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa7f947cc56b98ca75000ef69e1ebe5ac183e118802862066844909b5763f7e9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> CPPO is a reinforcement learning method for finetuning vision-language models that addresses the challenge of disentangling perception from reasoning tokens. It detects perception tokens using entropy shifts under image perturbations and applies a contrastive perception loss to optimize them. Experiments show CPPO outperforms previous methods without requiring extra models, making training more efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [physics-based simulation], [motion distillation, differentiable simulation, multimodal large language model, material parameter estimation, video diffusion models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Miaowei Wang, Jakub Zadrożny, Oisin Mac Aodha, Amir Vaxman</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Edinburgh</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00504" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00504</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://wangmiaowei.github.io/MotionPhysics.github.io/" target="_blank" rel="noopener noreferrer" class="">https://wangmiaowei.github.io/MotionPhysics.github.io/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An end-to-end differentiable framework that infers physical parameters from natural language prompts for 3D simulation, 2. A novel learnable motion distillation loss that extracts motion priors from video diffusion models while minimizing appearance/geometry bias, 3. A comprehensive evaluation across diverse scenarios (real-world, human-designed, AI-generated objects) and materials (solids, fluids) showing state-of-the-art performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1f5f328bdf213bd8c50e27a819223c1b50a9adbc0e2f36cc8adcb40bdbdd5bb8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1f5f328bdf213bd8c50e27a819223c1b50a9adbc0e2f36cc8adcb40bdbdd5bb8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces MotionPhysics, a framework that uses a multimodal LLM and a novel motion distillation loss from video diffusion models to automatically estimate plausible physical parameters from text prompts for 3D dynamic simulation. This approach eliminates the need for ground-truth trajectories or annotated videos. The method is shown to produce realistic simulations across a wide variety of materials and object types, outperforming prior work.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video restoration], [all-in-one restoration, smoothly evolving degradation, prompt learning, coarse intensity estimation, flow prompt generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenrui Li, Hongtao Chen, Yao Xiao, Wangmeng Zuo, Jiantao Zhou, Yonghong Tian, Xiaopeng Fan</p>
</li>
<li class="">
<p><strong>institution:</strong> Harbin Institute of Technology, University of Macau, Peking University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00533" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00533</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Friskknight/ORCANet-SEUD" target="_blank" rel="noopener noreferrer" class="">https://github.com/Friskknight/ORCANet-SEUD</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Smoothly Evolving Unknown Degradations (SEUD) scenario for video restoration, where degradation types and intensities change continuously over time. 2. Proposes ORCANet, featuring a Coarse Intensity Estimation Dehazing (CIED) module for initialization and a Flow Prompt Generation (FPG) module that generates static and dynamic prompts to capture degradation types and intensity variations. 3. Designs a flexible synthesis pipeline to generate temporally coherent videos with single, compound, and evolving degradations for training and evaluation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/314d098b5e34dadb4e9353f07d267f249935ece4ae21fe942fc2f57313e50e49_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/314d098b5e34dadb4e9353f07d267f249935ece4ae21fe942fc2f57313e50e49_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of all-in-one video restoration under temporally continuous and evolving weather degradations. It proposes ORCANet, a network that uses coarse intensity estimation and a novel prompting mechanism to adapt to degradation changes over time. Experiments show the method achieves superior restoration quality and temporal consistency compared to existing baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [text-to-image generation], [diffusion transformers, attention localization, spectral glyph injection, training-free, text rendering]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ruiqiang Zhang, Hengyi Wang, Chang Liu, Guanjie Wang, Zehua Ma, Weiming Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology of China</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00535" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00535</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a training-free framework (FreeText) that decomposes text rendering into &quot;where to write&quot; and &quot;what to write&quot; problems. 2. Introduces a method to localize writing regions using endogenous image-to-text attention with sink-like tokens and topology-aware refinement. 3. Presents Spectral-Modulated Glyph Injection (SGMI) to inject a noise-aligned glyph prior with frequency-domain modulation to enhance structure and suppress semantic leakage.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/463d01d9f126e0686a5a88ff8da2bf9d31c4f68b3b838fbd41e6842bdf70c735_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/463d01d9f126e0686a5a88ff8da2bf9d31c4f68b3b838fbd41e6842bdf70c735_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes FreeText, a training-free framework to improve text rendering in Diffusion Transformer models. It localizes writing regions via attention mechanisms and injects glyph structure using spectral modulation. Experiments show it improves text readability while preserving image quality with minimal overhead.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image editing], [drag-style editing, motion prediction, predict-and-move framework, motion supervision]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiacheng Sui, Yujie Zhou, Li Niu</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00542" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00542</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel &quot;predict-and-move&quot; framework for drag-style image editing, which is the first of its kind. 2. Introduces an iterative method combining Motion Prediction and Motion Supervision to proactively guide handle points. 3. Proposes a dynamic adjustment mechanism for valid handle points to improve editing performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c465837de0f92a45bd31ff245e5f10563376186d973cd5f4a47cbf125a05967a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c465837de0f92a45bd31ff245e5f10563376186d973cd5f4a47cbf125a05967a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies issues like miss tracking and poor editability in existing drag-style image editing methods. It proposes DynaDrag, a new method under a &quot;predict-and-move&quot; framework that iteratively uses motion prediction and motion supervision to guide handle points. Experiments on face and human datasets show it outperforms previous works.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image segmentation], [Segment Anything Model, zero-shot segmentation, low-level features, fine-tuning, visually non-salient]</p>
</li>
<li class="">
<p><strong>authors:</strong> Guangqian Guo, Pengfei Chen, Yong Guo, Huafeng Chen, Boqiang Zhang, Shan Gao</p>
</li>
<li class="">
<p><strong>institution:</strong> Northwestern Polytechnical University, University of Chinese Academy of Sciences, Max Planck Institute for Informatics, University of Science and Technology of China</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00537" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00537</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://guangqian-guo.github.io/VNS-SAM/" target="_blank" rel="noopener noreferrer" class="">https://guangqian-guo.github.io/VNS-SAM/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed VNS-SAM, a method to enhance SAM&#x27;s performance in visually non-salient (low-contrast) scenarios while preserving its zero-shot generalizability. 2. Introduced two key designs: a Mask-Edge Token Interactive decoder and a Non-Salient Feature Mining module to effectively exploit SAM&#x27;s low-level features. 3. Created VNS-SEG, a large-scale unified dataset with over 35K images for training and benchmarking models on various visually non-salient segmentation tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4bd924d5e254a323192d4b65b47fba860d34f9df6052e0b5cf36ea1076a78bef_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4bd924d5e254a323192d4b65b47fba860d34f9df6052e0b5cf36ea1076a78bef_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem where the Segment Anything Model (SAM) struggles with visually non-salient scenarios where foreground and background have low contrast. The authors propose VNS-SAM, which enhances SAM&#x27;s perception using a novel decoder and feature mining module, and introduce a new dataset called VNS-SEG. Experiments show VNS-SAM achieves superior performance, especially in zero-shot settings, demonstrating its practical potential.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] A Comprehensive Dataset for Human vs. AI Generated Image Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image forensics], [AI-Generated Images, Detection Techniques, Synthetic Media, Generative AI, Multimodal AI]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rajarshi Roy, Nasrin Imanpour, Ashhar Aziz, Shashwat Bajpai, Gurpreet Singh, Shwetangshu Biswas, Kapil Wanaskar, Parth Patwa, Subhankar Ghosh, Shreyas Dixit, Nilesh Ranjan Pal, Vipula Rawte, Ritvik Garimella, Gaytri Jena, Vasu Sharma, Vinija Jain, Aman Chadha, Aishwarya Naresh Reganti, Amitava Das</p>
</li>
<li class="">
<p><strong>institution:</strong> Kalyani Govt. Engg. College, AI Institute USC, IIIT Delhi, BITS Pilani, NIT Silchar, San José State Univ., UCLA, Washington State Univ., VIIT, GITA, Meta AI, Amazon AI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00553" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00553</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MS COCOAI, a novel large-scale dataset of 96,000 real and AI-generated images for detection research., 2. Proposes two benchmark tasks: binary real-vs-AI classification and multi-class AI model attribution., 3. Provides a diverse dataset using five state-of-the-art generators (Stable Diffusion 3, SD 2.1, SDXL, DALL-E 3, MidJourney v6) built upon MS COCO.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80669ac38c06acf6818488be6bd831b0e8cc20bc319a1b37c431681230968cd3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80669ac38c06acf6818488be6bd831b0e8cc20bc319a1b37c431681230968cd3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of detecting increasingly realistic AI-generated images by introducing MS COCOAI, a comprehensive dataset of 96,000 real and synthetic images created using five modern generators. The dataset enables two key tasks: distinguishing real from AI-generated images and identifying the specific AI model that created a synthetic image. The release of this dataset aims to advance research in AI-generated image detection and model attribution.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical imaging reconstruction], [photoacoustic imaging, point cloud, iterative reconstruction, irregular array, hierarchical optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuang Li, Yibing Wang, Jian Gao, Chulhong Kim, Seongwook Choi, Yu Zhang, Qian Chen, Yao Yao, Changhui Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University, Nanjing University, Pohang University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00551" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00551</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/ShuangLiPKU/SlingBAG-Pro" target="_blank" rel="noopener noreferrer" class="">https://github.com/ShuangLiPKU/SlingBAG-Pro</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed SlingBAG Pro, an advanced reconstruction algorithm extending point cloud iteration to arbitrary/irregular transducer array geometries. 2. Introduced a hierarchical optimization strategy combining zero-gradient filtering with progressively increased temporal sampling to accelerate convergence. 3. Demonstrated significant speed improvement (up to 2.2x) over the original method while maintaining quality, validated via simulation and in vivo experiments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4ac663649e0f38d28ffc4287bdae5acac62679da31ef28094152756b24cc0f8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4ac663649e0f38d28ffc4287bdae5acac62679da31ef28094152756b24cc0f8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces SlingBAG Pro, an accelerated iterative reconstruction algorithm for 3D photoacoustic imaging that works with arbitrary, irregular transducer arrays. It uses a point cloud-based method with a hierarchical optimization strategy to remove redundant computations, speeding up convergence. The method achieves up to 2.2x faster reconstruction than its predecessor while maintaining quality, as shown in simulations and mouse experiments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] A Cascaded Information Interaction Network for Precise Image Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image segmentation], [cascaded convolutional neural network, Global Information Guidance Module, multi-scale feature fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hewen Xiao, Jie Mei, Guangfu Ma, Weiren Wu</p>
</li>
<li class="">
<p><strong>institution:</strong> Harbin Institute of Technology, Shenzhen</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00562" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00562</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a cascaded convolutional neural network architecture for robust image segmentation. 2. Introduces a novel Global Information Guidance Module to fuse low-level texture and high-level semantic features. 3. Demonstrates superior performance on benchmark datasets, particularly in cluttered or blurred environments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b418eb7fd8e5a8768acb864c9583e6d602d0cbbea738237f0d183a02620d4ce_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b418eb7fd8e5a8768acb864c9583e6d602d0cbbea738237f0d183a02620d4ce_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of robust image segmentation in complex scenarios by proposing a cascaded CNN with a novel Global Information Guidance Module. This module effectively fuses multi-scale features to overcome the limitations of single-scale extraction. Experimental results show the method outperforms state-of-the-art approaches, highlighting its potential for practical robotic applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [multimodal evaluation], [unified multimodal models, world knowledge, deterministic evaluation, benchmark, reasoning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jintao Lin, Bowen Dong, Weikang Shi, Chenyang Lei, Suiyun Zhang, Rui Liu, Xihui Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Hong Kong, The Hong Kong Polytechnic University, The Chinese University of Hong Kong, Huawei Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00561" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00561</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes AEGIS, a comprehensive multi-task benchmark for evaluating world knowledge capabilities of Unified Multimodal Models across visual understanding, generation, editing, and interleaved generation. 2. Introduces Deterministic Checklist-based Evaluation (DCE), a protocol using atomic &quot;Y/N&quot; judgments to replace ambiguous prompt-based scoring for more reliable evaluation. 3. Conducts extensive experiments revealing severe world knowledge deficits in current UMMs, performance degradation with complex reasoning, and the partial mitigation offered by plug-in reasoning modules.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d213b94fc8cc3083a9cd6696a9015bb66bc67f497c2bbd6325bdba2cc4da71e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d213b94fc8cc3083a9cd6696a9015bb66bc67f497c2bbd6325bdba2cc4da71e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies a critical gap in evaluating Unified Multimodal Models&#x27; (UMMs) ability to apply world knowledge. To address this, it proposes the AEGIS benchmark and a Deterministic Checklist-based Evaluation (DCE) protocol. The experiments show that current UMMs have significant world knowledge deficiencies, especially in complex reasoning tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video moment retrieval], [zero-shot, granularity mismatch, query rewriting, caption generation, vision-language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mingyu Jeon, Sunjae Yoon, Jonghee Kim, Junyeoung Kim</p>
</li>
<li class="">
<p><strong>institution:</strong> Chung-Ang University, Korea Advanced Institute of Science and Technology (KAIST), Electronics and Telecommunications Research Institute (ETRI)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00584" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00584</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a training-free framework (GranAlign) to address semantic granularity mismatch in zero-shot video moment retrieval. 2. Introduces granularity-based query rewriting to generate queries at varied semantic levels. 3. Introduces query-aware caption generation to embed query intent into video content descriptions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23be1befed58b8e1274e0b349e1e52f23148fa93c049fd218714eaf38b8f9a43_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23be1befed58b8e1274e0b349e1e52f23148fa93c049fd218714eaf38b8f9a43_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the granularity mismatch problem in zero-shot video moment retrieval by proposing GranAlign, a training-free framework that uses query rewriting and query-aware caption generation to align multi-level semantic representations. The method achieves state-of-the-art performance on three major benchmarks, including a 3.23% mAP@avg improvement on QVHighlights.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [RGB-Infrared fusion, modality imbalance, cross-modal learning, optimization bias, multimodal perception]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xianhui Liu, Siqi Jiang, Yi Xie, Yuqing Lin, Siao Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Tongji University, Soochow University, The University of Arizona</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00598" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00598</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the Modality Dominance Index (MDI) to quantify optimization bias caused by asymmetric modality characteristics in RGB-IR fusion. 2. Develops the Modality Dominance-Aware Cross-modal Learning (MDACL) framework to regulate cross-modal optimization. 3. Introduces Hierarchical Cross-modal Guidance (HCG) and Adversarial Equilibrium Regularization (AER) within MDACL to enhance feature alignment and balance optimization dynamics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea14f519454d3351bfb99175ef4749ac8218341133d6b9799ea3b1f99453239f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea14f519454d3351bfb99175ef4749ac8218341133d6b9799ea3b1f99453239f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the optimization bias problem in RGB-Infrared multimodal object detection, where disparities in information density cause training to over-rely on a dominant modality. The authors propose a Modality Dominance Index (MDI) to measure this bias and a Modality Dominance-Aware Cross-modal Learning (MDACL) framework to mitigate it. Experiments show that MDACL effectively reduces optimization bias and achieves state-of-the-art performance on RGB-IR benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [text-to-motion generation], [machine unlearning, diffusion models, motion safety, continuous kinematics, safe dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yiling Wang, Zeyu Zhang, Yiran Wang, Hao Tang</p>
</li>
<li class="">
<p><strong>institution:</strong> The Australian National University, Peking University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00590" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00590</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/YilingWang98/SafeMo" target="_blank" rel="noopener noreferrer" class="">https://github.com/YilingWang98/SafeMo</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed SafeMo, a trustworthy motion generation framework integrating a two-stage Minimal Motion Unlearning (MMU) strategy for safe generation in continuous space. 2. Introduced the first safe text-to-motion dataset, SafeMoVAE-29K, with rewritten safe prompts and refined motions. 3. Demonstrated superior safety-utility trade-offs, achieving significantly higher forget-set FID scores than prior state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e31ba879f70fe1ccd3182c0dcf440f56ec134c723c4b1d1032dbe7d145432291_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e31ba879f70fe1ccd3182c0dcf440f56ec134c723c4b1d1032dbe7d145432291_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses safety concerns in text-to-motion generation by proposing SafeMo, a framework that uses a two-stage machine unlearning strategy to remove unsafe behaviors while preserving motion quality in continuous space. It also introduces a new safe dataset for training. Experiments show SafeMo effectively forgets unsafe prompts while maintaining good performance on safe ones, outperforming previous methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Noise-Robust Tiny Object Localization with Flows</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [Tiny Object Detection, Noise Robustness, Normalizing Flows, Uncertainty-Guided Optimization, Flow-Based Error Modeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Huixin Sun, Linlin Yang, Ronyu Chen, Kerui Gu, Baochang Zhang, Angela Yao, Xianbin Cao</p>
</li>
<li class="">
<p><strong>institution:</strong> Beihang University, Communication University of China, National University of Singapore</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00617" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00617</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Tiny Object Localization with Flows (TOLF), a noise-robust framework for tiny object detection. 2. Introduces flow-based error modeling to capture complex, non-Gaussian prediction distributions for robust learning under noisy supervision. 3. Designs an uncertainty-aware gradient modulation mechanism to suppress learning from high-uncertainty, noise-prone samples, mitigating overfitting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b7f3a1410f8fcc28ad5c6cee9bc7ead457cf793040466a768ce5024835633cf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b7f3a1410f8fcc28ad5c6cee9bc7ead457cf793040466a768ce5024835633cf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of tiny object detection being highly sensitive to annotation noise, which leads to overfitting. The authors propose TOLF, a framework using normalizing flows for flexible error modeling and uncertainty-guided optimization to learn robustly from noisy labels. Experiments show TOLF effectively improves performance, boosting a DINO baseline by 1.2% AP on the AI-TOD dataset.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [Hypergraph Neural Network, Learning Using Privileged Information, Knowledge Distillation, Severed Graph Strategy, dual-stream distillation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuren Gabriel Yu, Sikang Ren, Yongji Tian</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Beijing Tiantan Hospital</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00626" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00626</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information framework for preoperative ependymoma prognosis. 2. Introduces a Severed Graph Strategy with a shared encoder to process both a Teacher graph (with post-surgery text) and a Student graph (with pre-op MRI only). 3. Employs dual-stream distillation to enable the Student model to hallucinate semantic community structures from visual features alone, transferring expert knowledge without requiring text at inference.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b9ab4eaefd7ae386c2d1758797c14d52ec57c898fa468bb73264675d58297cb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b9ab4eaefd7ae386c2d1758797c14d52ec57c898fa468bb73264675d58297cb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of preoperative ependymoma prognosis where post-operative text reports are unavailable at inference. It proposes HyperPriv-EPN, a hypergraph learning framework that uses a severed graph strategy and dual-stream distillation to transfer knowledge from privileged text data to a model that only uses MRI. The method achieves state-of-the-art diagnostic accuracy and survival stratification on a multi-center cohort, enabling the use of historical post-operative data for new patient diagnosis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human pose estimation], [real-time 3D pose estimation, biomechanical analysis, SmoothNet, multi-camera tracking, Unity visualization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Junxiao Xue, Pavel Smirnov, Ziao Li, Yunyun Shi, Shi Chen, Xinyi Yin, Xiaohan Yue, Lei Wang, Yiduo Wang, Feng Lin, Yijia Chen, Xiao Ma, Xiaoran Yan, Qing Zhang, Fengjian Xue, Xuecheng Wu</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang Lab, Xi&#x27;an Jiaotong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00625" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00625</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A unified, end-to-end pipeline for real-time 3D human pose estimation and motion analysis from multi-camera RGB video for rehabilitation. 2. A fast tracking method for multi-person scenarios in medical rehabilitation, achieving tracking in under 1ms per frame. 3. A modification of SmoothNet for real-time posture estimation to reduce errors and produce smoother, more accurate motion states.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b90bb8653fd0f2988586244cf96968ebee6859d08f1685dd8be9db9a00f20b34_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b90bb8653fd0f2988586244cf96968ebee6859d08f1685dd8be9db9a00f20b34_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes RePose, a real-time framework for 3D human pose estimation and biomechanical analysis to assist rehabilitation training. The method uses a multi-camera pipeline, a fast multi-person tracker, and a modified SmoothNet for smooth pose estimation, integrated with Unity for real-time feedback and muscle stress visualization. The system aims to provide immediate guidance to help patients perform exercises correctly.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image classification], [transfer learning, vision transformer, DenseNet, sprout detection, shelf-life prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shrikant Kapse, Priyankkumar Dhrangdhariya, Priya Kedia, Manasi Patwardhan, Shankar Kausley, Soumyadipta Maiti, Beena Rai, Shirish Karande</p>
</li>
<li class="">
<p><strong>institution:</strong> TCS Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00645" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00645</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Designed a high-precision binary classifier for potato sprout detection using transfer learning, achieving 98.03% accuracy with DenseNet. 2. Developed an advanced multi-class predictor for estimating weight loss and forecasting remaining shelf-life, demonstrating best performance with coarse class divisions. 3. Demonstrated the feasibility of integrating image-based deep learning models into automated sorting systems for improved inventory management and reduced food waste.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a94f0adb825ef9b8afe22fb1ed85de1536913fd08af263977b5d34f95849c59_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a94f0adb825ef9b8afe22fb1ed85de1536913fd08af263977b5d34f95849c59_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes using transfer learning with CNN and Vision Transformer models for non-invasive quality detection of stored potatoes. The method involves a binary classifier for sprout detection and a multi-class predictor for weight loss and shelf-life estimation. The study concludes that this approach is effective for automated sorting, with coarse class divisions yielding robust performance for practical applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [hallucination mitigation, contrastive decoding, vision-language models, training-free]</p>
</li>
<li class="">
<p><strong>authors:</strong> Neeraj Anand, Samyak Jha, Udbhav Bamba, Rahul Rahaman</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology (ISM) Dhanbad, Transmute AI, National University of Singapore</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00659" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00659</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/ubamba98/CRoPS-Mitigate-Hallucinations-in-Vision-Language-Models" target="_blank" rel="noopener noreferrer" class="">https://github.com/ubamba98/CRoPS-Mitigate-Hallucinations-in-Vision-Language-Models</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel hallucinated model that captures hallucination effects by selectively removing key text tokens, addressing the limitation of visual information propagation. 2. Introduces Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. 3. Presents CRoPS, a training-free framework that significantly improves hallucination metrics (e.g., 20% CHAIR score gain) across multiple benchmarks and LVLM families.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13748e3293c59d4b319620d3651a674872996116396b3caafd1fd283bb3ab841_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13748e3293c59d4b319620d3651a674872996116396b3caafd1fd283bb3ab841_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of hallucinations in Large Vision-Language Models (LVLMs). It proposes CRoPS, a training-free framework that introduces a novel hallucinated model based on text token removal and a Generalized Contrastive Decoding method to mitigate diverse hallucination sources. The method achieves significant improvements in hallucination metrics across several benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D reconstruction], [SAR tomography, point cloud, deep learning, building height estimation, dual-topology network]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhaiyu Chen, Yuanyuan Wang, Yilei Shi, Xiao Xiang Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> Technical University of Munich (TUM)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00658" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00658</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/zhu-xlab/tomosar2height" target="_blank" rel="noopener noreferrer" class="">https://github.com/zhu-xlab/tomosar2height</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel learning-based framework for generating high-resolution building height maps directly from raw, noisy TomoSAR point clouds. 2. Introduces a dual-topology network architecture that alternates between a point branch and a grid branch to jointly model irregular scatterer features and enforce spatial consistency. 3. Demonstrates the first proof of concept for large-scale urban height mapping from TomoSAR and shows the framework&#x27;s extensibility to incorporate optical imagery for enhanced quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/37a1a6d415c67af4db85e8077f3ab9a67e110f03583e90820fa3c4b93634ac87_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/37a1a6d415c67af4db85e8077f3ab9a67e110f03583e90820fa3c4b93634ac87_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of accurate building height reconstruction from noisy and incomplete spaceborne TomoSAR point clouds. It proposes a dual-topology deep learning network that processes both point-based and grid-based representations to denoise and inpaint the data, producing continuous height maps. Experiments on urban datasets validate the method&#x27;s effectiveness, and the framework is shown to be extensible by fusing with optical imagery.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [talking head generation], [diffusion forcing, direct preference optimization, real-time interaction, low latency, multimodal inputs]</p>
</li>
<li class="">
<p><strong>authors:</strong> Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang</p>
</li>
<li class="">
<p><strong>institution:</strong> KAIST, NTU Singapore, DeepAuto.ai</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00664" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00664</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://taekyungki.github.io/AvatarForcing" target="_blank" rel="noopener noreferrer" class="">https://taekyungki.github.io/AvatarForcing</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Avatar Forcing, a framework using diffusion forcing for real-time interactive head avatar generation that processes multimodal user inputs with low latency. 2. Introduces a label-free direct preference optimization method using synthetic losing samples to learn expressive interactions. 3. Demonstrates real-time performance (~500ms latency, 6.8x speedup) and generates avatars preferred over 80% against the baseline for expressiveness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the lack of truly interactive and emotionally engaging talking head avatars by proposing Avatar Forcing, a framework that uses diffusion forcing for real-time, low-latency generation and a direct preference optimization method for label-free learning of expressive reactions. The method achieves a significant speedup and produces avatar motions that are strongly preferred by users in evaluations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [3D Gaussians, camera-controlled generation, temporal consistency, single-image-conditioned]</p>
</li>
<li class="">
<p><strong>authors:</strong> Melonie de Almeida, Daniela Ivanova, Tong Shi, John H. Williamson, Paul Henderson</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Glasgow</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00678" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00678</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://melonienimasha.github.io/Pixel-to-4D-Website/" target="_blank" rel="noopener noreferrer" class="">https://melonienimasha.github.io/Pixel-to-4D-Website/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion from a single image in a single forward pass., 2. Enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into rendered frames., 3. Achieves state-of-the-art video quality and inference efficiency on multiple datasets (KITTI, Waymo, RealEstate10K, DL3DV-10K).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05b260fc970977b430e0e91ebe61a5a7c0d11cd466b771b1dc287d76492ccd15_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05b260fc970977b430e0e91ebe61a5a7c0d11cd466b771b1dc287d76492ccd15_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of generating temporally consistent and geometrically sound videos from a single image with user-controlled camera paths. It proposes Pixel-to-4D, a method that builds a dynamic 3D Gaussian representation of the scene in one pass to enable fast, camera-guided video synthesis. The approach demonstrates superior video quality and efficiency compared to existing methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] DefVINS: Visual-Inertial Odometry for Deformable Scenes</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [visual-inertial odometry], [deformable scenes, observability analysis, embedded deformation graph, IMU anchoring]</p>
</li>
<li class="">
<p><strong>authors:</strong> Samuel Cerezo, Javier Civera</p>
</li>
<li class="">
<p><strong>institution:</strong> Universidad de Zaragoza</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00702" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00702</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A VIO framework (DefVINS) that explicitly separates rigid motion (IMU-anchored) from non-rigid deformation (modeled by an embedded deformation graph). 2. An observability analysis characterizing how inertial measurements constrain rigid motion and identify modes in deformable scenes. 3. A conditioning-based activation strategy that progressively enables non-rigid degrees of freedom to prevent ill-posed updates.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b312fb0bd7d9381dfbae957a5aabad3939766f96c58738070e2c5334cb31268_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b312fb0bd7d9381dfbae957a5aabad3939766f96c58738070e2c5334cb31268_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces DefVINS, a visual-inertial odometry framework designed for deformable scenes. It separates rigid and non-rigid motion, uses an observability analysis to guide a progressive activation strategy for deformation, and shows improved robustness in non-rigid environments through ablation studies.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Efficient Deep Demosaicing with Spatially Downsampled Isotropic Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image demosaicing], [isotropic networks, spatial downsampling, joint-demosaicing-and-denoising (JDD), DeepMAD, JD3Net]</p>
</li>
<li class="">
<p><strong>authors:</strong> Cory Fan, Wenchao Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Cornell University, OmniVision Technologies</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00703" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00703</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes that spatial downsampling can improve the efficiency and performance of isotropic networks for demosaicing, contrary to common design practices. 2. Designs and validates simple fully convolutional networks with downsampling using a mathematical architecture design technique adapted from DeepMAD. 3. Introduces JD3Net, a downsampled variant, which demonstrates strong empirical performance on various image demosaicing and JDD tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f98b6a6dc08bfce9cbda95473895cce4e107cbf797681c55dc3d3a75a0832b92_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f98b6a6dc08bfce9cbda95473895cce4e107cbf797681c55dc3d3a75a0832b92_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the computational inefficiency of isotropic networks for image demosaicing on mobile platforms. It proposes using spatial downsampling within these networks to improve efficiency and performance, validating the claim through network design and empirical testing of a model called JD3Net. The results show that the downsampled networks achieve strong performance on demosaicing and joint-demosaicing-and-denoising tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [SLAM (Simultaneous Localization and Mapping)], [Gaussian Splatting, Dense Initialization, DINOv3, Multi-view Triangulation, Real-time Mapping]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wei-Tse Cheng, Yen-Jen Chiou, Yuan-Fu Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> National Yang Ming Chiao Tung University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00705" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00705</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a one-shot, training-free dense Gaussian initialization via multi-view correspondence triangulation, replacing the iterative densification in GS-SLAM. 2. Introduces a robust correspondence generation method using DINOv3 descriptors refined by a confidence-aware inlier classifier. 3. Achieves faster convergence (~20% speedup), higher rendering fidelity, and maintains real-time performance (up to 925 FPS) while being compatible with existing GS-SLAM pipelines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4161c36bfb3fee2a260166b1caa94cf7f49f3bda268754e5be2f18620346aa62_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4161c36bfb3fee2a260166b1caa94cf7f49f3bda268754e5be2f18620346aa62_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces RGS-SLAM, a robust SLAM framework that improves upon Gaussian Splatting SLAM by replacing its iterative densification with a one-shot, dense Gaussian initialization from triangulated multi-view correspondences. This method, using refined DINOv3 features, stabilizes mapping, accelerates convergence, and enhances rendering quality. Evaluations show it achieves competitive or superior accuracy and real-time performance compared to state-of-the-art systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Multi-Level Feature Fusion for Continual Learning in Visual Quality Inspection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [continual learning], [multi-level feature fusion, catastrophic forgetting, visual quality inspection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Johannes C. Bauer, Paul Geng, Stephan Trattnig, Petr Dokládal, Rüdiger Daub</p>
</li>
<li class="">
<p><strong>institution:</strong> Technical University of Munich, MINES Paris, Fraunhofer IGCV</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00725" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00725</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a multi-level feature fusion (MLFF) approach for continual learning that utilizes representations from different depths of a pretrained network. 2. Demonstrates that MLFF matches end-to-end training performance for quality inspection tasks while using significantly fewer trainable parameters. 3. Shows that the approach reduces catastrophic forgetting and improves generalization robustness to new product types or defects.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6dc838232a328d451ea44fc50490b38b4e8ae077cf5f0ad87c37c75779e7a316_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6dc838232a328d451ea44fc50490b38b4e8ae077cf5f0ad87c37c75779e7a316_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of continual learning for visual quality inspection in volatile manufacturing scenarios like remanufacturing. It proposes a Multi-Level Feature Fusion (MLFF) method that fuses features from different network depths to enable efficient model adaptation. The results show that MLFF achieves performance comparable to full retraining with fewer parameters, while also mitigating catastrophic forgetting and improving robustness to new defects.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [domain adaptation], [data shift detection, performance degradation monitoring, vision-language model, confidence-based indicator, digital pathology]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hao Guan, Li Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> Brigham and Women&#x27;s Hospital, Harvard Medical School</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00716" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00716</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed DomainSAT, a lightweight toolbox with a graphical interface for systematic analysis and intuitive exploration of input data shift. 2. Introduced a label-free, confidence-based degradation indicator for output-based monitoring that directly captures changes in model prediction confidence. 3. Demonstrated that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a1e297333bf6471523693e104d6b047f585e96fad854f63687658f33d5b22d7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a1e297333bf6471523693e104d6b047f585e96fad854f63687658f33d5b22d7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates how to detect performance degradation in a pathology Vision-Language Model (VLM) when the input data distribution shifts after deployment. The authors propose a two-part framework: analyzing input-level data shift using their developed toolbox, DomainSAT, and monitoring output-level prediction confidence with a new label-free indicator. Their experiments show that combining these input and output monitoring methods provides a more reliable and complementary approach for detecting model degradation under data shift in digital pathology.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Grading Handwritten Engineering Exams with Multimodal Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [multimodal LLM, handwritten exam grading, reference grounding, ensemble grading, deterministic validation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Janez Perš, Jon Muhovič, Andrej Košir, Boštjan Murovec</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Ljubljana, Faculty of Electrical Engineering</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00730" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00730</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An end-to-end workflow for grading scanned handwritten engineering exams using multimodal LLMs that preserves standard paper-based exam processes, requiring only a handwritten reference solution and grading rules from the lecturer. 2. A multi-stage reliability design featuring format/presence checks, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. 3. Empirical evaluation demonstrating the pipeline&#x27;s effectiveness with state-of-the-art backends (GPT-5.2, Gemini-3 Pro), achieving ≈8-point mean absolute difference to lecturer grades and low bias, while ablations confirm the necessity of structured prompting and reference grounding.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb2f6c076cdb07cf31fcaea487e23c16eeb8f5f2479e5f44c8447c8ad253559_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb2f6c076cdb07cf31fcaea487e23c16eeb8f5f2479e5f44c8447c8ad253559_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents an automated workflow for grading handwritten STEM exams using multimodal large language models. The method uses a lecturer&#x27;s handwritten reference solution and grading rules, processed through a multi-stage pipeline with checks and ensemble grading for reliability. Evaluation shows the system achieves close agreement with human grades, confirming that structured prompting and reference grounding are essential for accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Unified Primitive Proxies for Structured Shape Completion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D shape completion], [primitive proxies, structured shape completion, primitive assembly, online target updates, Chamfer distance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhaiyu Chen, Yuqing Wang, Xiao Xiang Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> Technical University of Munich, Munich Center for Machine Learning</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00759" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00759</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://unico-completion.github.io" target="_blank" rel="noopener noreferrer" class="">https://unico-completion.github.io</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes UniCo, a model that predicts a complete set of primitives (geometry, semantics, inlier membership) in a single feed-forward pass for structured shape completion. 2. Introduces primitive proxies, learnable queries that are contextualized to produce assembly-ready outputs. 3. Presents a training strategy that couples primitives and points with online target updates for consistent optimization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21550f37418321862a038f9a28188f8255cfde85add7d3392abec68b2b44b586_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21550f37418321862a038f9a28188f8255cfde85add7d3392abec68b2b44b586_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of structured 3D shape completion from partial scans. It proposes UniCo, a model that uses primitive proxies to jointly predict a complete set of parametric primitives in a single pass, outperforming baselines by significantly lowering Chamfer distance and improving normal consistency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Investigating the Viability of Employing Multi-modal Large Language Models in the Context of Audio Deepfake Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [audio deepfake detection], [Multimodal Large Language Models, audio deepfake detection, zero-shot, fine-tuning, multi-prompt]</p>
</li>
<li class="">
<p><strong>authors:</strong> Akanksha Chuchra, Shukesh Reddy, Sudeepta Mishra, Abhijit Das, Abhinav Dhall</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology Ropar, Birla Institute of Technology and Science Pilani Hyderabad Campus, Monash University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00777" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00777</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Pioneering exploration of Multimodal Large Language Models for audio deepfake detection, a largely unexplored area. 2. Introduction of a text-aware, context-rich, question-answer based multi-prompt approach to facilitate multimodal understanding for the task. 3. Comprehensive evaluation of models (Qwen2-Audio-7B-Instruct, SALMONN) in zero-shot and fine-tuned modes, demonstrating their potential on in-domain data with minimal supervision.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e3660e734b6fff9841bbb8fb1f74d79456beeb841387a9fc33b145976d4701e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e3660e734b6fff9841bbb8fb1f74d79456beeb841387a9fc33b145976d4701e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the use of Multimodal Large Language Models for detecting audio deepfakes by combining audio inputs with text prompts. The method employs a multi-prompt, question-answer approach and evaluates models in zero-shot and fine-tuned settings. The results show that while models struggle without training and on out-of-domain data, they achieve promising performance on in-domain data with minimal supervision, indicating a viable path forward.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [deepfake detection], [self-supervised learning, auxiliary task, feature fusion, cross-dataset generalization, local directional pattern]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shukesh Reddy, Srijan Das, Abhijit Das</p>
</li>
<li class="">
<p><strong>institution:</strong> Birla Institute of Technology and Science, Pilani; University of North Carolina at Charlotte</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00789" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00789</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel framework that uses self-supervised learning as an auxiliary task to optimize the primary task of generalized deepfake detection. 2. Introduces a feature fusion strategy to combine representations from the self-supervised and primary tasks, creating a more powerful and unique feature set. 3. Demonstrates superior cross-dataset generalization performance on multiple deepfake datasets compared to state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9821725826aaddf6ca9d023a9670caa7eeef592d4bffd810a590e229d26cbb8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9821725826aaddf6ca9d023a9670caa7eeef592d4bffd810a590e229d26cbb8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of poor cross-dataset generalization in deepfake detection. The proposed method, Fusion-SSAT, leverages self-supervised learning as an auxiliary task and fuses its features with those from the primary detection task to create a more robust representation. The results show that this approach achieves better generalizability across multiple datasets than current state-of-the-art detectors.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [hypernetwork, conditional VAE, differential privacy, MMD alignment, client heterogeneity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sunny Gupta, Amit Sethi</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology Bombay</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00785" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00785</a></p>
</li>
<li class="">
<p><strong>code:</strong> github.com/sunnyinAI/FedHypeVAE</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A bi-level framework using a shared hypernetwork to generate personalized, client-aware decoders and class-conditional priors for a conditional VAE, decoupling local data from shared parameters. 2. Incorporation of differential privacy during hypernetwork optimization with noise-perturbed, clipped gradients to provide formal privacy guarantees against gradient leakage. 3. Introduction of a local MMD alignment loss and Lipschitz regularization to enhance stability and distributional coherence under non-IID data conditions, along with a neutral meta-code for domain-agnostic synthesis.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726225d41733e7b16755efae5f3b9ec28771c58479913c7f5581e9843368fd0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726225d41733e7b16755efae5f3b9ec28771c58479913c7f5581e9843368fd0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes FedHypeVAE, a federated learning framework that uses a differentially private hypernetwork to generate personalized conditional VAEs for synthesizing embedding-level data across decentralized clients. It addresses challenges of non-IID data and privacy by decoupling local data from shared parameters and ensuring formal privacy guarantees. The method establishes a foundation for privacy-preserving data synthesis in federated settings by unifying personalization, privacy, and distribution alignment at the generator level.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [U-Net, layer normalization, instance-batch normalization, left ventricle segmentation, cardiac MRI]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenhui Chu, Nikolaos V. Tsekos</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Houston</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00794" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00794</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed LNU-Net, a novel segmentation architecture derived from U-Net that applies layer normalization in each convolutional block. 2. Proposed IBU-Net, another novel architecture that incorporates instance and batch normalization together in the first convolutional block. 3. Demonstrated that the proposed methods outperform state-of-the-art approaches on a dataset of 805 MRI images using metrics like dice coefficient and average perpendicular distance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45a19599df20601908ea938c8bea28356e54685c5ce08dbe58a85a26dda95834_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45a19599df20601908ea938c8bea28356e54685c5ce08dbe58a85a26dda95834_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes two new deep learning models, LNU-Net and IBU-Net, for automated segmentation of the left ventricle in cardiac MRI images. The models are based on the U-Net architecture but incorporate different normalization strategies—layer normalization and a combined instance-batch normalization—to improve segmentation performance. Experimental results show that both proposed approaches outperform existing state-of-the-art methods on key evaluation metrics.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Automated electrostatic characterization of quantum dot devices in single- and bilayer heterostructures</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [quantum computing], [quantum dot, charge stability diagram, automated characterization, machine learning, image processing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Merritt P. R. Losert, Dario Denora, Barnaby van Straaten, Michael Chan, Stefan D. Oosterhout, Lucas Stehouwer, Giordano Scappucci, Menno Veldhorst, Justyna P. Zwolak</p>
</li>
<li class="">
<p><strong>institution:</strong> National Institute of Standards and Technology (NIST), Delft University of Technology (QuTech)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00067" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00067</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed an automated protocol combining ML, image processing, and object detection to extract capacitive properties from charge stability diagrams without manual labeling. 2. Demonstrated the method&#x27;s effectiveness on complex bilayer germanium heterostructures, which feature interlayer tunneling and distinct loading lines. 3. Enabled statistical estimation of physically relevant device parameters, such as lever arms and capacitive couplings, facilitating rapid device characterization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c536889ababa23206b42f144321036dbbfd329505ea45c505aa53c2a04bf7815_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c536889ababa23206b42f144321036dbbfd329505ea45c505aa53c2a04bf7815_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents an automated method for characterizing quantum dot devices by analyzing charge stability diagrams. The protocol integrates machine learning and image processing to identify charge transitions and extract capacitive properties from experimental data. It enables rapid, scalable extraction of key device parameters, which is critical for advancing large-scale quantum dot arrays.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Deep Learning Approach for the Diagnosis of Pediatric Pneumonia Using Chest X-ray Imaging</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [Convolutional Neural Network, Transfer Learning, Chest X-ray, Pediatric Pneumonia, RegNet]</p>
</li>
<li class="">
<p><strong>authors:</strong> Fatemeh Hosseinabadi, Mohammad Mojtaba Rohani</p>
</li>
<li class="">
<p><strong>institution:</strong> Zahedan University of Medical Sciences, Guilan University of Medical Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00041" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00041</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Evaluated and compared the performance of state-of-the-art CNN architectures (ResNetRS, RegNet, EfficientNetV2) for automated pediatric pneumonia diagnosis. 2. Applied transfer learning with ImageNet-pretrained weights to a curated dataset of pediatric chest X-rays to address data and expertise limitations. 3. Demonstrated that the RegNet model achieved the highest accuracy and sensitivity for this specific binary classification task.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbf67414c0b3072bcb906df1f0ca6e5942968a2374f2ac09a701ed20efb78f09_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbf67414c0b3072bcb906df1f0ca6e5942968a2374f2ac09a701ed20efb78f09_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes using transfer learning with deep convolutional neural networks to automate the diagnosis of pediatric pneumonia from chest X-ray images. The authors fine-tuned and compared three CNN models (ResNetRS, RegNet, EfficientNetV2) on a curated dataset, finding that RegNet performed best with 92.4% accuracy. The study concludes that such deep learning approaches can provide reliable diagnostic support, especially in settings with limited radiological expertise.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [neural fields, signal processing], [Neural Radiance Fields (NeRF), EEG, brain-computer interfaces, signal reconstruction, continuous representation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shahar Ain Kedem, Itamar Zimerman, Eliya Nachmani</p>
</li>
<li class="">
<p><strong>institution:</strong> Ben Gurion University of the Negev, Tel Aviv University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00012" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00012</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Shaharak88/neural-brain-fields" target="_blank" rel="noopener noreferrer" class="">https://github.com/Shaharak88/neural-brain-fields</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel NeRF-inspired method to learn a continuous representation of brain activity from discrete EEG electrode data. 2. Enables rendering of EEG signals at unseen time steps and spatial electrode positions, including simulating non-existent electrodes. 3. Demonstrates that the reconstructed signals can be used to improve the performance of standard EEG processing networks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79dda8675c25fe2c8906136ddc20e55e51c835cb08bed2a4e02388b7dadc16da_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79dda8675c25fe2c8906136ddc20e55e51c835cb08bed2a4e02388b7dadc16da_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Neural Brain Fields, a method inspired by Neural Radiance Fields (NeRF) to model EEG data. It trains a neural network on a single EEG sample to produce a fixed-size weight vector that encodes the continuous neural activity, allowing for signal reconstruction at any time or scalp location. The approach effectively generates data for non-existent electrodes, which can enhance downstream EEG analysis tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [dynamic scene reconstruction], [Adaptive Gabor Representation, Cubic Hermite Splines, Temporal Curvature Regularization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiewen Chan, Zhenjun Zhao, Yu-Lun Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> National Yang Ming Chiao Tung University, University of Zaragoza</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00796" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00796</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Adaptive Gabor Representation, which extends Gaussian primitives with learnable frequency weights and adaptive energy compensation to capture high-frequency details while maintaining stability. 2. Introduced a temporal continuity modeling approach using Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution and reduce interpolation artifacts. 3. Designed an Adaptive Initialization mechanism that leverages depth estimation, point tracking, and foreground masks to establish a stable initial point cloud distribution for efficient training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b1693b069cec7400ac55a2d212a74c9a9ef7f185c00490fc53dbec8796fa8f9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b1693b069cec7400ac55a2d212a74c9a9ef7f185c00490fc53dbec8796fa8f9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes AdaGaR, a novel framework for dynamic 3D scene reconstruction from monocular videos. It introduces an Adaptive Gabor Representation to capture high-frequency details and uses Cubic Hermite Splines with regularization to ensure temporal smoothness. Experiments show state-of-the-art performance on the DAVIS dataset, achieving superior rendering quality and strong generalization in tasks like frame interpolation and video editing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] The Impact of Lesion Focus on the Performance of AI-Based Melanoma Classification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [lesion attention, explainable AI, Grad-CAM, sensitivity analysis, transfer learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tanay Donde</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Illinois Urbana-Champaign</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00355" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00355</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Analyzed the relationship between model attention on lesion areas and diagnostic performance metrics (precision, recall, F1-score) in melanoma classification. 2. Employed a multi-faceted methodology involving masked images, bounding box detection, and transfer learning to investigate lesion focus. 3. Demonstrated that models with higher focus on lesion areas achieve better diagnostic performance, highlighting the value of interpretable AI for building trustworthy medical models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6577c58857fa2854c7a747e548163eec65a456f081e789308195cccd972d8c9f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6577c58857fa2854c7a747e548163eec65a456f081e789308195cccd972d8c9f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study investigates how the focus (attention) of AI models on lesion areas affects their performance in classifying melanoma from skin images. The authors used methods like masked images and explainability techniques (e.g., Grad-CAM) to analyze this relationship. They found that models which pay more attention to the actual lesion regions perform better, suggesting that improving model interpretability can lead to more accurate and reliable diagnostic tools.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2026-01-05T03:17:11.000Z" itemprop="dateModified">Jan 5, 2026</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/cscv/20251229-20260104"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251229-20260104 (cs.CV)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/category/cscy"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">cs.CY</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2026-01-05" class="table-of-contents__link toc-highlight">2026-01-05</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2026-01</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><div class="calendar-cell calendar-empty"></div><div class="calendar-cell calendar-empty"></div><div class="calendar-cell calendar-empty"></div><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251229-20260104#2026-01-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251229-20260104#2026-01-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251229-20260104#2026-01-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20251229-20260104#2026-01-04">4</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/cscv/20260105-20260111#2026-01-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260105-20260111#2026-01-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260105-20260111#2026-01-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260105-20260111#2026-01-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260105-20260111#2026-01-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260105-20260111#2026-01-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260105-20260111#2026-01-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260112-20260118#2026-01-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260112-20260118#2026-01-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260112-20260118#2026-01-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260112-20260118#2026-01-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260112-20260118#2026-01-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260112-20260118#2026-01-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260112-20260118#2026-01-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260119-20260125#2026-01-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260119-20260125#2026-01-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260119-20260125#2026-01-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260119-20260125#2026-01-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260119-20260125#2026-01-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260119-20260125#2026-01-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260119-20260125#2026-01-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260126-20260201#2026-01-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260126-20260201#2026-01-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260126-20260201#2026-01-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260126-20260201#2026-01-29">29</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260126-20260201#2026-01-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cscv/20260126-20260201#2026-01-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>