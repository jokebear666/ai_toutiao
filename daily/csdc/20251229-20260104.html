<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_DC/20251229-20260104" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251229-20260104 (cs.DC) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/csdc/20251229-20260104"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251229-20260104 (cs.DC) | AI头条"><meta data-rh="true" name="description" content="2025-12-29"><meta data-rh="true" property="og:description" content="2025-12-29"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/csdc/20251229-20260104"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/csdc/20251229-20260104" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/csdc/20251229-20260104" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.DC","item":"https://jokebear666.github.io/ai_toutiao/daily/csdc"},{"@type":"ListItem","position":3,"name":"20251229-20260104 (cs.DC)","item":"https://jokebear666.github.io/ai_toutiao/daily/csdc/20251229-20260104"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.38d649e8.css">
<script src="/ai_toutiao/assets/js/runtime~main.ded78d19.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.e580b2b9.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/daily">Daily</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/paper">Paper</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Collapse sidebar category &#x27;cs.DC&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cs_DC/20251215-20251221"><span title="20251215-20251221 (cs.DC)" class="linkLabel_WmDU">20251215-20251221 (cs.DC)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/csdc/20251222-20251228"><span title="20251222-20251228 (cs.DC)" class="linkLabel_WmDU">20251222-20251228 (cs.DC)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/csdc/20251229-20260104"><span title="20251229-20260104 (cs.DC)" class="linkLabel_WmDU">20251229-20260104 (cs.DC)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/daily/csdc"><span>cs.DC</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251229-20260104 (cs.DC)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251229-20260104 (cs.DC)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-29">2025-12-29<a href="#2025-12-29" class="hash-link" aria-label="Direct link to 2025-12-29" title="Direct link to 2025-12-29" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251229] Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [data spaces, cloud-edge continuum, containerized microservices, edge AI, intelligent infrastructure monitoring]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dimitrios Amaxilatis, Themistoklis Sarantakos, Nikolaos Tsironis, Souvik Sengupta, Kostas Ramantas, Jhofre Ojeda</p>
</li>
<li class="">
<p><strong>institution:</strong> Spark Works Ltd., IONOS SE, Iquadrat Informática S.L.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21340" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21340</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A real-world implementation of intelligent infrastructure monitoring within a data space-enabled cloud-edge framework, demonstrating practical integration. 2. Leveraging edge computing, containerized microservices, and interoperable data sharing to address challenges like sensor integration, data privacy, and scalability. 3. Showcasing the training and deployment of AI/ML services directly at the edge for optimized resource use and timely decision-making in smart city applications.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ae37cae2dac445e3682b661f116dd30e5d680105ac5070eb7b48c049ab9aff3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ae37cae2dac445e3682b661f116dd30e5d680105ac5070eb7b48c049ab9aff3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a real-world use case for smart cities, implementing an intelligent climate monitoring system within a cloud-edge continuum framework enabled by data spaces. The method combines edge computing, containerized microservices, and secure data sharing to facilitate localized analytics and AI deployment. The conclusion highlights the transformative potential of integrating AI, edge computing, and data spaces for building efficient and resilient smart city infrastructures.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [lossy compression, quality prediction, deep-surrogate, mixture-of-experts, feature-extraction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Khondoker Mirazul Mumenin, Robert Underwood, Dong Dai, Jinzhen Wang, Sheng Di, Zarija Lukić, Franck Cappello</p>
</li>
<li class="">
<p><strong>institution:</strong> University of North Carolina at Charlotte, Argonne National Laboratory, University of Delaware, Lawrence Berkeley National Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21433" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21433</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1) A generalizable surrogate model for predicting compression quality across different compressors, quality metrics, and datasets. 2) A novel two-stage design that decouples expensive feature extraction from lightweight prediction for efficient training and modular inference. 3) A mixture-of-experts design to optimize performance and robustness for time-evolving scientific data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06e831f83754144a92a117a184fdcc51da0584d4163390cf94b34d4175b77a1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06e831f83754144a92a117a184fdcc51da0584d4163390cf94b34d4175b77a1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes DeepCQ, a deep-surrogate framework to efficiently predict the quality of data after lossy compression, which is traditionally computationally expensive to assess. The method uses a two-stage and mixture-of-experts design for generalizability and robustness across different compressors, metrics, and time-evolving datasets. The framework achieves high predictive accuracy (errors under 10%) on real-world applications, enabling informed compression decisions and reducing I/O and computational overhead.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Demystifying ARM SME to Optimize General Matrix Multiplications</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [gpu kernels], [ARM SME, GEMM, cache-aware partitioning, micro-kernels, on-the-fly transposition]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chencheng Deng, Weiling Yang, Jianbin Fang, Dezun Dong</p>
</li>
<li class="">
<p><strong>institution:</strong> College of Computer Science and Technology, National University of Defense Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21473" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21473</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A systematic characterization of the ARM SME architecture that derives optimization guidelines for GEMM. 2. The design and implementation of MpGEMM, an open-source library featuring cache-aware partitioning and efficient data packing with on-the-fly transposition. 3. Specialized micro-kernels that fully utilize SME&#x27;s multi-vector loads and all available tile registers to maximize performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3bc3a0f3452bf6376dcfa53f5f9e56a621c5b526537a2008e7f55c112b765095_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3bc3a0f3452bf6376dcfa53f5f9e56a621c5b526537a2008e7f55c112b765095_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the underutilization of ARM&#x27;s Scalable Matrix Extension (SME) hardware for large-scale General Matrix Multiplication (GEMM). It proposes MpGEMM, an open-source library that optimizes GEMM through cache-aware partitioning, efficient data packing, and specialized micro-kernels tailored for SME. Evaluations on an Apple M4 Pro show MpGEMM achieves a 1.23x speedup over the vendor-optimized Apple Accelerate library.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [mixture-of-experts (MoE), disaggregated expert parallelism (DEP), task scheduling, inference throughput, fine-grained pipelining]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xinglin Pan, Shaohuai Shi, Wenxiang Lin, Yuxin Wang, Zhenheng Tang, Wei Wang, Xiaowen Chu</p>
</li>
<li class="">
<p><strong>institution:</strong> The Hong Kong University of Science and Technology (Guangzhou), Harbin Institute of Technology (Shenzhen), Hong Kong Baptist University, The Hong Kong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21487" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21487</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1) Partitioning intensive computation and communication tasks into smaller, fine-grained tasks to enable pipelining, including support for shared experts. 2) Formulating a fine-grained task scheduling optimization problem that supports variable task granularity and ordering. 3) Developing an efficient solver to navigate the large solution space and derive a near-optimal task schedule.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44cc55e59c66470ffb4e47c93ad8e48f60e8377f30eff6289fbad1cfcb862c96_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44cc55e59c66470ffb4e47c93ad8e48f60e8377f30eff6289fbad1cfcb862c96_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the memory-intensive inference problem in Mixture-of-Experts (MoE) models by proposing FinDEP, a fine-grained task scheduling algorithm for Disaggregated Expert Parallelism (DEP). FinDEP improves inference throughput by maximizing task overlap through computational partitioning and optimized scheduling. Experiments on systems with up to 32 GPUs show throughput improvements of up to 1.61x over prior methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [compiler &amp; ir], [e-graph, term rewriting, phase ordering, NUMA abstraction, auto vectorize]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hui Guo, Qihang Zheng, Chenghai Huo, Dongliang Guo, Haoqi Yang, Yang Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Canaan Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21571" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21571</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/kendryte/nncase" target="_blank" rel="noopener noreferrer" class="">https://github.com/kendryte/nncase</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an end-to-end compilation framework (nncase) that unifies LLM deployment across heterogeneous memory architectures using a NUMA abstraction for a &quot;compile once, adapt everywhere&quot; capability. 2. Introduces an e-graph-based term rewriting engine with equality saturation to mitigate the phase ordering problem and enable global optimization of computation and data movement. 3. Integrates three key automated optimization modules: Auto Vectorize for heterogeneous computing units, Auto Distribution for parallel strategies with communication optimization, and Auto Schedule for on-chip cache locality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a555b38ecd80e8c11606362e5402405bbf0053e11754e06f720d50ce213ee0d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a555b38ecd80e8c11606362e5402405bbf0053e11754e06f720d50ce213ee0d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents nncase, an end-to-end compiler framework designed to tackle the challenge of efficiently deploying large language models on heterogeneous memory architectures. Its core innovation is an e-graph-based rewriting engine that avoids the phase ordering problem, enabling unified optimization across diverse hardware targets. Evaluations show nncase outperforms frameworks like MLC LLM and Intel IPEX, achieving performance close to hand-optimized llama.cpp, demonstrating the viability of automated compilation for high-performance LLM deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Embedding Samples Dispatching for Recommendation Model Training in Edge Environments</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [memory &amp; caching], [edge computing, embedding cache, parameter server, sample dispatching, transmission cost]</p>
</li>
<li class="">
<p><strong>authors:</strong> Guopeng Li, Haisheng Tan, Chi Zhang, Hongqiu Ni, Zilong Wang, Xinyue Zhang, Yang Xu, Han Tian</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology of China (USTC), Hefei University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21615" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21615</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed ESD, a novel mechanism to optimize the dispatch of input embedding samples to edge workers to minimize embedding transmission cost. 2. Designed HybridDis, a dispatch decision method that combines an optimal algorithm and a heuristic to balance decision quality and resource consumption. 3. Implemented a prototype and demonstrated significant reductions in transmission cost (up to 36.76%) and training speedup (up to 1.74x) on real-world workloads.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f94cc43f65f5fd909f8762bda535a33eea94f879931ebe1a563280cb0db1be81_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f94cc43f65f5fd909f8762bda535a33eea94f879931ebe1a563280cb0db1be81_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high communication cost of embedding transmission during Deep Learning Recommendation Model (DLRM) training in edge environments. It proposes ESD, a mechanism that dispatches input samples to edge workers to minimize expected transmission cost, using a hybrid decision method called HybridDis. Experimental results show that ESD significantly reduces transmission cost and speeds up end-to-end training compared to state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] LEFT-RS: A Lock-Free Fault-Tolerant Resource Sharing Protocol for Multicore Real-Time Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [real-time systems], [lock-free, fault-tolerance, resource sharing, multicore, worst-case response time analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nan Chen, Xiaotian Dai, Tong Cheng, Alan Burns, Iain Bate, Shuai Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> University of York, Sun Yat-sen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21701" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21701</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the LEFT-RS protocol, a lock-free design that allows concurrent read access to global resources and parallel entry into critical sections, improving efficiency. 2. Enhances fault resilience by limiting overhead and enabling tasks to complete earlier if others experience faults, reducing blocking. 3. Provides a comprehensive worst-case response time analysis to ensure timing guarantees for the proposed protocol.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb19a780f9199527b92c55981536e4b4108e6135efe187882739942a46ebf5ed_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb19a780f9199527b92c55981536e4b4108e6135efe187882739942a46ebf5ed_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes LEFT-RS, a lock-free and fault-tolerant resource sharing protocol for multicore real-time systems. It allows tasks to concurrently access resources and enter critical sections in parallel, improving efficiency and resilience to transient faults. Evaluation shows it significantly outperforms existing methods, achieving up to an 84.5% average improvement in schedulability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Hyperion: Low-Latency Ultra-HD Video Analytics via Collaborative Vision Transformer Inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [vision transformer, cloud-device collaboration, dynamic scheduling, patch-level importance, weighted ensembling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Linyi Jiang, Yifei Zhu, Hao Yin, Bo Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, Tsinghua University, Hong Kong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21730" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21730</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A collaboration-aware importance scorer that identifies critical regions at the patch level for selective processing. 2. A dynamic scheduler that adaptively adjusts patch transmission quality to balance latency and accuracy under changing network conditions. 3. A weighted ensembler that fuses edge and cloud inference results to improve overall accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/883099a5be7486c0821b7ffc4858fa9de1fb7c6f3487310e5eb913db9f04c63e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/883099a5be7486c0821b7ffc4858fa9de1fb7c6f3487310e5eb913db9f04c63e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents Hyperion, a cloud-device collaborative framework designed to enable low-latency inference on Ultra-HD video using off-the-shelf vision transformers. It tackles computational and transmission bottlenecks by selectively processing critical patches, dynamically adjusting transmission quality, and fusing results. Experiments show Hyperion improves frame processing rate by up to 1.61x and accuracy by up to 20.2% compared to baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [cluster infrastructure], [LSTM, Random Forest, MQTT, InfluxDB, Streamlit]</p>
</li>
<li class="">
<p><strong>authors:</strong> Krishna Chaitanya Sunkara, Rambabu Konakanchi</p>
</li>
<li class="">
<p><strong>institution:</strong> Oracle, Charles Schwab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21801" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21801</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Probabilistic LSTM forecasting validated within ±30-minute windows for coolant leaks, 2. 96.5% F1-score Random Forest detection for immediate leak identification, 3. Integrated smart IoT architecture design with MQTT streaming, InfluxDB storage, and Streamlit dashboards for energy-efficient data center operations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa113d59b2dc6ec7a3bf00f9eebc8541689a6235867cf59ce376cdcfa30a2a5c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa113d59b2dc6ec7a3bf00f9eebc8541689a6235867cf59ce376cdcfa30a2a5c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes a smart IoT monitoring system combining LSTM neural networks for probabilistic leak forecasting and Random Forest classifiers for instant detection in liquid-cooled AI data centers. The system, tested on synthetic data, achieves 96.5% detection accuracy and 87% forecasting accuracy, potentially preventing significant energy waste through proactive maintenance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] LIME<!-- -->:Accelerating<!-- --> Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [collaborative inference, pipeline parallelism, model offloading, memory adaptation, edge computing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mingyu Sun, Xiao Zhang, Shen Qu, Yan Li, Mengbai Xiao, Yuan Yuan, Dongxiao Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> Shandong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21835" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21835</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LIME, a collaborative system for lossless LLM inference across multiple memory-constrained edge devices under limited bandwidth. 2. Employs an interleaved pipeline parallelism with model offloading to dynamically balance computation and communication. 3. Introduces a fine-grained offline allocation scheduler and an online memory adaptation strategy to optimize resource usage and minimize inference latency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4d2da6a7be206646ebc1b92d8a0053408a991ec073254f89b4182ecdc54fe1b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4d2da6a7be206646ebc1b92d8a0053408a991ec073254f89b4182ecdc54fe1b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes LIME, a system that enables lossless, collaborative LLM inference on multiple memory-constrained edge devices by using interleaved pipeline parallelism and model offloading, along with offline scheduling and online memory adaptation. Experiments on four Nvidia Jetson devices with LLaMA3.3-70B show that LIME achieves significant speedups over baselines without accuracy loss.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [distributed inference, block placement, request routing, performance modeling, resource allocation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tingyang Sun, Ting He, Bo Ji, Parimal Parag</p>
</li>
<li class="">
<p><strong>institution:</strong> Pennsylvania State University, Virginia Tech, Indian Institute of Science</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21884" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21884</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed experimentally validated performance models for distributed LLM inference under given block placement and request routing decisions. 2. Formulated the offline optimization problem as a MILP, proved its NP-hardness, and designed a polynomial-complexity algorithm with performance guarantees. 3. Adapted the offline algorithm for the online setting with the same performance guarantee under bounded load.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25938e1be55cbd072ba066aea4bb0e492f8b8c2a83e48eaa7e09e800b8697383_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25938e1be55cbd072ba066aea4bb0e492f8b8c2a83e48eaa7e09e800b8697383_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the resource allocation problem for geographically-distributed LLM inference, focusing on optimizing block placement and request routing. It proposes performance models, offline and online algorithms with theoretical guarantees, and a lightweight CPU-only simulator. The solution significantly reduces inference time compared to the state-of-the-art in diverse distributed settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] BLEST: Blazingly Efficient BFS using Tensor Cores</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [gpu kernels], [BFS, Tensor Cores, SpMSpV, Graph Reordering, Kernel Fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Deniz Elbek, Kamer Kaya</p>
</li>
<li class="">
<p><strong>institution:</strong> Sabanci University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21967" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21967</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Binarised Virtual Slice Sets (BVSS) for warp-level load balancing and eliminating frontier-oblivious work assignment in BFS., 2. Applies two complementary graph reordering strategies (compression-oriented and bandwidth-reducing) to improve memory efficiency and update locality., 3. Develops a batched SpMSpV multiplication pattern using bitwise Tensor Core tiles and combines kernel fusion with a lazy vertex update scheme to reduce synchronization and atomic overheads.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da59a50541aea7cf054914628e80911f7ca77a9353af15a6a412588e726ca791_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da59a50541aea7cf054914628e80911f7ca77a9353af15a6a412588e726ca791_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents BLEST, a framework that accelerates Breadth-First Search (BFS) on GPUs by efficiently mapping the irregular computation onto dense-math Tensor Cores. The method reformulates the BFS pipeline using a bitmap-oriented structure, specialized load balancing, graph reordering, and kernel fusion. Experiments show that BLEST achieves significant speedups (3.58x to 4.9x) over state-of-the-art GPU-based BFS implementations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Proceedings First Workshop on Adaptable Cloud Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Giuseppe De Palma, Saverio Giallorenzo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22054" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22054</a></li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [communication &amp; networking], [Mixture-of-Experts, expert parallelism, data shuffling, transformation-communication fusion, collective communication]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhuoran Zhu, Chunyang Zhu, Hao Lin, Xu Fu, Yiming Zhou, Quanlu Zhang, Zhenhua Li, Feng Qian, Chao Yu, Boxun Li, Guohao Dai, Yu Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Infinigence AI, University of Southern California, Zhongguancun Academy, Shanghai Jiao Tong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22036" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22036</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies the root cause of inefficiency in MoE data shuffling as the misalignment between expert-major and device-major data layouts, requiring disaggregated transformation and communication. 2. Proposes FUSCO, a communication library that fuses data transformation and communication operations into a single, efficient pipeline to eliminate redundant data movement. 3. Introduces lightweight planning and load-balancing mechanisms to eliminate redundant communication and disperse traffic, further optimizing the shuffling process.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7cf69e647d44d7f0b5d2cdef643280359c8d359bbdf2f836c065bb3b6fb214ae_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7cf69e647d44d7f0b5d2cdef643280359c8d359bbdf2f836c065bb3b6fb214ae_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the performance bottleneck of distributed data shuffling in Mixture-of-Experts (MoE) model training and inference. It proposes FUSCO, a communication library that fuses data transformation and communication to align expert-major and device-major data layouts efficiently. Evaluations show FUSCO achieves significant speedups over existing libraries like NCCL and DeepEP, reducing both training and inference latency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [federated fine-tuning, connection failures, adaptive aggregation, data heterogeneity, convergence guarantee]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yanmeng Wang, Zhiwen Dai, Shuai Wang, Jian Zhou, Fu Xiao, Tony Q. S. Quek, Tsung-Hui Chang</p>
</li>
<li class="">
<p><strong>institution:</strong> The affiliations include IEEE members, suggesting multiple institutions. Based on common patterns, likely institutions include The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen) and/or other Chinese universities/tech institutes, given authors like Tsung-Hui Chang and Tony Q. S. Quek are affiliated with such institutions.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22035" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22035</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FedAuto, a novel Federated Fine-Tuning framework that mitigates the combined effects of unreliable connections and data heterogeneity via adaptive aggregation, requiring no prior knowledge of network conditions. 2. Establishes a rigorous, per-round convergence guarantee for FedAuto that holds for each individual realization, removing common assumptions on failure probabilities or client selection. 3. Demonstrates through extensive experiments that FedAuto outperforms state-of-the-art baselines under diverse failure scenarios for both full and partial-parameter fine-tuning (e.g., LoRA).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09b9295640a8e9a219a19de76effe76cb1ea0696845676f4a5d9a059161538fb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09b9295640a8e9a219a19de76effe76cb1ea0696845676f4a5d9a059161538fb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the performance degradation of Federated Fine-Tuning (FFT) in real-world networks with unreliable connections and heterogeneous data. It proposes FedAuto, a framework that uses adaptive aggregation to handle these issues without prior network knowledge or infrastructure changes. Experiments show FedAuto consistently outperforms existing methods and provides stronger theoretical convergence guarantees.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [root cause analysis, service dependency graph, program dependence graph, LLM agent, cloud incident]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shengkun Cui, Rahul Krishna, Saurabh Jha, Ravishankar K. Iyer</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Illinois at Urbana-Champaign, IBM Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22113" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22113</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. PRAXIS, an agentic approach for cloud incident RCA with structured, LLM-driven graph reasoning and traversal over microservice and program dependency graphs. 2. An application of the hammock block program dependence graph for agentic RCA, leveraging its hierarchical structure for multi-granular code analysis. 3. A Code-Cloud-RCA Benchmark consisting of 30 real-world incident scenarios injected in a live Kubernetes environment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62ebd8a01fd966235e0d8d40581cb8352024a391331fada8ea23868c2235ada9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62ebd8a01fd966235e0d8d40581cb8352024a391331fada8ea23868c2235ada9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces PRAXIS, an orchestrator that uses an LLM-driven agent to traverse service dependency graphs and program dependence graphs to diagnose the root cause of code- and configuration-related cloud incidents. Compared to ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x, as demonstrated on a benchmark of 30 real-world incidents.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-29T06:08:20.000Z" itemprop="dateModified">Dec 29, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/csdc/20251222-20251228"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251222-20251228 (cs.DC)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/daily/csdl"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">cs.DL</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-29" class="table-of-contents__link toc-highlight">2025-12-29</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/csdc/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csdc/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>