<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_HC/20251222-20251228" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251222-20251228 (cs.HC) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/cshc/20251222-20251228"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251222-20251228 (cs.HC) | AI头条"><meta data-rh="true" name="description" content="2025-12-22"><meta data-rh="true" property="og:description" content="2025-12-22"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/cshc/20251222-20251228"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cshc/20251222-20251228" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cshc/20251222-20251228" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.HC","item":"https://jokebear666.github.io/ai_toutiao/category/cshc"},{"@type":"ListItem","position":3,"name":"20251222-20251228 (cs.HC)","item":"https://jokebear666.github.io/ai_toutiao/daily/cshc/20251222-20251228"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.647bd1ff.css">
<script src="/ai_toutiao/assets/js/runtime~main.500c5cf3.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.1e3609bc.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/arxiv-daily"><span title="Arxiv每日论文" class="linkLabel_WmDU">Arxiv每日论文</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Collapse sidebar category &#x27;cs.HC&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cs_HC/20251215-20251221"><span title="20251215-20251221 (cs.HC)" class="linkLabel_WmDU">20251215-20251221 (cs.HC)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/cshc/20251222-20251228"><span title="20251222-20251228 (cs.HC)" class="linkLabel_WmDU">20251222-20251228 (cs.HC)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cshc/20251229-20260104"><span title="20251229-20260104 (cs.HC)" class="linkLabel_WmDU">20251229-20260104 (cs.HC)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csoh"><span title="cs.OH" class="categoryLinkLabel_W154">cs.OH</span></a><button aria-label="Expand sidebar category &#x27;cs.OH&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/cshc"><span>cs.HC</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251222-20251228 (cs.HC)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251222-20251228 (cs.HC)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-22">2025-12-22<a href="#2025-12-22" class="hash-link" aria-label="Direct link to 2025-12-22" title="Direct link to 2025-12-22" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251222] Adversarial VR: An Open-Source Testbed for Evaluating Adversarial Robustness of VR Cybersickness Detection and Mitigation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [adversarial attacks, deep learning, cybersickness detection, visual tunneling, MI-FGSM, PGD, C&amp;W, DeepTCN, Transformer]</li>
<li class=""><strong>authors:</strong> Istiak Ahmed, Ripan Kumar Kundu, Khaza Anuarul Hoque</li>
<li class=""><strong>institution:</strong> University of Missouri-Columbia</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17029" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17029</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a313962e09ceaa54a617d0e446a38a50ffa44d10894d76830f87cd1e74c0749_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a313962e09ceaa54a617d0e446a38a50ffa44d10894d76830f87cd1e74c0749_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Adversarial-VR, an open-source Unity testbed that integrates DeepTCN and Transformer models for real-time cybersickness detection and mitigation, and evaluates their robustness against adversarial attacks like MI-FGSM, PGD, and C&amp;W. The results show these attacks can successfully fool the system, significantly degrading model accuracy and preventing correct mitigation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Bots Don&#x27;t Sit Still: A Longitudinal Study of Bot Behaviour Change, Temporal Drift, and Feature-Structure Evolution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [social media analysis], [Augmented Dickey-Fuller test, KPSS test, Spearman correlation, Chi-square test, time series analysis, stationarity testing]</li>
<li class=""><strong>authors:</strong> Ohoud Alzahrani, Russell Beale, Bob Hendley</li>
<li class=""><strong>institution:</strong> University of Birmingham</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17067" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17067</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts a longitudinal study analyzing the temporal behavior of promotional Twitter bots using time series analysis and statistical tests on ten content-based meta-features. It finds that bot behavior is non-stationary, with individual features and their interdependencies evolving systematically over time and across bot generations. The conclusion is that bot-detection systems must account for this dynamic adaptation and avoid treating behavioral features as static.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric and Trustworthy Explanations for Daily Use Cases</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [large language model, explainable ai, augmented reality, personalized explanations, real-time object detection, user study]</li>
<li class=""><strong>authors:</strong> Ripan Kumar Kundu, Istiak Ahmed, Khaza Anuarul Hoque</li>
<li class=""><strong>institution:</strong> University of Missouri-Columbia</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17172" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17172</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7767a7bc851a49f82148423782803913a5c377f60c4ce3a9cc7383c22a6d08a4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7767a7bc851a49f82148423782803913a5c377f60c4ce3a9cc7383c22a6d08a4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes PILAR, a framework that uses a pre-trained large language model (LLM) to generate unified, context-aware, and personalized explanations for AI-driven augmented reality systems. A user study on a recipe recommendation prototype showed that the LLM-based explanation interface significantly improved user task performance and perceived transparency compared to a traditional template-based approach.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [spatio-temporal feature representation, channel attention, self-attention, recurrent neural networks, video-based gaze estimation]</li>
<li class=""><strong>authors:</strong> Alexandre Personnic, Mihai Bâce</li>
<li class=""><strong>institution:</strong> KU Leuven</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17673" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17673</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes the Spatio-Temporal Gaze Network (ST-Gaze), which combines a CNN backbone with channel and self-attention modules to fuse eye and face features, then models intra- and inter-frame dynamics by treating features as a spatial sequence propagated through time. The method achieves state-of-the-art performance on the EVE dataset, demonstrating that preserving intra-frame spatial context is superior to premature spatial pooling for robust video-based gaze estimation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] ShareChat: A Dataset of Chatbot Conversations in the Wild</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [dataset collection, multi-turn conversations, platform affordances, source citations, temporal analysis, cross-platform corpus]</li>
<li class=""><strong>authors:</strong> Yueru Yan, Tuc Nguyen, Bo Su, Melissa Lieffers, Thai Le</li>
<li class=""><strong>institution:</strong> Indiana University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17843" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17843</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces ShareChat, a large-scale dataset of real-world chatbot conversations collected from five major platforms, preserving interface-specific features like reasoning traces and source links. It demonstrates the dataset&#x27;s utility through analyses of user intent satisfaction, citation behaviors, and evolving usage patterns, providing a resource for studying authentic user-LLM interactions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational social science], [computational text analysis, machine learning (ML), natural language processing (NLP), ethnography, in-depth interviews, mixed-methods]</li>
<li class=""><strong>authors:</strong> Corey M. Abramson</li>
<li class=""><strong>institution:</strong> Rice University, UC San Francisco</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17850" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17850</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper demonstrates how computational social science tools like machine learning and natural language processing can be integrated with traditional qualitative methods (e.g., ethnography, interviews) to study aging. It concludes that these computational methods can broaden qualitative research by streamlining workflows, scaling up projects, and enabling new multi-method insights, rather than replacing its foundational approaches.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-23">2025-12-23<a href="#2025-12-23" class="hash-link" aria-label="Direct link to 2025-12-23" title="Direct link to 2025-12-23" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251223] Design and Integration of Thermal and Vibrotactile Feedback for Lifelike Touch in Social Robots</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jacqueline Borgstedt, Jake Bhattacharyya, Matteo Iovino, Frank E. Pollick, Stephen Brewster</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18032" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18032</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab5ccc615f8631e1cb919fee6836987e7793d4803a41430b431092ecd0b25ec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab5ccc615f8631e1cb919fee6836987e7793d4803a41430b431092ecd0b25ec_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Design and Integration of Thermal and Vibrotactile Feedback for Lifelike Touch in Social Robots</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] From Prompt to Product: A Human-Centered Benchmark of Agentic App Generation Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Marcos Ortiz, Justin Hill, Collin Overbay, Ingrida Semenec, Frederic Sauve-Hoover, Jim Schwoebel, Joel Shor</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18080" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18080</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0154cb62824a09bcbf4a6476b89c563b0f548b2e6721f62b4207082ab09ee544_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0154cb62824a09bcbf4a6476b89c563b0f548b2e6721f62b4207082ab09ee544_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> From Prompt to Product: A Human-Centered Benchmark of Agentic App Generation Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Characterising Behavioural Families and Dynamics of Promotional Twitter Bots via Sequence-Based Modelling</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ohoud Alzahrani, Russell Beale, Robert J. Hendley</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18077" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18077</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ecf679a26dc73049a43a2ec8c3b4b9a5b43ae16a82684041393594d146210f5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ecf679a26dc73049a43a2ec8c3b4b9a5b43ae16a82684041393594d146210f5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Characterising Behavioural Families and Dynamics of Promotional Twitter Bots via Sequence-Based Modelling</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Dimensionality Reduction Considered Harmful (Some of the Time)</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hyeon Jeon</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18230" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18230</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6c2d73af1d47933ab3fe41a060a175bd40e436f32af5added065941e07d0688_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6c2d73af1d47933ab3fe41a060a175bd40e436f32af5added065941e07d0688_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dimensionality Reduction Considered Harmful (Some of the Time)</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] The Social Blindspot in Human-AI Collaboration: How Undetected AI Personas Reshape Team Dynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Lixiang Yan, Xibin Han, Yu Zhang, Samuel Greiff, Inge Molenaar, Roberto Martinez-Maldonado, Yizhou Fan, Linxuan Zhao, Xinyu Li, Yueqiao Jin, Dragan Gašević</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18234" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18234</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08ef726a4410ec8749e604356b03b8afe29560606553bd2e895fa2e42f8a4d5a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08ef726a4410ec8749e604356b03b8afe29560606553bd2e895fa2e42f8a4d5a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The Social Blindspot in Human-AI Collaboration: How Undetected AI Personas Reshape Team Dynamics</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Emergent Learner Agency in Implicit Human-AI Collaboration: How AI Personas Reshape Creative-Regulatory Interaction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yueqiao Jin, Roberto Martinez-Maldonado, Dragan Gašević, Lixiang Yan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18239" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18239</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90be08528a89abf308b5f3179521424f35d5720259e534256a475d02ffe5b615_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90be08528a89abf308b5f3179521424f35d5720259e534256a475d02ffe5b615_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Emergent Learner Agency in Implicit Human-AI Collaboration: How AI Personas Reshape Creative-Regulatory Interaction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Software Vulnerability Management in the Era of Artificial Intelligence: An Industry Perspective</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> M. Mehdi Kholoosi, Triet Huynh Minh Le, M. Ali Babar</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18261" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18261</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c527700e49403d8d115fcd9b121714ac7cc66ca4c4917d88ae5eeb6712cf705e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c527700e49403d8d115fcd9b121714ac7cc66ca4c4917d88ae5eeb6712cf705e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Software Vulnerability Management in the Era of Artificial Intelligence: An Industry Perspective</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MORPHEUS: A Multidimensional Framework for Modeling, Measuring, and Mitigating Human Factors in Cybersecurity</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Giuseppe Desolda, Francesco Greco, Rosa Lanzilotti, Cesare Tucci</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18303" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18303</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/013fffaf0ed613ccbcc22a6990caf81327b42deac91bfb1c67c73cad88e1ab96_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/013fffaf0ed613ccbcc22a6990caf81327b42deac91bfb1c67c73cad88e1ab96_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MORPHEUS: A Multidimensional Framework for Modeling, Measuring, and Mitigating Human Factors in Cybersecurity</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Leveraging Peer, Self, and Teacher Assessments for Generative AI-Enhanced Feedback</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Alvaro Becerra, Ruth Cobos</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18306" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18306</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5aea301733440fbfacf134ac6fba24972e030dde31a68ce5c51ab92a244cc90b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5aea301733440fbfacf134ac6fba24972e030dde31a68ce5c51ab92a244cc90b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Leveraging Peer, Self, and Teacher Assessments for Generative AI-Enhanced Feedback</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Exploration vs. Fixation: Scaffolding Divergent and Convergent Thinking for Human-AI Co-Creation with Generative Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chao Wen, Tung Phung, Pronita Mehrotra, Sumit Gulwani, Tomohiro Nagashima, Adish Singla</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18388" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18388</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26ae3275661776caa174169c0f345d17624c81b4d8a6d11a881528d1b3ebbea4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26ae3275661776caa174169c0f345d17624c81b4d8a6d11a881528d1b3ebbea4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Exploration vs. Fixation: Scaffolding Divergent and Convergent Thinking for Human-AI Co-Creation with Generative Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Towards Scalable Visual Data Wrangling via Direct Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> El Kindi Rezig, Mir Mahathir Mohammad, Nicolas Baret, Ricardo Mayerhofer, Andrew McNutt, Paul Rosen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18405" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18405</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/790118e217a177acb4ecd824d816066f84640176a4ebb5ada4193f1841afa44c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/790118e217a177acb4ecd824d816066f84640176a4ebb5ada4193f1841afa44c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards Scalable Visual Data Wrangling via Direct Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Listening to the Mind: Earable Acoustic Sensing of Cognitive Load</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xijia Wei, Ting Dang, Khaldoon Al-Naimi, Yang Liu, Fahim Kawsar, Alessandro Montanari</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18413" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18413</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe6c29fe2181ae377bd035ff882e6e32ace02c50f40c9f4359bd37cc647e34d7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe6c29fe2181ae377bd035ff882e6e32ace02c50f40c9f4359bd37cc647e34d7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Listening to the Mind: Earable Acoustic Sensing of Cognitive Load</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] When Robots Say No: The Empathic Ethical Disobedience Benchmark</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dmytro Kuzmenko, Nadiya Shvai</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18474" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18474</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/083ac27bf9c1ba565d26a4c6417b20f994336e4ffb3cb410b3b93d3ef36fb6aa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/083ac27bf9c1ba565d26a4c6417b20f994336e4ffb3cb410b3b93d3ef36fb6aa_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> When Robots Say No: The Empathic Ethical Disobedience Benchmark</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] From Scratch to Fine-Tuned: A Comparative Study of Transformer Training Strategies for Legal Machine Translation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Amit Barman, Atanu Mandal, Sudip Kumar Naskar</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18593" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18593</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4841f825fcb7cf75a5d51e2769fdede7c9af6b25f2faafea290804903c41f40_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4841f825fcb7cf75a5d51e2769fdede7c9af6b25f2faafea290804903c41f40_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> From Scratch to Fine-Tuned: A Comparative Study of Transformer Training Strategies for Legal Machine Translation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] DASH: Deception-Augmented Shared Mental Model for a Human-Machine Teaming System</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zelin Wan, Han Jun Yoon, Nithin Alluru, Terrence J. Moore, Frederica F. Nelson, Seunghyun Yoon, Hyuk Lim, Dan Dongseong Kim, Jin-Hee Cho</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18616" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18616</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fabd2c171e25c623bc7edccb8e1ef0506a926726f1d2a534aaa0d5113899beef_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fabd2c171e25c623bc7edccb8e1ef0506a926726f1d2a534aaa0d5113899beef_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DASH: Deception-Augmented Shared Mental Model for a Human-Machine Teaming System</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Thanh Dat Hoang, Thanh Trung Huynh, Matthias Weidlich, Thanh Tam Nguyen, Tong Chen, Hongzhi Yin, Quoc Viet Hung Nguyen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18622" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18622</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06b3e9f87e02f31154a7925036d456a7d4454e03d1f54e60c44ca1f788fae13_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06b3e9f87e02f31154a7925036d456a7d4454e03d1f54e60c44ca1f788fae13_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] &quot;Even GPT Can Reject Me&quot;: Conceptualizing Abrupt Refusal Secondary Harm (ARSH) and Reimagining Psychological AI Safety with Compassionate Completion Standard (CCS)</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yang Ni, Tong Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18776" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18776</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c436bec45a43d86e53531d2849399bd87e6bf3d4305bd5fa2744e56f7b83352_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c436bec45a43d86e53531d2849399bd87e6bf3d4305bd5fa2744e56f7b83352_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> &quot;Even GPT Can Reject Me&quot;: Conceptualizing Abrupt Refusal Secondary Harm (ARSH) and Reimagining Psychological AI Safety with Compassionate Completion Standard (CCS)</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] VizDefender: Unmasking Visualization Tampering through Proactive Localization and Intent Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sicheng Song, Yanjie Zhang, Zixin Chen, Huamin Qu, Changbo Wang, Chenhui Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18853" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18853</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6df051754b8f594a7ef88a46e4855d2eb43c4c662b1afbca9997caf2b4fbad53_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6df051754b8f594a7ef88a46e4855d2eb43c4c662b1afbca9997caf2b4fbad53_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VizDefender: Unmasking Visualization Tampering through Proactive Localization and Intent Inference</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Psychometric Validation of the Sophotechnic Mediation Scale and a New Understanding of the Development of GenAI Mastery: Lessons from 3,392 Adult Brazilian Workers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Bruno Campello de Souza</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18871" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18871</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/446f3fb717e9b018a154458859c845d2583ea717613eb2a7397f53ffedb8700f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/446f3fb717e9b018a154458859c845d2583ea717613eb2a7397f53ffedb8700f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Psychometric Validation of the Sophotechnic Mediation Scale and a New Understanding of the Development of GenAI Mastery: Lessons from 3,392 Adult Brazilian Workers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Household Plastic Recycling: Empirical Insights and Design Explorations</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ashley Colley, Emma Kirjavainen, Sari Tapio, Jonna Häkkilä</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18889" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18889</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e98f1d82cd1fbc384d43ddf57fd109888b3f5c78ff0d869daa0ef5d4c713eb3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e98f1d82cd1fbc384d43ddf57fd109888b3f5c78ff0d869daa0ef5d4c713eb3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Household Plastic Recycling: Empirical Insights and Design Explorations</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] An Empirical Study of Developer-Provided Context for AI Coding Assistants in Open-Source Projects</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shaokang Jiang, Daye Nam</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18925" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18925</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2089c7618775b9b9cdc54e25f2f7b14898adbf8c1ce8308d624be1f22c566408_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2089c7618775b9b9cdc54e25f2f7b14898adbf8c1ce8308d624be1f22c566408_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> An Empirical Study of Developer-Provided Context for AI Coding Assistants in Open-Source Projects</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Narrative Scaffolding: Transforming Data-Driven Sensemaking Through Narrative-First Exploration</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Oliver Huang, Muhammad Fatir, Steven Luo, Sangho Suh, Hariharan Subramonyam, Carolina Nobre</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18920" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18920</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a117e0515b80f1075c391fa0ed128ff0ec8754654d3747ba533f99d3d9311e6f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a117e0515b80f1075c391fa0ed128ff0ec8754654d3747ba533f99d3d9311e6f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Narrative Scaffolding: Transforming Data-Driven Sensemaking Through Narrative-First Exploration</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Advancing Accessibility: Augmented Reality Solutions for the Blind and Disabled in Bangladesh</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Md Minhazul Islam Munna, Al Amin, Xin Wang, Hongbin Ma</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19047" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19047</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e57af16bdaba33a67888f0850a24ef71eead80c5314fcfb4dde2288280f7820e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e57af16bdaba33a67888f0850a24ef71eead80c5314fcfb4dde2288280f7820e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Advancing Accessibility: Augmented Reality Solutions for the Blind and Disabled in Bangladesh</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Towards a collaborative digital platform for railway infrastructure projects</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pierre Jehel, Pierre-Étienne Gautier, Judicaël Dehotin, Flavien Viguier</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19169" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19169</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3874b7105990b7eea9564e4b1e9114f5f53c8e95ba6b7ddfc4422d6b9542ed5a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3874b7105990b7eea9564e4b1e9114f5f53c8e95ba6b7ddfc4422d6b9542ed5a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards a collaborative digital platform for railway infrastructure projects</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Epistemological Fault Lines Between Human and Artificial Intelligence</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Walter Quattrociocchi, Valerio Capraro, Matjaž Perc</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19466" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19466</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e38aa1bf279d77f222964e2fa6eaf6b1a85cc9955ae786124894e9ed3fb93c1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e38aa1bf279d77f222964e2fa6eaf6b1a85cc9955ae786124894e9ed3fb93c1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Epistemological Fault Lines Between Human and Artificial Intelligence</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Angjelin Hila</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19570" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19570</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14053cd17ec2f7fbfad183ca144e70fb650f93b135eca28453bda97a810f7db4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14053cd17ec2f7fbfad183ca144e70fb650f93b135eca28453bda97a810f7db4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] More code, less validation: Risk factors for over-reliance on AI coding tools among scientists</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Gabrielle O&#x27;Brien, Alexis Parker, Nasir Eisty, Jeffrey Carver</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19644" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19644</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fa42c82b2739bd1b4c7111c0c7d29876d12dbca90ed48e9b300fd1778d6e033_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fa42c82b2739bd1b4c7111c0c7d29876d12dbca90ed48e9b300fd1778d6e033_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> More code, less validation: Risk factors for over-reliance on AI coding tools among scientists</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-24">2025-12-24<a href="#2025-12-24" class="hash-link" aria-label="Direct link to 2025-12-24" title="Direct link to 2025-12-24" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251224] Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> James K Ruffle, Samia Mohinta, Guilherme Pombo, Asthik Biswas, Alan Campbell, Indran Davagnanam, David Doig, Ahmed Hamman, Harpreet Hyare, Farrah Jabeen, Emma Lim, Dermot Mallon, Stephanie Owen, Sophie Wilkinson, Sebastian Brandner, Parashkev Nachev</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19707" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19707</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e098f2b53a5d94739b784dac1a98f71b53ab4d9f759c65700bc9e1f9500bbafd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e098f2b53a5d94739b784dac1a98f71b53ab4d9f759c65700bc9e1f9500bbafd_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Taoran Sheng, Manfred Huber</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19713" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19713</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8069c49480aa4056d903366d9a07ef262001809b60e89caaaab99b1ab6318bb6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8069c49480aa4056d903366d9a07ef262001809b60e89caaaab99b1ab6318bb6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Predicting Student Actions in a Procedural Training Environment</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Diego Riofrío-Luzcando, Jaime Ramírez, Marta Berrocal-Lobo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19810" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19810</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5170ec33c553e90c5b65d0bc5448598b91b9a44fd63a92b56fd0d690965f3002_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5170ec33c553e90c5b65d0bc5448598b91b9a44fd63a92b56fd0d690965f3002_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Predicting Student Actions in a Procedural Training Environment</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] How Tech Workers Contend with Hazards of Humanlikeness in Generative AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mark Díaz, Renee Shelby, Eric Corbett, Andrew Smart</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19832" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19832</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8537a350df62ef095164c7748f634e18b0a4279fffa1f30668439646406288e2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8537a350df62ef095164c7748f634e18b0a4279fffa1f30668439646406288e2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> How Tech Workers Contend with Hazards of Humanlikeness in Generative AI</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Visualizing a Collective Student Model for Procedural Training Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Diego Riofrío-Luzcando, Jaime RamÍrez, Cristian Moral, Angélica de Antonio, Marta Berrocal-Lobo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19885" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19885</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cdb96b3b8530271a31011b3f10e569cebea1d14b4385eb418b43cd79244a07f4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cdb96b3b8530271a31011b3f10e569cebea1d14b4385eb418b43cd79244a07f4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Visualizing a Collective Student Model for Procedural Training Environments</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Detecting cyberbullying in Spanish texts through deep learning techniques</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Paúl Cumba-Armijos, Diego Riofrío-Luzcando, Verónica Rodríguez-Arboleda, Joe Carrión-Jumbo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19899" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19899</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d2f7ccac215958604ec2bafb628962c08cfada143b59dda8159af7e4af21661_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d2f7ccac215958604ec2bafb628962c08cfada143b59dda8159af7e4af21661_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Detecting cyberbullying in Spanish texts through deep learning techniques</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Free-Will vs Free-Wheel: Understanding Community Accessibility Requirements of Wheelchair Users through Interviews, Participatory Action, and Modeling</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hanna Noyce, Emily Olejniczak, Vaskar Raychoudhury, Roger O. Smith, Md Osman Gani</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19898" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19898</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/94f45bbd3f136fa9ca9486c93a06494dfae76a62569d1f5464b47a9b2a351451_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/94f45bbd3f136fa9ca9486c93a06494dfae76a62569d1f5464b47a9b2a351451_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Free-Will vs Free-Wheel: Understanding Community Accessibility Requirements of Wheelchair Users through Interviews, Participatory Action, and Modeling</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Developers&#x27; Experience with Generative AI -- First Insights from an Empirical Mixed-Methods Field Study</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Charlotte Brandebusemeyer, Tobias Schimmer, Bert Arnrich</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19926" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19926</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b805b7a034106a6a896aa4fa59536e4c24c1b4390bbad36d9762174995633c68_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b805b7a034106a6a896aa4fa59536e4c24c1b4390bbad36d9762174995633c68_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Developers&#x27; Experience with Generative AI -- First Insights from an Empirical Mixed-Methods Field Study</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Heet Bodara, Md Masum Mushfiq, Isma Farah Siddiqui</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19950" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19950</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25282dd436ce0f7a5fabd0438ec9d8be57567585d626e71c9a9af6d0ac8451b9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25282dd436ce0f7a5fabd0438ec9d8be57567585d626e71c9a9af6d0ac8451b9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Stories That Teach: Eastern Wisdom for Human-AI Creative Partnerships</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kexin Nie, Xin Tang, Mengyao Guo, Ze Gao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19999" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19999</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/10e9f9340ffb8b63217712d4aa9b0cc137057244e1829ea40c95f6374ff796aa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/10e9f9340ffb8b63217712d4aa9b0cc137057244e1829ea40c95f6374ff796aa_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Stories That Teach: Eastern Wisdom for Human-AI Creative Partnerships</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] /UnmuteAll: Modeling Verbal Communication Patterns of Collaborative Contexts in MOBA Games</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yongchan Son, Jahun Jang, Been An, Jimoon Kang, Eunji Park</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20116" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20116</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50853e5216c134f7f20eb67d59c4cc443da21824aa6454f1a64a094910c9658a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50853e5216c134f7f20eb67d59c4cc443da21824aa6454f1a64a094910c9658a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> /UnmuteAll: Modeling Verbal Communication Patterns of Collaborative Contexts in MOBA Games</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Cyrus Vachha, Yixiao Kang, Zach Dive, Ashwat Chidambaram, Anik Gupta, Eunice Jun, Bjoern Hartmann</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20129" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20129</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8400f1033a93635b0a5b706c568585f557f04cf8533a2b77ce1c55f08e14bef6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8400f1033a93635b0a5b706c568585f557f04cf8533a2b77ce1c55f08e14bef6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Competing or Collaborating? The Role of Hackathon Formats in Shaping Team Dynamics and Project Choices</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sadia Nasrin Tisha, Md Nazmus Sakib, Sanorita Dey</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20181" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20181</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dadc4f214409965b0df1005c180cb9f7bf76c16e776028372175b81d9eed7768_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dadc4f214409965b0df1005c180cb9f7bf76c16e776028372175b81d9eed7768_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Competing or Collaborating? The Role of Hackathon Formats in Shaping Team Dynamics and Project Choices</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] RESPOND: Risk-Enhanced Structured Pattern for LLM-driven Online Node-level Decision-making</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dan Chen, Heye Huang, Tiantian Chen, Zheng Li, Yongji Li, Yuhui Xu, Sikai Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20179" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20179</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba879d80c9d58b4f8d7942ebda77c8c4b993bb9018f484a08adfe360eb40c02c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba879d80c9d58b4f8d7942ebda77c8c4b993bb9018f484a08adfe360eb40c02c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RESPOND: Risk-Enhanced Structured Pattern for LLM-driven Online Node-level Decision-making</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] The Effect of Empathic Expression Levels in Virtual Human Interaction: A Controlled Experiment</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sung Park, Daeho Yoon, Jungmin Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20221" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20221</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e8f259bc1362a522f2cf043a9f6b686f7f77b7e14bfd0671a5c5cba5ec63c8af_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e8f259bc1362a522f2cf043a9f6b686f7f77b7e14bfd0671a5c5cba5ec63c8af_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The Effect of Empathic Expression Levels in Virtual Human Interaction: A Controlled Experiment</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Karolina Drożdż, Kacper Dudzic, Anna Sterna, Marcin Moskalewicz</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20298" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20298</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e46ae09622d9142a3896f2528685617edcbaae96bc105754e16929ed630e3f0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e46ae09622d9142a3896f2528685617edcbaae96bc105754e16929ed630e3f0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Structured Visualization Design Knowledge for Grounding Generative Reasoning and Situated Feedback</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Péter Ferenc Gyarmati, Dominik Moritz, Torsten Möller, Laura Koesten</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20306" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20306</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e3c684df7664b2bee4d03afb8801d141ceb15c7e18014fd3e28b28a59515f72_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e3c684df7664b2bee4d03afb8801d141ceb15c7e18014fd3e28b28a59515f72_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Structured Visualization Design Knowledge for Grounding Generative Reasoning and Situated Feedback</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] A human-centered approach to reframing job satisfaction in the BIM-enabled construction industry</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sharareh Mirzaei, Stephanie Bunt, Susan M Bogus</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20584" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20584</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/510c6ffa270afe4e2d7d2de70c289fafe8fd64d4a4f5f29123b3c70e64963e1e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/510c6ffa270afe4e2d7d2de70c289fafe8fd64d4a4f5f29123b3c70e64963e1e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A human-centered approach to reframing job satisfaction in the BIM-enabled construction industry</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Humza Nusrat, Luke Francisco, Bing Luo, Hassan Bagher-Ebadian, Joshua Kim, Karen Chin-Snyder, Salim Siddiqui, Mira Shah, Eric Mellon, Mohammad Ghassemi, Anthony Doemer, Benjamin Movsas, Kundan Thind</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20586" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20586</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39917d1df3de96bd690d947b78c3c6d1ac037b54cc0591faa464cbc08cd8c729_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39917d1df3de96bd690d947b78c3c6d1ac037b54cc0591faa464cbc08cd8c729_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-25">2025-12-25<a href="#2025-12-25" class="hash-link" aria-label="Direct link to 2025-12-25" title="Direct link to 2025-12-25" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251225] Cooperation Through Indirect Reciprocity in Child-Robot Interactions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [human-robot interaction], [indirect reciprocity, multi-armed bandit, coordination dilemmas]</p>
</li>
<li class="">
<p><strong>authors:</strong> Isabel Neto, Alexandre S. Pires, Filipa Correia, Fernando P. Santos</p>
</li>
<li class="">
<p><strong>institution:</strong> Universidade de Lisboa, University of Amsterdam, Instituto Superior Técnico</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20621" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20621</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrated that the mechanism of indirect reciprocity can be successfully transposed from human-human interactions to child-robot interactions. 2. Showed that children&#x27;s behavioral strategies provide a sufficient signal for multi-armed bandit algorithms to learn cooperative actions. 3. Analyzed how differences in learning algorithms impact the dynamics and outcomes of human-AI cooperation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a49edd2299eeb19bce07b564c8061c35b829f55eb48557d15dbeabbbd028e0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a49edd2299eeb19bce07b564c8061c35b829f55eb48557d15dbeabbbd028e0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether indirect reciprocity, a mechanism for sustaining cooperation, applies to child-robot interactions. The authors combine laboratory experiments with theoretical modeling, using multi-armed bandit algorithms for the robots. They find that indirect reciprocity does extend to these interactions and that robots can learn to cooperate based on children&#x27;s strategies, though this learning is highly dependent on the human strategies revealed.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Uncovering Patterns of Brain Activity from EEG Data Consistently Associated with Cybersickness Using Neural Network Interpretability Maps</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [brain-computer interface], [EEG, cybersickness, interpretability maps, convolutional neural networks, event-related potentials]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jacqueline Yau, Katherine J. Mimnaugh, Evan G. Center, Timo Ojala, Steven M. LaValle, Wenzhen Yuan, Nancy Amato, Minje Kim, Kara Federmeier</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Illinois Urbana-Champaign, University of Oulu</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20620" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20620</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a method using CNNs and transformers with interpretability maps (integrated gradients and class activation) to identify EEG features for cybersickness classification. 2. Identified a consistent and surprising pattern: amplitudes near the left prefrontal cortex electrode are important for cybersickness classification. 3. Proposed using the identified scalp location as a tagged feature for better real-time cybersickness classification with EEG.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5601623e3cdfb620242f065ddf726ad49d48b3d131d2e3cc1f6fa48032be9946_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5601623e3cdfb620242f065ddf726ad49d48b3d131d2e3cc1f6fa48032be9946_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of detecting cybersickness from EEG data by using event-related potentials to isolate sickness-related brain activity from visual stimulus confounds. The authors employ trained convolutional neural networks and transformer models with interpretability maps to identify key EEG features. The main finding is that amplitudes recorded near the left prefrontal cortex are consistently important for classification, suggesting this location as a valuable feature for real-time detection.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Signal, Noise, and Burnout: A Human-Information Interaction Analysis of Voter Verification in a High-Volatility Environment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [human-information interaction], [epistemic self-efficacy, information fatigue, algorithmic filtering theory]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kijung Lee</p>
</li>
<li class="">
<p><strong>institution:</strong> (Institution not explicitly stated in provided content; author name &quot;Kijung Lee&quot; present but no affiliation/email domain. Based on data source, likely an academic institution collaborating with Pew Research Center.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20679" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20679</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Empirically tests the relationship between information source (social media vs. mainstream news) and perceived verification difficulty (epistemic self-efficacy) during a high-volatility election. 2. Identifies perceived exposure to inaccurate information as a mediator and information fatigue as a key moderator in the verification process. 3. Challenges platform-deterministic theories by finding that demographics and universal information fatigue, not platform type, are primary drivers of epistemic burden in volatile environments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e84ec2c80314744bb65f09fdbb5472f486e9a21a743399f645a4b93420c8f3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e84ec2c80314744bb65f09fdbb5472f486e9a21a743399f645a4b93420c8f3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study analyzes how voters perceive their ability to verify news (epistemic self-efficacy) during the volatile 2024 U.S. election, using survey data to test hypotheses about information sources, misinformation exposure, and fatigue. Contrary to expectations, it finds no significant difference in verification difficulty between social media and mainstream news users, concluding that cognitive factors like universal information fatigue are more critical than platform choice.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [educational technology], [generative AI, personalization, adaptive learning, large language models, intelligent tutoring systems]</p>
</li>
<li class="">
<p><strong>authors:</strong> Iman Reihanian, Yunfei Hou, Qingquan Sun</p>
</li>
<li class="">
<p><strong>institution:</strong> California State University, San Bernardino</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20714" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20714</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identified and analyzed five key application domains for GenAI-enabled personalization in CS education: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review. 2. Synthesized four design patterns for successful implementations: context-aware tutoring anchored in student artifacts, multi-level hint structures, composition with traditional CS infrastructure, and human-in-the-loop quality assurance. 3. Proposed an exploration-first adoption framework for integrating GenAI, emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling, while pairing recurrent risks with operational mitigations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e46f313e494a41a4a12873eddb6320db4cd59b6fb958bb008fd6f6512729af4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e46f313e494a41a4a12873eddb6320db4cd59b6fb958bb008fd6f6512729af4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This scoping review maps how generative AI enables personalized computer science education. It analyzes design choices across 32 studies and finds that structured implementations with explanation-first guidance and artifact grounding lead to more positive learning outcomes than unconstrained chat interfaces. The paper concludes that generative AI can provide precision scaffolding when embedded in audit-ready workflows that preserve productive struggle.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] YCB-Handovers Dataset: Analyzing Object Weight Impact on Human Handovers to Adapt Robotic Handover Motion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human-robot interaction], [motion capture, dataset, handover, weight adaptation, YCB dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Parag Khanna, Karen Jane Dsouza, Chunyu Wang, Mårten Björkman, Christian Smith</p>
</li>
<li class="">
<p><strong>institution:</strong> KTH Royal Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20847" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20847</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the novel YCB-Handovers dataset, capturing 2771 human-human handover motions with varied object weights., 2. Provides an analysis of the impact of object weight on human reaching motion during handovers., 3. Bridges a gap in human-robot collaboration research by enabling data-driven, human-inspired models for weight-sensitive robotic motion planning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af98ffe543cfd02847143e303696cbfb3dbc3991e06d43e1e59dda76a0ca8a49_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af98ffe543cfd02847143e303696cbfb3dbc3991e06d43e1e59dda76a0ca8a49_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces the YCB-Handovers dataset, a motion capture dataset of human-human handovers with objects of varying weights. The dataset is built upon the YCB object set and aims to provide insights for developing intuitive, weight-adaptive robotic handover motions. The analysis shows that object weight significantly impacts human reaching motion, which can inform more natural and safe robotic handover behaviors.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Pioneering Multimodal Emotion Recognition in the Era of Large Models: From Closed Sets to Open Vocabularies</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal emotion recognition], [multimodal large language models, open-vocabulary, benchmarking, fusion strategies, prompt engineering]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jing Han, Zhiqiang Gao, Shihao Gao, Jialing Liu, Hongyu Chen, Zixing Zhang, Björn W. Schuller</p>
</li>
<li class="">
<p><strong>institution:</strong> Hunan University, University of Cambridge, Imperial College London</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20938" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20938</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted the first large-scale benchmarking study of open-vocabulary multimodal emotion recognition (MER-OV) using 19 mainstream MLLMs. 2. Systematically analyzed key factors affecting MLLM performance in MER-OV, including reasoning capacity, fusion strategies, and prompt design. 3. Identified that a two-stage, trimodal (audio, video, text) fusion approach with video as the most critical modality yields optimal performance, and found a narrow performance gap between open- and closed-source LLMs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f45d32d66d378f46ef5f60f8b0f7b530f5e3f17019ab0a207910d809d72b3e5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f45d32d66d378f46ef5f60f8b0f7b530f5e3f17019ab0a207910d809d72b3e5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the underexplored potential of Multimodal Large Language Models (MLLMs) for fine-grained, open-vocabulary emotion recognition. It presents a comprehensive benchmark on the OV-MERD dataset, evaluating 19 MLLMs and analyzing factors like fusion strategies and prompt design. The key findings are that a two-stage trimodal fusion works best, video is the most critical modality, and the performance gap between open- and closed-source models is surprisingly small.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] From Human Bias to Robot Choice: How Occupational Contexts and Racial Priming Shape Robot Selection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [human-robot interaction], [racial bias, occupational stereotypes, stereotype priming, skin tone discrimination, anthropomorphism]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiangen He, Wanqi Zhang, Jessica Barfield</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Tennessee, University of Kentucky</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20951" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20951</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrated that human occupational biases and skin-tone-based discrimination directly transfer to robot selection decisions. 2. Revealed distinct, context-dependent patterns of robot preference, with lighter-skinned agents favored in healthcare/education and darker-toned agents in construction/athletics. 3. Showed that exposure to human professionals of specific races can prime and systematically shift subsequent robot preferences in stereotype-consistent directions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e05e8cac9fadcb40ef8a6f45c93ec8405326df2cb55fbb423081a31a9baebd6a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e05e8cac9fadcb40ef8a6f45c93ec8405326df2cb55fbb423081a31a9baebd6a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates how societal biases influence human decisions when selecting robots for professional roles. Through two experiments with over 1000 participants, the study found that preferences for robots with different skin tones vary by occupational context and can be primed by exposure to human racial stereotypes. The main conclusion is that robotic deployment risks perpetuating existing social inequalities by inheriting human biases from human-human evaluation contexts.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] A Design Study Process Model for Medical Visualization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [visualization], [design study, process model, medical visualization, visual analysis, interdisciplinary research]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mengjie Fan, Liang Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University Health Science Center</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21034" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21034</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel design study process model specifically tailored for medical visualization, emphasizing stakeholder distinction, stage differentiation by analytic logic, and task classification. 2. Refines previous general visualization design models by incorporating characteristics of medical problems and providing actionable guidance for each step. 3. Demonstrates the model&#x27;s utility by applying it to guide a new visual analysis method design and by reanalyzing three existing works, validating its practical framework.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e5fe74c72d51a03dca133118f52d887d90317100c302c2b1d5fca972d1219ded_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e5fe74c72d51a03dca133118f52d887d90317100c302c2b1d5fca972d1219ded_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a specialized design study process model for medical visualization, developed through literature review and interdisciplinary experience. The model emphasizes stakeholder analysis, task classification, and provides step-by-step guidance to make visualization design more targeted and adaptable to medical complexity. The authors demonstrate its application and argue it provides a systematic theoretical and practical framework for interdisciplinary medical visualization research.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] When LLMs fall short in Deductive Coding: Model Comparison and Human AI Collaboration Workflow Design</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [text classification], [deductive coding, large language models, human-AI collaboration, BERT, learning analytics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zijian Li, Luzhen Tang, Mengyu Xia, Xinyu Li, Naping Chen, Dragan Gašević, Yizhou Fan</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University, Monash University, Shantou University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21041" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21041</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a comparative performance evaluation of LLMs and BERT-based models for deductive coding, revealing LLMs&#x27; limitations in handling imbalanced data and theoretical interpretation. 2. Identified systematic errors and biases exhibited by LLMs in theory-driven coding tasks, highlighting their difficulty with semantic similarity. 3. Designed and evaluated a novel human-AI collaborative workflow that improves coding efficiency while maintaining reliability, demonstrating a practical path for scaling such tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39f7b78a482db70949a4efca7f09dde7775d6e85ff7942e944bb807ed535f120_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39f7b78a482db70949a4efca7f09dde7775d6e85ff7942e944bb807ed535f120_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the use of Large Language Models (LLMs) for automated, theory-driven (deductive) coding of educational dialogue data. It finds that LLMs do not outperform smaller BERT-based classifiers and exhibit systematic biases, leading to the design of a human-AI collaborative workflow. The main conclusion is that while LLMs have limitations in this specific task, a human-AI partnership offers a promising approach to maintain reliability while improving efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D human pose estimation], [3D sign language reconstruction, biomechanical accuracy, hand and body pose priors, monocular video, SMPL-X]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kaustubh Kundu, Hrishav Bakul Barua, Lucy Robertson-Bell, Zhixi Cai, Kalin Stefanov</p>
</li>
<li class="">
<p><strong>institution:</strong> Monash University, TCS Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21054" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21054</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/kaustesseract/DexAvatar" target="_blank" rel="noopener noreferrer" class="">https://github.com/kaustesseract/DexAvatar</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel framework (DexAvatar) for reconstructing biomechanically accurate 3D hand and body poses from monocular sign language videos. 2. The use of learned 3D hand and body pose priors to guide the reconstruction and overcome challenges like self-occlusion and motion blur. 3. Demonstrating strong performance on the SGNify benchmark, achieving a 35.11% improvement over the state-of-the-art.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/594bef871fe9a00d58a9f3f12c9a0b4bf4f66d3d738bd3f02dedcbad04bdcd25_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/594bef871fe9a00d58a9f3f12c9a0b4bf4f66d3d738bd3f02dedcbad04bdcd25_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces DexAvatar, a framework that uses learned 3D hand and body pose priors to reconstruct accurate 3D sign language poses from monocular videos. It addresses the limitations of existing 2D datasets and noisy 3D estimations. The method significantly outperforms prior work on the SGNify motion capture benchmark.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Making AI Work: An Autoethnography of a Workaround in Higher Education</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [Information Systems], [Autoethnography, Invisible Labour, Workaround, Sociotechnical Systems, User Innovation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shang Chieh Lee, Bhuva Narayan, Simon Buckingham Shum, Stella Ng, A. Baki Kocaballi</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Technology Sydney</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21055" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21055</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides an insider, autoethnographic account of the sociotechnical friction and &quot;invisible labour&quot; required to make enterprise GenAI functional in higher education. 2. Applies and extends Alter&#x27;s theory of workarounds to interpret user-driven adaptations as integral acts of sociotechnical integration, not mere deviations. 3. Highlights the central paradox of GenAI workarounds: they enable functionality but can create unofficial &quot;shadow&quot; systems and obscure the crucial, politically charged labour involved.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c38d81a4b275e82620a51814d2cbc6d349963475ab4224ec58e70714de1ab37_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c38d81a4b275e82620a51814d2cbc6d349963475ab4224ec58e70714de1ab37_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study uses analytic autoethnography to examine a workaround developed when an institutional goal of empowering staff with GenAI clashed with technical and political constraints. It argues such workarounds are essential acts of sociotechnical integration that reveal the &quot;invisible labour&quot; needed to make AI functional, but this labour is often obscured, creating a paradox. The findings position this invisible labour as a core, rather than peripheral, component of practical GenAI implementation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [Agentic AI, SHAP, Large Language Model, Iterative Refinement, Bias-Variance Trade-off]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tomoaki Yamaguchi, Yutong Zhou, Masahiro Ryo, Keisuke Katsura</p>
</li>
<li class="">
<p><strong>institution:</strong> Gifu University, Leibniz Centre for Agricultural Landscape Research (ZALF), Brandenburg University of Technology Cottbus–Senftenberg, Kyoto University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21066" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21066</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Agentic XAI framework integrating SHAP-based explainability with multimodal LLM-driven iterative refinement for generating progressively enhanced explanations. 2. Demonstrates the framework&#x27;s application and evaluation in a real-world agricultural recommendation system using rice yield data. 3. Identifies a bias-variance trade-off in iterative refinement, showing that early stopping (regularization) is crucial for optimizing explanation quality, challenging assumptions of monotonic improvement.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76e69e3950a2d09bc883a9cee931300bdfbeacc4e1e6094150a08db35366449e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76e69e3950a2d09bc883a9cee931300bdfbeacc4e1e6094150a08db35366449e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an Agentic Explainable AI (XAI) framework that combines SHAP analysis with iterative refinement by a multimodal Large Language Model (LLM) to generate better explanations. The framework was tested as an agricultural recommendation system, and evaluations by both human experts and LLMs showed that explanation quality improved over initial rounds but declined with excessive refinement, revealing a bias-variance trade-off. The findings indicate that strategic early stopping is necessary to optimize the practical utility of such agentic XAI systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Volatile Organic Compounds for Stress Detection: A Scoping Review and Exploratory Feasibility Study with Low-Cost Sensors</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [affective computing], [volatile organic compounds (VOCs), multimodal classification, low-cost sensors, stress detection, Random Forest]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nicolai Plintz, Marcus Vetter, Dirk Ifenthaler</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Mannheim, Technische Hochschule Mannheim, Curtis University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21105" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21105</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Comprehensive mapping of VOC biomarker evidence and technological gaps for emotion recognition. 2. Initial demonstration that low-cost sensors can capture stress-related VOC patterns in multimodal fusion. 3. Identification of key implementation challenges, such as interindividual variability and the need for individual calibration.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1d1525e62afe978f13051c6114b1fe52c0caad526ab1d3c2dfb71cd0f5c3f29_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1d1525e62afe978f13051c6114b1fe52c0caad526ab1d3c2dfb71cd0f5c3f29_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the use of volatile organic compounds (VOCs) for stress detection by conducting a scoping review and an exploratory feasibility study with low-cost sensors. The study combines low-cost TVOC sensors with physiological monitoring and uses Random Forest for multimodal classification to detect laboratory-induced stress. The results show that VOC sensors contribute to model performance, but substantial interindividual variability highlights the need for larger samples and individual calibration.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Learning Factors in AI-Augmented Education: A Comparative Study of Middle and High School Students</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [educational technology], [learning analytics, correlation analysis, text mining, student perceptions, human-ai interaction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gaia Ebli, Bianca Raimondi, Maurizio Gabbrielli</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Bologna</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21246" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21246</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Investigated the interrelationships of four key learning factors (experience, clarity, comfort, motivation) in AI-augmented education, a gap in prior research. 2. Revealed a developmental moderator by comparing middle and high school students, finding holistic vs. differentiated evaluation patterns between age groups. 3. Established a foundation for age-specific AI integration strategies by showing perception dimensions actively mediate learning and their structure varies with developmental stage.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/37485735380aa9bf03e242b5a0fe4f8958c120a691a693af44376fb7f27c2b0b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/37485735380aa9bf03e242b5a0fe4f8958c120a691a693af44376fb7f27c2b0b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study investigates how key learning factors relate to each other in AI-augmented programming education for middle and high school students. Using a multimethod analysis combining correlation analysis and text mining on classroom data, it finds that middle school students evaluate AI tools holistically, while high school students assess different factors independently. The conclusion is that the structure of student perceptions is moderated by developmental stage, which should inform age-appropriate AI integration strategies.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Quadrupped-Legged Robot Movement Plan Generation using Large Language Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [Large Language Model, quadruped robot, ROS navigation, sensor fusion, offloaded inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Muhtadin, Vincentius Gusti Putu A. B. M., Ahmad Zaini, Mauridhi Hery Purnomo, I Ketut Eddy Purnama, Chastine Fatichah</p>
</li>
<li class="">
<p><strong>institution:</strong> Institut Teknologi Sepuluh Nopember (ITS)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21293" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21293</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel distributed control architecture that offloads LLM-based high-level planning to an external server to overcome the computational constraints of a lightweight quadruped robot platform. 2. A system that grounds natural language instructions into executable ROS navigation commands using real-time sensor fusion (LiDAR, IMU, Odometry). 3. Experimental validation in a structured indoor environment demonstrating over 90% aggregate success rate, proving the feasibility of the offloaded LLM planning approach for real-world deployment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70190e79023c7f7dad391e0e2059aa027ac578d23c266728acc6d3e205234b01_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70190e79023c7f7dad391e0e2059aa027ac578d23c266728acc6d3e205234b01_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a distributed control framework that uses a Large Language Model (LLM) to generate navigation plans for a quadruped robot from natural language commands. The computationally intensive LLM inference is offloaded to an external server, while the robot executes the plans locally using ROS and sensor data. Experiments in indoor environments showed the system is robust, achieving over 90% success rate and validating the approach for intuitive robot control.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Scaling Laws for Economic Productivity: Experimental Evidence in LLM-Assisted Consulting, Data Analyst, and Management Tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [large language models], [scaling laws, economic productivity, agentic workflows, compute scaling, algorithmic progress]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ali Merali</p>
</li>
<li class="">
<p><strong>institution:</strong> Yale University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21316" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21316</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Derives empirical scaling laws linking LLM training compute to professional productivity gains. 2. Quantifies the relative contributions of increased compute (56%) versus algorithmic progress (44%) to annual productivity improvements. 3. Identifies a significant disparity in productivity gains between non-agentic analytical tasks and agentic workflows requiring tool use.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f148c34bb4d8aa66926afe66d00135fb826ae28f1253bfed833d9ff39c8b046d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f148c34bb4d8aa66926afe66d00135fb826ae28f1253bfed833d9ff39c8b046d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the relationship between LLM capabilities and professional productivity through a preregistered experiment with over 500 professionals. It finds that each year of AI progress reduces task time by 8%, driven by both compute and algorithmic scaling, but gains are larger for analytical tasks than for agentic ones. The results suggest continued model scaling could significantly boost U.S. productivity over the next decade.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-30T11:07:23.000Z" itemprop="dateModified">Dec 30, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/cs_HC/20251215-20251221"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251215-20251221 (cs.HC)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/daily/cshc/20251229-20260104"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251229-20260104 (cs.HC)</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-22" class="table-of-contents__link toc-highlight">2025-12-22</a></li><li><a href="#2025-12-23" class="table-of-contents__link toc-highlight">2025-12-23</a></li><li><a href="#2025-12-24" class="table-of-contents__link toc-highlight">2025-12-24</a></li><li><a href="#2025-12-25" class="table-of-contents__link toc-highlight">2025-12-25</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/cshc/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cshc/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>