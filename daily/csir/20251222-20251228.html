<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_IR/20251222-20251228" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251222-20251228 (cs.IR) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/csir/20251222-20251228"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251222-20251228 (cs.IR) | AI头条"><meta data-rh="true" name="description" content="2025-12-22"><meta data-rh="true" property="og:description" content="2025-12-22"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/csir/20251222-20251228"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/csir/20251222-20251228" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/csir/20251222-20251228" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.IR","item":"https://jokebear666.github.io/ai_toutiao/category/csir"},{"@type":"ListItem","position":3,"name":"20251222-20251228 (cs.IR)","item":"https://jokebear666.github.io/ai_toutiao/daily/csir/20251222-20251228"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.9ae66a68.css">
<script src="/ai_toutiao/assets/js/runtime~main.0e9c9a8b.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.b0d4a715.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/arxiv-daily"><span title="Arxiv每日论文" class="linkLabel_WmDU">Arxiv每日论文</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Collapse sidebar category &#x27;cs.IR&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cs_IR/20251215-20251221"><span title="20251215-20251221 (cs.IR)" class="linkLabel_WmDU">20251215-20251221 (cs.IR)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/csir/20251222-20251228"><span title="20251222-20251228 (cs.IR)" class="linkLabel_WmDU">20251222-20251228 (cs.IR)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/csir/20251229-20260104"><span title="20251229-20260104 (cs.IR)" class="linkLabel_WmDU">20251229-20260104 (cs.IR)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csoh"><span title="cs.OH" class="categoryLinkLabel_W154">cs.OH</span></a><button aria-label="Expand sidebar category &#x27;cs.OH&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/csir"><span>cs.IR</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251222-20251228 (cs.IR)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251222-20251228 (cs.IR)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-22">2025-12-22<a href="#2025-12-22" class="hash-link" aria-label="Direct link to 2025-12-22" title="Direct link to 2025-12-22" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251222] V-Agent: An Interactive Video Search System Using Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [vision-language model, fine-tuning, retrieval vector, re-ranking, multi-agent system]</li>
<li class=""><strong>authors:</strong> SunYoung Park, Jong-Hyeon Lee, Youngjune Kim, Daegyu Sung, Younghyun Yu, Young-rok Cha, Jeongho Ju</li>
<li class=""><strong>institution:</strong> NC AI, Kakao, Korea Advanced Institute of Science and Technology (KAIST)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16925" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16925</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e827044257e239766415ac9500a06f7ddb67bfc47c517c041d42cc61ac33ad18_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e827044257e239766415ac9500a06f7ddb67bfc47c517c041d42cc61ac33ad18_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> V-Agent is a multi-agent video search system that fine-tunes a vision-language model with a small video preference dataset and enhances it with a retrieval vector to embed video frames and audio transcriptions into a shared multimodal space. It uses three agents—routing, search, and chat—to refine searches and interact with users, achieving state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Unexpected Knowledge: Auditing Wikipedia and Grokipedia Search Recommendations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [search engine audit, semantic alignment, topical annotation, trajectory analysis]</li>
<li class=""><strong>authors:</strong> Erica Coppolillo, Simone Mungari</li>
<li class=""><strong>institution:</strong> University of Calabria, ICAR-CNR, University of Southern California</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17027" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17027</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper conducts a comparative audit of search engine recommendations on Wikipedia and Grokipedia by analyzing over 70,000 results from nearly 10,000 neutral English word queries. It finds that both platforms frequently generate weakly related or unexpected results from innocuous queries, though their recommendation sets often differ substantially in topical distribution and exploration trajectories.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [CLIP, semantic refinement mechanism, local token-patch alignment, attribute-object binding, compositional image-text matching]</li>
<li class=""><strong>authors:</strong> Qi Zhang, Yuxu Chen, Lei Deng, Lili Shen</li>
<li class=""><strong>institution:</strong> Sichuan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17178" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17178</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea9a6c4b5b64bc9b1ed96212005ee492c3fd2ae615fc413250d7a6ef7c3bc712_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea9a6c4b5b64bc9b1ed96212005ee492c3fd2ae615fc413250d7a6ef7c3bc712_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes ABE-CLIP, a training-free method to enhance attribute-object binding in CLIP models. It uses a semantic refinement mechanism to improve text token embeddings and a local token-patch alignment strategy to compute image-text similarity. Experiments show the method significantly improves compositional matching performance, even surpassing some trained approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Warmer for Less: A Cost-Efficient Strategy for Cold-Start Recommendations at Pinterest</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [residual connection, score regularization, manifold mixup]</li>
<li class=""><strong>authors:</strong> Saeed Ebrahimi, Weijie Jiang, Jaewon Yang, Olafur Gudmundsson, Yucheng Tu, Huizhong Duan</li>
<li class=""><strong>institution:</strong> Pinterest</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17277" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17277</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f351bf91dbfa3f8f187d67a36cb3c88f1014690a13606c70e860bd376621c71f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f351bf91dbfa3f8f187d67a36cb3c88f1014690a13606c70e860bd376621c71f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a cost-efficient strategy to improve cold-start recommendations by introducing lightweight techniques: a residual connection for non-historical features, a score regularization term, and manifold mixup for data sparsity. These methods collectively increased fresh content engagement by 10% without harming overall engagement or cost, and have been deployed at Pinterest.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A Systematic Reproducibility Study of BSARec for Sequential Recommendation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [sequential recommendation], [BSARec, Transformer, Fourier transform, discrete wavelet transform, padding strategies, frequency rescaling]</li>
<li class=""><strong>authors:</strong> Jan Hutter, Hua Chang Bakker, Stan Fris, Madelon Bernardy, Yuanna Liu</li>
<li class=""><strong>institution:</strong> University of Amsterdam</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17442" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17442</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper reproduces and evaluates BSARec, a sequential recommendation method that enhances Transformer encoders with a frequency layer using Fourier transforms to capture high-frequency signals. The study finds that BSARec outperforms other methods on some datasets, but digital signal processing techniques like discrete wavelet transform offer only marginal improvements over Fourier transforms, and non-constant padding significantly boosts performance while constant padding hinders high-frequency signal capture.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [marketing personalisation], [randomised controlled trial, agentic messaging, rule-based campaign, causal inference, contextual bandits]</li>
<li class=""><strong>authors:</strong> Olivier Jeunen, Schaun Wheeler</li>
<li class=""><strong>institution:</strong> aampe</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17462" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17462</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates an agentic messaging approach for customer communication, comparing it against a traditional rule-based system in a financial service application via a randomized controlled trial. The results show that the agentic system reduced unsubscribe events by 21% and encouraged earlier tax filing, demonstrating its effectiveness in improving user engagement and retention.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [recommendation systems], [causal deconfounding, LightGCN, Unbiased Asymmetric Co-purchase Relationship (UACR), counterfactual exposure, BPR loss]</li>
<li class=""><strong>authors:</strong> Jingmao Zhang, Zhiting Zhao, Yunqi Lin, Jianghong Ma, Tianjun Wei, Haijun Zhang, Xiaofeng Zhang</li>
<li class=""><strong>institution:</strong> Harbin Institute of Technology (Shenzhen), Nanyang Technological University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17733" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17733</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0101cc9ad451bf594fefde69475b9ae7433ae4f0ec8954e841150d68840d01_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0101cc9ad451bf594fefde69475b9ae7433ae4f0ec8954e841150d68840d01_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Cadence, a plug-and-play framework built on LightGCN that uses causal deconfounding to compute unbiased item-item relationships and counterfactual exposure simulation to enhance recommendation diversity. The method constructs a deconfounded directed item graph and identifies diverse, causally relevant items a user has not interacted with. Experiments show it outperforms state-of-the-art models in both diversity and accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Intelligent Knowledge Mining Framework: Bridging AI Analysis and Trustworthy Preservation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Knowledge Mining, Digital Preservation, Semantic Web, Data Integration, Dual-Stream Architecture]</li>
<li class=""><strong>authors:</strong> Binh Vu</li>
<li class=""><strong>institution:</strong> FernUniversität in Hagen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17795" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17795</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes the Intelligent Knowledge Mining Framework (IKMF), a conceptual dual-stream architecture that combines a horizontal AI-driven mining process with a parallel trustworthy archiving stream. It aims to bridge the gap between dynamic analysis and long-term preservation, transforming static data repositories into living, actionable knowledge ecosystems.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-23">2025-12-23<a href="#2025-12-23" class="hash-link" aria-label="Direct link to 2025-12-23" title="Direct link to 2025-12-23" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251223] Factorized Transport Alignment for Multimodal and Multiview E-commerce Representation Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiwen Chen, Yen-Chieh Lien, Susan Liu, María Castaños, Abolfazl Razi, Xiaoting Zhao, Congzhe Su</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18117" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18117</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54d34ab26112b6d842aa7d95d121b9688afb4448ef70a36cd3b95fb4d5a05c30_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54d34ab26112b6d842aa7d95d121b9688afb4448ef70a36cd3b95fb4d5a05c30_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Factorized Transport Alignment for Multimodal and Multiview E-commerce Representation Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Improving Data Reusability in Interactive Information Retrieval: Insights from the Community</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tianji Jiang, Wenqi Li, Jiqun Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18283" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18283</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad5fd35b3eb397c1cdccdf1de651dbc282de84bed1120a1a715ed8c742a75b03_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad5fd35b3eb397c1cdccdf1de651dbc282de84bed1120a1a715ed8c742a75b03_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Improving Data Reusability in Interactive Information Retrieval: Insights from the Community</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Datasets for machine learning and for assessing the intelligence level of automatic patent search systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Boris Genin, Alexander Gorbunov, Dmitry Zolkin, Igor Nekrasov</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18384" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18384</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac80e35326c113d249d5bd28c4aafe01da23a8e0c681cde84257daab6b92b651_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac80e35326c113d249d5bd28c4aafe01da23a8e0c681cde84257daab6b92b651_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Datasets for machine learning and for assessing the intelligence level of automatic patent search systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Efficient Optimization of Hierarchical Identifiers for Generative Recommendation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Federica Valeau, Odysseas Boufalis, Polytimi Gkotsi, Joshua Rosenthal, David Vos</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18434" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18434</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9351bb1bdc8edb2cb50c73d61a8cbfe17eefd091e911a8e3f3f3df8e19c8ee7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9351bb1bdc8edb2cb50c73d61a8cbfe17eefd091e911a8e3f3f3df8e19c8ee7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Efficient Optimization of Hierarchical Identifiers for Generative Recommendation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CIRR: Causal-Invariant Retrieval-Augmented Recommendation with Faithful Explanations under Distribution Shift</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sebastian Sun</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18683" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18683</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/774fd9266a894d1c9a35161bf04dd7ead5c3acad03d8d090d9728f6f69f12262_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/774fd9266a894d1c9a35161bf04dd7ead5c3acad03d8d090d9728f6f69f12262_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CIRR: Causal-Invariant Retrieval-Augmented Recommendation with Faithful Explanations under Distribution Shift</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Modular Layout Synthesis (MLS): Front-end Code via Structure Normalization and Constrained Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chong Liu, Ming Zhang, Fei Li, Hao Zhou, Xiaoshuang Chen, Ye Yuan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18996" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18996</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bc934518ca031428cca5fd8c00520837bb98e48a55a7433a62c9c6ec593d172_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bc934518ca031428cca5fd8c00520837bb98e48a55a7433a62c9c6ec593d172_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Modular Layout Synthesis (MLS): Front-end Code via Structure Normalization and Constrained Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dehai Min, Kailin Zhang, Tongtong Wu, Lu Cheng</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19134" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19134</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/829ce6a74a55abeea6f6d10fec5338ad05441e95283687a525b0f2525bbf8a12_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/829ce6a74a55abeea6f6d10fec5338ad05441e95283687a525b0f2525bbf8a12_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Generative vector search to improve pathology foundation models across multimodal vision-language tasks</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Markus Ekvall, Ludvig Bergenstråhle, Patrick Truong, Ben Murrell, Joakim Lundeberg</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19360" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19360</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e4366142df495ffd095e4dff889e4df1f04fade9bef7320147bdd655dab870fd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e4366142df495ffd095e4dff889e4df1f04fade9bef7320147bdd655dab870fd_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generative vector search to improve pathology foundation models across multimodal vision-language tasks</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-24">2025-12-24<a href="#2025-12-24" class="hash-link" aria-label="Direct link to 2025-12-24" title="Direct link to 2025-12-24" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251224] Towards Analysing Invoices and Receipts with Amazon Textract</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sneha Oommen, Gabby Sanchez, Cassandra T. Britto, Di Wang, Jordan Chiou, Maria Spichkova</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19958" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19958</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8027409d62cb78b143f7d21e2c36fb093ab6a4978dfa9fe88c1a8339d40565fd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8027409d62cb78b143f7d21e2c36fb093ab6a4978dfa9fe88c1a8339d40565fd_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards Analysing Invoices and Receipts with Amazon Textract</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] IGDMRec: Behavior Conditioned Item Graph Diffusion for Multimodal Recommendation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ziyuan Guo, Jie Guo, Zhenghao Chen, Bin Song, Fei Richard Yu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19983" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19983</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6d1b0c632ea03f65951be60ec7a07744efabc63fd50bc3718c0feceaf6929db4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6d1b0c632ea03f65951be60ec7a07744efabc63fd50bc3718c0feceaf6929db4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> IGDMRec: Behavior Conditioned Item Graph Diffusion for Multimodal Recommendation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kian Godhwani, David Benrimoh</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20022" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20022</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd553d2c8dfa3e24074d8dc0752a5348a2dc9e7e83526071de4a16c28bb018b6_w640_q70.png" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd553d2c8dfa3e24074d8dc0752a5348a2dc9e7e83526071de4a16c28bb018b6_w640_q70.png</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] VSA<!-- -->:Visual-Structural<!-- --> Alignment for UI-to-Code</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xian Wu, Ming Zhang, Zhiyu Fang, Fei Li, Bin Wang, Yong Jiang, Hao Zhou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20034" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20034</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96d552b0d871e15200119995d18b2c4223e07f8d868c7cc4caaa9ee14883b636_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96d552b0d871e15200119995d18b2c4223e07f8d868c7cc4caaa9ee14883b636_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VSA<!-- -->:Visual-Structural<!-- --> Alignment for UI-to-Code</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Retrieval-augmented Prompt Learning for Pre-trained Foundation Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiang Chen, Yixin Ou, Quan Feng, Lei Li, Piji Li, Haibo Ye, Sheng-Jun Huang, Shuofei Qiao, Shumin Deng, Huajun Chen, Ningyu Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20145" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20145</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028ab8e7171d7f497cbdc48e136d419e8642bf876111786382aee29b15d0992_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028ab8e7171d7f497cbdc48e136d419e8642bf876111786382aee29b15d0992_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Retrieval-augmented Prompt Learning for Pre-trained Foundation Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Collaborative Group-Aware Hashing for Fast Recommender Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yan Zhang, Li Deng, Lixin Duan, Ivor W. Tsang, Guowu Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20172" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20172</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88ca67dd050ef45a91576d3d9023639ade9c0749505763fab48ef17a2472cf5d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88ca67dd050ef45a91576d3d9023639ade9c0749505763fab48ef17a2472cf5d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Collaborative Group-Aware Hashing for Fast Recommender Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hao Guo, Xugong Qin, Jun Jie Ou Yang, Peng Zhang, Gangyan Zeng, Yubo Li, Hailun Lin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20174" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20174</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a74295652803d99c4b0631ef38e9684d12032ddd9797f217033f356497ff647_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a74295652803d99c4b0631ef38e9684d12032ddd9797f217033f356497ff647_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Memory as Resonance: A Biomimetic Architecture for Infinite Context Memory on Ergodic Phonetic Manifolds</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tarik Houichime, Abdelghani Souhar, Younes El Amrani</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20245" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20245</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1001e3781e678219db00950fa667bbe46bbaa2f98cd7e5064d91ede9a2cbc6fe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1001e3781e678219db00950fa667bbe46bbaa2f98cd7e5064d91ede9a2cbc6fe_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Memory as Resonance: A Biomimetic Architecture for Infinite Context Memory on Ergodic Phonetic Manifolds</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shuting Wang, Qiaolin Xia, Hao Wang, Yu Lu, Bobsimons, Zhicheng Dou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20458" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20458</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a13a1563c00a3e27fe1f913d8759ce111fa71a7790c6903507a37b4af30d53cf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a13a1563c00a3e27fe1f913d8759ce111fa71a7790c6903507a37b4af30d53cf_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Making Large Language Models Efficient Dense Retrievers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yibin Lei, Shwai He, Ang Li, Andrew Yates</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20612" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20612</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40238eef858f9a1a1327758d04b0c4c31e71fbbf6df6898a51ccf0f7ff9a8f36_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40238eef858f9a1a1327758d04b0c4c31e71fbbf6df6898a51ccf0f7ff9a8f36_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Making Large Language Models Efficient Dense Retrievers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Siyuan Fu, Xuchen Guo, Mingjun Liu, Hongxiang Li, Boyin Tan, Gongxi Zhu, Xianwei Zhuang, Jinghan Ru, Yuxin Xie, Yuguo Yin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19703" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19703</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ac2c582d6058b0a98adaddd2fc52e7fcb4b2f111f05471b7984fd691dc97aed_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ac2c582d6058b0a98adaddd2fc52e7fcb4b2f111f05471b7984fd691dc97aed_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Towards a point-to-point CV-QKD system: Implementation challenges and perspectives</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Davi Juvêncio Gomes de Sousa, Nelson Alves Ferreira Neto, Christiano M. S. Nascimento, Lucas Q. Galvão, Mauro Queiroz Nooblath Neto, Micael Andrade Dias, Cássio de Castro Silva, Braian Pinheiro da Silva, Alexandre B. Tacla, Valéria Loureiro da Silva</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19834" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19834</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfa7fbf5acad2dd0ae547f58bc198ca67d3b36ccc31ad024285f0358a08cb6c9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfa7fbf5acad2dd0ae547f58bc198ca67d3b36ccc31ad024285f0358a08cb6c9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards a point-to-point CV-QKD system: Implementation challenges and perspectives</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-25">2025-12-25<a href="#2025-12-25" class="hash-link" aria-label="Direct link to 2025-12-25" title="Direct link to 2025-12-25" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251225] MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [multimodal knowledge graph, cross-modal reasoning, visual document understanding, retrieval-augmented generation, entity-centric structure]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chi-Hsiang Hsiao, Yi-Cheng Wang, Tzung-Sheng Lin, Yi-Ren Yeh, Chu-Song Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> National Taiwan University, E.SUN Financial Holding Co., Ltd., National Kaohsiung Normal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20626" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20626</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a multimodal knowledge graph-based RAG framework that integrates visual cues into KG construction, retrieval, and answer generation for cross-modal reasoning. 2. Addresses the limitation of existing text-only KG-RAG methods by automatically building KGs that capture text-to-figure and figure-to-figure relationships. 3. Demonstrates superior performance over existing RAG approaches on both textual and multimodal question-answering tasks through comprehensive experiments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/425d6eb853edb40749e686474d27dc018d8a86017a4cd69160f9ac2081d36385_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/425d6eb853edb40749e686474d27dc018d8a86017a4cd69160f9ac2081d36385_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces MegaRAG, a multimodal knowledge graph-based retrieval-augmented generation method designed to overcome the limitations of text-only RAG systems in understanding complex, long-form visual documents. It integrates visual information into the knowledge graph construction and retrieval process to enable better cross-modal reasoning. Experimental results show it consistently outperforms existing RAG methods on various question-answering tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Soft Filtering: Guiding Zero-shot Composed Image Retrieval with Prescriptive and Proscriptive Constraints</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image retrieval], [composed image retrieval, zero-shot, multimodal llm, semantic filtering, re-ranking]</p>
</li>
<li class="">
<p><strong>authors:</strong> Youjin Jung, Seongwoo Cho, Hyun-seok Min, Sungchul Choi</p>
</li>
<li class="">
<p><strong>institution:</strong> Pukyong National University, Tomocube Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20781" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20781</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/jjungyujin/SoFT" target="_blank" rel="noopener noreferrer" class="">https://github.com/jjungyujin/SoFT</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SoFT, a training-free, plug-and-play filtering module for ZS-CIR that uses multimodal LLMs to extract prescriptive and proscriptive constraints for re-ranking. 2. Introduces a two-stage dataset pipeline to refine CIR benchmarks by constructing multi-target triplets and rewriting modification texts to handle ambiguity. 3. Demonstrates significant performance improvements on multiple CIR benchmarks (CIRR, CIRCO, FashionIQ) without modifying the base retrieval model.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1143c63f4b8369d6ea6c4b9a2d3107b565118e12056bbcf37eb527e47bc75cca_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1143c63f4b8369d6ea6c4b9a2d3107b565118e12056bbcf37eb527e47bc75cca_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses limitations in Zero-shot Composed Image Retrieval (ZS-CIR), where fused queries dilute information and benchmarks ignore text ambiguity. The authors propose SoFT, a method that uses multimodal LLMs to extract &quot;must-have&quot; and &quot;must-avoid&quot; constraints to filter and re-rank candidate images, and they refine evaluation datasets to better capture user intent. The approach significantly boosts retrieval performance on standard benchmarks without requiring additional training.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] How important is Recall for Measuring Retrieval Quality?</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [retrieval quality, recall estimation, LLM-based evaluation, nDCG, RAG]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shelly Schwartz, Oleg Vasilyev, Randy Sawaya</p>
</li>
<li class="">
<p><strong>institution:</strong> Primer Technologies Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20854" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20854</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Evaluates established strategies for measuring retrieval quality when the total number of relevant documents (and thus recall) is unknown, by correlating metrics with LLM-based judgments of response quality. 2. Conducts experiments across multiple datasets with a low number of relevant documents (2-15) to assess these strategies. 3. Introduces a new, simple retrieval quality measure that performs well without requiring knowledge of the total number of relevant documents.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8385a132c621f6e8d9e71274fc95535412aa2b9f0d0f40e834705d3548f0a601_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8385a132c621f6e8d9e71274fc95535412aa2b9f0d0f40e834705d3548f0a601_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of evaluating retrieval quality in realistic settings where the total number of relevant documents is unknown, making recall uncomputable. It evaluates existing strategies and proposes a new simple metric by measuring their correlation with LLM-generated response quality. The main conclusion is that a simple measure can perform effectively without needing to know the total relevant documents, offering a practical solution for dynamic knowledge bases.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Accurate and Diverse Recommendations via Propensity-Weighted Linear Autoencoders</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [recommender systems], [Inverse Propensity Scoring, Missing Not At Random, Linear Autoencoder, Diversity, Propensity Score]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kazuma Onishi, Katsuhiko Hayashi, Hidetaka Kamigaito</p>
</li>
<li class="">
<p><strong>institution:</strong> Hokkaido University, The University of Tokyo, Nara Institute of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20896" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20896</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/cars1015/IPS-LAE" target="_blank" rel="noopener noreferrer" class="">https://github.com/cars1015/IPS-LAE</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identified that standard power-law-based Inverse Propensity Scoring (IPS) overly penalizes popular items, harming recommendation performance. 2. Proposed a novel propensity score definition using a sigmoid function on the log of item observation frequency, offering a flexible correction for popularity bias. 3. Successfully integrated the redefined propensity score into a linear autoencoder model to improve recommendation diversity without sacrificing accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3366553060af980681971cfbf2bdb6b5845b43caacef978c0195bc0df1fe29f6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3366553060af980681971cfbf2bdb6b5845b43caacef978c0195bc0df1fe29f6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of popularity bias in recommender systems caused by Missing Not At Random (MNAR) data. It proposes a new propensity score formulation using a sigmoid function to correct for this bias more flexibly than traditional power-law methods and integrates it into a linear autoencoder. Experiments show this method significantly improves recommendation diversity while maintaining accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] MMSRARec: Summarization and Retrieval Augumented Sequential Recommendation Based on Multimodal Large Language Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [multimodal large language model, sequential recommendation, retrieval-augmented generation, supervised fine-tuning, multi-task learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haoyu Wang, Yitong Wang, Jining Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Fudan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20916" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20916</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a method to use MLLMs to adaptively summarize multimodal items into concise keywords via fine-tuning with a custom reward function. 2. Integrates collaborative signals into the recommendation process by transforming them into keywords and using them as supplementary context, inspired by RAG. 3. Aligns the MLLM for multimodal sequential recommendation through supervised fine-tuning with multi-task learning, balancing performance, interpretability, and computational cost.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24628be848ba62fad6bf4a863ec676471dac0625d17315b1ad3a57018467d1d9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24628be848ba62fad6bf4a863ec676471dac0625d17315b1ad3a57018467d1d9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes MMSRARec, a method that uses a Multimodal Large Language Model (MLLM) to summarize user behavior sequences and integrate collaborative signals for sequential recommendation. The approach fine-tunes the MLLM with adaptive summarization and retrieval-augmented context to improve efficiency and interpretability. Evaluations show it effectively understands user histories for accurate recommendations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [crosslingual information retrieval], [dual-encoder, contrastive learning, hard negative sampling, data augmentation, multi-source alignment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mohammad Mahdi Abootorabi, Alireza Ghahramani Kure, Mohammadali Mohammadkhani, Sina Elahimanesh, Mohammad Ali Ali Panah</p>
</li>
<li class="">
<p><strong>institution:</strong> Based on the provided email domains (gmail.com), no specific institution can be reliably inferred. The team name is &quot;MultiMind&quot;.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20950" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20950</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces TriAligner, a novel dual-encoder architecture with contrastive learning for crosslingual claim retrieval. 2. Proposes a method to learn the relative importance of different information sources (e.g., native text, English translations) for alignment. 3. Enhances robustness through LLM-based data preprocessing/augmentation and hard negative sampling strategies.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccb6e1762a9617f640573a86ea65e0e68afff48d53006aa74213e0a557970889_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccb6e1762a9617f640573a86ea65e0e68afff48d53006aa74213e0a557970889_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of retrieving fact-checked claims across multiple languages to combat misinformation. The proposed TriAligner system uses a dual-encoder with contrastive learning and multi-source alignment, enhanced by LLM-based data processing. The method shows significant improvements in retrieval accuracy on monolingual and crosslingual benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Towards Better Search with Domain-Aware Text Embeddings for C2C Marketplaces</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [text embeddings], [Matryoshka Representation Learning, role-specific prefixes, purchase-driven fine-tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Andre Rusli, Miao Cao, Shoma Ishimoto, Sho Akiyama, Max Frenzel</p>
</li>
<li class="">
<p><strong>institution:</strong> Mercari, Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21021" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21021</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a domain-aware Japanese text-embedding model fine-tuned on purchase-driven query-title pairs for C2C marketplace search. 2. Introduced the use of role-specific prefixes to model the query-item asymmetry inherent in search tasks. 3. Applied Matryoshka Representation Learning to create compact, truncation-robust embeddings that meet production latency and throughput constraints.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f78c022541a8ea7125744d1404efda5d54f335700045da22b53146a30bf9632_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f78c022541a8ea7125744d1404efda5d54f335700045da22b53146a30bf9632_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of improving search relevance in noisy, user-generated C2C marketplaces by fine-tuning a Japanese text-embedding model with role-specific prefixes and Matryoshka Representation Learning. The method produces compact embeddings that are robust to truncation for efficiency. Offline and online evaluations show significant improvements in retrieval quality and business metrics, providing a practical foundation for enhanced search experiences.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Agentic Multi-Persona Framework for Evidence-Aware Fake News Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [misinformation detection], [multi-persona agent, LLM-SLM synergy, evidence-grounded, multimodal fusion, credibility fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Roopa Bukke, Soumya Pandey, Suraj Kumar, Soumi Chattopadhyay, Chandranath Adak</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology (Indore, Patna)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21039" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21039</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed AMPEND-LS, an agentic multi-persona framework that integrates textual, visual, and contextual evidence through a structured LLM reasoning pipeline. 2. Introduced a credibility fusion mechanism combining semantic similarity, domain trustworthiness, and temporal context to improve reliability. 3. Designed a complementary SLM classifier to mitigate LLM uncertainty and hallucinations, enhancing robustness and explainability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e8f4708c11ed1d6f5c25e0cab8a4e68e02e81ca73057ce067c85265f0213b46_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e8f4708c11ed1d6f5c25e0cab8a4e68e02e81ca73057ce067c85265f0213b46_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of multimodal fake news detection by proposing AMPEND-LS, a framework that synergizes LLMs and SLMs within a multi-persona agent structure to reason over diverse evidence. It demonstrates superior performance over state-of-the-art baselines in accuracy and robustness across multiple datasets. The work contributes to developing more adaptive and explainable systems for combating online misinformation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Blurb-Refined Inference from Crowdsourced Book Reviews using Hierarchical Genre Mining with Dual-Path Graph Convolutions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [text classification], [hierarchical genre classification, zero-shot semantic alignment, dual-path graph convolution, label co-occurrence graph, blurb-refined inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Suraj Kumar, Utsav Kumar Nareti, Soumi Chattopadhyay, Chandranath Adak, Prolay Mallick</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology Indore, Indian Institute of Technology Patna</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21076" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21076</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a two-phase hierarchical genre mining framework (HiGeMine) that integrates noisy user reviews with authoritative blurbs using a zero-shot semantic alignment strategy to filter reviews. 2. Introduces a dual-path, two-level graph-based classification architecture that models inter-genre dependencies via a label co-occurrence graph for coarse and fine-grained prediction. 3. Curates a new hierarchical book genre dataset to facilitate systematic evaluation of the proposed method.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3afc3f4ebea9058118426cd2d72f37e4c09ae5bcc05f191e73c452f78fc501af_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3afc3f4ebea9058118426cd2d72f37e4c09ae5bcc05f191e73c452f78fc501af_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of noisy and unreliable book genre classification by proposing HiGeMine, a framework that filters user reviews using semantic alignment with book blurbs and then performs hierarchical classification using a dual-path graph-based model. The method outperforms baselines on a newly curated dataset, demonstrating an effective solution for leveraging structured and unstructured text in hierarchical genre analysis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [conversational ai], [multi-turn clarification, ambiguity taxonomy, agentic approach]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sichun Luo, Yi Huang, Mukai Li, Shichang Meng, Fengyuan Liu, Zefa Hu, Junlan Feng, Qi Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Hong Kong, JIUTIAN Research (China Mobile), City University of Hong Kong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21120" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21120</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces ClarifyMT-Bench, a novel benchmark for multi-turn clarification featuring a five-dimensional ambiguity taxonomy and diverse simulated user personas. 2. Uncovers a consistent &quot;under-clarification bias&quot; in LLMs, where they answer prematurely and performance degrades with dialogue depth. 3. Proposes ClarifyAgent, an agentic framework that decomposes clarification into perception, forecasting, tracking, and planning to improve robustness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c251a364522d772d4c0ccb3f8108c9a5bafb4d76316e5783c96374fcd1c4488_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c251a364522d772d4c0ccb3f8108c9a5bafb4d76316e5783c96374fcd1c4488_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that LLMs tend to answer ambiguous user queries prematurely in multi-turn conversations. To study this, the authors introduce ClarifyMT-Bench, a multi-turn clarification benchmark, and propose ClarifyAgent, an agentic method that improves clarification robustness. The main finding is that current LLMs have an under-clarification bias, which the proposed agentic approach helps mitigate.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] ReaSeq: Unleashing World Knowledge via Reasoning for Sequential Modeling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [sequential modeling, chain-of-thought reasoning, diffusion large language models, multi-agent collaboration, world knowledge]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chuan Wang, Gaoming Yang, Han Wu, Jiakai Tang, Jiahao Yu, Jian Wu, Jianwu Hu, Junjun Zheng, Shuwen Xiao, Yeqiu Yang, Yuning Jiang, Ahjol Nurlanbek, Binbin Cao, Bo Zheng, Fangmei Zhu, Gaoming Zhou, Huimin Yi, Huiping Chu, Jin Huang, Jinzhe Shan, Kenan Cui, Longbin Li, Silu Zhou, Wen Chen, Xia Ming, Xiang Gao, Xin Yao, Xingyu Wen, Yan Zhang, Yiwen Hu, Yulin Wang, Ziheng Bao, Zongyuan Wu</p>
</li>
<li class="">
<p><strong>institution:</strong> TaoRank Team (Alibaba Group / Taobao)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21257" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21257</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ReaSeq, a reasoning-enhanced framework that leverages LLM world knowledge to overcome limitations of log-driven recommender systems. 2. Introduces explicit Chain-of-Thought reasoning via multi-agent collaboration to distill structured product knowledge into enriched item representations. 3. Employs latent reasoning via Diffusion LLMs to infer plausible beyond-log user behaviors, enhancing interest modeling.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f98d5c26b79acf6e7c20219639988198127de51ba1227ceaeb063216243ba42_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f98d5c26b79acf6e7c20219639988198127de51ba1227ceaeb063216243ba42_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces ReaSeq, a framework that uses Large Language Models&#x27; world knowledge for explicit and implicit reasoning to address knowledge poverty and systemic blindness in log-driven industrial recommender systems. It enhances item representations and infers beyond-log user behaviors. Deployed on Taobao, it achieved significant improvements in key business metrics like CTR and GMV.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-30T09:28:57.000Z" itemprop="dateModified">Dec 30, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/cs_IR/20251215-20251221"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251215-20251221 (cs.IR)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/daily/csir/20251229-20260104"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251229-20260104 (cs.IR)</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-22" class="table-of-contents__link toc-highlight">2025-12-22</a></li><li><a href="#2025-12-23" class="table-of-contents__link toc-highlight">2025-12-23</a></li><li><a href="#2025-12-24" class="table-of-contents__link toc-highlight">2025-12-24</a></li><li><a href="#2025-12-25" class="table-of-contents__link toc-highlight">2025-12-25</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/csir/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csir/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>