<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_LG/20251222-20251228" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251222-20251228 (cs.LG) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/cslg/20251222-20251228"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251222-20251228 (cs.LG) | AI头条"><meta data-rh="true" name="description" content="2025-12-22"><meta data-rh="true" property="og:description" content="2025-12-22"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/cslg/20251222-20251228"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cslg/20251222-20251228" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cslg/20251222-20251228" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.LG","item":"https://jokebear666.github.io/ai_toutiao/daily/cslg"},{"@type":"ListItem","position":3,"name":"20251222-20251228 (cs.LG)","item":"https://jokebear666.github.io/ai_toutiao/daily/cslg/20251222-20251228"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.bc6dbd00.css">
<script src="/ai_toutiao/assets/js/runtime~main.a48d6d4f.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.becdf5a1.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/daily">Daily</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/paper">Paper</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Collapse sidebar category &#x27;cs.LG&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cs_LG/20251215-20251221"><span title="20251215-20251221 (cs.LG)" class="linkLabel_WmDU">20251215-20251221 (cs.LG)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/cslg/20251222-20251228"><span title="20251222-20251228 (cs.LG)" class="linkLabel_WmDU">20251222-20251228 (cs.LG)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/daily/cslg"><span>cs.LG</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251222-20251228 (cs.LG)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251222-20251228 (cs.LG)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-22">2025-12-22<a href="#2025-12-22" class="hash-link" aria-label="Direct link to 2025-12-22" title="Direct link to 2025-12-22" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251222] Dion2: A Simple Method to Shrink Matrix in Muon</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [Muon optimizer, orthonormalization, matrix shrinking, sampling, Newton-Schulz iterations, FSDP2]</li>
<li class=""><strong>authors:</strong> Kwangjun Ahn, Noah Amsel, John Langford</li>
<li class=""><strong>institution:</strong> Microsoft Research, AI Frontiers, NYU</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16928" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16928</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Dion2, a simple method to reduce the computational cost of the Muon optimizer by sampling a fraction of rows or columns for orthonormalization at each iteration. This sparsifies the update, lowering computation and communication overhead. The method maintains update quality close to full Muon while improving scalability, as shown in training benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] BIONIX: A Wireless, Low-Cost Prosthetic Arm with Dual-Signal EEG and EMG Control</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [EEG, EMG, sliding window classification, threshold-based detection, ESP32 microcontroller, NeuroSky MindWave Mobile 2, MyoWare 2.0]</li>
<li class=""><strong>authors:</strong> Pranesh Sathish Kumar</li>
<li class=""><strong>institution:</strong> Alliance Academy for Innovation, Georgia Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16929" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16929</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a low-cost prosthetic arm controlled by a dual-signal system using EEG (for hand open/close via blink detection) and EMG (for elbow movement via threshold-based detection). The prototype, built with ESP32 microcontrollers and commercial sensors, demonstrates a feasible, intuitive control method for upper-limb prostheses in resource-limited settings. The main conclusion is that this integrated neuro-muscular approach offers a viable pathway to affordable and biologically intuitive prosthetic control.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [dense embedding retrieval, graph-based exploration, BM25, LLM-based reasoning, code localization]</li>
<li class=""><strong>authors:</strong> Shravan Chaudhari, Rahul Thomas Jacob, Mononito Goswami, Jiajun Cao, Shihab Rashid, Christian Bock</li>
<li class=""><strong>institution:</strong> Johns Hopkins University, AWS AI Labs</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16956" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16956</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c743ebe3d40c5416b7ff367b0e7e93ca8ff7bf1bd771b2359d8a7333521abcbc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c743ebe3d40c5416b7ff367b0e7e93ca8ff7bf1bd771b2359d8a7333521abcbc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SpIDER, a method that enhances dense retrieval for code localization by using graph-based exploration of a codebase to gather auxiliary context, which is then reasoned over by an LLM. This approach addresses the limitations of standard embedding methods that underutilize code structure. Empirical results show that SpIDER consistently improves retrieval performance across multiple programming languages.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Optimizing Text Search: A Novel Pattern Matching Algorithm Based on Ukkonen&#x27;s Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [pattern matching algorithms], [Ukkonen&#x27;s Algorithm, Suffix Trees, pattern recognition, text-search algorithms]</li>
<li class=""><strong>authors:</strong> Xinyu Guan, Shaohua Zhang</li>
<li class=""><strong>institution:</strong> Not specified</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16927" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16927</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b065cfa04bf6f3a4b29a1299ffe0b7dd4f84fbabb6368c76abaa339e1a0a77c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b065cfa04bf6f3a4b29a1299ffe0b7dd4f84fbabb6368c76abaa339e1a0a77c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a novel pattern matching algorithm that combines Ukkonen&#x27;s Algorithm for constructing Suffix Trees with a new search technique using Python&#x27;s dynamic link attributes. The optimized algorithm demonstrates linear time and space efficiency, outperforming traditional methods like Naive Search, KMP, and Boyer-Moore, and achieves 100% accuracy in tasks such as genomic sequence pattern recognition.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Compression is Routing: Reconstruction Error as an Intrinsic Signal for Modular Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [transformer autoencoder, mixture-of-experts (MoE), reconstruction error, latent vectors, sequence compression, routing mechanism]</li>
<li class=""><strong>authors:</strong> Zhongpan Tang</li>
<li class=""><strong>institution:</strong> Independent Researcher (based on gmail address)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16963" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16963</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a novel &quot;Compression is Routing&quot; architecture using a Transformer Autoencoder to compress long sequences into latent vectors. It demonstrates that reconstruction error serves as an intrinsic domain fingerprint, enabling expert module scheduling without explicit gating networks. The method offers a scalable approach for handling long contexts and modular language model design.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] PAACE: A Plan-Aware Automated Agent Context Engineering Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [context engineering, plan-aware compression, next-k-task relevance, instruction co-refinement, function-preserving compression, synthetic data generation, knowledge distillation]</li>
<li class=""><strong>authors:</strong> Kamer Ali Yuksel</li>
<li class=""><strong>institution:</strong> aiXplain Inc</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16970" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16970</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces PAACE, a framework for compressing the expanding context of LLM agents in multi-step workflows. It uses plan-aware techniques like next-k-task relevance modeling and function-preserving compression, trained on synthetic data and distilled into efficient models. The method improves agent accuracy while significantly reducing context load and inference costs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [retrieval-augmented generation, long-term memory, semantic imitation, indirect injection attack, memory poisoning, MetaGPT, DataInterpreter]</li>
<li class=""><strong>authors:</strong> Saksham Sahai Srivastava, Haoyu He</li>
<li class=""><strong>institution:</strong> University of Georgia</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16962" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16962</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MemoryGraft, a novel attack that poisons an LLM agent&#x27;s long-term memory by implanting malicious successful experiences, which are then retrieved and imitated during future tasks. The method exploits the agent&#x27;s semantic imitation heuristic through a poisoned RAG store, leading to persistent behavioral compromise. The authors demonstrate that this attack can cause significant and stealthy behavioral drift in agents like MetaGPT&#x27;s DataInterpreter.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Physics-Informed Lightweight Machine Learning for Aviation Visibility Nowcasting Across Multiple Climatic Regimes</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [aviation meteorology], [XGBoost, physics-guided feature engineering, SHAP analysis, METAR data, gradient boosting]</li>
<li class=""><strong>authors:</strong> Marcelo Cerda Castillo</li>
<li class=""><strong>institution:</strong> Pulsetech.cl</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16967" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16967</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a lightweight XGBoost model for aviation visibility nowcasting, trained on surface observation data (METAR) and enhanced with physics-informed feature engineering. The model outperforms operational TAF forecasts at 3-hour horizons, achieving 2.5x to 4x higher recall with fewer false alarms across multiple climates. The SHAP analysis shows the model implicitly reconstructs local physical drivers like advection and radiation, providing explainable predictions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] QSMOTE-PGM/kPGM: QSMOTE Based PGM and kPGM for Imbalanced Dataset Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [quantum-inspired machine learning], [Pretty Good Measurement (PGM), kernelized PGM (KPGM), Quantum SMOTE (QSMOTE), quantum state discrimination, density matrix-based learning]</li>
<li class=""><strong>authors:</strong> Bikash K. Behera, Giuseppe Sergioli, Robert Giuntini</li>
<li class=""><strong>institution:</strong> Università degli Studi di Cagliari, Technische Universität München</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16960" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16960</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces and compares quantum-inspired classifiers, specifically Pretty Good Measurement (PGM) and kernelized PGM (kPGM), for imbalanced datasets, using Quantum SMOTE (QSMOTE) for synthetic oversampling. The results show that both PGM and kPGM outperform a classical random forest baseline, with PGM achieving the highest accuracy and kPGM demonstrating greater robustness across different data sampling strategies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [Proximal Policy Optimization (PPO), Group Relative Policy Optimization (GRPO), turn-level MDP, advantage estimation, multi-turn RL]</li>
<li class=""><strong>authors:</strong> Junbo Li, Peng Zhou, Rui Meng, Meet P. Vadera, Lihong Li, Yang Li</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin, Amazon</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17008" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17008</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Turn-PPO, a reinforcement learning method that applies Proximal Policy Optimization at the turn level instead of the token level for training LLM agents in multi-turn tasks. It demonstrates that this approach is more robust and effective than the commonly used GRPO method, particularly for long-horizon reasoning scenarios, as validated on the WebShop and Sokoban datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [scientific ai evaluation], [Practical Inquiry Model (PIM), SGI-Bench, Test-Time Reinforcement Learning (TTRL), retrieval-augmented novelty, agent-based evaluation]</li>
<li class=""><strong>authors:</strong> Wanghan Xu, Yuhao Zhou, Yifan Zhou, Qinglong Cao, Shuo Li, Jia Bu, Bo Liu, Yixin Chen, Xuming He, Xiangyu Zhao, Xiang Zhuang, Fengxiang Wang, Zhiwang Zhou, Qiantai Feng, Wenxuan Huang, Jiaqi Wei, Hao Wu, Yuejin Yang, Guangshuai Wang, Sheng Xu, Ziyan Huang, Xinyao Liu, Jiyao Liu, Cheng Tang, Wei Li, Ying Chen, Junzhi Ning, Pengfei Jiang, Chenglong Ma, Ye Du, Changkai Ji, Huihui Xu, Ming Hu, Jiangbin Zheng, Xin Chen, Yucheng Wu, Feifei Jiang, Xi Chen, Xiangru Tang, Yuchen Fu, Yingzhou Lu, Yuanyuan Zhang, Lihao Sun, Chengbo Li, Jinzhe Ma, Wanhao Liu, Yating Liu, Kuo-Cheng Wu, Shengdu Chai, Yizhou Wang, Ouwen Zhangjin, Chen Tang, Shufei Zhang, Wenbo Cao, Junjie Ren, Taoyong Cui, Zhouheng Yao, Juntao Deng, Yijie Sun, Feng Liu, Wangxu Wei, Jingyi Xu, Zhangrui Li, Junchao Gong, Zijie Guo, Zhiyu Yao, Zaoyu Chen, Tianhao Peng, Fangchen Yu, Bo Zhang, Dongzhan Zhou, Shixiang Tang, Jiaheng Liu, Fenghua Ling, Yan Lu, Yuchen Ren, Ben Fei, Zhen Zhao, Xinyu Gu, Rui Su, Xiao-Ming Wu, Weikang Si, Yang Liu, Hao Chen, Xiangchao Yan, Xue Yang, Junchi Yan, Jiamin Wu, Qihao Zheng, Chenhui Li, Zhiqiang Gao, Hao Kong, Junjun He, Mao Su, Tianfan Fu, Peng Ye, Chunfeng Song, Nanqing Dong, Yuqiang Li, Huazhu Fu</li>
<li class=""><strong>institution:</strong> Shanghai Artificial Intelligence Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16969" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16969</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40fba3081819027f6af6208a55e87bd4bfc888d4ba6ce07d9baa5f158fbe6fa2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40fba3081819027f6af6208a55e87bd4bfc888d4ba6ce07d9baa5f158fbe6fa2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a framework for evaluating Scientific General Intelligence (SGI) in LLMs, grounded in the Practical Inquiry Model and operationalized through the SGI-Bench benchmark. The results reveal significant performance gaps across tasks like deep research and experimental reasoning. The authors also introduce Test-Time Reinforcement Learning (TTRL) to enhance hypothesis novelty without requiring reference answers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A Women&#x27;s Health Benchmark for Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [healthcare AI evaluation], [women&#x27;s health benchmark, large language models, error types, model stumps, query types]</li>
<li class=""><strong>authors:</strong> Victoria-Elisabeth Gruber, Razvan Marinescu, Diego Fajardo, Amin H. Nassar, Christopher Arkfeld, Alexandria Ludlow, Shama Patel, Mehrnoosh Samaei, Valerie Klug, Anna Huber, Marcel Gühner, Albert Botta i Orfila, Irene Lagoja, Kimya Tarr, Haleigh Larson, Mary Beth Howard</li>
<li class=""><strong>institution:</strong> Lumos AI, Yale Cancer Center, Harvard Medical School, UCSF, Brown University, Emory University, Clinic Ottakring, NHS, Yale School of Medicine, Johns Hopkins University School of Medicine</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17028" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17028</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Women&#x27;s Health Benchmark (WHB), a novel evaluation framework comprising 96 validated model stumps across five medical specialties, three query types, and eight error types to assess LLM performance in women&#x27;s health. It finds that current LLMs have approximately 60% failure rates, with significant weaknesses in detecting urgency, indicating they are not yet reliable for providing women&#x27;s health advice.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] GB-DQN: Gradient Boosted DQN Models for Non-stationary Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [gradient boosting, DQN, ensemble learning, Bellman residual, non-stationary environments]</li>
<li class=""><strong>authors:</strong> Chang-Hwan Lee, Chanseung Lee</li>
<li class=""><strong>institution:</strong> Florida Atlantic University, Morrow Company</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17034" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17034</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes GB-DQN, a method that uses gradient boosting to create an ensemble of Q-networks, where each new network learns the residual error of the current ensemble to adapt to non-stationary environments. Experiments show that GB-DQN achieves faster recovery and greater robustness compared to standard DQN and other baselines in tasks with changing dynamics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SFBD-OMNI: Bridge models for lossy measurement restoration with limited clean samples</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [generative models], [diffusion models, bridge models, entropic optimal transport, distribution restoration, SFBD-OMNI]</li>
<li class=""><strong>authors:</strong> Haoye Lu, Yaoliang Yu, Darren Ho</li>
<li class=""><strong>institution:</strong> University of Waterloo, Vector Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17051" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17051</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SFBD-OMNI, a bridge model framework that generalizes Stochastic Forward-Backward Deconvolution to restore a clean data distribution from abundant noisy samples and a black-box corruption process, framed as a one-sided entropic optimal transport problem. It shows that with a small number of clean samples, the underlying distribution becomes largely recoverable even in cases of per-sample information loss. Experiments demonstrate significant improvements across diverse measurement settings beyond Gaussian corruption.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Dynamic Tool Dependency Retrieval for Efficient Function Calling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [dynamic tool dependency retrieval, function calling, tool retrieval, on-device agents, tool-augmented llms]</li>
<li class=""><strong>authors:</strong> Bhrij Patel, Davide Belli, Amir Jalalirad, Maximilian Arnold, Aleksandr Ermovol, Bence Major</li>
<li class=""><strong>institution:</strong> Qualcomm Research, University of Maryland, College Park</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17052" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17052</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1d880d31eb189fff50465aa10379459bec7bfbbf666206f8ad6ea98793a534a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1d880d31eb189fff50465aa10379459bec7bfbbf666206f8ad6ea98793a534a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Dynamic Tool Dependency Retrieval (DTDR), a lightweight method that retrieves relevant tools for LLM function calling by conditioning on both the initial user query and the evolving execution context. It models tool dependencies from demonstrations to adaptively retrieve tools as a plan unfolds. The results show that this dynamic retrieval improves function calling success rates by 23% to 104% compared to static retrieval methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Universal consistency of the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span>-NN rule in metric spaces and Nagata dimension. III</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [statistical learning theory], [k-nearest neighbor classifier, universal consistency, Nagata dimension, Lebesgue-Besicovitch differentiation property, metric spaces]</li>
<li class=""><strong>authors:</strong> Vladimir G. Pestov</li>
<li class=""><strong>institution:</strong> Universidade Federal de Santa Catarina, University of Ottawa</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17058" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17058</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proves the final implication needed to establish the equivalence between the universal consistency of the k-nearest neighbor classifier, the strong Lebesgue-Besicovitch differentiation property, and a metric space being sigma-finite dimensional in the sense of Nagata. The core method extends a measure construction from Hilbert spaces to any complete separable metric space lacking the Nagata property. The main conclusion is that the k-NN classifier is universally consistent in a metric space if and only if the space is sigma-finite dimensional.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [mathematical reasoning], [chain-of-thought prompting, reinforcement learning, GRPO, fine-tuning, error recovery]</li>
<li class=""><strong>authors:</strong> Saraswathy Amjith, Mihika Dusad, Neha Muramalla, Shweta Shah</li>
<li class=""><strong>institution:</strong> MIT</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17079" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17079</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9501255b38adfbd4ed3cb05e9a136df6cf358b6420281716744f14c17554a871_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9501255b38adfbd4ed3cb05e9a136df6cf358b6420281716744f14c17554a871_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper fine-tunes the Qwen3-4B model using GRPO reinforcement learning on intentionally flawed chain-of-thought reasoning traces to improve error detection and recovery. It finds that this mixed training on both calculation and reasoning errors improves robustness to misleading prefills without sacrificing accuracy on clean problems, unlike standard fine-tuning which degrades robustness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [mixture-of-experts, offloading, dynamic quantization, low-rank compensation, router-guided precision restoration]</li>
<li class=""><strong>authors:</strong> Zhenyu Liu, Yunzhen Liu, Zehao Fan, Garrett Gagnon, Yayue Hou, Nan Wu, Yangwook Kang, Liu Liu</li>
<li class=""><strong>institution:</strong> Rensselaer Polytechnic Institute, University of Massachusetts Amherst, George Washington University, Samsung Semiconductor</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17073" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17073</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c6e9afd2a3ff75dfe81968ba3c73da71d577f341a443ab96f8bb14a681ed3f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c6e9afd2a3ff75dfe81968ba3c73da71d577f341a443ab96f8bb14a681ed3f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a bandwidth-efficient method for Mixture-of-Experts (MoE) inference that uses router-guided, low-rank compensation to dynamically restore precision for the most important experts while keeping others in low-bit form. This approach reduces I/O traffic during offloading without significantly harming model accuracy. The method demonstrates a superior bandwidth-accuracy trade-off and improved throughput on GPU and GPU-NDP systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Perturb Your Data: Paraphrase-Guided Training Data Watermarking</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [SPECTRA, watermarking, training data detection, membership inference attack, paraphrase generation, scoring model, token probability comparison]</li>
<li class=""><strong>authors:</strong> Pranav Shetty, Mirazul Haque, Petr Babkin, Zhiqiang Ma, Xiaomo Liu, Manuela Veloso</li>
<li class=""><strong>institution:</strong> JPMorgan AI Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17075" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17075</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b47f34679362a94a5413b86758bfca6d1690e7158a9b8bc7a21706264c5e833c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b47f34679362a94a5413b86758bfca6d1690e7158a9b8bc7a21706264c5e833c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SPECTRA, a watermarking method that subtly paraphrases text using an LLM to embed a detectable signature into training data without altering its statistical distribution. It verifies unauthorized use by comparing token probabilities between a suspect model and a scoring model. The approach reliably detects watermarked data even when it constitutes a minuscule fraction of the training corpus, providing a scalable pre-release watermark for data owners.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] How to Square Tensor Networks and Circuits Without Squaring Them</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [probabilistic modeling], [tensor networks, squared circuits, probabilistic circuits, marginalization, canonical forms, unitary matrices, distribution estimation]</li>
<li class=""><strong>authors:</strong> Lorenzo Loconte, Adrián Javaloy, Antonio Vergari</li>
<li class=""><strong>institution:</strong> University of Edinburgh</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17090" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17090</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method to parameterize squared circuits (a generalization of squared tensor networks) using conditions inspired by orthogonality and determinism, enabling efficient marginalization without squaring. This approach overcomes computational overhead while maintaining expressiveness for distribution estimation. Experiments confirm the method allows more efficient learning without loss of expressiveness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [reinforcement learning, model predictive control, MPPI, hierarchical planning, adaptive sampling]</li>
<li class=""><strong>authors:</strong> Toshiaki Hori, Jonathan DeCastro, Deepak Gopinath, Avinash Balachandran, Guy Rosman</li>
<li class=""><strong>institution:</strong> Toyota Research Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17091" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17091</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method that fuses reinforcement learning and model-predictive control (MPC) into an adaptive hierarchical framework. It uses RL actions to guide the MPPI sampler and adaptively aggregates MPPI samples to improve value estimation, leading to more robust and sample-efficient policies. The approach demonstrates improved data efficiency, performance, and convergence speed in domains like race driving and Lunar Lander.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [interpretability], [counterfactual explanations, model-agnostic, time series, ECG, LIME, SHAP]</li>
<li class=""><strong>authors:</strong> Justin Li, Efe Sencan, Jasper Zheng Duan, Vitus J. Leung, Stephan Tsaur, Ayse K. Coskun</li>
<li class=""><strong>institution:</strong> Boston University, Sandia National Laboratories, Boston Medical Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17100" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17100</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces UniCoMTE, a universal, model-agnostic framework for generating counterfactual explanations for time series classifiers by modifying input samples to identify influential temporal features. It is evaluated on an ECG classifier and shown to produce more concise, stable, and human-aligned explanations than established methods like LIME and SHAP, thereby improving model interpretability for real-world applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [modular reuse, on-device execution, model decomposition, parallel execution, quantization]</li>
<li class=""><strong>authors:</strong> Kunjal Panchal, Saayan Mitra, Somdeb Sarkhel, Haoliang Wang, Ishita Dasgupta, Gang Wu, Hui Guan</li>
<li class=""><strong>institution:</strong> University of Massachusetts Amherst, Adobe Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17108" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17108</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ae01de3bb190d7134b2995b14582f1e46ed434d4588a9e6017b7c9282b01074_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ae01de3bb190d7134b2995b14582f1e46ed434d4588a9e6017b7c9282b01074_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Atom, an on-device system that decomposes large video-language models into reusable modules (e.g., visual encoder, language decoder) to eliminate redundant model loading and enable parallel execution across subtasks like captioning and retrieval. This modular reuse approach reduces end-to-end latency by 27–33% on commodity smartphones with only marginal performance drops, making it a practical solution for efficient video-language pipelines on edge devices.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Fault Diagnosis and Quantification for Photovoltaic Arrays based on Differentiable Physical Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [differentiable physical model, gradient-based fault parameters identification, Adahessian optimizer, I-V curve reconstruction]</li>
<li class=""><strong>authors:</strong> Zenan Yang, Yuanliang Li, Jingwei Zhang, Yongjie Liu, Kun Ding</li>
<li class=""><strong>institution:</strong> Hohai University, Concordia University, Aalborg University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17107" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17107</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5899b49765c1a033a8227b9785fb68b2ee7e7efe7e5a6fd98b6b7f264267e08c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5899b49765c1a033a8227b9785fb68b2ee7e7efe7e5a6fd98b6b7f264267e08c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a differentiable fast fault simulation model (DFFSM) and a gradient-based fault parameters identification (GFPI) method using the Adahessian optimizer to quantify faults in photovoltaic arrays. The method accurately models I-V characteristics under multiple faults and uses analytical gradients for efficient parameter identification. Experimental results show high quantification accuracy with I-V reconstruction errors below 3%, demonstrating the effectiveness of differentiable physical simulators for PV fault diagnosis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Bridging Training and Merging Through Momentum-Aware Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [low-rank factorization, momentum-aware optimization, curvature-aware merging, model composition, memory-efficient training]</li>
<li class=""><strong>authors:</strong> Alireza Moayedikia, Alicia Troncoso</li>
<li class=""><strong>institution:</strong> Swinburne University of Technology, Universidad Pablo de Olavide</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17109" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17109</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a unified framework that maintains factorized momentum and curvature statistics during model training, then reuses this information for geometry-aware model merging. This approach eliminates the need to recompute curvature data, saving computation and enabling more principled model composition. The method demonstrates improved performance on language understanding benchmarks and offers better hyperparameter robustness compared to existing low-rank optimizers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [contrastive fine-tuning, token attribution, t-SNE projection, attention-head ablation, image retrieval, negation handling]</li>
<li class=""><strong>authors:</strong> Jasmine Vu, Shivanand Sheshappanavar</li>
<li class=""><strong>institution:</strong> Santa Clara University, University of Wyoming</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17121" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17121</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcadf51218711774df214ce0c80d2175e2b72d14b2fab69a5a27aebd9bc17bd2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcadf51218711774df214ce0c80d2175e2b72d14b2fab69a5a27aebd9bc17bd2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper fine-tunes the CLIP-based CheXagent model to improve its ability to understand negated phrases in medical image retrieval tasks. The method uses contrastive fine-tuning and analyzes internal model behavior through techniques like token attribution. The results show improved negation handling with a slight trade-off in accuracy for positive prompts.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Digitizing Nepal&#x27;s Written Heritage: A Comprehensive HTR Pipeline for Old Nepali Manuscripts</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [handwritten text recognition], [encoder-decoder architectures, data-centric techniques, line-level transcription, transfer learning, data augmentation, character error rate]</li>
<li class=""><strong>authors:</strong> Anjali Sarawgi, Esteban Garces Arias, Christof Zotter</li>
<li class=""><strong>institution:</strong> LMU Munich, Heidelberg Academy of Sciences and Humanities, Munich Center for Machine Learning (MCML)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17111" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17111</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bae7b70bc5a974847e0434967f87fb01f452c81f46d95e3cea515746090bd7f5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bae7b70bc5a974847e0434967f87fb01f452c81f46d95e3cea515746090bd7f5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an end-to-end Handwritten Text Recognition (HTR) pipeline for Old Nepali manuscripts, employing line-level transcription and exploring encoder-decoder architectures with data-centric methods. The best model achieves a low Character Error Rate (CER) of 4.9%, demonstrating effective digitization for this low-resource historical language. The authors release their training code and evaluation scripts to support further research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] DiffeoMorph: Learning to Morph 3D Shapes Using Differentiable Agent-Based Simulations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [differentiable simulation, graph neural network, SE(3)-equivariance, attention mechanism, 3D Zernike polynomials, shape-matching loss, implicit differentiation, bilevel optimization]</li>
<li class=""><strong>authors:</strong> Seong Ho Pahng, Guoye Guan, Benjamin Fefferman, Sahand Hormoz</li>
<li class=""><strong>institution:</strong> Harvard University, Harvard Medical School, Dana-Farber Cancer Institute, Broad Institute of MIT and Harvard</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17129" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17129</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces DiffeoMorph, a differentiable framework that uses an attention-based SE(3)-equivariant graph neural network to train agents to collectively morph into target 3D shapes. It employs a novel shape-matching loss based on 3D Zernike polynomials and uses implicit differentiation to handle a bilevel optimization problem for rotation alignment. The method successfully generates complex shapes from simple ellipsoids using minimal spatial cues.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [Generalized Primal Averaging (GPA), DiLoCo, Schedule-Free, AdamW, Nesterov&#x27;s method, primal averaging, optimizer, iterate averaging]</li>
<li class=""><strong>authors:</strong> Aaron Defazio, Konstantin Mishchenko, Parameswaran Raman, Hao-Jun Michael Shi, Lin Xiao</li>
<li class=""><strong>institution:</strong> Meta Superintelligence Labs</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17131" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17131</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Generalized Primal Averaging (GPA), a new optimizer that extends Nesterov&#x27;s method to perform smooth, per-step averaging of model iterates, addressing limitations of periodic averaging methods like single-worker DiLoCo. It demonstrates that GPA outperforms single-worker DiLoCo, simplifies hyperparameter tuning, reduces memory overhead, and achieves significant speedups in training LLMs and vision models compared to AdamW.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Biosecurity-Aware AI: Agentic Risk Auditing of Soft Prompt Attacks on ESM-Based Variant Predictors</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [soft prompt attacks, adversarial auditing, agentic framework, risk metrics, embedding-space robustness]</li>
<li class=""><strong>authors:</strong> Huixin Zhan</li>
<li class=""><strong>institution:</strong> New Mexico Institute of Mining and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17146" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17146</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edf3a867526cb70d49c0816641a925b129fe30dc5f7871777c74f463824654df_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edf3a867526cb70d49c0816641a925b129fe30dc5f7871777c74f463824654df_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SAGE, an agentic framework that audits genomic foundation models by injecting soft prompt perturbations and evaluating performance degradation. It finds that models like ESM2 are vulnerable to such attacks, revealing hidden security risks in biomedical applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Distributed Learning in Markovian Restless Bandits over Interference Graphs for Stable Spectrum Sharing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [wireless communication, distributed learning], [restless multi-armed bandit (RMAB), Gale-Shapley matching, interference graph, SMILE algorithm, distributed optimization]</li>
<li class=""><strong>authors:</strong> Liad Lea Didi, Kobi Cohen</li>
<li class=""><strong>institution:</strong> Ben-Gurion University of the Negev</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17161" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17161</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SMILE, a distributed learning algorithm that integrates restless bandit learning with graph-constrained coordination for stable spectrum sharing in interference graphs. It proves that SMILE converges to the optimal stable allocation and achieves logarithmic regret. Simulations validate the algorithm&#x27;s robustness, scalability, and efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] BumpNet: A Sparse Neural Network Framework for Learning PDE Solutions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [pde solving], [BumpNet, sparse neural network, basis function expansion, sigmoid activation, physics-informed neural networks, PINNs, DeepONet, EDNN, meshless method, h-adaptivity]</li>
<li class=""><strong>authors:</strong> Shao-Ting Chiu, Ioannis G. Kevrekidis, Ulisses Braga-Neto</li>
<li class=""><strong>institution:</strong> Texas A&amp;M University, The Johns Hopkins University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17198" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17198</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces BumpNet, a sparse neural network framework that constructs adaptive, trainable basis functions from sigmoid activations for solving PDEs. It combines BumpNet with existing architectures like PINNs, DeepONet, and EDNN to create efficient, interpretable models for PDE solution and operator learning. The proposed methods demonstrate improved accuracy and reduced computational cost compared to standard approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning solution operator of dynamical systems with diffusion maps kernel ridge regression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [dynamical systems modeling], [kernel ridge regression, diffusion maps, operator learning, geometry-aware learning, data-driven kernels]</li>
<li class=""><strong>authors:</strong> Jiwoo Song, Daning Huang, John Harlim</li>
<li class=""><strong>institution:</strong> The Pennsylvania State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17203" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17203</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Diffusion Maps Kernel Ridge Regression (DM-KRR) method for learning solution operators of complex dynamical systems. It combines a simple kernel ridge regression framework with a data-driven kernel from diffusion maps to adapt to the system&#x27;s intrinsic geometry without explicit manifold modeling. The method is shown to outperform state-of-the-art approaches in accuracy and data efficiency across various systems, highlighting the importance of geometric constraints for long-term prediction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Do Foundational Audio Encoders Understand Music Structure?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [music information retrieval], [music structure analysis, foundational audio encoders, self-supervised learning, masked language modeling, boundary detection, function prediction]</li>
<li class=""><strong>authors:</strong> Keisuke Toyama, Zhi Zhong, Akira Takahashi, Shusuke Takahashi, Yuki Mitsufuji</li>
<li class=""><strong>institution:</strong> Sony Group Corporation, Sony AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17209" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17209</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates the use of pretrained foundational audio encoders (FAEs) for music structure analysis (MSA). Through comprehensive experiments on 11 FAE types, it finds that models using self-supervised learning with masked language modeling on music data are particularly effective for MSA tasks like boundary detection and function prediction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [reinforcement learning, group relative policy optimization, knowledge graph consistency, entity-relation matching, process supervision, hard-example mining]</li>
<li class=""><strong>authors:</strong> Xiao Liang, Yuxuan An, Di Wang, Jiawei Hu, Zhicheng Jiao, Bin Jing, Quan Wang</li>
<li class=""><strong>institution:</strong> Xidian University, Brown University, Capital Medical University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17213" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17213</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c86c0e9aa8fe8870d86c5f63baf1ccd89856e7434ece25e03ba384660733fc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c86c0e9aa8fe8870d86c5f63baf1ccd89856e7434ece25e03ba384660733fc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes CheXPO-v2, an alignment framework that uses a Knowledge Graph Consistency Reward for process supervision to reduce hallucinations in medical VLMs. It parses reasoning into structured triplets to penalize incoherent logic, outperforming prior methods on benchmarks with high data efficiency. The approach achieves state-of-the-art accuracy on MIMIC-CXR-VQA using only 5k samples.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [differential privacy, local differential privacy, RAPPOR, PAC indistinguishability, hybrid privacy, rarity-aware protection]</li>
<li class=""><strong>authors:</strong> Madhava Gaikwad</li>
<li class=""><strong>institution:</strong> Microsoft</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17251" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17251</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes AlignDP, a hybrid privacy mechanism that protects large language models by separating data into rare and non-rare fields. Rare fields are shielded with PAC indistinguishability for strong privacy, while non-rare fields are privatized using RAPPOR to allow useful frequency estimation. This approach aims to prevent knowledge extraction and unauthorized fine-tuning by design, making models more secure against distillation and editing attacks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [federated learning, byzantine-robust aggregation, privacy-preserving, dimensionality reduction, secure multi-party computation, adaptive tuning]</li>
<li class=""><strong>authors:</strong> Baolei Zhang, Minghong Fang, Zhuqing Liu, Biao Yi, Peizhao Zhou, Yuan Wang, Tong Li, Zheli Liu</li>
<li class=""><strong>institution:</strong> Nankai University, University of Louisville, University of North Texas</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17254" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17254</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ABBR, a practical framework for federated learning that combines Byzantine-robust aggregation with privacy-preserving techniques. Its core method uses dimensionality reduction to speed up private computations and an adaptive tuning strategy to minimize the impact of malicious models. The framework is shown to run significantly faster with minimal overhead while maintaining strong Byzantine resilience.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Electric Vehicle Charging Load Forecasting: An Experimental Comparison of Machine Learning Methods</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [time series forecasting, LSTM, GRU, Transformer, ARIMA, XGBoost]</li>
<li class=""><strong>authors:</strong> Iason Kyriakopoulos, Yannis Theodoridis</li>
<li class=""><strong>institution:</strong> University of Piraeus</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17257" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17257</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper experimentally compares five time series forecasting models, including traditional statistical methods, machine learning, and deep learning (e.g., ARIMA, XGBoost, LSTM, GRU, Transformer), for predicting electric vehicle charging load. The evaluation across multiple real-world datasets, temporal horizons, and spatial aggregation levels shows that recurrent neural networks (GRU, LSTM) generally perform best for mid- and long-term forecasting, while Transformers excel in short-term forecasting at higher aggregation levels.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SHARP-QoS: Sparsely-gated Hierarchical Adaptive Routing for joint Prediction of QoS</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hyperbolic convolution, Poincaré ball, adaptive feature-sharing, gated feature fusion, EMA-based loss balancing, multi-task learning, graph convolution networks]</li>
<li class=""><strong>authors:</strong> Suraj Kumar, Arvind Kumar, Soumi Chattopadhyay</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology Indore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17262" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17262</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SHARP-QoS, a unified model for joint QoS prediction that uses hyperbolic convolution to extract hierarchical features, an adaptive feature-sharing mechanism with gated fusion, and an EMA-based loss balancing strategy. It demonstrates superior performance over single- and multi-task baselines across multiple datasets, effectively handling sparsity, outliers, and cold-start scenarios with moderate computational cost.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A Theoretical Analysis of State Similarity Between Markov Decision Processes</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [bisimulation metric, generalized bisimulation metric, Markov decision process, policy transfer, state aggregation, sampling-based estimation]</li>
<li class=""><strong>authors:</strong> Zhenyu Tao, Wei Xu, Xiaohu You</li>
<li class=""><strong>institution:</strong> Southeast University, Purple Mountain Laboratories</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17265" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17265</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a Generalized Bisimulation Metric (GBSM) to measure state similarity between different Markov Decision Processes, establishing its fundamental mathematical properties. The authors leverage GBSM to theoretically analyze tasks like policy transfer and state aggregation, obtaining tighter performance bounds than previous methods. Numerical results validate the effectiveness of GBSM for multi-MDP scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [audit agents, attestation protocols, constrained reasoning, cryptographic attestation, symbolic methods, benchmark suite, verifiability]</li>
<li class=""><strong>authors:</strong> Abhivansh Gupta</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology, Roorkee</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17259" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17259</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a60caf6cec7c2ab0bdf36d4e5ba8513e86e73ba9dc3d215615ad6190a8df9091_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a60caf6cec7c2ab0bdf36d4e5ba8513e86e73ba9dc3d215615ad6190a8df9091_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Verifiability-First architecture for LLM-based agents, integrating runtime attestations, lightweight audit agents for continuous verification, and challenge-response protocols for high-risk operations. It introduces the OPERA benchmark to evaluate the detectability and speed of remediation for misaligned behavior, shifting the focus from measuring the propensity for misalignment to ensuring reliable detection and control.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Alzheimer&#x27;s Disease Brain Network Mining</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical AI], [semi-supervised learning, optimal transport, label propagation, graph neural networks, Wasserstein distance]</li>
<li class=""><strong>authors:</strong> Alireza Moayedikia, Sara Fin</li>
<li class=""><strong>institution:</strong> Swinburne University of Technology, Monash University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17276" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17276</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MATCH-AD, a semi-supervised framework that combines deep representation learning, graph-based label propagation, and optimal transport theory to diagnose Alzheimer&#x27;s disease from neuroimaging data with limited labeled samples. The method achieves near-perfect diagnostic accuracy on a large dataset, demonstrating that semi-supervised learning can effectively leverage partially annotated data for clinical deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Understanding Generalization in Role-Playing Models via Information Theory</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [natural language processing], [information theory, mutual information, reinforcement learning, distribution shift, role-playing models]</li>
<li class=""><strong>authors:</strong> Yongqi Li, Hao Lang, Fei Huang, Tieyun Qian, Yongbin Li</li>
<li class=""><strong>institution:</strong> Wuhan University, Tongyi Lab, Zhongguancun Academy</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17270" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17270</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85ede876c98e7e8d1d360bf9eb2cb767cc2570e218ebc72489f68b5cec65ce55_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85ede876c98e7e8d1d360bf9eb2cb767cc2570e218ebc72489f68b5cec65ce55_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an information-theoretic metric called reasoning-based effective mutual information difference (R-EMID) to measure and analyze the generalization degradation of role-playing models under distribution shifts. It also proposes a co-evolving reinforcement learning framework to improve response probability estimation for calculating R-EMID. The main conclusion is that user shift poses the highest risk to model performance and reinforcement learning is the most effective approach for enhancing generalization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Kolmogorov-Arnold Networks (KANs), multilayer perceptron networks (MLPs), Physics-informed Neural Networks, nonlocal consistency loss, integro-differential equations (IDEs), fractional PDEs]</li>
<li class=""><strong>authors:</strong> Farinaz Mostajeran, Aruzhan Tleubek, Salah A Faroughi</li>
<li class=""><strong>institution:</strong> University of Utah</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17273" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17273</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MINPO, a unified neural framework that learns nonlocal operators and their inverses using KANs or MLPs to solve integro-differential equations. It enforces coherence between the learned operator and solution via a nonlocal consistency loss. The method is shown to be accurate and robust across diverse kernel types and dimensionalities, generalizing beyond problem-specific formulations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Warmer for Less: A Cost-Efficient Strategy for Cold-Start Recommendations at Pinterest</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [residual connection, score regularization, manifold mixup]</li>
<li class=""><strong>authors:</strong> Saeed Ebrahimi, Weijie Jiang, Jaewon Yang, Olafur Gudmundsson, Yucheng Tu, Huizhong Duan</li>
<li class=""><strong>institution:</strong> Pinterest</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17277" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17277</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f351bf91dbfa3f8f187d67a36cb3c88f1014690a13606c70e860bd376621c71f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f351bf91dbfa3f8f187d67a36cb3c88f1014690a13606c70e860bd376621c71f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a cost-efficient strategy to improve cold-start recommendations by introducing lightweight techniques: a residual connection for non-historical features, a score regularization term, and manifold mixup for data sparsity. These methods collectively increased fresh content engagement by 10% without harming overall engagement or cost, and have been deployed at Pinterest.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [speech processing], [voice activity detection, vision transformer, MFCC, Gammatone filter bank cepstral coefficients, dataset augmentation, out-of-distribution evaluation]</li>
<li class=""><strong>authors:</strong> Ioannis Stylianou, Achintya kr. Sarkar, Nauman Dawalatabad, James Glass, Zheng-Hua Tan</li>
<li class=""><strong>institution:</strong> Aalborg University, Pioneer Centre for AI, IIIT SriCity, Zoom Communications Inc., Massachusetts Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17281" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17281</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces LibriVAD, a scalable open dataset for voice activity detection (VAD) created by augmenting LibriSpeech with diverse noise sources. It benchmarks several feature-model combinations and proposes using a Vision Transformer (ViT) architecture for VAD. The main conclusion is that ViT with MFCC features outperforms established models across various conditions, and scaling the dataset size and balancing its silence-to-speech ratio consistently improves out-of-distribution generalization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [memristive architecture, minion recurrent unit, weighted-bit streaming, experience replay, mixed-signal accelerator, on-chip continual learning]</li>
<li class=""><strong>authors:</strong> Abdullah M. Zyarah, Dhireesha Kudithipudi</li>
<li class=""><strong>institution:</strong> University of Texas at San Antonio, University of Baghdad</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17299" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17299</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces M2RU, a mixed-signal hardware architecture that implements the Minion Recurrent Unit for efficient on-chip continual learning at the edge. It uses weighted-bit streaming and experience replay to enable energy-efficient temporal processing and stable adaptation. The results show significant energy efficiency improvements and a long operational lifetime, establishing M2RU as a scalable platform for edge-level temporal intelligence.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [explainable AI (XAI)], [graph theory, model decomposition, hypothesis-evidence structure, Cox proportional hazards model, PREDICT]</li>
<li class=""><strong>authors:</strong> Michael Merry, Pat Riddle, Jim Warren</li>
<li class=""><strong>institution:</strong> University of Auckland</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17316" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17316</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a formal, testable criterion for inherent explainability in AI, using graph theory to decompose models into verifiable structure-local explanations called annotations. The method is applied to demonstrate the inherent explainability of a clinical cardiovascular risk model (PREDICT). The work provides a rigorous foundation for regulators and formalizes the distinction between an explainable model and an explained one.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Task Schema and Binding: A Double Dissociation Study of In-Context Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [in-context learning], [activation patching, double dissociation, task schema, binding, transformer, mamba]</li>
<li class=""><strong>authors:</strong> Chaeha Kim</li>
<li class=""><strong>institution:</strong> Changwon National University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17325" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17325</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper uses activation patching experiments across multiple Transformer models and Mamba to causally dissect in-context learning. It concludes that ICL decomposes into two separable mechanisms: Task Schema (abstract task recognition) and Binding (specific input-output associations), with their reliance governed by a trade-off with the model&#x27;s prior knowledge.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [adaptive graph pruning, Spatio-Temporal Graph Neural Networks (ST-GNNs), Sudden Event Prediction Accuracy (SEPA), online semi-decentralized training, Federated Learning (FL), Gossip Learning]</li>
<li class=""><strong>authors:</strong> Ivan Kralj, Lodovico Giaretta, Gordan Ježić, Ivana Podnar Žarko, Šarūnas Girdzijauskas</li>
<li class=""><strong>institution:</strong> University of Zagreb, Faculty of Electrical Engineering and Computing; RISE Research Institutes of Sweden; KTH Royal Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17352" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17352</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a9feeec6bf4367fbf82a987484881d47da3c3be2e4b769373b648eb200942b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a9feeec6bf4367fbf82a987484881d47da3c3be2e4b769373b648eb200942b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an adaptive graph pruning algorithm for Spatio-Temporal Graph Neural Networks (ST-GNNs) to reduce communication overhead in online semi-decentralized traffic prediction systems. It also introduces a novel evaluation metric, SEPA, to measure responsiveness to sudden traffic events. The method maintains prediction accuracy while significantly lowering communication costs across different decentralized learning settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [ensemble learning, adversarial training, Bayesian inference, LLM-based sample generation, weight assignment]</li>
<li class=""><strong>authors:</strong> Yidong Chai, Yi Liu, Mohammadreza Ebrahimi, Weifeng Li, Balaji Padmanabhan</li>
<li class=""><strong>institution:</strong> Hefei University of Technology, University of South Florida, University of Georgia, University of Maryland</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17367" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17367</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a novel framework called LLM-SGA and instantiates a detector named ARHOCD, which uses an ensemble of base detectors, a dynamic Bayesian weight assignment method, and an iterative adversarial training strategy to improve robustness. The results show that ARHOCD achieves strong generalizability and improves detection accuracy for harmful online content under adversarial conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [adversarial control tokens, beam-search exploration, last-layer logit gap, LoRA-based adversarial training, reward hacking]</li>
<li class=""><strong>authors:</strong> Tung-Ling Li, Yuhao Wu, Hongliang Liu</li>
<li class=""><strong>institution:</strong> Palo Alto Networks</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17375" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17375</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces AdvJudge-Zero, a method that uses beam-search on a model&#x27;s next-token distribution to discover short, low-perplexity control token sequences that can flip the binary decisions of LLM-as-a-Judge systems from &quot;No&quot; to &quot;Yes&quot;. It concludes that these tokens represent a realistic reward-hacking vulnerability in post-training pipelines, and shows that adversarial training can mitigate the issue while preserving evaluation quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Timely Information Updating for Mobile Devices Without and With ML Advice</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [online scheduling], [competitive online algorithm, age of information, consistency-robustness trade-off, ML-augmented algorithm, adversarial environment]</li>
<li class=""><strong>authors:</strong> Yu-Pin Hsu, Yi-Hsuan Tseng</li>
<li class=""><strong>institution:</strong> National Taipei University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17381" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17381</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an online algorithm for a mobile device to decide when to send status updates to an access point, balancing information timeliness and update cost. The algorithm achieves an optimal competitive ratio against adversarial uncertainties and, when augmented with machine learning advice, attains an optimal consistency-robustness trade-off. The main conclusion is that an optimal competitive algorithm exhibits a threshold-like response to ML advice, either fully trusting or completely ignoring it.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] DeepShare: Sharing ReLU Across Channels and Layers for Efficient Private Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Private Inference, ReLU sharing, DReLU, cryptographic protocols, activation sharing]</li>
<li class=""><strong>authors:</strong> Yonathan Bornfeld, Shai Avidan</li>
<li class=""><strong>institution:</strong> Tel Aviv University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17398" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17398</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f38ab38719abeec4c59d997e57b2ecf1dd76acec3485b40aa5ccef81258f3179_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f38ab38719abeec4c59d997e57b2ecf1dd76acec3485b40aa5ccef81258f3179_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces DeepShare, a method for efficient Private Inference (PI) that reduces computational costs by sharing the DReLU (the non-linear step function of ReLU) across channels and layers within a neural network. It achieves state-of-the-art results by drastically decreasing the number of expensive DReLU operations while maintaining model performance on tasks like classification and segmentation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [stratified analysis, statistical methods, bias assessment, performance metrics, multiple comparisons correction, intersectional analysis]</li>
<li class=""><strong>authors:</strong> Dishantkumar Sutariya, Eike Petersen</li>
<li class=""><strong>institution:</strong> Fraunhofer Institute for Digital Medicine MEVIS</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17409" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17409</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a statistical toolbox called &#x27;meval&#x27; designed for rigorous, fine-grained analysis of machine learning model performance across different subgroups. It addresses challenges like metric selection, uncertainty estimation, and multiple comparisons to identify performance disparities. The main conclusion is that this toolbox enables practitioners to easily detect potential biases and failure modes, particularly in medical imaging applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [SWE-Bench++, automated benchmark generation, pull request harvesting, environment synthesis, test oracle extraction, hint-guided trajectory synthesis, fine-tuning]</li>
<li class=""><strong>authors:</strong> Lilin Wang, Lucas Ramalho, Alan Celestino, Phuc Anthony Pham, Yu Liu, Umang Kumar Sinha, Andres Portillo, Onassis Osunwa, Gabriel Maduekwe</li>
<li class=""><strong>institution:</strong> Turing</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17419" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17419</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/077c20705c707ee562f1935988b006695cf25f213f2df392cb27846fedaf0d4a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/077c20705c707ee562f1935988b006695cf25f213f2df392cb27846fedaf0d4a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SWE-Bench++, an automated framework that generates software engineering benchmarks by harvesting pull requests from GitHub to create reproducible, execution-based coding tasks across multiple languages. The method involves programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance, with a final step to create training trajectories from failed instances. The main conclusion is that this scalable, multilingual approach provides a valuable benchmark for evaluating and improving LLMs on repository-level code generation, as demonstrated by model performance metrics and fine-tuning improvements.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-agent reinforcement learning, independent proximal policy optimization, agent-based modeling, electricity markets, capacity markets, contracts for difference]</li>
<li class=""><strong>authors:</strong> Javier Gonzalez-Ruiz, Carlos Rodriguez-Pardo, Iacopo Savelli, Alice Di Bella, Massimo Tavoni</li>
<li class=""><strong>institution:</strong> Politecnico di Milano, CMCC Foundation - Euro-Mediterranean Center on Climate Change, RFF-CMCC European Institute on Economics and the Environment, Bocconi University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17444" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17444</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a multi-agent reinforcement learning framework, using independent proximal policy optimization, to model investment decisions by generation companies in long-term electricity markets. The model is applied to a stylized Italian electricity system to test various market designs and policy scenarios. The results demonstrate that market design is critical for achieving decarbonization targets while mitigating price volatility.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [multimodal semantic segmentation, deep neural network, robust training strategies, synchronized sensor data, daytime training for nighttime performance]</li>
<li class=""><strong>authors:</strong> Jon Muhovič, Janez Perš</li>
<li class=""><strong>institution:</strong> University of Ljubljana</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17450" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17450</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6eb14eac17bf3aa4e9ce145e08ce4eae06c5adaaf8ccf31ca3fa25a7f3835fa0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6eb14eac17bf3aa4e9ce145e08ce4eae06c5adaaf8ccf31ca3fa25a7f3835fa0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MULTIAQUA, a multimodal maritime dataset with synchronized RGB, thermal, IR, and LIDAR data, and proposes robust training strategies for multimodal semantic segmentation. The core method enables training a deep neural network using only daytime images to achieve reliable performance in challenging conditions like near-complete darkness. The main conclusion is that this approach simplifies data acquisition and annotation while maintaining robust scene interpretation for unmanned surface vehicles.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] When Data Quality Issues Collide: A Large-Scale Empirical Study of Co-Occurring Data Quality Issues in Software Defect Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [software defect prediction], [Explainable Boosting Machines, stratified interaction analysis, class imbalance, class overlap, irrelevant features, attribute noise, outliers]</li>
<li class=""><strong>authors:</strong> Emmanuel Charleson Dapaah, Jens Grabowski</li>
<li class=""><strong>institution:</strong> University of Göttingen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17460" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17460</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts a large-scale empirical study using Explainable Boosting Machines and stratified interaction analysis to examine five co-occurring data quality issues in software defect prediction across 374 datasets. It finds that co-occurrence is nearly universal, identifies tipping points for issues like class overlap and imbalance, and reveals context-dependent effects, concluding that no single model performs best under all conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning What to Write: Write-Gated KV for Efficient Long-Context Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [KV cache management, Write-Gated KV, KV Admission, KV cache eviction, KV cache selection, FlashAttention, paged-KV systems]</li>
<li class=""><strong>authors:</strong> Yen-Chieh Huang, Rui Fang, Ming-Syan Chen, Pi-Cheng Hsiu</li>
<li class=""><strong>institution:</strong> National Taiwan University, Academia Sinica</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17452" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17452</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Write-Gated KV, a learnable KV Admission mechanism that predicts token utility before it enters the KV cache to reduce memory usage and speed up inference. By filtering low-utility tokens early and maintaining a compact global cache, the method significantly reduces memory usage and improves prefill and decode speeds for long-context LLMs with minimal accuracy loss. The results demonstrate that proactive KV cache management is a practical solution for efficient long-context inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [time series forecasting], [spatial-temporal graph neural network, trend-seasonal decomposition, low-rank Top-K adjacency learning, horizon-wise gating, linear baseline]</li>
<li class=""><strong>authors:</strong> Henok Tenaw Moges, Deshendran Moodley</li>
<li class=""><strong>institution:</strong> University of Cape Town</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17453" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17453</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e930d6ea2e25d38e443cede1cee5618870d8bd50e1307a179be023dcac63701d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e930d6ea2e25d38e443cede1cee5618870d8bd50e1307a179be023dcac63701d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Lite-STGNN, a lightweight model combining decomposition-based linear temporal modeling with a learnable sparse graph module for spatial corrections. It achieves state-of-the-art accuracy on long-term multivariate forecasting benchmarks while being parameter-efficient and faster than transformer-based methods. The learned adjacency matrices also provide interpretable insights into domain-specific variable interactions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [marketing personalisation], [randomised controlled trial, agentic messaging, rule-based campaign, causal inference, contextual bandits]</li>
<li class=""><strong>authors:</strong> Olivier Jeunen, Schaun Wheeler</li>
<li class=""><strong>institution:</strong> aampe</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17462" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17462</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates an agentic messaging approach for customer communication, comparing it against a traditional rule-based system in a financial service application via a randomized controlled trial. The results show that the agentic system reduced unsubscribe events by 21% and encouraged earlier tax filing, demonstrating its effectiveness in improving user engagement and retention.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Linear Attention for Joint Power Optimization and User-Centric Clustering in Cell-Free Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [transformer, linear attention, supervised learning, user-centric clustering, power optimization]</li>
<li class=""><strong>authors:</strong> Irched Chafaa, Giacomo Bacci, Luca Sanguinetti</li>
<li class=""><strong>institution:</strong> University of Pisa</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17466" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17466</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a lightweight transformer model with a customized linear attention mechanism to jointly predict access point clusters and transmission powers in user-centric cell-free massive MIMO networks, using only spatial coordinates. This approach eliminates channel estimation overhead and pilot contamination while ensuring linear scalability with the number of users. Numerical results show the model provides near-optimal performance in maximizing minimum spectral efficiency and is adaptable to dynamic network scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Translating the Rashomon Effect to Sequential Decision-Making Tasks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [sequential decision-making], [Rashomon effect, formal verification, policy ensembles, behavioral cloning, permissive policies]</li>
<li class=""><strong>authors:</strong> Dennis Gross, Jørn Eirik Betten, Helge Spieker</li>
<li class=""><strong>institution:</strong> University of Oslo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17470" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17470</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper translates the Rashomon effect from classification to sequential decision-making by defining it for policies that behave identically but have different internal structures. It uses formal verification methods to compare the complete probabilistic behavior of policies in stochastic environments. The study concludes that the effect exists in this domain and that ensembles from the Rashomon set are more robust to distribution shifts.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Deep Learning-Based Surrogate Creep Modelling in Inconel 625: A High-Temperature Alloy Study</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [BiLSTM-Variational Autoencoder, BiLSTM-Transformer, surrogate modelling, Norton creep law, self-attention, probabilistic prediction, deterministic prediction]</li>
<li class=""><strong>authors:</strong> Shubham Das, Kaushal Singhania, Amit Sadhu, Suprabhat Das, Arghya Nandi</li>
<li class=""><strong>institution:</strong> Jadavpur University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17477" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17477</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes deep learning-based surrogate models, specifically a BiLSTM-Variational Autoencoder and a BiLSTM-Transformer, to rapidly predict creep strain in Inconel 625, replacing computationally expensive finite-element simulations. The models, trained on ANSYS-generated data, achieve high accuracy and provide predictions within seconds compared to the 30-40 minutes required by traditional simulations, enabling faster design optimization and structural health monitoring for high-temperature alloys.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, digital twin, Vision Transformer, ViT-UNet, privacy-preserving AI, brain tumor segmentation]</li>
<li class=""><strong>authors:</strong> Almustapha A. Wakili, Adamu Hussaini, Abubakar A. Musa, Woosub Jung, Wei Yu</li>
<li class=""><strong>institution:</strong> Towson University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17488" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17488</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67c12280225e186b761a6952a367a2b042a6fcd1886ac481e9bc15f5f81ee64a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67c12280225e186b761a6952a367a2b042a6fcd1886ac481e9bc15f5f81ee64a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes TwinSegNet, a federated learning framework that uses a hybrid ViT-UNet model and personalized digital twins for brain tumor segmentation without sharing raw data. It achieves high accuracy on heterogeneous MRI datasets, demonstrating that privacy can be preserved without sacrificing segmentation performance in multi-institutional settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multiple instance learning, AutoML, feature extraction, whole-slide images, benchmarking, computational pathology]</li>
<li class=""><strong>authors:</strong> Siemen Brussee, Pieter A. Valkema, Jurre A. J. Weijer, Thom Doeleman, Anne M.R. Schrader, Jesper Kers</li>
<li class=""><strong>institution:</strong> Leiden University Medical Center, Utrecht University Medical Center, Amsterdam University Medical Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17517" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17517</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces PathBench-MIL, an automated machine learning and benchmarking framework designed for Multiple Instance Learning in histopathology. It automates the entire pipeline from preprocessing to model aggregation, enabling standardized and reproducible evaluation of various models and feature extractors on whole-slide image datasets. The main conclusion is that this open-source framework facilitates rapid experimentation and standardization in computational pathology research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [protein hazard screening], [homology clustering, cluster-level holdout, logistic regression, random forest, linear SVM, calibrated probabilities, AUROC, AUPRC, Brier score, Expected Calibration Error]</li>
<li class=""><strong>authors:</strong> Muhammad Haris Khan</li>
<li class=""><strong>institution:</strong> University of Copenhagen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17527" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17527</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SafeBench-Seq, a benchmark and baseline classifier for screening hazardous protein sequences using only interpretable physicochemical and compositional features. The method employs homology clustering at ≤40% identity with cluster-level holdouts to evaluate performance on novel threats. The main conclusion is that random data splits overestimate robustness compared to this stricter homology-controlled evaluation, and that calibrated linear models provide good probability calibration for this CPU-only screening task.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] NetworkFF: Unified Layer Optimization in Forward-Only Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [neural network training algorithms], [Forward-Forward algorithm, Collaborative Forward-Forward, inter-layer cooperation, goodness function, neuromorphic computing]</li>
<li class=""><strong>authors:</strong> Salar Beigzad</li>
<li class=""><strong>institution:</strong> University of St. Thomas, Minnesota</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17531" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17531</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Collaborative Forward-Forward (CFF) learning, an enhancement to the Forward-Forward algorithm that addresses inter-layer isolation by incorporating weighted contributions from all layers into a collaborative goodness function. It proposes two variants, Fixed CFF and Adaptive CFF, which enable coordinated feature learning while preserving forward-only computation. The method demonstrates significant performance improvements on benchmark datasets, establishing inter-layer collaboration as a fundamental enhancement for biologically plausible and memory-efficient neural network training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Bayesian Optimisation: Which Constraints Matter?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [Bayesian optimisation], [Bayesian optimisation, Knowledge Gradient, Gaussian Processes, decoupled constraints, acquisition functions]</li>
<li class=""><strong>authors:</strong> Xietao Wang Lin, Juan Ungredda, Max Butler, James Town, Alma Rahat, Hemant Singh, Juergen Branke</li>
<li class=""><strong>institution:</strong> University of Warwick, ESTECO SpA, Swansea University, University of New South Wales</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17569" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17569</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes new Bayesian optimisation variants of the Knowledge Gradient acquisition function for problems with decoupled black-box constraints, focusing on evaluating only the most relevant constraints. The methods are empirically benchmarked and shown to be superior to existing state-of-the-art approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] When De-noising Hurts: A Systematic Study of Speech Enhancement Effects on Modern Medical ASR Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [MetricGAN-plus-voicebank, semantic WER, noise robustness, speech enhancement]</li>
<li class=""><strong>authors:</strong> Sujal Chondhekar, Vasanth Murukuri, Rushabh Vasani, Sanika Goyal, Rajshree Badami, Anushree Rana, Sanjana SN, Karthik Pandia, Sulabh Katiyar, Neha Jagadeesh, Sankalp Gulati</li>
<li class=""><strong>institution:</strong> EkaCare (Orbi Health Private Limited)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17562" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17562</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically evaluates the effect of MetricGAN-plus-voicebank speech enhancement on four modern ASR systems using medical speech under nine noise conditions. The study finds that denoising preprocessing consistently degrades ASR performance across all models and conditions, suggesting modern ASR models are inherently noise-robust and enhancement may remove critical acoustic features.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [vertical scheduling, optimizer step overlapping, SSD-offloaded training, gradient accumulation]</li>
<li class=""><strong>authors:</strong> Yikang Yue, Yishu Yin, Xuehai Qian</li>
<li class=""><strong>institution:</strong> Tsinghua University, University of Illinois at Urbana-Champaign</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17570" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17570</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces GreedySnake, a system that accelerates SSD-offloaded LLM training by using vertical scheduling to process all micro-batches per layer before moving to the next, and by overlapping the optimizer step with the next forward pass. This approach significantly reduces I/O bottlenecks and improves throughput compared to prior systems like ZeRO-Infinity, achieving up to 2.53x speedup for large models like GPT-175B.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [GPU-internal scheduling, resource sharing, collaborative multi-GPU video decoding, logically decoupled execution, FlashCodec, UnifiedServe]</li>
<li class=""><strong>authors:</strong> Lingxiao Zhao, Haoran Zhou, Yuezhi Che, Dazhao Cheng</li>
<li class=""><strong>institution:</strong> Wuhan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17574" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17574</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65d961bcef453cdff273d0991a97111419acf476f8d34e21f66dbd356156dfa7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65d961bcef453cdff273d0991a97111419acf476f8d34e21f66dbd356156dfa7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FlashCodec and UnifiedServe, a framework that optimizes multimodal large language model (MLLM) serving by accelerating video decoding and enabling resource sharing across the vision and text stages. This approach reduces latency and eliminates inter-stage blocking, leading to significantly higher throughput and better SLO adherence compared to existing systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, asynchronous collaboration, model stitching, ensembles, multi-objective learning]</li>
<li class=""><strong>authors:</strong> Arthur Guijt, Dirk Thierens, Ellen Kerkhof, Jan Wiersma, Tanja Alderliesten, Peter A.N. Bosman</li>
<li class=""><strong>institution:</strong> Not specified (inferred from author names only; no affiliations or email domains provided)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17592" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17592</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using model stitching to combine disjointly trained deep learning models as an asynchronous alternative to federated learning, allowing collaboration without sharing raw data. The method inserts stitching layers to merge intermediate representations, improving generalization across different parties&#x27; datasets while maintaining competitive performance on each party&#x27;s own data. The results show that asynchronous collaboration through stitching can yield competitive performance without requiring synchronous training or data exchange.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Machine Learning for Static and Single-Event Dynamic Complex Network Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [graph representation learning], [latent space models, latent distance model, graph representation learning, network embeddings, static networks, dynamic networks]</li>
<li class=""><strong>authors:</strong> Nikolaos Nakis</li>
<li class=""><strong>institution:</strong> arXiv</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17577" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17577</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a74e4c69d21da3d9d39474c8e185aec564c37a98f7b8fa85563d3b895bd7a484_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a74e4c69d21da3d9d39474c8e185aec564c37a98f7b8fa85563d3b895bd7a484_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This thesis develops novel algorithmic approaches for graph representation learning using latent space models, specifically the latent distance model, to capture network characteristics like homophily and transitivity. It aims to create structural-aware network representations for tasks such as community characterization and impact dynamics quantification in temporal networks. The methods are designed as unified learning processes to avoid heuristics and multi-stage post-processing, advancing towards comprehensive and powerful network embeddings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A Unified Representation of Neural Networks Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [neural networks theory], [continuous neural networks, neural ODEs, distributed parameter systems, approximation error, discretization]</li>
<li class=""><strong>authors:</strong> Christophe Prieur, Mircea Lazar, Bogdan Robu</li>
<li class=""><strong>institution:</strong> Univ. Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab, Eindhoven University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17593" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17593</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a unified representation of neural network architectures called Distributed Parameter neural Network (DiPaNet), derived by considering the limiting case where the number of neurons and layers tends to infinity. The method merges integral infinite-width representations with neural ODEs using homogenization and discretization techniques. The main conclusion is that most existing finite and infinite-dimensional neural network architectures can be related through this unified DiPaNet framework.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Gaussian Discriminant Analysis, Z-score distance analysis, cluster-driven deep learning, two-stage framework]</li>
<li class=""><strong>authors:</strong> Tosin Ige, Christopher Kiekintveld, Aritran Piplai, Asif Rahman, Olukunle Kolade, Sasidhar Kunapuli</li>
<li class=""><strong>institution:</strong> The University of Texas at El Paso, University of North Carolina</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17594" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17594</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes MAD-OOD, a two-stage cluster-driven deep learning framework for out-of-distribution malware detection. It uses Gaussian Discriminant Analysis to model class embeddings and Z-score distance analysis to identify anomalies, then integrates these predictions with a neural network for final classification. The method significantly outperforms state-of-the-art approaches on benchmark datasets, achieving high AUC for detecting unseen malware families.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A Systems-Theoretic View on the Convergence of Algorithms under Disturbances</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [converse Lyapunov theorems, stability bounds, convergence rates, distributed learning, noise injection, disturbance modeling]</li>
<li class=""><strong>authors:</strong> Guner Dilsad Er, Sebastian Trimpe, Michael Muehlebach</li>
<li class=""><strong>institution:</strong> Max Planck Institute for Intelligent Systems, RWTH Aachen University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17598" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17598</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper uses converse Lyapunov theorems to derive stability bounds and convergence rates for iterative algorithms operating under disturbances. It provides a unifying framework to quantify how noise and interconnections affect algorithmic performance, with applications in distributed learning and privacy-preserving computation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning Safe Autonomous Driving Policies Using Predictive Safety Representations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [safe reinforcement learning], [safe reinforcement learning, predictive safety representations, constrained markov decision processes, waymo open motion dataset, nuplan, srpl]</li>
<li class=""><strong>authors:</strong> Mahesh Keswani, Raunak Bhattacharyya</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology Delhi</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17586" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17586</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40dd4c52ca67a3f1ecc6ec7068de338a3616940aa4e51935c5ce5f30430ebbfe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40dd4c52ca67a3f1ecc6ec7068de338a3616940aa4e51935c5ce5f30430ebbfe_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates the Safety Representations for Safer Policy Learning (SRPL) framework, which augments SafeRL agents with a predictive model of future constraint violations to improve the safety-performance trade-off in autonomous driving. Experiments on real-world datasets (Waymo Open Motion Dataset and NuPlan) show that SRPL can lead to statistically significant improvements in success rate and cost reduction, and enhances robustness to noise and generalization in cross-dataset evaluation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] More Consistent Accuracy PINN via Alternating Easy-Hard Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [scientific machine learning], [physics-informed neural networks, easy-hard prioritization, hybrid training strategy, alternating scheme]</li>
<li class=""><strong>authors:</strong> Zhaoqian Gao, Min Yanga</li>
<li class=""><strong>institution:</strong> Yantai University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17607" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17607</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid training strategy for Physics-Informed Neural Networks (PINNs) that alternates between easy and hard prioritization to improve performance. The method achieves consistently high accuracy on challenging PDEs, significantly outperforming baseline approaches. The work demonstrates that this alternating scheme enhances the robustness and reliability of PINNs across diverse problem types.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SCOPE: Sequential Causal Optimization of Process Interventions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [prescriptive process monitoring], [backward induction, causal learning, reinforcement learning, sequential decision-making]</li>
<li class=""><strong>authors:</strong> Jakob De Moor, Hans Weytjens, Johannes De Smedt, Jochen De Weerdt</li>
<li class=""><strong>institution:</strong> KU Leuven, Technical University of Munich (TUM)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17629" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17629</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SCOPE, a prescriptive process monitoring approach that uses backward induction and causal learning to recommend aligned sequences of interventions for optimizing key performance indicators. It directly leverages observational data without needing process approximations for reinforcement learning. Experiments show SCOPE outperforms existing techniques in optimizing KPIs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [natural language processing], [ensemble learning, weighted voting, Condorcet’s Jury Theorem, fine-tuning, transformer models]</li>
<li class=""><strong>authors:</strong> Menna Elgabry, Ali Hamdi</li>
<li class=""><strong>institution:</strong> MSA University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17630" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17630</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a confidence- and credibility-weighted ensemble framework using diverse small transformer models (BERT, RoBERTa, etc.) for emotion detection. The method combines global validation performance and instance-level confidence to weight model votes. The ensemble achieves a 93.5% macro F1-score on the DAIR-AI dataset, outperforming larger LLMs while being more parameter-efficient.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Trust-Region Adaptive Policy Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [post-training], [Trust-Region Adaptive Policy Optimization, Trust-Region SFT, forward KL divergence, reverse KL, adaptive prefix-selection, supervised fine-tuning, reinforcement learning]</li>
<li class=""><strong>authors:</strong> Mingyu Su, Jian Guan, Yuxian Gu, Minlie Huang, Hongning Wang</li>
<li class=""><strong>institution:</strong> Tsinghua University, Ant Group</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17636" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17636</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a948c761d13b2b79a39ce9d5e992677a2054a69ebce16f21dcf30264ab34a82f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a948c761d13b2b79a39ce9d5e992677a2054a69ebce16f21dcf30264ab34a82f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces TRAPO, a hybrid framework that interleaves supervised fine-tuning and reinforcement learning within each training instance to unify expert supervision and self-exploration. It stabilizes training with Trust-Region SFT and an adaptive prefix-selection mechanism. Experiments on mathematical reasoning benchmarks show TRAPO outperforms standard pipelines and state-of-the-art approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Estimating Spatially Resolved Radiation Fields Using Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Monte-Carlo Simulation, Geant4, Convolutional Neural Networks, Fully Connected Neural Networks, U-Net, FCNN, Dataset Generation]</li>
<li class=""><strong>authors:</strong> Felix Lehner, Pasquale Lombardo, Susana Castillo, Oliver Hupe, Marcus Magnor</li>
<li class=""><strong>institution:</strong> Physikalisch-Technische Bundesanstalt (PTB), Technical University Braunschweig, Belgian Nuclear Research Centre (SCK CEN), University of New Mexico, Leibniz University Hannover</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17654" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17654</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper uses neural networks, including convolutional and fully connected architectures, to estimate spatial radiation fields for medical dosimetry, trained on synthetic datasets generated via Monte-Carlo simulations with Geant4. It evaluates design decisions for reconstructing fluence and spectra distributions, concluding with open-source release of datasets and training pipelines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Polyharmonic Cascade</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [deep learning architecture], [polyharmonic cascade, polyharmonic splines, random functions, principles of indifference, global linear system, GPU matrix operations]</li>
<li class=""><strong>authors:</strong> Yuriy N. Bakhvalov</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17671" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17671</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the &quot;polyharmonic cascade,&quot; a deep learning architecture built from packages of polyharmonic splines, derived from random function theory and principles of indifference. It proposes a training method that solves a global linear system per batch instead of using gradient descent, enabling synchronized layer updates and efficient GPU computation. The method demonstrates fast learning without overfitting on the MNIST dataset.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Vidarc: Embodied Video Diffusion Model for Closed-loop Control</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [video diffusion, autoregressive generation, masked inverse dynamics model, closed-loop control, cross-embodiment pre-training, KV cache]</li>
<li class=""><strong>authors:</strong> Yao Feng, Chendong Xiang, Xinyi Mao, Hengkai Tan, Zuyue Zhang, Shuhe Huang, Kaiwen Zheng, Haitian Liu, Hang Su, Jun Zhu</li>
<li class=""><strong>institution:</strong> Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17661" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17661</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7977f39cb797f71021c4776c090587d8f5e8e7c33c06e677b445877e8ad4c5d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7977f39cb797f71021c4776c090587d8f5e8e7c33c06e677b445877e8ad4c5d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Vidarc, a method for robotic control that combines an autoregressive video diffusion model with a masked inverse dynamics model to enable fast, closed-loop operation. It is pre-trained on a large dataset of diverse robotic episodes and achieves state-of-the-art performance, including higher success rates and significantly lower latency compared to baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] You Only Train Once: Differentiable Subset Selection for Omics Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [bioinformatics], [differentiable subset selection, multi-task learning, end-to-end training, sparsity, single-cell RNA-seq]</li>
<li class=""><strong>authors:</strong> Daphné Chopard, Jorge da Silva Gonçalves, Irene Cannistraci, Thomas M. Sutter, Julia E. Vogt</li>
<li class=""><strong>institution:</strong> ETH Zurich, University Children’s Hospital Zurich</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17678" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17678</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces YOTO, an end-to-end framework that jointly selects discrete gene subsets and performs prediction in a single differentiable model, using sparsity and multi-task learning. It demonstrates improved predictive performance and yields compact, meaningful gene subsets on single-cell RNA-seq datasets, advancing biomarker discovery.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [spatio-temporal forecasting], [transformer, self-attention, geostatistical covariance, Gaussian processes, spatial decay parameters]</li>
<li class=""><strong>authors:</strong> Yuri Calleo</li>
<li class=""><strong>institution:</strong> Universitas Mercatorum</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17696" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17696</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a spatially-informed transformer that injects a learnable geostatistical covariance kernel into the self-attention mechanism to incorporate spatial distance information for spatio-temporal forecasting. The method decomposes attention into a stationary physical prior and a data-driven residual, allowing the network to recover spatial decay parameters. Experiments show it outperforms graph neural networks and provides well-calibrated probabilistic forecasts.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Mitigating Forgetting in Low Rank Adaptation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [LoRA, Laplace approximation, catastrophic forgetting, regularization, parameter-efficient fine-tuning]</li>
<li class=""><strong>authors:</strong> Joanna Sliwa, Frank Schneider, Philipp Hennig, Jose Miguel Hernandez-Lobato</li>
<li class=""><strong>institution:</strong> University of Tübingen, University of Cambridge</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17720" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17720</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces LaLoRA, a method that applies a Laplace approximation to LoRA weights to estimate parameter confidence and regularize updates, mitigating catastrophic forgetting during fine-tuning. The approach improves the learning-forgetting trade-off for large language models, as demonstrated by fine-tuning a Llama model for mathematical reasoning, while remaining computationally lightweight.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Convergence Guarantees for Federated SARSA with Local Training and Heterogeneous Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [federated learning, SARSA, linear function approximation, convergence analysis, heterogeneous agents, local training]</li>
<li class=""><strong>authors:</strong> Paul Mangold, Eloïse Berthier, Eric Moulines</li>
<li class=""><strong>institution:</strong> CNRS, École polytechnique, Institut Polytechnique de Paris, ENSTA, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17688" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17688</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a theoretical analysis of Federated SARSA (FedSARSA), an on-policy reinforcement learning algorithm with linear function approximation and local training across heterogeneous agents. It establishes the first convergence guarantees and complexity bounds for this setting, showing that FedSARSA achieves linear speed-up with the number of agents despite heterogeneity in transitions and rewards.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Can You Hear Me Now? A Benchmark for Long-Range Graph Propagation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [graph neural networks], [ECHO benchmark, long-range propagation, message-passing, over-smoothing, over-squashing, synthetic graph tasks, molecular property prediction]</li>
<li class=""><strong>authors:</strong> Luca Miglior, Matteo Tolloso, Alessio Gravina, Davide Bacciu</li>
<li class=""><strong>institution:</strong> University of Pisa</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17762" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17762</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces ECHO, a benchmark designed to evaluate the ability of Graph Neural Networks (GNNs) to handle long-range information propagation. It includes synthetic tasks and real-world molecular datasets to test GNNs on challenging, long-distance dependencies. The benchmarking reveals significant performance gaps in existing GNNs, highlighting the difficulty of long-range propagation and the need for improved architectural designs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [Easy Adaptation, Parameter-Efficient Fine-Tuning, LoRA, Specific Small Models, task adaptation, resource-constrained]</li>
<li class=""><strong>authors:</strong> Dong Chen, Zhengqing Hu, Shixing Zhao, Yibo Guo</li>
<li class=""><strong>institution:</strong> Not explicitly provided in the given text.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17771" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17771</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Easy Adaptation (EA), a method that uses Specific Small Models (SSMs) to complement the data distribution for Large Models, enabling task adaptation without accessing the LM&#x27;s internal parameters. This approach matches the performance of Parameter-Efficient Fine-Tuning (PEFT) like LoRA on diverse tasks while requiring only minimal computational resources, making it suitable for resource-constrained environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [sequential recommendation], [ensembling, ID embeddings, text embeddings, modality features]</li>
<li class=""><strong>authors:</strong> Liam Collins, Bhuvesh Kumar, Clark Mingxuan Ju, Tong Zhao, Donald Loveland, Leonardo Neves, Neil Shah</li>
<li class=""><strong>institution:</strong> Snap Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17820" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17820</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a sequential recommendation method that independently trains ID-based and text-based models and then combines them via a simple ensembling strategy. It demonstrates that ID and text features learn complementary signals. The main conclusion is that both feature types are necessary for state-of-the-art performance, but complex fusion architectures are not required.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [weakly supervised learning], [multi-instance partial-label learning, calibratable disambiguation loss, model calibration]</li>
<li class=""><strong>authors:</strong> Wei Tang, Yin-Fang Yang, Weijia Zhang, Min-Ling Zhang</li>
<li class=""><strong>institution:</strong> Southeast University, The University of Newcastle</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17788" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17788</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec09016ca3c7c620b69a1f11fd5eac67d6cf0e41b74b5709b9e32e919c36ac8f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec09016ca3c7c620b69a1f11fd5eac67d6cf0e41b74b5709b9e32e919c36ac8f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a plug-and-play Calibratable Disambiguation Loss (CDL) to improve classification accuracy and model calibration in Multi-Instance Partial-Label Learning. The loss has two instantiations that calibrate predictions using candidate label probabilities or both candidate and non-candidate sets. Experimental results show that CDL significantly enhances both classification and calibration performance over conventional methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [imitation learning, distributionally robust control, layered control architecture, Taylor Series Imitation Learning (TaSIL), L1-Distributionally Robust Adaptive Control (L1-DRAC), certifiable autonomy]</li>
<li class=""><strong>authors:</strong> Aditya Gahlawat, Ahmed Aboudonia, Sandeep Banik, Naira Hovakimyan, Nikolai Matni, Aaron D. Ames, Gioele Zardini, Alberto Speranzon</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign, University of Pennsylvania, California Institute of Technology, Massachusetts Institute of Technology, Lockheed Martin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17899" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17899</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Distributionally Robust Imitation Policy (DRIP) architecture, a layered control framework that integrates Taylor Series Imitation Learning (TaSIL) and L1-Distributionally Robust Adaptive Control (L1-DRAC) to address different sources of distribution shift. The main conclusion is that this integration enables the design of certifiable autonomy pipelines by guaranteeing performance certificates for the entire control system, combining learning-based components with model-based decision-making.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [generative modeling], [Wasserstein-Fisher-Rao gradient flow, weighted stochastic differential equations, Feynman-Kac representation, score-based diffusion models, Langevin dynamics]</li>
<li class=""><strong>authors:</strong> Herlock Rahimi</li>
<li class=""><strong>institution:</strong> Yale University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17878" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17878</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a new sampling method for generative modeling by implementing Wasserstein-Fisher-Rao gradient flow via weighted stochastic differential equations, using the Feynman-Kac representation. This approach aims to overcome the slow mixing rates of traditional diffusion models in non-log-concave, multimodal target distributions by incorporating controlled mass reweighting. The study provides a rigorous geometric and operator-theoretic foundation for future developments in this area.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Regularized Random Fourier Features and Finite Element Reconstruction for Operator Learning in Sobolev Space</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [operator learning], [random Fourier features, Tikhonov regularization, finite element reconstruction, Student&#x27;s t distribution]</li>
<li class=""><strong>authors:</strong> Xinyue Yu, Hayden Schaeffer</li>
<li class=""><strong>institution:</strong> University of California, Los Angeles</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17884" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17884</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a regularized random Fourier feature method combined with finite element reconstruction (RRFF-FEM) for learning operators from noisy data, using Student&#x27;s t-distributed random features and frequency-weighted regularization. It proves theoretical guarantees on conditioning and generalization when features scale appropriately. Numerical experiments on PDE problems show the method is robust to noise, faster to train, and maintains competitive accuracy compared to kernel and neural operator baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Visually Prompted Benchmarks Are Surprisingly Fragile</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [visual prompting, benchmark evaluation, vision-language models, visual marker design, JPEG compression, dataset size, VPBench]</li>
<li class=""><strong>authors:</strong> Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang, Renhao Wang, Trevor Darrell, Alane Suhr, Angjoo Kanazawa</li>
<li class=""><strong>institution:</strong> UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17875" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17875</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1578e85cb159e2bf39916b0de61ec0b774772fc5c07fb3edc1f869eea3ea32e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1578e85cb159e2bf39916b0de61ec0b774772fc5c07fb3edc1f869eea3ea32e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper evaluates vision-language models (VLMs) on visually prompted benchmarks, where questions refer to coordinates marked directly on images. It finds that model performance and leaderboard rankings are surprisingly fragile to minor changes in visual marker design (e.g., color, size) and low-level inference settings like JPEG compression. To address this instability, the authors introduce VPBench, a larger benchmark with multiple visual marker variants.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] RadarGen: Automotive Radar Point Cloud Generation from Cameras</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [diffusion model, bird&#x27;s-eye-view, radar cross section, Doppler, point cloud generation, foundation models]</li>
<li class=""><strong>authors:</strong> Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany</li>
<li class=""><strong>institution:</strong> Technion, MIT, NVIDIA, University of Toronto, Vector Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17897" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17897</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RadarGen is a diffusion model that generates realistic automotive radar point clouds from multi-view camera images by representing radar data in bird&#x27;s-eye-view and conditioning on visual cues. It uses a lightweight recovery step to reconstruct point clouds from the generated maps. Evaluations show it captures real radar statistics and reduces the performance gap for perception models trained on synthetic data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Disentangled representations via score-based variational autoencoders</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [representation learning], [variational autoencoder, diffusion models, score-based guidance, disentanglement, evidence lower bound]</li>
<li class=""><strong>authors:</strong> Benjamin S. H. Lyo, Eero P. Simoncelli, Cristina Savin</li>
<li class=""><strong>institution:</strong> New York University, Flatiron Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17127" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17127</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SAMI, a method that combines diffusion models and variational autoencoders into a unified framework for unsupervised representation learning. It uses score-based guidance to learn disentangled, semantically meaningful latent representations from data. The results show that this approach can make implicit structural information in diffusion models explicit and interpretable.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Colormap-Enhanced Vision Transformers for MRI-Based Multiclass (4-Class) Alzheimer&#x27;s Disease Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Vision Transformers, Pseudo-Color Enhancement, MRI, Multi-Class Classification]</li>
<li class=""><strong>authors:</strong> Faisal Ahmed</li>
<li class=""><strong>institution:</strong> Embry-Riddle Aeronautical University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16964" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16964</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes PseudoColorViT-Alz, a method that enhances MRI scans with pseudo-color transformations and processes them using a Vision Transformer for Alzheimer&#x27;s disease classification. The model achieves state-of-the-art accuracy of 99.79% on the OASIS-1 dataset, demonstrating that combining color augmentation with Vision Transformers significantly improves classification performance over previous methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [test-time refinement, self-supervised learning, shape from shading, score distillation sampling, monocular depth estimation, diffusion models]</li>
<li class=""><strong>authors:</strong> Ananta R. Bhattarai, Helge Rhodin</li>
<li class=""><strong>institution:</strong> Bielefeld University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17908" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17908</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb5eab47c4353057728b3e9889e13b412f6c11ae6703f713d247c4724fbb909_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb5eab47c4353057728b3e9889e13b412f6c11ae6703f713d247c4724fbb909_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Re-Depth Anything, a test-time self-supervision framework that refines monocular depth predictions by re-lighting the geometry and using a 2D diffusion model&#x27;s priors via Score Distillation Sampling. It updates only specific model embeddings and the decoder to prevent collapse, rather than fine-tuning the entire network. The method shows substantial improvements in depth accuracy and realism over the baseline Depth Anything V2 model.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Graph Attention Networks for Detecting Epilepsy from EEG Signals Using Accessible Hardware in Low-Resource Settings</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [graph attention networks, electroencephalography, spatio-temporal graphs, edge analysis, low-cost hardware, RaspberryPi]</li>
<li class=""><strong>authors:</strong> Szymon Mazurek, Stephen Moore, Alessandro Crimi</li>
<li class=""><strong>institution:</strong> AGH University of Krakow, University of Cape Coast</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2507.15118" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2507.15118</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d57b4723f1c065c80f840b86af58e96a683cea596741b961e8c90f8c5680da8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d57b4723f1c065c80f840b86af58e96a683cea596741b961e8c90f8c5680da8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a graph attention network (GAT) framework that models EEG signals as spatio-temporal graphs to detect epilepsy, with a focus on low-cost hardware for deployment in low-resource settings. The method adapts GATs to analyze edge connectivity for biomarker identification and is designed for lightweight training and deployment. The results demonstrate promising classification performance and highlight the potential for scalable, accessible diagnostic support in underserved regions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Application of machine learning to predict food processing level using Open Food Facts</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [food science and nutrition informatics], [LightGBM, Random Forest, CatBoost, NOVA classification, nutrient concentration data]</li>
<li class=""><strong>authors:</strong> Nalin Arora, Aviral Chauhan, Siddhant Rana, Mahansh Aditya, Sumit Bhagat, Aditya Kumar, Akash Kumar, Akanksh Semar, Ayush Vikram Singh, Ganesh Bagler</li>
<li class=""><strong>institution:</strong> Indraprastha Institute of Information Technology Delhi (IIIT-Delhi), Infosys Center for Artificial Intelligence, Center of Excellence in Healthcare, Foodoscope Technologies Pvt Ltd</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17169" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17169</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This study applies machine learning models, including LightGBM, Random Forest, and CatBoost, to classify food processing levels (NOVA) using nutrient data from the Open Food Facts dataset. The best-performing model, LightGBM, achieved 80-85% accuracy and effectively distinguished minimally from ultra-processed foods. The research concludes that higher processing levels are strongly associated with poorer nutritional quality, greater environmental impact, and common allergens like gluten and milk.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Systemic Risk Radar: A Multi-Layer Graph Framework for Early Market Crash Warning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-layer graph, GNN, temporal GNN, logistic regression, Random Forest, correlation-based, systemic risk]</li>
<li class=""><strong>authors:</strong> Sandeep Neela</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17185" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17185</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the Systemic Risk Radar (SRR), a framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility. It demonstrates that graph-derived features from this model provide useful early-warning signals for market crashes, outperforming standard feature-based models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [fair machine learning], [penalized regression, cost-sensitive classification, true positive rate disparity penalties]</li>
<li class=""><strong>authors:</strong> Carter H. Nakamoto, Lucia Lushi Chen, Agata Foryciarz, Sherri Rose</li>
<li class=""><strong>institution:</strong> Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17340" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17340</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a penalized fair regression framework using unfairness penalties for multiple groups, implemented via reduction to cost-sensitive classification. The method is applied to predict end-stage renal disease in a chronic kidney disease study, showing substantial fairness improvements for multiple race and ethnicity groups without appreciable loss in overall model fit.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Alternating Direction Method of Multipliers for Nonlinear Matrix Decompositions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [optimization algorithms], [alternating direction method of multipliers, nonlinear matrix decomposition, rectified linear unit, component-wise square, MinMax transform]</li>
<li class=""><strong>authors:</strong> Atharva Awari, Nicolas Gillis, Arnaud Vandaele</li>
<li class=""><strong>institution:</strong> University of Mons</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17473" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17473</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an ADMM-based algorithm for solving nonlinear matrix decompositions, where a matrix is approximated via a nonlinear function applied to a low-rank product. It demonstrates flexibility across various nonlinear models and loss functions, showing applicability to real-world datasets in areas like sparse data approximation and recommender systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Resource-efficient medical image classification for edge devices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [quantization, quantization-aware training, post-training quantization, convolutional neural networks]</li>
<li class=""><strong>authors:</strong> Mahsa Lavaei, Zahra Abadi, Salar Beigzad, Alireza Maleki</li>
<li class=""><strong>institution:</strong> University of Tehran, Tehran University, University of St. Thomas, Minnesota</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17515" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17515</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates a resource-efficient approach for medical image classification on edge devices using model quantization techniques, specifically quantization-aware training (QAT) and post-training quantization (PTQ). The study demonstrates that quantized models achieve significant reductions in model size and inference latency while maintaining clinically acceptable diagnostic accuracy, enabling real-time processing in resource-limited settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Machine Learning Assisted Parameter Tuning on Wavelet Transform Amorphous Radial Distribution Function</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [wavelet-transform radial distribution function (WT-RDF), parameter optimization, machine learning, RBF, LSTM]</li>
<li class=""><strong>authors:</strong> Deriyan Senjaya, Stephen Ekaputra Limantoro</li>
<li class=""><strong>institution:</strong> National Tsing Hua University, National Yang Ming Chiao Tung University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17245" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17245</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0fcb546d51f53548e0556b6cedd431012b18af3f725ab1ca0c73a63485285ea_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0fcb546d51f53548e0556b6cedd431012b18af3f725ab1ca0c73a63485285ea_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper enhances the wavelet-transform radial distribution function (WT-RDF) for analyzing amorphous materials by using machine learning to optimize its parameters, creating the WT-RDF+ framework. The improved model provides more accurate peak predictions and outperforms benchmark ML models like RBF and LSTM, demonstrating its robustness for characterizing Ge-Se amorphous systems and aiding in the design of phase-change thin films.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Perfect reconstruction of sparse signals using nonconvexity control and one-step RSB message passing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [compressed sensing, sparse signal reconstruction], [SCAD penalty, approximate message passing, replica symmetry breaking, state evolution, nonconvexity control]</li>
<li class=""><strong>authors:</strong> Xiaosi Gu, Ayaka Sakata, Tomoyuki Obuchi</li>
<li class=""><strong>institution:</strong> Kyoto University, RIKEN center for AIP, Ochanomizu University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17426" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17426</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops a one-step replica-symmetry-breaking extension of approximate message passing (1RSB-AMP) for sparse signal reconstruction using the SCAD penalty. The authors propose a new criterion for selecting the Parisi parameter and combine it with nonconvexity control, which improves the algorithmic limit for perfect reconstruction compared to the replica-symmetric AMP, though the gain is modest and remains below the Bayes-optimal threshold.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Sharp Structure-Agnostic Lower Bounds for General Functional Estimation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [causal inference], [structure-agnostic estimation, double machine learning, debiased learning, doubly robust learning, nuisance functions, functional estimation]</li>
<li class=""><strong>authors:</strong> Jikai Jin, Vasilis Syrgkanis</li>
<li class=""><strong>institution:</strong> Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17341" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17341</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper establishes sharp lower bounds for structure-agnostic functional estimation, analyzing the optimal error rates achievable without imposing structural priors. It shows that doubly robust learning and double machine learning (DML) are optimal for a general class of functionals, including the average treatment effect (ATE), across different regimes of double robustness. The results provide theoretical validation for first-order debiasing methods and guidance for practitioners in the absence of strong structural assumptions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] HydroGym: A Reinforcement Learning Platform for Fluid Dynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [reinforcement learning, flow control, differentiable solvers, transfer learning, benchmark platform]</li>
<li class=""><strong>authors:</strong> Christian Lagemann, Sajeda Mokbel, Miro Gondrum, Mario Rüttgers, Jared Callaham, Ludger Paehler, Samuel Ahnert, Nicholas Zolman, Kai Lagemann, Nikolaus Adams, Matthias Meinke, Wolfgang Schröder, Jean-Christophe Loiseau, Esther Lagemann, Steven L. Brunton</li>
<li class=""><strong>institution:</strong> University of Washington, RWTH Aachen University, Inha University, Technical University of Munich, German Center for Neurodegenerative Diseases, Arts et Métiers Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17534" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17534</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11a2356fa2a2cafe6887d85b978c67e18494f41d547e787460e381132d0f9508_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11a2356fa2a2cafe6887d85b978c67e18494f41d547e787460e381132d0f9508_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces HydroGym, a reinforcement learning platform designed for fluid dynamics control, featuring both non-differentiable and differentiable solvers to improve sample efficiency. The platform includes 42 validated environments and demonstrates that RL agents can discover robust control principles, achieving significant drag reduction and efficient adaptation to new conditions via transfer learning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [molecular design], [Bayesian optimization, generative models, multi-objective optimization, acquisition function, qPMHI, Monte Carlo sampling]</li>
<li class=""><strong>authors:</strong> Madhav R. Muthyala, Farshud Sorourifar, Tianhong Tan, You Peng, Joel A. Paulson</li>
<li class=""><strong>institution:</strong> University of Wisconsin–Madison, The Ohio State University, The Dow Chemical Company</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17659" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17659</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a &quot;generate-then-optimize&quot; framework for de novo molecular design, which uses a generative model to create candidate molecules and a novel acquisition function called qPMHI to efficiently select batches for evaluation. The method demonstrates significant improvements over existing approaches in sample efficiency and performance, as shown in benchmarks and a case study on designing organic cathode materials for batteries.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Imputation Uncertainty in Interpretable Machine Learning Methods</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [interpretable machine learning], [permutation feature importance, partial dependence plots, Shapley values, single imputation, multiple imputation, imputation uncertainty, confidence intervals]</li>
<li class=""><strong>authors:</strong> Pegah Golchian, Marvin N. Wright</li>
<li class=""><strong>institution:</strong> Leibniz Institute for Prevention Research &amp; Epidemiology – BIPS, University of Bremen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17689" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17689</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates the impact of missing data imputation methods on the uncertainty quantification of interpretable machine learning (IML) methods. It compares single and multiple imputation, showing that single imputation underestimates variance in IML explanations, while multiple imputation provides confidence interval coverage closer to the nominal level.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [StyleGAN2-ADA, Denoising Diffusion Probabilistic Models (DDPMs), FID, KID, Inception Score, ViT-B/16, synthetic data augmentation]</li>
<li class=""><strong>authors:</strong> N. A. Adarsh Pritam, Jeba Shiney O, Sanyam Jain</li>
<li class=""><strong>institution:</strong> Alliance University, Østfold University College</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17585" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17585</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/497a14da92ec4e58a3716d9bb6044a8b43d59d0f8ab0aff8c6e70b0d8e5325c9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/497a14da92ec4e58a3716d9bb6044a8b43d59d0f8ab0aff8c6e70b0d8e5325c9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SkinGenBench, a benchmark that evaluates the effects of generative models (StyleGAN2-ADA and DDPMs) and preprocessing pipelines on synthetic dermoscopic image generation for melanoma diagnosis. The main conclusion is that the choice of generative architecture has a stronger impact on image quality and diagnostic utility than preprocessing complexity, with StyleGAN2-ADA outperforming diffusion models in fidelity, and synthetic augmentation significantly boosting downstream classifier performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Revisiting the Broken Symmetry Phase of Solid Hydrogen: A Neural Network Variational Monte Carlo Study</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational physics], [neural network variational Monte Carlo, quantum Monte Carlo, deep neural network wave function, constant pressure ensemble, density functional theory, group-theoretical analysis]</li>
<li class=""><strong>authors:</strong> Shengdu Chai, Chen Lin, Xinyang Dong, Yuqiang Li, Wanli Ouyang, Lei Wang, X.C. Xie</li>
<li class=""><strong>institution:</strong> Fudan University, Shanghai Artificial Intelligence Laboratory, University of Oxford, Chinese Academy of Sciences, Peking University, Hefei National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17703" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17703</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper develops a neural network variational Monte Carlo method to treat electrons and nuclei quantum mechanically, revealing a new Cmcm crystal structure candidate for high-pressure solid hydrogen. This structure matches experimental data but is unstable in static DFT calculations, highlighting the need for full quantum many-body treatment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Domain-Aware Quantum Circuit for QML</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [quantum machine learning], [parameterized quantum circuits, domain-aware encoding, locality-preserving entanglement, barren plateau mitigation, DCT-style zigzag windows, encode-entangle-train cycles]</li>
<li class=""><strong>authors:</strong> Gurinder Singh, Thaddeus Pellegrini, Kenneth M. Merz Jr</li>
<li class=""><strong>institution:</strong> Cleveland Clinic, IBM Quantum</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17800" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17800</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a Domain-Aware Quantum Circuit (DAQC) for quantum machine learning, which uses image priors to guide locality-preserving encoding and entanglement via non-overlapping zigzag windows. The method mitigates barren plateaus and hardware noise, achieving performance on real quantum hardware competitive with strong classical neural networks for image classification tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical imaging], [image registration, radiomics, deep learning, logistic regression, feature selection]</li>
<li class=""><strong>authors:</strong> Rahul Ravi, Ruizhe Li, Tarek Abdelfatah, Stephen Chan, Xin Chen</li>
<li class=""><strong>institution:</strong> University of Nottingham, Nottingham City Hospital</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17759" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17759</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe1fee674daeeb2ff62e605cac35448ddb7b933f1a6948f0aa461b318710117_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe1fee674daeeb2ff62e605cac35448ddb7b933f1a6948f0aa461b318710117_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a framework using aligned longitudinal MRI and clinical data to predict breast cancer treatment response. The method involves tumor segmentation, image registration, and feature extraction, comparing radiomics and deep learning models. The main conclusion is that image registration significantly improves prediction, with radiomics features outperforming deep learning features in predicting pathologic complete response and relapse-free survival.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Fraud detection in credit card transactions using Quantum-Assisted Restricted Boltzmann Machines</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [quantum annealing, restricted boltzmann machine, qubo, quantum-assisted machine learning, fraud detection]</li>
<li class=""><strong>authors:</strong> João Marcos Cavalcanti de Albuquerque Neto, Gustavo Castro do Amaral, Guilherme Penello Temporão</li>
<li class=""><strong>institution:</strong> Pontifícia Universidade Católica do Rio de Janeiro, The Netherlands Organization for Applied Scientific Research (TNO)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17660" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17660</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43a1f86f2b2d05435580200ee936d437f042be18dc15953d8144f949af07568c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43a1f86f2b2d05435580200ee936d437f042be18dc15953d8144f949af07568c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates the use of Quantum-Assisted Restricted Boltzmann Machines (RBMs) for credit card fraud detection, applying quantum annealing to solve the QUBO problem derived from the RBM&#x27;s energy function. The method was tested on a real-world dataset of 145 million transactions. The results indicate that the quantum-assisted approach achieves superior performance compared to classical methods, even on current noisy quantum hardware.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning vertical coordinates via automatic differentiation of a dynamical core</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [automatic differentiation, neural network, terrain-following coordinates, differentiable dynamical core, NEUVE, Arakawa C-grid, non-hydrostatic Euler equations]</li>
<li class=""><strong>authors:</strong> Tim Whittaker, Seth Taylor, Elsa Cardoso-Bihlo, Alejandro Di Luca, Alex Bihlo</li>
<li class=""><strong>institution:</strong> Université du Québec à Montréal, University of Saskatchewan, Memorial University of Newfoundland</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17877" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17877</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a framework to learn vertical coordinates by integrating a parametric, neural network-based coordinate system (NEUVE) into a differentiable dynamical core for atmospheric modeling. Using automatic differentiation to compute exact geometric terms, the method optimizes the grid structure for physics and numerics. The learned coordinates reduce errors in benchmarks and eliminate spurious velocity patterns over steep topography.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [3D ConvNeXt, Global Response Normalization, depth scaling, width scaling, context scaling, supervised pretraining, volumetric segmentation]</li>
<li class=""><strong>authors:</strong> Saikat Roy, Yannick Kirchhoff, Constantin Ulrich, Maximillian Rokuss, Tassilo Wald, Fabian Isensee, Klaus Maier-Hein</li>
<li class=""><strong>institution:</strong> German Cancer Research Center (DKFZ), Heidelberg University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17774" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17774</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MedNeXt-v2, a compound-scaled 3D ConvNeXt architecture enhanced with a Global Response Normalization module for large-scale supervised representation learning in medical image segmentation. The authors demonstrate that scaling the backbone network&#x27;s architecture is crucial for effective pretraining, and MedNeXt-v2 achieves state-of-the-art performance when fine-tuned on diverse CT and MR benchmarks. The work establishes that a stronger backbone design yields better downstream segmentation results than simply increasing dataset size.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-23">2025-12-23<a href="#2025-12-23" class="hash-link" aria-label="Direct link to 2025-12-23" title="Direct link to 2025-12-23" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251223] Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [spiking neural networks, Hopfield networks, Hierarchical Gated Recurrent Networks, supervised contrastive learning, cross-modal ablation study, engram analysis]</li>
<li class=""><strong>authors:</strong> Effiong Blessing, Chiung-Yi Tseng, Somshubhra Roy, Junaid Rehman, Isaac Nkrumah</li>
<li class=""><strong>institution:</strong> Saint Louis University, Luxmuse AI, North Carolina State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18575" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18575</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts a cross-modal ablation study of memory mechanisms in spiking neural networks (SNNs), evaluating Hopfield networks, HGRNs, and supervised contrastive learning on visual and auditory neuromorphic datasets. It finds that memory mechanisms exhibit strong modality-specific specialization rather than universal applicability, with Hopfield networks excelling on visual tasks but performing poorly on auditory ones. The work provides empirical evidence for modality-specific memory optimization in neuromorphic systems, achieving significant energy efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [cross-modal gated attention, positive-negative awareness attention, pseudo-matched pairs mitigation]</li>
<li class=""><strong>authors:</strong> Pengxiang Ouyang, Qing Ma, Zheng Wang, Cong Bai</li>
<li class=""><strong>institution:</strong> Zhejiang University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18660" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18660</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/55a03600bbbf89f98190599bd29b1a2f12ec3a4ac217e9251256ba5b1f3c6a42_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/55a03600bbbf89f98190599bd29b1a2f12ec3a4ac217e9251256ba5b1f3c6a42_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes PMPGuard, a retrieval framework that uses Cross-Modal Gated Attention and Positive-Negative Awareness Attention to handle noisy image-text pairs in remote sensing datasets. It dynamically regulates cross-modal information flow and distinguishes informative from misleading cues during alignment learning. Experiments on RSICD, RSITMD, and RS5M datasets show state-of-the-art performance, demonstrating robustness against pseudo-matched pairs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Qwen2.5-Omni, auxiliary modality-specific perception tasks, emotion keyword extraction, facial expression analysis, prosody analysis, multimodal adaptation framework]</li>
<li class=""><strong>authors:</strong> Xueming Yan, Boyan Xu, Yaochu Jin, Lixian Xiao, Wenlong Ye, Runyang Cai, Zeqi Zheng, Jingfa Liu, Aimin Yang</li>
<li class=""><strong>institution:</strong> Not explicitly provided in the given text.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19379" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19379</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes OmniMER, a multimodal adaptation framework built on Qwen2.5-Omni that enhances emotion recognition by using auxiliary modality-specific perception tasks (text, video, audio) to identify emotion cues before fusion. It introduces the IndoMER benchmark for Indonesian and shows that OmniMER significantly outperforms the base model on this dataset, with cross-lingual evaluation on CH-SIMS demonstrating its generalizability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [neural operator, adaptive numerical correction, exponential moving average, PDE residual, hybrid inference, error control, autoregressive framework]</li>
<li class=""><strong>authors:</strong> Rajyasri Roy, Dibyajyoti Nayak, Somdatta Goswami</li>
<li class=""><strong>institution:</strong> Johns Hopkins University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19643" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19643</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2db71241f8b136e671bfbd60878ebfc82de055944b09b4fd5d0a608d13528f8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2db71241f8b136e671bfbd60878ebfc82de055944b09b4fd5d0a608d13528f8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ANCHOR, a hybrid framework that combines a pretrained neural operator with a classical numerical solver, using an adaptive, residual-based error estimator to monitor and correct errors during long-horizon PDE inference. This method stabilizes predictions and bounds error growth, offering a reliable and efficient alternative to standalone neural operators or expensive high-fidelity solvers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] GLUE: Generative Latent Unification of Expertise-Informed Engineering Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [generative design, latent space unification, data-free training, differentiable geometry, multidisciplinary design optimization, UAV design]</li>
<li class=""><strong>authors:</strong> Tim Aebersold, Soheyl Massoudi, Mark D. Fuge</li>
<li class=""><strong>institution:</strong> ETH Zurich</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19469" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19469</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfb699958bc64d8b342419ffb48e5f2645d78f06ee90425f67231f7ebceffcfc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfb699958bc64d8b342419ffb48e5f2645d78f06ee90425f67231f7ebceffcfc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces GLUE, a method that coordinates frozen, pre-trained generative models for engineering subsystems to produce feasible, diverse, and high-performing full-system designs. It proposes both data-driven and data-free training approaches, with the data-free method being particularly efficient, training a full generative model in about 10 minutes while requiring far fewer computational resources than data-driven methods. The work demonstrates a path to scaling generative design for complex engineering systems by integrating domain-specific submodels.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-23T10:56:01.000Z" itemprop="dateModified">Dec 23, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/cs_LG/20251215-20251221"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251215-20251221 (cs.LG)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/category/cslo"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">cs.LO</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-22" class="table-of-contents__link toc-highlight">2025-12-22</a></li><li><a href="#2025-12-23" class="table-of-contents__link toc-highlight">2025-12-23</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>