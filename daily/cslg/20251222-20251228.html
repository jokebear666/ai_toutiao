<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_LG/20251222-20251228" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251222-20251228 (cs.LG) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/cslg/20251222-20251228"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251222-20251228 (cs.LG) | AI头条"><meta data-rh="true" name="description" content="2025-12-22"><meta data-rh="true" property="og:description" content="2025-12-22"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/cslg/20251222-20251228"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cslg/20251222-20251228" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cslg/20251222-20251228" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.LG","item":"https://jokebear666.github.io/ai_toutiao/daily/cslg"},{"@type":"ListItem","position":3,"name":"20251222-20251228 (cs.LG)","item":"https://jokebear666.github.io/ai_toutiao/daily/cslg/20251222-20251228"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.38d649e8.css">
<script src="/ai_toutiao/assets/js/runtime~main.fd2ba062.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.919db729.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/daily">Daily</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/paper">Paper</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Collapse sidebar category &#x27;cs.LG&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cs_LG/20251215-20251221"><span title="20251215-20251221 (cs.LG)" class="linkLabel_WmDU">20251215-20251221 (cs.LG)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/cslg/20251222-20251228"><span title="20251222-20251228 (cs.LG)" class="linkLabel_WmDU">20251222-20251228 (cs.LG)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/daily/cslg"><span>cs.LG</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251222-20251228 (cs.LG)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251222-20251228 (cs.LG)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-22">2025-12-22<a href="#2025-12-22" class="hash-link" aria-label="Direct link to 2025-12-22" title="Direct link to 2025-12-22" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251222] Dion2: A Simple Method to Shrink Matrix in Muon</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [Muon optimizer, orthonormalization, matrix shrinking, sampling, Newton-Schulz iterations, FSDP2]</li>
<li class=""><strong>authors:</strong> Kwangjun Ahn, Noah Amsel, John Langford</li>
<li class=""><strong>institution:</strong> Microsoft Research, AI Frontiers, NYU</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16928" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16928</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Dion2, a simple method to reduce the computational cost of the Muon optimizer by sampling a fraction of rows or columns for orthonormalization at each iteration. This sparsifies the update, lowering computation and communication overhead. The method maintains update quality close to full Muon while improving scalability, as shown in training benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] BIONIX: A Wireless, Low-Cost Prosthetic Arm with Dual-Signal EEG and EMG Control</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [EEG, EMG, sliding window classification, threshold-based detection, ESP32 microcontroller, NeuroSky MindWave Mobile 2, MyoWare 2.0]</li>
<li class=""><strong>authors:</strong> Pranesh Sathish Kumar</li>
<li class=""><strong>institution:</strong> Alliance Academy for Innovation, Georgia Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16929" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16929</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a low-cost prosthetic arm controlled by a dual-signal system using EEG (for hand open/close via blink detection) and EMG (for elbow movement via threshold-based detection). The prototype, built with ESP32 microcontrollers and commercial sensors, demonstrates a feasible, intuitive control method for upper-limb prostheses in resource-limited settings. The main conclusion is that this integrated neuro-muscular approach offers a viable pathway to affordable and biologically intuitive prosthetic control.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [dense embedding retrieval, graph-based exploration, BM25, LLM-based reasoning, code localization]</li>
<li class=""><strong>authors:</strong> Shravan Chaudhari, Rahul Thomas Jacob, Mononito Goswami, Jiajun Cao, Shihab Rashid, Christian Bock</li>
<li class=""><strong>institution:</strong> Johns Hopkins University, AWS AI Labs</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16956" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16956</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c743ebe3d40c5416b7ff367b0e7e93ca8ff7bf1bd771b2359d8a7333521abcbc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c743ebe3d40c5416b7ff367b0e7e93ca8ff7bf1bd771b2359d8a7333521abcbc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SpIDER, a method that enhances dense retrieval for code localization by using graph-based exploration of a codebase to gather auxiliary context, which is then reasoned over by an LLM. This approach addresses the limitations of standard embedding methods that underutilize code structure. Empirical results show that SpIDER consistently improves retrieval performance across multiple programming languages.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Optimizing Text Search: A Novel Pattern Matching Algorithm Based on Ukkonen&#x27;s Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [pattern matching algorithms], [Ukkonen&#x27;s Algorithm, Suffix Trees, pattern recognition, text-search algorithms]</li>
<li class=""><strong>authors:</strong> Xinyu Guan, Shaohua Zhang</li>
<li class=""><strong>institution:</strong> Not specified</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16927" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16927</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b065cfa04bf6f3a4b29a1299ffe0b7dd4f84fbabb6368c76abaa339e1a0a77c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b065cfa04bf6f3a4b29a1299ffe0b7dd4f84fbabb6368c76abaa339e1a0a77c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a novel pattern matching algorithm that combines Ukkonen&#x27;s Algorithm for constructing Suffix Trees with a new search technique using Python&#x27;s dynamic link attributes. The optimized algorithm demonstrates linear time and space efficiency, outperforming traditional methods like Naive Search, KMP, and Boyer-Moore, and achieves 100% accuracy in tasks such as genomic sequence pattern recognition.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Compression is Routing: Reconstruction Error as an Intrinsic Signal for Modular Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [transformer autoencoder, mixture-of-experts (MoE), reconstruction error, latent vectors, sequence compression, routing mechanism]</li>
<li class=""><strong>authors:</strong> Zhongpan Tang</li>
<li class=""><strong>institution:</strong> Independent Researcher (based on gmail address)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16963" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16963</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a novel &quot;Compression is Routing&quot; architecture using a Transformer Autoencoder to compress long sequences into latent vectors. It demonstrates that reconstruction error serves as an intrinsic domain fingerprint, enabling expert module scheduling without explicit gating networks. The method offers a scalable approach for handling long contexts and modular language model design.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] PAACE: A Plan-Aware Automated Agent Context Engineering Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [context engineering, plan-aware compression, next-k-task relevance, instruction co-refinement, function-preserving compression, synthetic data generation, knowledge distillation]</li>
<li class=""><strong>authors:</strong> Kamer Ali Yuksel</li>
<li class=""><strong>institution:</strong> aiXplain Inc</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16970" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16970</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces PAACE, a framework for compressing the expanding context of LLM agents in multi-step workflows. It uses plan-aware techniques like next-k-task relevance modeling and function-preserving compression, trained on synthetic data and distilled into efficient models. The method improves agent accuracy while significantly reducing context load and inference costs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [retrieval-augmented generation, long-term memory, semantic imitation, indirect injection attack, memory poisoning, MetaGPT, DataInterpreter]</li>
<li class=""><strong>authors:</strong> Saksham Sahai Srivastava, Haoyu He</li>
<li class=""><strong>institution:</strong> University of Georgia</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16962" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16962</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MemoryGraft, a novel attack that poisons an LLM agent&#x27;s long-term memory by implanting malicious successful experiences, which are then retrieved and imitated during future tasks. The method exploits the agent&#x27;s semantic imitation heuristic through a poisoned RAG store, leading to persistent behavioral compromise. The authors demonstrate that this attack can cause significant and stealthy behavioral drift in agents like MetaGPT&#x27;s DataInterpreter.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Physics-Informed Lightweight Machine Learning for Aviation Visibility Nowcasting Across Multiple Climatic Regimes</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [aviation meteorology], [XGBoost, physics-guided feature engineering, SHAP analysis, METAR data, gradient boosting]</li>
<li class=""><strong>authors:</strong> Marcelo Cerda Castillo</li>
<li class=""><strong>institution:</strong> Pulsetech.cl</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16967" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16967</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a lightweight XGBoost model for aviation visibility nowcasting, trained on surface observation data (METAR) and enhanced with physics-informed feature engineering. The model outperforms operational TAF forecasts at 3-hour horizons, achieving 2.5x to 4x higher recall with fewer false alarms across multiple climates. The SHAP analysis shows the model implicitly reconstructs local physical drivers like advection and radiation, providing explainable predictions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] QSMOTE-PGM/kPGM: QSMOTE Based PGM and kPGM for Imbalanced Dataset Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [quantum-inspired machine learning], [Pretty Good Measurement (PGM), kernelized PGM (KPGM), Quantum SMOTE (QSMOTE), quantum state discrimination, density matrix-based learning]</li>
<li class=""><strong>authors:</strong> Bikash K. Behera, Giuseppe Sergioli, Robert Giuntini</li>
<li class=""><strong>institution:</strong> Università degli Studi di Cagliari, Technische Universität München</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16960" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16960</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces and compares quantum-inspired classifiers, specifically Pretty Good Measurement (PGM) and kernelized PGM (kPGM), for imbalanced datasets, using Quantum SMOTE (QSMOTE) for synthetic oversampling. The results show that both PGM and kPGM outperform a classical random forest baseline, with PGM achieving the highest accuracy and kPGM demonstrating greater robustness across different data sampling strategies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [Proximal Policy Optimization (PPO), Group Relative Policy Optimization (GRPO), turn-level MDP, advantage estimation, multi-turn RL]</li>
<li class=""><strong>authors:</strong> Junbo Li, Peng Zhou, Rui Meng, Meet P. Vadera, Lihong Li, Yang Li</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin, Amazon</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17008" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17008</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Turn-PPO, a reinforcement learning method that applies Proximal Policy Optimization at the turn level instead of the token level for training LLM agents in multi-turn tasks. It demonstrates that this approach is more robust and effective than the commonly used GRPO method, particularly for long-horizon reasoning scenarios, as validated on the WebShop and Sokoban datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [scientific ai evaluation], [Practical Inquiry Model (PIM), SGI-Bench, Test-Time Reinforcement Learning (TTRL), retrieval-augmented novelty, agent-based evaluation]</li>
<li class=""><strong>authors:</strong> Wanghan Xu, Yuhao Zhou, Yifan Zhou, Qinglong Cao, Shuo Li, Jia Bu, Bo Liu, Yixin Chen, Xuming He, Xiangyu Zhao, Xiang Zhuang, Fengxiang Wang, Zhiwang Zhou, Qiantai Feng, Wenxuan Huang, Jiaqi Wei, Hao Wu, Yuejin Yang, Guangshuai Wang, Sheng Xu, Ziyan Huang, Xinyao Liu, Jiyao Liu, Cheng Tang, Wei Li, Ying Chen, Junzhi Ning, Pengfei Jiang, Chenglong Ma, Ye Du, Changkai Ji, Huihui Xu, Ming Hu, Jiangbin Zheng, Xin Chen, Yucheng Wu, Feifei Jiang, Xi Chen, Xiangru Tang, Yuchen Fu, Yingzhou Lu, Yuanyuan Zhang, Lihao Sun, Chengbo Li, Jinzhe Ma, Wanhao Liu, Yating Liu, Kuo-Cheng Wu, Shengdu Chai, Yizhou Wang, Ouwen Zhangjin, Chen Tang, Shufei Zhang, Wenbo Cao, Junjie Ren, Taoyong Cui, Zhouheng Yao, Juntao Deng, Yijie Sun, Feng Liu, Wangxu Wei, Jingyi Xu, Zhangrui Li, Junchao Gong, Zijie Guo, Zhiyu Yao, Zaoyu Chen, Tianhao Peng, Fangchen Yu, Bo Zhang, Dongzhan Zhou, Shixiang Tang, Jiaheng Liu, Fenghua Ling, Yan Lu, Yuchen Ren, Ben Fei, Zhen Zhao, Xinyu Gu, Rui Su, Xiao-Ming Wu, Weikang Si, Yang Liu, Hao Chen, Xiangchao Yan, Xue Yang, Junchi Yan, Jiamin Wu, Qihao Zheng, Chenhui Li, Zhiqiang Gao, Hao Kong, Junjun He, Mao Su, Tianfan Fu, Peng Ye, Chunfeng Song, Nanqing Dong, Yuqiang Li, Huazhu Fu</li>
<li class=""><strong>institution:</strong> Shanghai Artificial Intelligence Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16969" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16969</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40fba3081819027f6af6208a55e87bd4bfc888d4ba6ce07d9baa5f158fbe6fa2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40fba3081819027f6af6208a55e87bd4bfc888d4ba6ce07d9baa5f158fbe6fa2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a framework for evaluating Scientific General Intelligence (SGI) in LLMs, grounded in the Practical Inquiry Model and operationalized through the SGI-Bench benchmark. The results reveal significant performance gaps across tasks like deep research and experimental reasoning. The authors also introduce Test-Time Reinforcement Learning (TTRL) to enhance hypothesis novelty without requiring reference answers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A Women&#x27;s Health Benchmark for Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [healthcare AI evaluation], [women&#x27;s health benchmark, large language models, error types, model stumps, query types]</li>
<li class=""><strong>authors:</strong> Victoria-Elisabeth Gruber, Razvan Marinescu, Diego Fajardo, Amin H. Nassar, Christopher Arkfeld, Alexandria Ludlow, Shama Patel, Mehrnoosh Samaei, Valerie Klug, Anna Huber, Marcel Gühner, Albert Botta i Orfila, Irene Lagoja, Kimya Tarr, Haleigh Larson, Mary Beth Howard</li>
<li class=""><strong>institution:</strong> Lumos AI, Yale Cancer Center, Harvard Medical School, UCSF, Brown University, Emory University, Clinic Ottakring, NHS, Yale School of Medicine, Johns Hopkins University School of Medicine</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17028" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17028</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Women&#x27;s Health Benchmark (WHB), a novel evaluation framework comprising 96 validated model stumps across five medical specialties, three query types, and eight error types to assess LLM performance in women&#x27;s health. It finds that current LLMs have approximately 60% failure rates, with significant weaknesses in detecting urgency, indicating they are not yet reliable for providing women&#x27;s health advice.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] GB-DQN: Gradient Boosted DQN Models for Non-stationary Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [gradient boosting, DQN, ensemble learning, Bellman residual, non-stationary environments]</li>
<li class=""><strong>authors:</strong> Chang-Hwan Lee, Chanseung Lee</li>
<li class=""><strong>institution:</strong> Florida Atlantic University, Morrow Company</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17034" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17034</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes GB-DQN, a method that uses gradient boosting to create an ensemble of Q-networks, where each new network learns the residual error of the current ensemble to adapt to non-stationary environments. Experiments show that GB-DQN achieves faster recovery and greater robustness compared to standard DQN and other baselines in tasks with changing dynamics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SFBD-OMNI: Bridge models for lossy measurement restoration with limited clean samples</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [generative models], [diffusion models, bridge models, entropic optimal transport, distribution restoration, SFBD-OMNI]</li>
<li class=""><strong>authors:</strong> Haoye Lu, Yaoliang Yu, Darren Ho</li>
<li class=""><strong>institution:</strong> University of Waterloo, Vector Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17051" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17051</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SFBD-OMNI, a bridge model framework that generalizes Stochastic Forward-Backward Deconvolution to restore a clean data distribution from abundant noisy samples and a black-box corruption process, framed as a one-sided entropic optimal transport problem. It shows that with a small number of clean samples, the underlying distribution becomes largely recoverable even in cases of per-sample information loss. Experiments demonstrate significant improvements across diverse measurement settings beyond Gaussian corruption.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Dynamic Tool Dependency Retrieval for Efficient Function Calling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [dynamic tool dependency retrieval, function calling, tool retrieval, on-device agents, tool-augmented llms]</li>
<li class=""><strong>authors:</strong> Bhrij Patel, Davide Belli, Amir Jalalirad, Maximilian Arnold, Aleksandr Ermovol, Bence Major</li>
<li class=""><strong>institution:</strong> Qualcomm Research, University of Maryland, College Park</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17052" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17052</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1d880d31eb189fff50465aa10379459bec7bfbbf666206f8ad6ea98793a534a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1d880d31eb189fff50465aa10379459bec7bfbbf666206f8ad6ea98793a534a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Dynamic Tool Dependency Retrieval (DTDR), a lightweight method that retrieves relevant tools for LLM function calling by conditioning on both the initial user query and the evolving execution context. It models tool dependencies from demonstrations to adaptively retrieve tools as a plan unfolds. The results show that this dynamic retrieval improves function calling success rates by 23% to 104% compared to static retrieval methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Universal consistency of the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span>-NN rule in metric spaces and Nagata dimension. III</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [statistical learning theory], [k-nearest neighbor classifier, universal consistency, Nagata dimension, Lebesgue-Besicovitch differentiation property, metric spaces]</li>
<li class=""><strong>authors:</strong> Vladimir G. Pestov</li>
<li class=""><strong>institution:</strong> Universidade Federal de Santa Catarina, University of Ottawa</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17058" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17058</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proves the final implication needed to establish the equivalence between the universal consistency of the k-nearest neighbor classifier, the strong Lebesgue-Besicovitch differentiation property, and a metric space being sigma-finite dimensional in the sense of Nagata. The core method extends a measure construction from Hilbert spaces to any complete separable metric space lacking the Nagata property. The main conclusion is that the k-NN classifier is universally consistent in a metric space if and only if the space is sigma-finite dimensional.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [mathematical reasoning], [chain-of-thought prompting, reinforcement learning, GRPO, fine-tuning, error recovery]</li>
<li class=""><strong>authors:</strong> Saraswathy Amjith, Mihika Dusad, Neha Muramalla, Shweta Shah</li>
<li class=""><strong>institution:</strong> MIT</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17079" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17079</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9501255b38adfbd4ed3cb05e9a136df6cf358b6420281716744f14c17554a871_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9501255b38adfbd4ed3cb05e9a136df6cf358b6420281716744f14c17554a871_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper fine-tunes the Qwen3-4B model using GRPO reinforcement learning on intentionally flawed chain-of-thought reasoning traces to improve error detection and recovery. It finds that this mixed training on both calculation and reasoning errors improves robustness to misleading prefills without sacrificing accuracy on clean problems, unlike standard fine-tuning which degrades robustness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [mixture-of-experts, offloading, dynamic quantization, low-rank compensation, router-guided precision restoration]</li>
<li class=""><strong>authors:</strong> Zhenyu Liu, Yunzhen Liu, Zehao Fan, Garrett Gagnon, Yayue Hou, Nan Wu, Yangwook Kang, Liu Liu</li>
<li class=""><strong>institution:</strong> Rensselaer Polytechnic Institute, University of Massachusetts Amherst, George Washington University, Samsung Semiconductor</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17073" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17073</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c6e9afd2a3ff75dfe81968ba3c73da71d577f341a443ab96f8bb14a681ed3f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c6e9afd2a3ff75dfe81968ba3c73da71d577f341a443ab96f8bb14a681ed3f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a bandwidth-efficient method for Mixture-of-Experts (MoE) inference that uses router-guided, low-rank compensation to dynamically restore precision for the most important experts while keeping others in low-bit form. This approach reduces I/O traffic during offloading without significantly harming model accuracy. The method demonstrates a superior bandwidth-accuracy trade-off and improved throughput on GPU and GPU-NDP systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Perturb Your Data: Paraphrase-Guided Training Data Watermarking</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [SPECTRA, watermarking, training data detection, membership inference attack, paraphrase generation, scoring model, token probability comparison]</li>
<li class=""><strong>authors:</strong> Pranav Shetty, Mirazul Haque, Petr Babkin, Zhiqiang Ma, Xiaomo Liu, Manuela Veloso</li>
<li class=""><strong>institution:</strong> JPMorgan AI Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17075" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17075</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b47f34679362a94a5413b86758bfca6d1690e7158a9b8bc7a21706264c5e833c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b47f34679362a94a5413b86758bfca6d1690e7158a9b8bc7a21706264c5e833c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SPECTRA, a watermarking method that subtly paraphrases text using an LLM to embed a detectable signature into training data without altering its statistical distribution. It verifies unauthorized use by comparing token probabilities between a suspect model and a scoring model. The approach reliably detects watermarked data even when it constitutes a minuscule fraction of the training corpus, providing a scalable pre-release watermark for data owners.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] How to Square Tensor Networks and Circuits Without Squaring Them</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [probabilistic modeling], [tensor networks, squared circuits, probabilistic circuits, marginalization, canonical forms, unitary matrices, distribution estimation]</li>
<li class=""><strong>authors:</strong> Lorenzo Loconte, Adrián Javaloy, Antonio Vergari</li>
<li class=""><strong>institution:</strong> University of Edinburgh</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17090" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17090</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method to parameterize squared circuits (a generalization of squared tensor networks) using conditions inspired by orthogonality and determinism, enabling efficient marginalization without squaring. This approach overcomes computational overhead while maintaining expressiveness for distribution estimation. Experiments confirm the method allows more efficient learning without loss of expressiveness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [reinforcement learning, model predictive control, MPPI, hierarchical planning, adaptive sampling]</li>
<li class=""><strong>authors:</strong> Toshiaki Hori, Jonathan DeCastro, Deepak Gopinath, Avinash Balachandran, Guy Rosman</li>
<li class=""><strong>institution:</strong> Toyota Research Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17091" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17091</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method that fuses reinforcement learning and model-predictive control (MPC) into an adaptive hierarchical framework. It uses RL actions to guide the MPPI sampler and adaptively aggregates MPPI samples to improve value estimation, leading to more robust and sample-efficient policies. The approach demonstrates improved data efficiency, performance, and convergence speed in domains like race driving and Lunar Lander.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [interpretability], [counterfactual explanations, model-agnostic, time series, ECG, LIME, SHAP]</li>
<li class=""><strong>authors:</strong> Justin Li, Efe Sencan, Jasper Zheng Duan, Vitus J. Leung, Stephan Tsaur, Ayse K. Coskun</li>
<li class=""><strong>institution:</strong> Boston University, Sandia National Laboratories, Boston Medical Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17100" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17100</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces UniCoMTE, a universal, model-agnostic framework for generating counterfactual explanations for time series classifiers by modifying input samples to identify influential temporal features. It is evaluated on an ECG classifier and shown to produce more concise, stable, and human-aligned explanations than established methods like LIME and SHAP, thereby improving model interpretability for real-world applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [modular reuse, on-device execution, model decomposition, parallel execution, quantization]</li>
<li class=""><strong>authors:</strong> Kunjal Panchal, Saayan Mitra, Somdeb Sarkhel, Haoliang Wang, Ishita Dasgupta, Gang Wu, Hui Guan</li>
<li class=""><strong>institution:</strong> University of Massachusetts Amherst, Adobe Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17108" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17108</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ae01de3bb190d7134b2995b14582f1e46ed434d4588a9e6017b7c9282b01074_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ae01de3bb190d7134b2995b14582f1e46ed434d4588a9e6017b7c9282b01074_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Atom, an on-device system that decomposes large video-language models into reusable modules (e.g., visual encoder, language decoder) to eliminate redundant model loading and enable parallel execution across subtasks like captioning and retrieval. This modular reuse approach reduces end-to-end latency by 27–33% on commodity smartphones with only marginal performance drops, making it a practical solution for efficient video-language pipelines on edge devices.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Fault Diagnosis and Quantification for Photovoltaic Arrays based on Differentiable Physical Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [differentiable physical model, gradient-based fault parameters identification, Adahessian optimizer, I-V curve reconstruction]</li>
<li class=""><strong>authors:</strong> Zenan Yang, Yuanliang Li, Jingwei Zhang, Yongjie Liu, Kun Ding</li>
<li class=""><strong>institution:</strong> Hohai University, Concordia University, Aalborg University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17107" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17107</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5899b49765c1a033a8227b9785fb68b2ee7e7efe7e5a6fd98b6b7f264267e08c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5899b49765c1a033a8227b9785fb68b2ee7e7efe7e5a6fd98b6b7f264267e08c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a differentiable fast fault simulation model (DFFSM) and a gradient-based fault parameters identification (GFPI) method using the Adahessian optimizer to quantify faults in photovoltaic arrays. The method accurately models I-V characteristics under multiple faults and uses analytical gradients for efficient parameter identification. Experimental results show high quantification accuracy with I-V reconstruction errors below 3%, demonstrating the effectiveness of differentiable physical simulators for PV fault diagnosis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Bridging Training and Merging Through Momentum-Aware Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [low-rank factorization, momentum-aware optimization, curvature-aware merging, model composition, memory-efficient training]</li>
<li class=""><strong>authors:</strong> Alireza Moayedikia, Alicia Troncoso</li>
<li class=""><strong>institution:</strong> Swinburne University of Technology, Universidad Pablo de Olavide</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17109" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17109</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a unified framework that maintains factorized momentum and curvature statistics during model training, then reuses this information for geometry-aware model merging. This approach eliminates the need to recompute curvature data, saving computation and enabling more principled model composition. The method demonstrates improved performance on language understanding benchmarks and offers better hyperparameter robustness compared to existing low-rank optimizers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [contrastive fine-tuning, token attribution, t-SNE projection, attention-head ablation, image retrieval, negation handling]</li>
<li class=""><strong>authors:</strong> Jasmine Vu, Shivanand Sheshappanavar</li>
<li class=""><strong>institution:</strong> Santa Clara University, University of Wyoming</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17121" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17121</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcadf51218711774df214ce0c80d2175e2b72d14b2fab69a5a27aebd9bc17bd2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcadf51218711774df214ce0c80d2175e2b72d14b2fab69a5a27aebd9bc17bd2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper fine-tunes the CLIP-based CheXagent model to improve its ability to understand negated phrases in medical image retrieval tasks. The method uses contrastive fine-tuning and analyzes internal model behavior through techniques like token attribution. The results show improved negation handling with a slight trade-off in accuracy for positive prompts.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Digitizing Nepal&#x27;s Written Heritage: A Comprehensive HTR Pipeline for Old Nepali Manuscripts</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [handwritten text recognition], [encoder-decoder architectures, data-centric techniques, line-level transcription, transfer learning, data augmentation, character error rate]</li>
<li class=""><strong>authors:</strong> Anjali Sarawgi, Esteban Garces Arias, Christof Zotter</li>
<li class=""><strong>institution:</strong> LMU Munich, Heidelberg Academy of Sciences and Humanities, Munich Center for Machine Learning (MCML)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17111" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17111</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bae7b70bc5a974847e0434967f87fb01f452c81f46d95e3cea515746090bd7f5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bae7b70bc5a974847e0434967f87fb01f452c81f46d95e3cea515746090bd7f5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an end-to-end Handwritten Text Recognition (HTR) pipeline for Old Nepali manuscripts, employing line-level transcription and exploring encoder-decoder architectures with data-centric methods. The best model achieves a low Character Error Rate (CER) of 4.9%, demonstrating effective digitization for this low-resource historical language. The authors release their training code and evaluation scripts to support further research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] DiffeoMorph: Learning to Morph 3D Shapes Using Differentiable Agent-Based Simulations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [differentiable simulation, graph neural network, SE(3)-equivariance, attention mechanism, 3D Zernike polynomials, shape-matching loss, implicit differentiation, bilevel optimization]</li>
<li class=""><strong>authors:</strong> Seong Ho Pahng, Guoye Guan, Benjamin Fefferman, Sahand Hormoz</li>
<li class=""><strong>institution:</strong> Harvard University, Harvard Medical School, Dana-Farber Cancer Institute, Broad Institute of MIT and Harvard</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17129" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17129</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces DiffeoMorph, a differentiable framework that uses an attention-based SE(3)-equivariant graph neural network to train agents to collectively morph into target 3D shapes. It employs a novel shape-matching loss based on 3D Zernike polynomials and uses implicit differentiation to handle a bilevel optimization problem for rotation alignment. The method successfully generates complex shapes from simple ellipsoids using minimal spatial cues.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [Generalized Primal Averaging (GPA), DiLoCo, Schedule-Free, AdamW, Nesterov&#x27;s method, primal averaging, optimizer, iterate averaging]</li>
<li class=""><strong>authors:</strong> Aaron Defazio, Konstantin Mishchenko, Parameswaran Raman, Hao-Jun Michael Shi, Lin Xiao</li>
<li class=""><strong>institution:</strong> Meta Superintelligence Labs</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17131" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17131</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Generalized Primal Averaging (GPA), a new optimizer that extends Nesterov&#x27;s method to perform smooth, per-step averaging of model iterates, addressing limitations of periodic averaging methods like single-worker DiLoCo. It demonstrates that GPA outperforms single-worker DiLoCo, simplifies hyperparameter tuning, reduces memory overhead, and achieves significant speedups in training LLMs and vision models compared to AdamW.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Biosecurity-Aware AI: Agentic Risk Auditing of Soft Prompt Attacks on ESM-Based Variant Predictors</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [soft prompt attacks, adversarial auditing, agentic framework, risk metrics, embedding-space robustness]</li>
<li class=""><strong>authors:</strong> Huixin Zhan</li>
<li class=""><strong>institution:</strong> New Mexico Institute of Mining and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17146" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17146</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edf3a867526cb70d49c0816641a925b129fe30dc5f7871777c74f463824654df_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edf3a867526cb70d49c0816641a925b129fe30dc5f7871777c74f463824654df_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SAGE, an agentic framework that audits genomic foundation models by injecting soft prompt perturbations and evaluating performance degradation. It finds that models like ESM2 are vulnerable to such attacks, revealing hidden security risks in biomedical applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Distributed Learning in Markovian Restless Bandits over Interference Graphs for Stable Spectrum Sharing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [wireless communication, distributed learning], [restless multi-armed bandit (RMAB), Gale-Shapley matching, interference graph, SMILE algorithm, distributed optimization]</li>
<li class=""><strong>authors:</strong> Liad Lea Didi, Kobi Cohen</li>
<li class=""><strong>institution:</strong> Ben-Gurion University of the Negev</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17161" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17161</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SMILE, a distributed learning algorithm that integrates restless bandit learning with graph-constrained coordination for stable spectrum sharing in interference graphs. It proves that SMILE converges to the optimal stable allocation and achieves logarithmic regret. Simulations validate the algorithm&#x27;s robustness, scalability, and efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] BumpNet: A Sparse Neural Network Framework for Learning PDE Solutions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [pde solving], [BumpNet, sparse neural network, basis function expansion, sigmoid activation, physics-informed neural networks, PINNs, DeepONet, EDNN, meshless method, h-adaptivity]</li>
<li class=""><strong>authors:</strong> Shao-Ting Chiu, Ioannis G. Kevrekidis, Ulisses Braga-Neto</li>
<li class=""><strong>institution:</strong> Texas A&amp;M University, The Johns Hopkins University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17198" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17198</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces BumpNet, a sparse neural network framework that constructs adaptive, trainable basis functions from sigmoid activations for solving PDEs. It combines BumpNet with existing architectures like PINNs, DeepONet, and EDNN to create efficient, interpretable models for PDE solution and operator learning. The proposed methods demonstrate improved accuracy and reduced computational cost compared to standard approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning solution operator of dynamical systems with diffusion maps kernel ridge regression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [dynamical systems modeling], [kernel ridge regression, diffusion maps, operator learning, geometry-aware learning, data-driven kernels]</li>
<li class=""><strong>authors:</strong> Jiwoo Song, Daning Huang, John Harlim</li>
<li class=""><strong>institution:</strong> The Pennsylvania State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17203" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17203</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Diffusion Maps Kernel Ridge Regression (DM-KRR) method for learning solution operators of complex dynamical systems. It combines a simple kernel ridge regression framework with a data-driven kernel from diffusion maps to adapt to the system&#x27;s intrinsic geometry without explicit manifold modeling. The method is shown to outperform state-of-the-art approaches in accuracy and data efficiency across various systems, highlighting the importance of geometric constraints for long-term prediction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Do Foundational Audio Encoders Understand Music Structure?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [music information retrieval], [music structure analysis, foundational audio encoders, self-supervised learning, masked language modeling, boundary detection, function prediction]</li>
<li class=""><strong>authors:</strong> Keisuke Toyama, Zhi Zhong, Akira Takahashi, Shusuke Takahashi, Yuki Mitsufuji</li>
<li class=""><strong>institution:</strong> Sony Group Corporation, Sony AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17209" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17209</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates the use of pretrained foundational audio encoders (FAEs) for music structure analysis (MSA). Through comprehensive experiments on 11 FAE types, it finds that models using self-supervised learning with masked language modeling on music data are particularly effective for MSA tasks like boundary detection and function prediction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [reinforcement learning, group relative policy optimization, knowledge graph consistency, entity-relation matching, process supervision, hard-example mining]</li>
<li class=""><strong>authors:</strong> Xiao Liang, Yuxuan An, Di Wang, Jiawei Hu, Zhicheng Jiao, Bin Jing, Quan Wang</li>
<li class=""><strong>institution:</strong> Xidian University, Brown University, Capital Medical University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17213" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17213</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c86c0e9aa8fe8870d86c5f63baf1ccd89856e7434ece25e03ba384660733fc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c86c0e9aa8fe8870d86c5f63baf1ccd89856e7434ece25e03ba384660733fc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes CheXPO-v2, an alignment framework that uses a Knowledge Graph Consistency Reward for process supervision to reduce hallucinations in medical VLMs. It parses reasoning into structured triplets to penalize incoherent logic, outperforming prior methods on benchmarks with high data efficiency. The approach achieves state-of-the-art accuracy on MIMIC-CXR-VQA using only 5k samples.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [differential privacy, local differential privacy, RAPPOR, PAC indistinguishability, hybrid privacy, rarity-aware protection]</li>
<li class=""><strong>authors:</strong> Madhava Gaikwad</li>
<li class=""><strong>institution:</strong> Microsoft</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17251" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17251</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes AlignDP, a hybrid privacy mechanism that protects large language models by separating data into rare and non-rare fields. Rare fields are shielded with PAC indistinguishability for strong privacy, while non-rare fields are privatized using RAPPOR to allow useful frequency estimation. This approach aims to prevent knowledge extraction and unauthorized fine-tuning by design, making models more secure against distillation and editing attacks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [federated learning, byzantine-robust aggregation, privacy-preserving, dimensionality reduction, secure multi-party computation, adaptive tuning]</li>
<li class=""><strong>authors:</strong> Baolei Zhang, Minghong Fang, Zhuqing Liu, Biao Yi, Peizhao Zhou, Yuan Wang, Tong Li, Zheli Liu</li>
<li class=""><strong>institution:</strong> Nankai University, University of Louisville, University of North Texas</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17254" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17254</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ABBR, a practical framework for federated learning that combines Byzantine-robust aggregation with privacy-preserving techniques. Its core method uses dimensionality reduction to speed up private computations and an adaptive tuning strategy to minimize the impact of malicious models. The framework is shown to run significantly faster with minimal overhead while maintaining strong Byzantine resilience.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Electric Vehicle Charging Load Forecasting: An Experimental Comparison of Machine Learning Methods</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [time series forecasting, LSTM, GRU, Transformer, ARIMA, XGBoost]</li>
<li class=""><strong>authors:</strong> Iason Kyriakopoulos, Yannis Theodoridis</li>
<li class=""><strong>institution:</strong> University of Piraeus</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17257" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17257</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper experimentally compares five time series forecasting models, including traditional statistical methods, machine learning, and deep learning (e.g., ARIMA, XGBoost, LSTM, GRU, Transformer), for predicting electric vehicle charging load. The evaluation across multiple real-world datasets, temporal horizons, and spatial aggregation levels shows that recurrent neural networks (GRU, LSTM) generally perform best for mid- and long-term forecasting, while Transformers excel in short-term forecasting at higher aggregation levels.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SHARP-QoS: Sparsely-gated Hierarchical Adaptive Routing for joint Prediction of QoS</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hyperbolic convolution, Poincaré ball, adaptive feature-sharing, gated feature fusion, EMA-based loss balancing, multi-task learning, graph convolution networks]</li>
<li class=""><strong>authors:</strong> Suraj Kumar, Arvind Kumar, Soumi Chattopadhyay</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology Indore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17262" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17262</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SHARP-QoS, a unified model for joint QoS prediction that uses hyperbolic convolution to extract hierarchical features, an adaptive feature-sharing mechanism with gated fusion, and an EMA-based loss balancing strategy. It demonstrates superior performance over single- and multi-task baselines across multiple datasets, effectively handling sparsity, outliers, and cold-start scenarios with moderate computational cost.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A Theoretical Analysis of State Similarity Between Markov Decision Processes</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [bisimulation metric, generalized bisimulation metric, Markov decision process, policy transfer, state aggregation, sampling-based estimation]</li>
<li class=""><strong>authors:</strong> Zhenyu Tao, Wei Xu, Xiaohu You</li>
<li class=""><strong>institution:</strong> Southeast University, Purple Mountain Laboratories</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17265" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17265</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a Generalized Bisimulation Metric (GBSM) to measure state similarity between different Markov Decision Processes, establishing its fundamental mathematical properties. The authors leverage GBSM to theoretically analyze tasks like policy transfer and state aggregation, obtaining tighter performance bounds than previous methods. Numerical results validate the effectiveness of GBSM for multi-MDP scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [audit agents, attestation protocols, constrained reasoning, cryptographic attestation, symbolic methods, benchmark suite, verifiability]</li>
<li class=""><strong>authors:</strong> Abhivansh Gupta</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology, Roorkee</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17259" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17259</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a60caf6cec7c2ab0bdf36d4e5ba8513e86e73ba9dc3d215615ad6190a8df9091_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a60caf6cec7c2ab0bdf36d4e5ba8513e86e73ba9dc3d215615ad6190a8df9091_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Verifiability-First architecture for LLM-based agents, integrating runtime attestations, lightweight audit agents for continuous verification, and challenge-response protocols for high-risk operations. It introduces the OPERA benchmark to evaluate the detectability and speed of remediation for misaligned behavior, shifting the focus from measuring the propensity for misalignment to ensuring reliable detection and control.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Alzheimer&#x27;s Disease Brain Network Mining</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical AI], [semi-supervised learning, optimal transport, label propagation, graph neural networks, Wasserstein distance]</li>
<li class=""><strong>authors:</strong> Alireza Moayedikia, Sara Fin</li>
<li class=""><strong>institution:</strong> Swinburne University of Technology, Monash University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17276" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17276</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MATCH-AD, a semi-supervised framework that combines deep representation learning, graph-based label propagation, and optimal transport theory to diagnose Alzheimer&#x27;s disease from neuroimaging data with limited labeled samples. The method achieves near-perfect diagnostic accuracy on a large dataset, demonstrating that semi-supervised learning can effectively leverage partially annotated data for clinical deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Understanding Generalization in Role-Playing Models via Information Theory</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [natural language processing], [information theory, mutual information, reinforcement learning, distribution shift, role-playing models]</li>
<li class=""><strong>authors:</strong> Yongqi Li, Hao Lang, Fei Huang, Tieyun Qian, Yongbin Li</li>
<li class=""><strong>institution:</strong> Wuhan University, Tongyi Lab, Zhongguancun Academy</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17270" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17270</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85ede876c98e7e8d1d360bf9eb2cb767cc2570e218ebc72489f68b5cec65ce55_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85ede876c98e7e8d1d360bf9eb2cb767cc2570e218ebc72489f68b5cec65ce55_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an information-theoretic metric called reasoning-based effective mutual information difference (R-EMID) to measure and analyze the generalization degradation of role-playing models under distribution shifts. It also proposes a co-evolving reinforcement learning framework to improve response probability estimation for calculating R-EMID. The main conclusion is that user shift poses the highest risk to model performance and reinforcement learning is the most effective approach for enhancing generalization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Kolmogorov-Arnold Networks (KANs), multilayer perceptron networks (MLPs), Physics-informed Neural Networks, nonlocal consistency loss, integro-differential equations (IDEs), fractional PDEs]</li>
<li class=""><strong>authors:</strong> Farinaz Mostajeran, Aruzhan Tleubek, Salah A Faroughi</li>
<li class=""><strong>institution:</strong> University of Utah</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17273" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17273</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MINPO, a unified neural framework that learns nonlocal operators and their inverses using KANs or MLPs to solve integro-differential equations. It enforces coherence between the learned operator and solution via a nonlocal consistency loss. The method is shown to be accurate and robust across diverse kernel types and dimensionalities, generalizing beyond problem-specific formulations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Warmer for Less: A Cost-Efficient Strategy for Cold-Start Recommendations at Pinterest</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [residual connection, score regularization, manifold mixup]</li>
<li class=""><strong>authors:</strong> Saeed Ebrahimi, Weijie Jiang, Jaewon Yang, Olafur Gudmundsson, Yucheng Tu, Huizhong Duan</li>
<li class=""><strong>institution:</strong> Pinterest</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17277" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17277</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f351bf91dbfa3f8f187d67a36cb3c88f1014690a13606c70e860bd376621c71f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f351bf91dbfa3f8f187d67a36cb3c88f1014690a13606c70e860bd376621c71f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a cost-efficient strategy to improve cold-start recommendations by introducing lightweight techniques: a residual connection for non-historical features, a score regularization term, and manifold mixup for data sparsity. These methods collectively increased fresh content engagement by 10% without harming overall engagement or cost, and have been deployed at Pinterest.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [speech processing], [voice activity detection, vision transformer, MFCC, Gammatone filter bank cepstral coefficients, dataset augmentation, out-of-distribution evaluation]</li>
<li class=""><strong>authors:</strong> Ioannis Stylianou, Achintya kr. Sarkar, Nauman Dawalatabad, James Glass, Zheng-Hua Tan</li>
<li class=""><strong>institution:</strong> Aalborg University, Pioneer Centre for AI, IIIT SriCity, Zoom Communications Inc., Massachusetts Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17281" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17281</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces LibriVAD, a scalable open dataset for voice activity detection (VAD) created by augmenting LibriSpeech with diverse noise sources. It benchmarks several feature-model combinations and proposes using a Vision Transformer (ViT) architecture for VAD. The main conclusion is that ViT with MFCC features outperforms established models across various conditions, and scaling the dataset size and balancing its silence-to-speech ratio consistently improves out-of-distribution generalization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [memristive architecture, minion recurrent unit, weighted-bit streaming, experience replay, mixed-signal accelerator, on-chip continual learning]</li>
<li class=""><strong>authors:</strong> Abdullah M. Zyarah, Dhireesha Kudithipudi</li>
<li class=""><strong>institution:</strong> University of Texas at San Antonio, University of Baghdad</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17299" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17299</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces M2RU, a mixed-signal hardware architecture that implements the Minion Recurrent Unit for efficient on-chip continual learning at the edge. It uses weighted-bit streaming and experience replay to enable energy-efficient temporal processing and stable adaptation. The results show significant energy efficiency improvements and a long operational lifetime, establishing M2RU as a scalable platform for edge-level temporal intelligence.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [explainable AI (XAI)], [graph theory, model decomposition, hypothesis-evidence structure, Cox proportional hazards model, PREDICT]</li>
<li class=""><strong>authors:</strong> Michael Merry, Pat Riddle, Jim Warren</li>
<li class=""><strong>institution:</strong> University of Auckland</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17316" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17316</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a formal, testable criterion for inherent explainability in AI, using graph theory to decompose models into verifiable structure-local explanations called annotations. The method is applied to demonstrate the inherent explainability of a clinical cardiovascular risk model (PREDICT). The work provides a rigorous foundation for regulators and formalizes the distinction between an explainable model and an explained one.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Task Schema and Binding: A Double Dissociation Study of In-Context Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [in-context learning], [activation patching, double dissociation, task schema, binding, transformer, mamba]</li>
<li class=""><strong>authors:</strong> Chaeha Kim</li>
<li class=""><strong>institution:</strong> Changwon National University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17325" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17325</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper uses activation patching experiments across multiple Transformer models and Mamba to causally dissect in-context learning. It concludes that ICL decomposes into two separable mechanisms: Task Schema (abstract task recognition) and Binding (specific input-output associations), with their reliance governed by a trade-off with the model&#x27;s prior knowledge.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [adaptive graph pruning, Spatio-Temporal Graph Neural Networks (ST-GNNs), Sudden Event Prediction Accuracy (SEPA), online semi-decentralized training, Federated Learning (FL), Gossip Learning]</li>
<li class=""><strong>authors:</strong> Ivan Kralj, Lodovico Giaretta, Gordan Ježić, Ivana Podnar Žarko, Šarūnas Girdzijauskas</li>
<li class=""><strong>institution:</strong> University of Zagreb, Faculty of Electrical Engineering and Computing; RISE Research Institutes of Sweden; KTH Royal Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17352" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17352</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a9feeec6bf4367fbf82a987484881d47da3c3be2e4b769373b648eb200942b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a9feeec6bf4367fbf82a987484881d47da3c3be2e4b769373b648eb200942b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an adaptive graph pruning algorithm for Spatio-Temporal Graph Neural Networks (ST-GNNs) to reduce communication overhead in online semi-decentralized traffic prediction systems. It also introduces a novel evaluation metric, SEPA, to measure responsiveness to sudden traffic events. The method maintains prediction accuracy while significantly lowering communication costs across different decentralized learning settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [ensemble learning, adversarial training, Bayesian inference, LLM-based sample generation, weight assignment]</li>
<li class=""><strong>authors:</strong> Yidong Chai, Yi Liu, Mohammadreza Ebrahimi, Weifeng Li, Balaji Padmanabhan</li>
<li class=""><strong>institution:</strong> Hefei University of Technology, University of South Florida, University of Georgia, University of Maryland</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17367" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17367</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a novel framework called LLM-SGA and instantiates a detector named ARHOCD, which uses an ensemble of base detectors, a dynamic Bayesian weight assignment method, and an iterative adversarial training strategy to improve robustness. The results show that ARHOCD achieves strong generalizability and improves detection accuracy for harmful online content under adversarial conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [adversarial control tokens, beam-search exploration, last-layer logit gap, LoRA-based adversarial training, reward hacking]</li>
<li class=""><strong>authors:</strong> Tung-Ling Li, Yuhao Wu, Hongliang Liu</li>
<li class=""><strong>institution:</strong> Palo Alto Networks</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17375" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17375</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces AdvJudge-Zero, a method that uses beam-search on a model&#x27;s next-token distribution to discover short, low-perplexity control token sequences that can flip the binary decisions of LLM-as-a-Judge systems from &quot;No&quot; to &quot;Yes&quot;. It concludes that these tokens represent a realistic reward-hacking vulnerability in post-training pipelines, and shows that adversarial training can mitigate the issue while preserving evaluation quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Timely Information Updating for Mobile Devices Without and With ML Advice</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [online scheduling], [competitive online algorithm, age of information, consistency-robustness trade-off, ML-augmented algorithm, adversarial environment]</li>
<li class=""><strong>authors:</strong> Yu-Pin Hsu, Yi-Hsuan Tseng</li>
<li class=""><strong>institution:</strong> National Taipei University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17381" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17381</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an online algorithm for a mobile device to decide when to send status updates to an access point, balancing information timeliness and update cost. The algorithm achieves an optimal competitive ratio against adversarial uncertainties and, when augmented with machine learning advice, attains an optimal consistency-robustness trade-off. The main conclusion is that an optimal competitive algorithm exhibits a threshold-like response to ML advice, either fully trusting or completely ignoring it.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] DeepShare: Sharing ReLU Across Channels and Layers for Efficient Private Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Private Inference, ReLU sharing, DReLU, cryptographic protocols, activation sharing]</li>
<li class=""><strong>authors:</strong> Yonathan Bornfeld, Shai Avidan</li>
<li class=""><strong>institution:</strong> Tel Aviv University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17398" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17398</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f38ab38719abeec4c59d997e57b2ecf1dd76acec3485b40aa5ccef81258f3179_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f38ab38719abeec4c59d997e57b2ecf1dd76acec3485b40aa5ccef81258f3179_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces DeepShare, a method for efficient Private Inference (PI) that reduces computational costs by sharing the DReLU (the non-linear step function of ReLU) across channels and layers within a neural network. It achieves state-of-the-art results by drastically decreasing the number of expensive DReLU operations while maintaining model performance on tasks like classification and segmentation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [stratified analysis, statistical methods, bias assessment, performance metrics, multiple comparisons correction, intersectional analysis]</li>
<li class=""><strong>authors:</strong> Dishantkumar Sutariya, Eike Petersen</li>
<li class=""><strong>institution:</strong> Fraunhofer Institute for Digital Medicine MEVIS</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17409" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17409</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a statistical toolbox called &#x27;meval&#x27; designed for rigorous, fine-grained analysis of machine learning model performance across different subgroups. It addresses challenges like metric selection, uncertainty estimation, and multiple comparisons to identify performance disparities. The main conclusion is that this toolbox enables practitioners to easily detect potential biases and failure modes, particularly in medical imaging applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [SWE-Bench++, automated benchmark generation, pull request harvesting, environment synthesis, test oracle extraction, hint-guided trajectory synthesis, fine-tuning]</li>
<li class=""><strong>authors:</strong> Lilin Wang, Lucas Ramalho, Alan Celestino, Phuc Anthony Pham, Yu Liu, Umang Kumar Sinha, Andres Portillo, Onassis Osunwa, Gabriel Maduekwe</li>
<li class=""><strong>institution:</strong> Turing</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17419" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17419</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/077c20705c707ee562f1935988b006695cf25f213f2df392cb27846fedaf0d4a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/077c20705c707ee562f1935988b006695cf25f213f2df392cb27846fedaf0d4a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SWE-Bench++, an automated framework that generates software engineering benchmarks by harvesting pull requests from GitHub to create reproducible, execution-based coding tasks across multiple languages. The method involves programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance, with a final step to create training trajectories from failed instances. The main conclusion is that this scalable, multilingual approach provides a valuable benchmark for evaluating and improving LLMs on repository-level code generation, as demonstrated by model performance metrics and fine-tuning improvements.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-agent reinforcement learning, independent proximal policy optimization, agent-based modeling, electricity markets, capacity markets, contracts for difference]</li>
<li class=""><strong>authors:</strong> Javier Gonzalez-Ruiz, Carlos Rodriguez-Pardo, Iacopo Savelli, Alice Di Bella, Massimo Tavoni</li>
<li class=""><strong>institution:</strong> Politecnico di Milano, CMCC Foundation - Euro-Mediterranean Center on Climate Change, RFF-CMCC European Institute on Economics and the Environment, Bocconi University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17444" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17444</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a multi-agent reinforcement learning framework, using independent proximal policy optimization, to model investment decisions by generation companies in long-term electricity markets. The model is applied to a stylized Italian electricity system to test various market designs and policy scenarios. The results demonstrate that market design is critical for achieving decarbonization targets while mitigating price volatility.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [multimodal semantic segmentation, deep neural network, robust training strategies, synchronized sensor data, daytime training for nighttime performance]</li>
<li class=""><strong>authors:</strong> Jon Muhovič, Janez Perš</li>
<li class=""><strong>institution:</strong> University of Ljubljana</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17450" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17450</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6eb14eac17bf3aa4e9ce145e08ce4eae06c5adaaf8ccf31ca3fa25a7f3835fa0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6eb14eac17bf3aa4e9ce145e08ce4eae06c5adaaf8ccf31ca3fa25a7f3835fa0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MULTIAQUA, a multimodal maritime dataset with synchronized RGB, thermal, IR, and LIDAR data, and proposes robust training strategies for multimodal semantic segmentation. The core method enables training a deep neural network using only daytime images to achieve reliable performance in challenging conditions like near-complete darkness. The main conclusion is that this approach simplifies data acquisition and annotation while maintaining robust scene interpretation for unmanned surface vehicles.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] When Data Quality Issues Collide: A Large-Scale Empirical Study of Co-Occurring Data Quality Issues in Software Defect Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [software defect prediction], [Explainable Boosting Machines, stratified interaction analysis, class imbalance, class overlap, irrelevant features, attribute noise, outliers]</li>
<li class=""><strong>authors:</strong> Emmanuel Charleson Dapaah, Jens Grabowski</li>
<li class=""><strong>institution:</strong> University of Göttingen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17460" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17460</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts a large-scale empirical study using Explainable Boosting Machines and stratified interaction analysis to examine five co-occurring data quality issues in software defect prediction across 374 datasets. It finds that co-occurrence is nearly universal, identifies tipping points for issues like class overlap and imbalance, and reveals context-dependent effects, concluding that no single model performs best under all conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning What to Write: Write-Gated KV for Efficient Long-Context Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [KV cache management, Write-Gated KV, KV Admission, KV cache eviction, KV cache selection, FlashAttention, paged-KV systems]</li>
<li class=""><strong>authors:</strong> Yen-Chieh Huang, Rui Fang, Ming-Syan Chen, Pi-Cheng Hsiu</li>
<li class=""><strong>institution:</strong> National Taiwan University, Academia Sinica</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17452" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17452</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Write-Gated KV, a learnable KV Admission mechanism that predicts token utility before it enters the KV cache to reduce memory usage and speed up inference. By filtering low-utility tokens early and maintaining a compact global cache, the method significantly reduces memory usage and improves prefill and decode speeds for long-context LLMs with minimal accuracy loss. The results demonstrate that proactive KV cache management is a practical solution for efficient long-context inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [time series forecasting], [spatial-temporal graph neural network, trend-seasonal decomposition, low-rank Top-K adjacency learning, horizon-wise gating, linear baseline]</li>
<li class=""><strong>authors:</strong> Henok Tenaw Moges, Deshendran Moodley</li>
<li class=""><strong>institution:</strong> University of Cape Town</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17453" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17453</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e930d6ea2e25d38e443cede1cee5618870d8bd50e1307a179be023dcac63701d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e930d6ea2e25d38e443cede1cee5618870d8bd50e1307a179be023dcac63701d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Lite-STGNN, a lightweight model combining decomposition-based linear temporal modeling with a learnable sparse graph module for spatial corrections. It achieves state-of-the-art accuracy on long-term multivariate forecasting benchmarks while being parameter-efficient and faster than transformer-based methods. The learned adjacency matrices also provide interpretable insights into domain-specific variable interactions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [marketing personalisation], [randomised controlled trial, agentic messaging, rule-based campaign, causal inference, contextual bandits]</li>
<li class=""><strong>authors:</strong> Olivier Jeunen, Schaun Wheeler</li>
<li class=""><strong>institution:</strong> aampe</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17462" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17462</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates an agentic messaging approach for customer communication, comparing it against a traditional rule-based system in a financial service application via a randomized controlled trial. The results show that the agentic system reduced unsubscribe events by 21% and encouraged earlier tax filing, demonstrating its effectiveness in improving user engagement and retention.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Linear Attention for Joint Power Optimization and User-Centric Clustering in Cell-Free Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [transformer, linear attention, supervised learning, user-centric clustering, power optimization]</li>
<li class=""><strong>authors:</strong> Irched Chafaa, Giacomo Bacci, Luca Sanguinetti</li>
<li class=""><strong>institution:</strong> University of Pisa</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17466" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17466</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a lightweight transformer model with a customized linear attention mechanism to jointly predict access point clusters and transmission powers in user-centric cell-free massive MIMO networks, using only spatial coordinates. This approach eliminates channel estimation overhead and pilot contamination while ensuring linear scalability with the number of users. Numerical results show the model provides near-optimal performance in maximizing minimum spectral efficiency and is adaptable to dynamic network scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Translating the Rashomon Effect to Sequential Decision-Making Tasks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [sequential decision-making], [Rashomon effect, formal verification, policy ensembles, behavioral cloning, permissive policies]</li>
<li class=""><strong>authors:</strong> Dennis Gross, Jørn Eirik Betten, Helge Spieker</li>
<li class=""><strong>institution:</strong> University of Oslo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17470" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17470</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper translates the Rashomon effect from classification to sequential decision-making by defining it for policies that behave identically but have different internal structures. It uses formal verification methods to compare the complete probabilistic behavior of policies in stochastic environments. The study concludes that the effect exists in this domain and that ensembles from the Rashomon set are more robust to distribution shifts.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Deep Learning-Based Surrogate Creep Modelling in Inconel 625: A High-Temperature Alloy Study</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [BiLSTM-Variational Autoencoder, BiLSTM-Transformer, surrogate modelling, Norton creep law, self-attention, probabilistic prediction, deterministic prediction]</li>
<li class=""><strong>authors:</strong> Shubham Das, Kaushal Singhania, Amit Sadhu, Suprabhat Das, Arghya Nandi</li>
<li class=""><strong>institution:</strong> Jadavpur University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17477" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17477</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes deep learning-based surrogate models, specifically a BiLSTM-Variational Autoencoder and a BiLSTM-Transformer, to rapidly predict creep strain in Inconel 625, replacing computationally expensive finite-element simulations. The models, trained on ANSYS-generated data, achieve high accuracy and provide predictions within seconds compared to the 30-40 minutes required by traditional simulations, enabling faster design optimization and structural health monitoring for high-temperature alloys.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, digital twin, Vision Transformer, ViT-UNet, privacy-preserving AI, brain tumor segmentation]</li>
<li class=""><strong>authors:</strong> Almustapha A. Wakili, Adamu Hussaini, Abubakar A. Musa, Woosub Jung, Wei Yu</li>
<li class=""><strong>institution:</strong> Towson University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17488" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17488</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67c12280225e186b761a6952a367a2b042a6fcd1886ac481e9bc15f5f81ee64a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67c12280225e186b761a6952a367a2b042a6fcd1886ac481e9bc15f5f81ee64a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes TwinSegNet, a federated learning framework that uses a hybrid ViT-UNet model and personalized digital twins for brain tumor segmentation without sharing raw data. It achieves high accuracy on heterogeneous MRI datasets, demonstrating that privacy can be preserved without sacrificing segmentation performance in multi-institutional settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multiple instance learning, AutoML, feature extraction, whole-slide images, benchmarking, computational pathology]</li>
<li class=""><strong>authors:</strong> Siemen Brussee, Pieter A. Valkema, Jurre A. J. Weijer, Thom Doeleman, Anne M.R. Schrader, Jesper Kers</li>
<li class=""><strong>institution:</strong> Leiden University Medical Center, Utrecht University Medical Center, Amsterdam University Medical Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17517" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17517</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces PathBench-MIL, an automated machine learning and benchmarking framework designed for Multiple Instance Learning in histopathology. It automates the entire pipeline from preprocessing to model aggregation, enabling standardized and reproducible evaluation of various models and feature extractors on whole-slide image datasets. The main conclusion is that this open-source framework facilitates rapid experimentation and standardization in computational pathology research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [protein hazard screening], [homology clustering, cluster-level holdout, logistic regression, random forest, linear SVM, calibrated probabilities, AUROC, AUPRC, Brier score, Expected Calibration Error]</li>
<li class=""><strong>authors:</strong> Muhammad Haris Khan</li>
<li class=""><strong>institution:</strong> University of Copenhagen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17527" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17527</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SafeBench-Seq, a benchmark and baseline classifier for screening hazardous protein sequences using only interpretable physicochemical and compositional features. The method employs homology clustering at ≤40% identity with cluster-level holdouts to evaluate performance on novel threats. The main conclusion is that random data splits overestimate robustness compared to this stricter homology-controlled evaluation, and that calibrated linear models provide good probability calibration for this CPU-only screening task.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] NetworkFF: Unified Layer Optimization in Forward-Only Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [neural network training algorithms], [Forward-Forward algorithm, Collaborative Forward-Forward, inter-layer cooperation, goodness function, neuromorphic computing]</li>
<li class=""><strong>authors:</strong> Salar Beigzad</li>
<li class=""><strong>institution:</strong> University of St. Thomas, Minnesota</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17531" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17531</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Collaborative Forward-Forward (CFF) learning, an enhancement to the Forward-Forward algorithm that addresses inter-layer isolation by incorporating weighted contributions from all layers into a collaborative goodness function. It proposes two variants, Fixed CFF and Adaptive CFF, which enable coordinated feature learning while preserving forward-only computation. The method demonstrates significant performance improvements on benchmark datasets, establishing inter-layer collaboration as a fundamental enhancement for biologically plausible and memory-efficient neural network training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Bayesian Optimisation: Which Constraints Matter?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [Bayesian optimisation], [Bayesian optimisation, Knowledge Gradient, Gaussian Processes, decoupled constraints, acquisition functions]</li>
<li class=""><strong>authors:</strong> Xietao Wang Lin, Juan Ungredda, Max Butler, James Town, Alma Rahat, Hemant Singh, Juergen Branke</li>
<li class=""><strong>institution:</strong> University of Warwick, ESTECO SpA, Swansea University, University of New South Wales</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17569" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17569</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes new Bayesian optimisation variants of the Knowledge Gradient acquisition function for problems with decoupled black-box constraints, focusing on evaluating only the most relevant constraints. The methods are empirically benchmarked and shown to be superior to existing state-of-the-art approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] When De-noising Hurts: A Systematic Study of Speech Enhancement Effects on Modern Medical ASR Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [MetricGAN-plus-voicebank, semantic WER, noise robustness, speech enhancement]</li>
<li class=""><strong>authors:</strong> Sujal Chondhekar, Vasanth Murukuri, Rushabh Vasani, Sanika Goyal, Rajshree Badami, Anushree Rana, Sanjana SN, Karthik Pandia, Sulabh Katiyar, Neha Jagadeesh, Sankalp Gulati</li>
<li class=""><strong>institution:</strong> EkaCare (Orbi Health Private Limited)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17562" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17562</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically evaluates the effect of MetricGAN-plus-voicebank speech enhancement on four modern ASR systems using medical speech under nine noise conditions. The study finds that denoising preprocessing consistently degrades ASR performance across all models and conditions, suggesting modern ASR models are inherently noise-robust and enhancement may remove critical acoustic features.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [vertical scheduling, optimizer step overlapping, SSD-offloaded training, gradient accumulation]</li>
<li class=""><strong>authors:</strong> Yikang Yue, Yishu Yin, Xuehai Qian</li>
<li class=""><strong>institution:</strong> Tsinghua University, University of Illinois at Urbana-Champaign</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17570" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17570</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces GreedySnake, a system that accelerates SSD-offloaded LLM training by using vertical scheduling to process all micro-batches per layer before moving to the next, and by overlapping the optimizer step with the next forward pass. This approach significantly reduces I/O bottlenecks and improves throughput compared to prior systems like ZeRO-Infinity, achieving up to 2.53x speedup for large models like GPT-175B.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [GPU-internal scheduling, resource sharing, collaborative multi-GPU video decoding, logically decoupled execution, FlashCodec, UnifiedServe]</li>
<li class=""><strong>authors:</strong> Lingxiao Zhao, Haoran Zhou, Yuezhi Che, Dazhao Cheng</li>
<li class=""><strong>institution:</strong> Wuhan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17574" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17574</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65d961bcef453cdff273d0991a97111419acf476f8d34e21f66dbd356156dfa7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65d961bcef453cdff273d0991a97111419acf476f8d34e21f66dbd356156dfa7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FlashCodec and UnifiedServe, a framework that optimizes multimodal large language model (MLLM) serving by accelerating video decoding and enabling resource sharing across the vision and text stages. This approach reduces latency and eliminates inter-stage blocking, leading to significantly higher throughput and better SLO adherence compared to existing systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, asynchronous collaboration, model stitching, ensembles, multi-objective learning]</li>
<li class=""><strong>authors:</strong> Arthur Guijt, Dirk Thierens, Ellen Kerkhof, Jan Wiersma, Tanja Alderliesten, Peter A.N. Bosman</li>
<li class=""><strong>institution:</strong> Not specified (inferred from author names only; no affiliations or email domains provided)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17592" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17592</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using model stitching to combine disjointly trained deep learning models as an asynchronous alternative to federated learning, allowing collaboration without sharing raw data. The method inserts stitching layers to merge intermediate representations, improving generalization across different parties&#x27; datasets while maintaining competitive performance on each party&#x27;s own data. The results show that asynchronous collaboration through stitching can yield competitive performance without requiring synchronous training or data exchange.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Machine Learning for Static and Single-Event Dynamic Complex Network Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [graph representation learning], [latent space models, latent distance model, graph representation learning, network embeddings, static networks, dynamic networks]</li>
<li class=""><strong>authors:</strong> Nikolaos Nakis</li>
<li class=""><strong>institution:</strong> arXiv</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17577" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17577</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a74e4c69d21da3d9d39474c8e185aec564c37a98f7b8fa85563d3b895bd7a484_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a74e4c69d21da3d9d39474c8e185aec564c37a98f7b8fa85563d3b895bd7a484_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This thesis develops novel algorithmic approaches for graph representation learning using latent space models, specifically the latent distance model, to capture network characteristics like homophily and transitivity. It aims to create structural-aware network representations for tasks such as community characterization and impact dynamics quantification in temporal networks. The methods are designed as unified learning processes to avoid heuristics and multi-stage post-processing, advancing towards comprehensive and powerful network embeddings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A Unified Representation of Neural Networks Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [neural networks theory], [continuous neural networks, neural ODEs, distributed parameter systems, approximation error, discretization]</li>
<li class=""><strong>authors:</strong> Christophe Prieur, Mircea Lazar, Bogdan Robu</li>
<li class=""><strong>institution:</strong> Univ. Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab, Eindhoven University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17593" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17593</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a unified representation of neural network architectures called Distributed Parameter neural Network (DiPaNet), derived by considering the limiting case where the number of neurons and layers tends to infinity. The method merges integral infinite-width representations with neural ODEs using homogenization and discretization techniques. The main conclusion is that most existing finite and infinite-dimensional neural network architectures can be related through this unified DiPaNet framework.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Gaussian Discriminant Analysis, Z-score distance analysis, cluster-driven deep learning, two-stage framework]</li>
<li class=""><strong>authors:</strong> Tosin Ige, Christopher Kiekintveld, Aritran Piplai, Asif Rahman, Olukunle Kolade, Sasidhar Kunapuli</li>
<li class=""><strong>institution:</strong> The University of Texas at El Paso, University of North Carolina</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17594" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17594</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes MAD-OOD, a two-stage cluster-driven deep learning framework for out-of-distribution malware detection. It uses Gaussian Discriminant Analysis to model class embeddings and Z-score distance analysis to identify anomalies, then integrates these predictions with a neural network for final classification. The method significantly outperforms state-of-the-art approaches on benchmark datasets, achieving high AUC for detecting unseen malware families.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A Systems-Theoretic View on the Convergence of Algorithms under Disturbances</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [converse Lyapunov theorems, stability bounds, convergence rates, distributed learning, noise injection, disturbance modeling]</li>
<li class=""><strong>authors:</strong> Guner Dilsad Er, Sebastian Trimpe, Michael Muehlebach</li>
<li class=""><strong>institution:</strong> Max Planck Institute for Intelligent Systems, RWTH Aachen University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17598" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17598</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper uses converse Lyapunov theorems to derive stability bounds and convergence rates for iterative algorithms operating under disturbances. It provides a unifying framework to quantify how noise and interconnections affect algorithmic performance, with applications in distributed learning and privacy-preserving computation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning Safe Autonomous Driving Policies Using Predictive Safety Representations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [safe reinforcement learning], [safe reinforcement learning, predictive safety representations, constrained markov decision processes, waymo open motion dataset, nuplan, srpl]</li>
<li class=""><strong>authors:</strong> Mahesh Keswani, Raunak Bhattacharyya</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology Delhi</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17586" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17586</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40dd4c52ca67a3f1ecc6ec7068de338a3616940aa4e51935c5ce5f30430ebbfe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40dd4c52ca67a3f1ecc6ec7068de338a3616940aa4e51935c5ce5f30430ebbfe_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates the Safety Representations for Safer Policy Learning (SRPL) framework, which augments SafeRL agents with a predictive model of future constraint violations to improve the safety-performance trade-off in autonomous driving. Experiments on real-world datasets (Waymo Open Motion Dataset and NuPlan) show that SRPL can lead to statistically significant improvements in success rate and cost reduction, and enhances robustness to noise and generalization in cross-dataset evaluation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] More Consistent Accuracy PINN via Alternating Easy-Hard Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [scientific machine learning], [physics-informed neural networks, easy-hard prioritization, hybrid training strategy, alternating scheme]</li>
<li class=""><strong>authors:</strong> Zhaoqian Gao, Min Yanga</li>
<li class=""><strong>institution:</strong> Yantai University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17607" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17607</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid training strategy for Physics-Informed Neural Networks (PINNs) that alternates between easy and hard prioritization to improve performance. The method achieves consistently high accuracy on challenging PDEs, significantly outperforming baseline approaches. The work demonstrates that this alternating scheme enhances the robustness and reliability of PINNs across diverse problem types.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SCOPE: Sequential Causal Optimization of Process Interventions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [prescriptive process monitoring], [backward induction, causal learning, reinforcement learning, sequential decision-making]</li>
<li class=""><strong>authors:</strong> Jakob De Moor, Hans Weytjens, Johannes De Smedt, Jochen De Weerdt</li>
<li class=""><strong>institution:</strong> KU Leuven, Technical University of Munich (TUM)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17629" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17629</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SCOPE, a prescriptive process monitoring approach that uses backward induction and causal learning to recommend aligned sequences of interventions for optimizing key performance indicators. It directly leverages observational data without needing process approximations for reinforcement learning. Experiments show SCOPE outperforms existing techniques in optimizing KPIs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [natural language processing], [ensemble learning, weighted voting, Condorcet’s Jury Theorem, fine-tuning, transformer models]</li>
<li class=""><strong>authors:</strong> Menna Elgabry, Ali Hamdi</li>
<li class=""><strong>institution:</strong> MSA University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17630" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17630</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a confidence- and credibility-weighted ensemble framework using diverse small transformer models (BERT, RoBERTa, etc.) for emotion detection. The method combines global validation performance and instance-level confidence to weight model votes. The ensemble achieves a 93.5% macro F1-score on the DAIR-AI dataset, outperforming larger LLMs while being more parameter-efficient.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Trust-Region Adaptive Policy Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [post-training], [Trust-Region Adaptive Policy Optimization, Trust-Region SFT, forward KL divergence, reverse KL, adaptive prefix-selection, supervised fine-tuning, reinforcement learning]</li>
<li class=""><strong>authors:</strong> Mingyu Su, Jian Guan, Yuxian Gu, Minlie Huang, Hongning Wang</li>
<li class=""><strong>institution:</strong> Tsinghua University, Ant Group</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17636" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17636</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a948c761d13b2b79a39ce9d5e992677a2054a69ebce16f21dcf30264ab34a82f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a948c761d13b2b79a39ce9d5e992677a2054a69ebce16f21dcf30264ab34a82f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces TRAPO, a hybrid framework that interleaves supervised fine-tuning and reinforcement learning within each training instance to unify expert supervision and self-exploration. It stabilizes training with Trust-Region SFT and an adaptive prefix-selection mechanism. Experiments on mathematical reasoning benchmarks show TRAPO outperforms standard pipelines and state-of-the-art approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Estimating Spatially Resolved Radiation Fields Using Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Monte-Carlo Simulation, Geant4, Convolutional Neural Networks, Fully Connected Neural Networks, U-Net, FCNN, Dataset Generation]</li>
<li class=""><strong>authors:</strong> Felix Lehner, Pasquale Lombardo, Susana Castillo, Oliver Hupe, Marcus Magnor</li>
<li class=""><strong>institution:</strong> Physikalisch-Technische Bundesanstalt (PTB), Technical University Braunschweig, Belgian Nuclear Research Centre (SCK CEN), University of New Mexico, Leibniz University Hannover</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17654" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17654</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper uses neural networks, including convolutional and fully connected architectures, to estimate spatial radiation fields for medical dosimetry, trained on synthetic datasets generated via Monte-Carlo simulations with Geant4. It evaluates design decisions for reconstructing fluence and spectra distributions, concluding with open-source release of datasets and training pipelines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Polyharmonic Cascade</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [deep learning architecture], [polyharmonic cascade, polyharmonic splines, random functions, principles of indifference, global linear system, GPU matrix operations]</li>
<li class=""><strong>authors:</strong> Yuriy N. Bakhvalov</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17671" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17671</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the &quot;polyharmonic cascade,&quot; a deep learning architecture built from packages of polyharmonic splines, derived from random function theory and principles of indifference. It proposes a training method that solves a global linear system per batch instead of using gradient descent, enabling synchronized layer updates and efficient GPU computation. The method demonstrates fast learning without overfitting on the MNIST dataset.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Vidarc: Embodied Video Diffusion Model for Closed-loop Control</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [video diffusion, autoregressive generation, masked inverse dynamics model, closed-loop control, cross-embodiment pre-training, KV cache]</li>
<li class=""><strong>authors:</strong> Yao Feng, Chendong Xiang, Xinyi Mao, Hengkai Tan, Zuyue Zhang, Shuhe Huang, Kaiwen Zheng, Haitian Liu, Hang Su, Jun Zhu</li>
<li class=""><strong>institution:</strong> Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17661" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17661</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7977f39cb797f71021c4776c090587d8f5e8e7c33c06e677b445877e8ad4c5d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7977f39cb797f71021c4776c090587d8f5e8e7c33c06e677b445877e8ad4c5d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Vidarc, a method for robotic control that combines an autoregressive video diffusion model with a masked inverse dynamics model to enable fast, closed-loop operation. It is pre-trained on a large dataset of diverse robotic episodes and achieves state-of-the-art performance, including higher success rates and significantly lower latency compared to baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] You Only Train Once: Differentiable Subset Selection for Omics Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [bioinformatics], [differentiable subset selection, multi-task learning, end-to-end training, sparsity, single-cell RNA-seq]</li>
<li class=""><strong>authors:</strong> Daphné Chopard, Jorge da Silva Gonçalves, Irene Cannistraci, Thomas M. Sutter, Julia E. Vogt</li>
<li class=""><strong>institution:</strong> ETH Zurich, University Children’s Hospital Zurich</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17678" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17678</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces YOTO, an end-to-end framework that jointly selects discrete gene subsets and performs prediction in a single differentiable model, using sparsity and multi-task learning. It demonstrates improved predictive performance and yields compact, meaningful gene subsets on single-cell RNA-seq datasets, advancing biomarker discovery.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [spatio-temporal forecasting], [transformer, self-attention, geostatistical covariance, Gaussian processes, spatial decay parameters]</li>
<li class=""><strong>authors:</strong> Yuri Calleo</li>
<li class=""><strong>institution:</strong> Universitas Mercatorum</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17696" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17696</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a spatially-informed transformer that injects a learnable geostatistical covariance kernel into the self-attention mechanism to incorporate spatial distance information for spatio-temporal forecasting. The method decomposes attention into a stationary physical prior and a data-driven residual, allowing the network to recover spatial decay parameters. Experiments show it outperforms graph neural networks and provides well-calibrated probabilistic forecasts.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Mitigating Forgetting in Low Rank Adaptation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [LoRA, Laplace approximation, catastrophic forgetting, regularization, parameter-efficient fine-tuning]</li>
<li class=""><strong>authors:</strong> Joanna Sliwa, Frank Schneider, Philipp Hennig, Jose Miguel Hernandez-Lobato</li>
<li class=""><strong>institution:</strong> University of Tübingen, University of Cambridge</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17720" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17720</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces LaLoRA, a method that applies a Laplace approximation to LoRA weights to estimate parameter confidence and regularize updates, mitigating catastrophic forgetting during fine-tuning. The approach improves the learning-forgetting trade-off for large language models, as demonstrated by fine-tuning a Llama model for mathematical reasoning, while remaining computationally lightweight.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Convergence Guarantees for Federated SARSA with Local Training and Heterogeneous Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [federated learning, SARSA, linear function approximation, convergence analysis, heterogeneous agents, local training]</li>
<li class=""><strong>authors:</strong> Paul Mangold, Eloïse Berthier, Eric Moulines</li>
<li class=""><strong>institution:</strong> CNRS, École polytechnique, Institut Polytechnique de Paris, ENSTA, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17688" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17688</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a theoretical analysis of Federated SARSA (FedSARSA), an on-policy reinforcement learning algorithm with linear function approximation and local training across heterogeneous agents. It establishes the first convergence guarantees and complexity bounds for this setting, showing that FedSARSA achieves linear speed-up with the number of agents despite heterogeneity in transitions and rewards.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Can You Hear Me Now? A Benchmark for Long-Range Graph Propagation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [graph neural networks], [ECHO benchmark, long-range propagation, message-passing, over-smoothing, over-squashing, synthetic graph tasks, molecular property prediction]</li>
<li class=""><strong>authors:</strong> Luca Miglior, Matteo Tolloso, Alessio Gravina, Davide Bacciu</li>
<li class=""><strong>institution:</strong> University of Pisa</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17762" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17762</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces ECHO, a benchmark designed to evaluate the ability of Graph Neural Networks (GNNs) to handle long-range information propagation. It includes synthetic tasks and real-world molecular datasets to test GNNs on challenging, long-distance dependencies. The benchmarking reveals significant performance gaps in existing GNNs, highlighting the difficulty of long-range propagation and the need for improved architectural designs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [Easy Adaptation, Parameter-Efficient Fine-Tuning, LoRA, Specific Small Models, task adaptation, resource-constrained]</li>
<li class=""><strong>authors:</strong> Dong Chen, Zhengqing Hu, Shixing Zhao, Yibo Guo</li>
<li class=""><strong>institution:</strong> Not explicitly provided in the given text.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17771" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17771</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Easy Adaptation (EA), a method that uses Specific Small Models (SSMs) to complement the data distribution for Large Models, enabling task adaptation without accessing the LM&#x27;s internal parameters. This approach matches the performance of Parameter-Efficient Fine-Tuning (PEFT) like LoRA on diverse tasks while requiring only minimal computational resources, making it suitable for resource-constrained environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [sequential recommendation], [ensembling, ID embeddings, text embeddings, modality features]</li>
<li class=""><strong>authors:</strong> Liam Collins, Bhuvesh Kumar, Clark Mingxuan Ju, Tong Zhao, Donald Loveland, Leonardo Neves, Neil Shah</li>
<li class=""><strong>institution:</strong> Snap Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17820" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17820</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a sequential recommendation method that independently trains ID-based and text-based models and then combines them via a simple ensembling strategy. It demonstrates that ID and text features learn complementary signals. The main conclusion is that both feature types are necessary for state-of-the-art performance, but complex fusion architectures are not required.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [weakly supervised learning], [multi-instance partial-label learning, calibratable disambiguation loss, model calibration]</li>
<li class=""><strong>authors:</strong> Wei Tang, Yin-Fang Yang, Weijia Zhang, Min-Ling Zhang</li>
<li class=""><strong>institution:</strong> Southeast University, The University of Newcastle</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17788" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17788</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec09016ca3c7c620b69a1f11fd5eac67d6cf0e41b74b5709b9e32e919c36ac8f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec09016ca3c7c620b69a1f11fd5eac67d6cf0e41b74b5709b9e32e919c36ac8f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a plug-and-play Calibratable Disambiguation Loss (CDL) to improve classification accuracy and model calibration in Multi-Instance Partial-Label Learning. The loss has two instantiations that calibrate predictions using candidate label probabilities or both candidate and non-candidate sets. Experimental results show that CDL significantly enhances both classification and calibration performance over conventional methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [imitation learning, distributionally robust control, layered control architecture, Taylor Series Imitation Learning (TaSIL), L1-Distributionally Robust Adaptive Control (L1-DRAC), certifiable autonomy]</li>
<li class=""><strong>authors:</strong> Aditya Gahlawat, Ahmed Aboudonia, Sandeep Banik, Naira Hovakimyan, Nikolai Matni, Aaron D. Ames, Gioele Zardini, Alberto Speranzon</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign, University of Pennsylvania, California Institute of Technology, Massachusetts Institute of Technology, Lockheed Martin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17899" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17899</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Distributionally Robust Imitation Policy (DRIP) architecture, a layered control framework that integrates Taylor Series Imitation Learning (TaSIL) and L1-Distributionally Robust Adaptive Control (L1-DRAC) to address different sources of distribution shift. The main conclusion is that this integration enables the design of certifiable autonomy pipelines by guaranteeing performance certificates for the entire control system, combining learning-based components with model-based decision-making.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [generative modeling], [Wasserstein-Fisher-Rao gradient flow, weighted stochastic differential equations, Feynman-Kac representation, score-based diffusion models, Langevin dynamics]</li>
<li class=""><strong>authors:</strong> Herlock Rahimi</li>
<li class=""><strong>institution:</strong> Yale University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17878" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17878</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a new sampling method for generative modeling by implementing Wasserstein-Fisher-Rao gradient flow via weighted stochastic differential equations, using the Feynman-Kac representation. This approach aims to overcome the slow mixing rates of traditional diffusion models in non-log-concave, multimodal target distributions by incorporating controlled mass reweighting. The study provides a rigorous geometric and operator-theoretic foundation for future developments in this area.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Regularized Random Fourier Features and Finite Element Reconstruction for Operator Learning in Sobolev Space</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [operator learning], [random Fourier features, Tikhonov regularization, finite element reconstruction, Student&#x27;s t distribution]</li>
<li class=""><strong>authors:</strong> Xinyue Yu, Hayden Schaeffer</li>
<li class=""><strong>institution:</strong> University of California, Los Angeles</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17884" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17884</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a regularized random Fourier feature method combined with finite element reconstruction (RRFF-FEM) for learning operators from noisy data, using Student&#x27;s t-distributed random features and frequency-weighted regularization. It proves theoretical guarantees on conditioning and generalization when features scale appropriately. Numerical experiments on PDE problems show the method is robust to noise, faster to train, and maintains competitive accuracy compared to kernel and neural operator baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Visually Prompted Benchmarks Are Surprisingly Fragile</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [visual prompting, benchmark evaluation, vision-language models, visual marker design, JPEG compression, dataset size, VPBench]</li>
<li class=""><strong>authors:</strong> Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang, Renhao Wang, Trevor Darrell, Alane Suhr, Angjoo Kanazawa</li>
<li class=""><strong>institution:</strong> UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17875" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17875</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1578e85cb159e2bf39916b0de61ec0b774772fc5c07fb3edc1f869eea3ea32e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1578e85cb159e2bf39916b0de61ec0b774772fc5c07fb3edc1f869eea3ea32e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper evaluates vision-language models (VLMs) on visually prompted benchmarks, where questions refer to coordinates marked directly on images. It finds that model performance and leaderboard rankings are surprisingly fragile to minor changes in visual marker design (e.g., color, size) and low-level inference settings like JPEG compression. To address this instability, the authors introduce VPBench, a larger benchmark with multiple visual marker variants.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] RadarGen: Automotive Radar Point Cloud Generation from Cameras</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [diffusion model, bird&#x27;s-eye-view, radar cross section, Doppler, point cloud generation, foundation models]</li>
<li class=""><strong>authors:</strong> Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany</li>
<li class=""><strong>institution:</strong> Technion, MIT, NVIDIA, University of Toronto, Vector Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17897" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17897</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RadarGen is a diffusion model that generates realistic automotive radar point clouds from multi-view camera images by representing radar data in bird&#x27;s-eye-view and conditioning on visual cues. It uses a lightweight recovery step to reconstruct point clouds from the generated maps. Evaluations show it captures real radar statistics and reduces the performance gap for perception models trained on synthetic data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Disentangled representations via score-based variational autoencoders</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [representation learning], [variational autoencoder, diffusion models, score-based guidance, disentanglement, evidence lower bound]</li>
<li class=""><strong>authors:</strong> Benjamin S. H. Lyo, Eero P. Simoncelli, Cristina Savin</li>
<li class=""><strong>institution:</strong> New York University, Flatiron Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17127" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17127</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SAMI, a method that combines diffusion models and variational autoencoders into a unified framework for unsupervised representation learning. It uses score-based guidance to learn disentangled, semantically meaningful latent representations from data. The results show that this approach can make implicit structural information in diffusion models explicit and interpretable.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Colormap-Enhanced Vision Transformers for MRI-Based Multiclass (4-Class) Alzheimer&#x27;s Disease Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Vision Transformers, Pseudo-Color Enhancement, MRI, Multi-Class Classification]</li>
<li class=""><strong>authors:</strong> Faisal Ahmed</li>
<li class=""><strong>institution:</strong> Embry-Riddle Aeronautical University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16964" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16964</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes PseudoColorViT-Alz, a method that enhances MRI scans with pseudo-color transformations and processes them using a Vision Transformer for Alzheimer&#x27;s disease classification. The model achieves state-of-the-art accuracy of 99.79% on the OASIS-1 dataset, demonstrating that combining color augmentation with Vision Transformers significantly improves classification performance over previous methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [test-time refinement, self-supervised learning, shape from shading, score distillation sampling, monocular depth estimation, diffusion models]</li>
<li class=""><strong>authors:</strong> Ananta R. Bhattarai, Helge Rhodin</li>
<li class=""><strong>institution:</strong> Bielefeld University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17908" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17908</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb5eab47c4353057728b3e9889e13b412f6c11ae6703f713d247c4724fbb909_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb5eab47c4353057728b3e9889e13b412f6c11ae6703f713d247c4724fbb909_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Re-Depth Anything, a test-time self-supervision framework that refines monocular depth predictions by re-lighting the geometry and using a 2D diffusion model&#x27;s priors via Score Distillation Sampling. It updates only specific model embeddings and the decoder to prevent collapse, rather than fine-tuning the entire network. The method shows substantial improvements in depth accuracy and realism over the baseline Depth Anything V2 model.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Graph Attention Networks for Detecting Epilepsy from EEG Signals Using Accessible Hardware in Low-Resource Settings</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [graph attention networks, electroencephalography, spatio-temporal graphs, edge analysis, low-cost hardware, RaspberryPi]</li>
<li class=""><strong>authors:</strong> Szymon Mazurek, Stephen Moore, Alessandro Crimi</li>
<li class=""><strong>institution:</strong> AGH University of Krakow, University of Cape Coast</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2507.15118" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2507.15118</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d57b4723f1c065c80f840b86af58e96a683cea596741b961e8c90f8c5680da8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d57b4723f1c065c80f840b86af58e96a683cea596741b961e8c90f8c5680da8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a graph attention network (GAT) framework that models EEG signals as spatio-temporal graphs to detect epilepsy, with a focus on low-cost hardware for deployment in low-resource settings. The method adapts GATs to analyze edge connectivity for biomarker identification and is designed for lightweight training and deployment. The results demonstrate promising classification performance and highlight the potential for scalable, accessible diagnostic support in underserved regions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Application of machine learning to predict food processing level using Open Food Facts</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [food science and nutrition informatics], [LightGBM, Random Forest, CatBoost, NOVA classification, nutrient concentration data]</li>
<li class=""><strong>authors:</strong> Nalin Arora, Aviral Chauhan, Siddhant Rana, Mahansh Aditya, Sumit Bhagat, Aditya Kumar, Akash Kumar, Akanksh Semar, Ayush Vikram Singh, Ganesh Bagler</li>
<li class=""><strong>institution:</strong> Indraprastha Institute of Information Technology Delhi (IIIT-Delhi), Infosys Center for Artificial Intelligence, Center of Excellence in Healthcare, Foodoscope Technologies Pvt Ltd</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17169" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17169</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This study applies machine learning models, including LightGBM, Random Forest, and CatBoost, to classify food processing levels (NOVA) using nutrient data from the Open Food Facts dataset. The best-performing model, LightGBM, achieved 80-85% accuracy and effectively distinguished minimally from ultra-processed foods. The research concludes that higher processing levels are strongly associated with poorer nutritional quality, greater environmental impact, and common allergens like gluten and milk.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Systemic Risk Radar: A Multi-Layer Graph Framework for Early Market Crash Warning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-layer graph, GNN, temporal GNN, logistic regression, Random Forest, correlation-based, systemic risk]</li>
<li class=""><strong>authors:</strong> Sandeep Neela</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17185" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17185</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the Systemic Risk Radar (SRR), a framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility. It demonstrates that graph-derived features from this model provide useful early-warning signals for market crashes, outperforming standard feature-based models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [fair machine learning], [penalized regression, cost-sensitive classification, true positive rate disparity penalties]</li>
<li class=""><strong>authors:</strong> Carter H. Nakamoto, Lucia Lushi Chen, Agata Foryciarz, Sherri Rose</li>
<li class=""><strong>institution:</strong> Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17340" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17340</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a penalized fair regression framework using unfairness penalties for multiple groups, implemented via reduction to cost-sensitive classification. The method is applied to predict end-stage renal disease in a chronic kidney disease study, showing substantial fairness improvements for multiple race and ethnicity groups without appreciable loss in overall model fit.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Alternating Direction Method of Multipliers for Nonlinear Matrix Decompositions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [optimization algorithms], [alternating direction method of multipliers, nonlinear matrix decomposition, rectified linear unit, component-wise square, MinMax transform]</li>
<li class=""><strong>authors:</strong> Atharva Awari, Nicolas Gillis, Arnaud Vandaele</li>
<li class=""><strong>institution:</strong> University of Mons</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17473" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17473</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an ADMM-based algorithm for solving nonlinear matrix decompositions, where a matrix is approximated via a nonlinear function applied to a low-rank product. It demonstrates flexibility across various nonlinear models and loss functions, showing applicability to real-world datasets in areas like sparse data approximation and recommender systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Resource-efficient medical image classification for edge devices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [quantization, quantization-aware training, post-training quantization, convolutional neural networks]</li>
<li class=""><strong>authors:</strong> Mahsa Lavaei, Zahra Abadi, Salar Beigzad, Alireza Maleki</li>
<li class=""><strong>institution:</strong> University of Tehran, Tehran University, University of St. Thomas, Minnesota</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17515" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17515</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates a resource-efficient approach for medical image classification on edge devices using model quantization techniques, specifically quantization-aware training (QAT) and post-training quantization (PTQ). The study demonstrates that quantized models achieve significant reductions in model size and inference latency while maintaining clinically acceptable diagnostic accuracy, enabling real-time processing in resource-limited settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Machine Learning Assisted Parameter Tuning on Wavelet Transform Amorphous Radial Distribution Function</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [wavelet-transform radial distribution function (WT-RDF), parameter optimization, machine learning, RBF, LSTM]</li>
<li class=""><strong>authors:</strong> Deriyan Senjaya, Stephen Ekaputra Limantoro</li>
<li class=""><strong>institution:</strong> National Tsing Hua University, National Yang Ming Chiao Tung University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17245" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17245</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0fcb546d51f53548e0556b6cedd431012b18af3f725ab1ca0c73a63485285ea_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0fcb546d51f53548e0556b6cedd431012b18af3f725ab1ca0c73a63485285ea_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper enhances the wavelet-transform radial distribution function (WT-RDF) for analyzing amorphous materials by using machine learning to optimize its parameters, creating the WT-RDF+ framework. The improved model provides more accurate peak predictions and outperforms benchmark ML models like RBF and LSTM, demonstrating its robustness for characterizing Ge-Se amorphous systems and aiding in the design of phase-change thin films.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Perfect reconstruction of sparse signals using nonconvexity control and one-step RSB message passing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [compressed sensing, sparse signal reconstruction], [SCAD penalty, approximate message passing, replica symmetry breaking, state evolution, nonconvexity control]</li>
<li class=""><strong>authors:</strong> Xiaosi Gu, Ayaka Sakata, Tomoyuki Obuchi</li>
<li class=""><strong>institution:</strong> Kyoto University, RIKEN center for AIP, Ochanomizu University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17426" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17426</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops a one-step replica-symmetry-breaking extension of approximate message passing (1RSB-AMP) for sparse signal reconstruction using the SCAD penalty. The authors propose a new criterion for selecting the Parisi parameter and combine it with nonconvexity control, which improves the algorithmic limit for perfect reconstruction compared to the replica-symmetric AMP, though the gain is modest and remains below the Bayes-optimal threshold.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Sharp Structure-Agnostic Lower Bounds for General Functional Estimation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [causal inference], [structure-agnostic estimation, double machine learning, debiased learning, doubly robust learning, nuisance functions, functional estimation]</li>
<li class=""><strong>authors:</strong> Jikai Jin, Vasilis Syrgkanis</li>
<li class=""><strong>institution:</strong> Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17341" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17341</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper establishes sharp lower bounds for structure-agnostic functional estimation, analyzing the optimal error rates achievable without imposing structural priors. It shows that doubly robust learning and double machine learning (DML) are optimal for a general class of functionals, including the average treatment effect (ATE), across different regimes of double robustness. The results provide theoretical validation for first-order debiasing methods and guidance for practitioners in the absence of strong structural assumptions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] HydroGym: A Reinforcement Learning Platform for Fluid Dynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [reinforcement learning, flow control, differentiable solvers, transfer learning, benchmark platform]</li>
<li class=""><strong>authors:</strong> Christian Lagemann, Sajeda Mokbel, Miro Gondrum, Mario Rüttgers, Jared Callaham, Ludger Paehler, Samuel Ahnert, Nicholas Zolman, Kai Lagemann, Nikolaus Adams, Matthias Meinke, Wolfgang Schröder, Jean-Christophe Loiseau, Esther Lagemann, Steven L. Brunton</li>
<li class=""><strong>institution:</strong> University of Washington, RWTH Aachen University, Inha University, Technical University of Munich, German Center for Neurodegenerative Diseases, Arts et Métiers Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17534" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17534</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11a2356fa2a2cafe6887d85b978c67e18494f41d547e787460e381132d0f9508_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11a2356fa2a2cafe6887d85b978c67e18494f41d547e787460e381132d0f9508_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces HydroGym, a reinforcement learning platform designed for fluid dynamics control, featuring both non-differentiable and differentiable solvers to improve sample efficiency. The platform includes 42 validated environments and demonstrates that RL agents can discover robust control principles, achieving significant drag reduction and efficient adaptation to new conditions via transfer learning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [molecular design], [Bayesian optimization, generative models, multi-objective optimization, acquisition function, qPMHI, Monte Carlo sampling]</li>
<li class=""><strong>authors:</strong> Madhav R. Muthyala, Farshud Sorourifar, Tianhong Tan, You Peng, Joel A. Paulson</li>
<li class=""><strong>institution:</strong> University of Wisconsin–Madison, The Ohio State University, The Dow Chemical Company</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17659" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17659</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a &quot;generate-then-optimize&quot; framework for de novo molecular design, which uses a generative model to create candidate molecules and a novel acquisition function called qPMHI to efficiently select batches for evaluation. The method demonstrates significant improvements over existing approaches in sample efficiency and performance, as shown in benchmarks and a case study on designing organic cathode materials for batteries.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Imputation Uncertainty in Interpretable Machine Learning Methods</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [interpretable machine learning], [permutation feature importance, partial dependence plots, Shapley values, single imputation, multiple imputation, imputation uncertainty, confidence intervals]</li>
<li class=""><strong>authors:</strong> Pegah Golchian, Marvin N. Wright</li>
<li class=""><strong>institution:</strong> Leibniz Institute for Prevention Research &amp; Epidemiology – BIPS, University of Bremen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17689" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17689</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates the impact of missing data imputation methods on the uncertainty quantification of interpretable machine learning (IML) methods. It compares single and multiple imputation, showing that single imputation underestimates variance in IML explanations, while multiple imputation provides confidence interval coverage closer to the nominal level.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [StyleGAN2-ADA, Denoising Diffusion Probabilistic Models (DDPMs), FID, KID, Inception Score, ViT-B/16, synthetic data augmentation]</li>
<li class=""><strong>authors:</strong> N. A. Adarsh Pritam, Jeba Shiney O, Sanyam Jain</li>
<li class=""><strong>institution:</strong> Alliance University, Østfold University College</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17585" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17585</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/497a14da92ec4e58a3716d9bb6044a8b43d59d0f8ab0aff8c6e70b0d8e5325c9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/497a14da92ec4e58a3716d9bb6044a8b43d59d0f8ab0aff8c6e70b0d8e5325c9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SkinGenBench, a benchmark that evaluates the effects of generative models (StyleGAN2-ADA and DDPMs) and preprocessing pipelines on synthetic dermoscopic image generation for melanoma diagnosis. The main conclusion is that the choice of generative architecture has a stronger impact on image quality and diagnostic utility than preprocessing complexity, with StyleGAN2-ADA outperforming diffusion models in fidelity, and synthetic augmentation significantly boosting downstream classifier performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Revisiting the Broken Symmetry Phase of Solid Hydrogen: A Neural Network Variational Monte Carlo Study</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational physics], [neural network variational Monte Carlo, quantum Monte Carlo, deep neural network wave function, constant pressure ensemble, density functional theory, group-theoretical analysis]</li>
<li class=""><strong>authors:</strong> Shengdu Chai, Chen Lin, Xinyang Dong, Yuqiang Li, Wanli Ouyang, Lei Wang, X.C. Xie</li>
<li class=""><strong>institution:</strong> Fudan University, Shanghai Artificial Intelligence Laboratory, University of Oxford, Chinese Academy of Sciences, Peking University, Hefei National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17703" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17703</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper develops a neural network variational Monte Carlo method to treat electrons and nuclei quantum mechanically, revealing a new Cmcm crystal structure candidate for high-pressure solid hydrogen. This structure matches experimental data but is unstable in static DFT calculations, highlighting the need for full quantum many-body treatment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Domain-Aware Quantum Circuit for QML</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [quantum machine learning], [parameterized quantum circuits, domain-aware encoding, locality-preserving entanglement, barren plateau mitigation, DCT-style zigzag windows, encode-entangle-train cycles]</li>
<li class=""><strong>authors:</strong> Gurinder Singh, Thaddeus Pellegrini, Kenneth M. Merz Jr</li>
<li class=""><strong>institution:</strong> Cleveland Clinic, IBM Quantum</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17800" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17800</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a Domain-Aware Quantum Circuit (DAQC) for quantum machine learning, which uses image priors to guide locality-preserving encoding and entanglement via non-overlapping zigzag windows. The method mitigates barren plateaus and hardware noise, achieving performance on real quantum hardware competitive with strong classical neural networks for image classification tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical imaging], [image registration, radiomics, deep learning, logistic regression, feature selection]</li>
<li class=""><strong>authors:</strong> Rahul Ravi, Ruizhe Li, Tarek Abdelfatah, Stephen Chan, Xin Chen</li>
<li class=""><strong>institution:</strong> University of Nottingham, Nottingham City Hospital</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17759" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17759</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe1fee674daeeb2ff62e605cac35448ddb7b933f1a6948f0aa461b318710117_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe1fee674daeeb2ff62e605cac35448ddb7b933f1a6948f0aa461b318710117_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a framework using aligned longitudinal MRI and clinical data to predict breast cancer treatment response. The method involves tumor segmentation, image registration, and feature extraction, comparing radiomics and deep learning models. The main conclusion is that image registration significantly improves prediction, with radiomics features outperforming deep learning features in predicting pathologic complete response and relapse-free survival.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Fraud detection in credit card transactions using Quantum-Assisted Restricted Boltzmann Machines</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [quantum annealing, restricted boltzmann machine, qubo, quantum-assisted machine learning, fraud detection]</li>
<li class=""><strong>authors:</strong> João Marcos Cavalcanti de Albuquerque Neto, Gustavo Castro do Amaral, Guilherme Penello Temporão</li>
<li class=""><strong>institution:</strong> Pontifícia Universidade Católica do Rio de Janeiro, The Netherlands Organization for Applied Scientific Research (TNO)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17660" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17660</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43a1f86f2b2d05435580200ee936d437f042be18dc15953d8144f949af07568c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43a1f86f2b2d05435580200ee936d437f042be18dc15953d8144f949af07568c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates the use of Quantum-Assisted Restricted Boltzmann Machines (RBMs) for credit card fraud detection, applying quantum annealing to solve the QUBO problem derived from the RBM&#x27;s energy function. The method was tested on a real-world dataset of 145 million transactions. The results indicate that the quantum-assisted approach achieves superior performance compared to classical methods, even on current noisy quantum hardware.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning vertical coordinates via automatic differentiation of a dynamical core</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [automatic differentiation, neural network, terrain-following coordinates, differentiable dynamical core, NEUVE, Arakawa C-grid, non-hydrostatic Euler equations]</li>
<li class=""><strong>authors:</strong> Tim Whittaker, Seth Taylor, Elsa Cardoso-Bihlo, Alejandro Di Luca, Alex Bihlo</li>
<li class=""><strong>institution:</strong> Université du Québec à Montréal, University of Saskatchewan, Memorial University of Newfoundland</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17877" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17877</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a framework to learn vertical coordinates by integrating a parametric, neural network-based coordinate system (NEUVE) into a differentiable dynamical core for atmospheric modeling. Using automatic differentiation to compute exact geometric terms, the method optimizes the grid structure for physics and numerics. The learned coordinates reduce errors in benchmarks and eliminate spurious velocity patterns over steep topography.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [3D ConvNeXt, Global Response Normalization, depth scaling, width scaling, context scaling, supervised pretraining, volumetric segmentation]</li>
<li class=""><strong>authors:</strong> Saikat Roy, Yannick Kirchhoff, Constantin Ulrich, Maximillian Rokuss, Tassilo Wald, Fabian Isensee, Klaus Maier-Hein</li>
<li class=""><strong>institution:</strong> German Cancer Research Center (DKFZ), Heidelberg University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17774" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17774</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MedNeXt-v2, a compound-scaled 3D ConvNeXt architecture enhanced with a Global Response Normalization module for large-scale supervised representation learning in medical image segmentation. The authors demonstrate that scaling the backbone network&#x27;s architecture is crucial for effective pretraining, and MedNeXt-v2 achieves state-of-the-art performance when fine-tuned on diverse CT and MR benchmarks. The work establishes that a stronger backbone design yields better downstream segmentation results than simply increasing dataset size.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-23">2025-12-23<a href="#2025-12-23" class="hash-link" aria-label="Direct link to 2025-12-23" title="Direct link to 2025-12-23" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251223] Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hongji Li, Junchi yao, Manjiang Yu, Priyanka Singh, Xue Li, Di Wang, Lijie Hu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17911" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17911</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/218d6b0cd750a67af41f0ec36e744aed0a36e9ad83656c3a4c9ed70aa75b977d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/218d6b0cd750a67af41f0ec36e744aed0a36e9ad83656c3a4c9ed70aa75b977d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Irwindeep Singh, Sukhpal Singh Gill, Jinzhao Sun, Jan Mol</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17918" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17918</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ffe2289ec69b8a1b313e066cc4c4de736d5becc210f22958bfd52daff8ff5e6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ffe2289ec69b8a1b313e066cc4c4de736d5becc210f22958bfd52daff8ff5e6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Allison Li, Kristjan Greenewald, Thomas Parnell, Navid Azizan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17910" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17910</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69e52c70ff632453dafd08b1f3a3463c9d2df49fdb8300df0a3e128192436717_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69e52c70ff632453dafd08b1f3a3463c9d2df49fdb8300df0a3e128192436717_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Supplementary Resources and Analysis for Automatic Speech Recognition Systems Trained on the Loquacious Dataset</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nick Rossenbach, Robin Schmitt, Tina Raissi, Simon Berger, Larissa Kleppel, Ralf Schlüter</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17915" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17915</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6036cb69e2cb0f832a1b1088209441a1b5309cc94cf18961ca3b07bffec7a52c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6036cb69e2cb0f832a1b1088209441a1b5309cc94cf18961ca3b07bffec7a52c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Supplementary Resources and Analysis for Automatic Speech Recognition Systems Trained on the Loquacious Dataset</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] chatter: a Python library for applying information theory and AI/ML models to animal communication</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mason Youngblood</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17935" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17935</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03a1f96f640c75358ed76ff8b7bb2631f43b52386c0c7724516992109d54aa7a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03a1f96f640c75358ed76ff8b7bb2631f43b52386c0c7724516992109d54aa7a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> chatter: a Python library for applying information theory and AI/ML models to animal communication</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] What&#x27;s the Price of Monotonicity? A Multi-Dataset Benchmark of Monotone-Constrained Gradient Boosting for Credit PD</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Petr Koklev</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17945" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17945</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/564ec84544e92a29e57fb456987e278879803262a1ad7f11b5331621181314ae_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/564ec84544e92a29e57fb456987e278879803262a1ad7f11b5331621181314ae_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> What&#x27;s the Price of Monotonicity? A Multi-Dataset Benchmark of Monotone-Constrained Gradient Boosting for Credit PD</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Soheil Hashtarkhani, Brianna M. White, Benyamin Hoseini, David L. Schwartz, Arash Shaban-Nejad</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17934" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17934</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76d1304c99e1089257b9c3f4d81ee78901872f3818b6e4743be9843bd9a60488_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76d1304c99e1089257b9c3f4d81ee78901872f3818b6e4743be9843bd9a60488_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Bin Wang, Fadi Dornaika</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17954" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17954</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bf3a9a797cfd9bf487a467cfbf96c2912048bb9780e4ea911d8505de4a3fd09_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bf3a9a797cfd9bf487a467cfbf96c2912048bb9780e4ea911d8505de4a3fd09_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Gunho Park, Jeongin Bae, Byeongwook Kim, Baeseong park, Jiwon Ryu, Hoseung Kim, Se Jung Kwon, Dongsoo Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17970" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17970</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8eee89dc0a0f6c26eab8715e73b23d4aa516792d2237ec4f38c8323363f37c37_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8eee89dc0a0f6c26eab8715e73b23d4aa516792d2237ec4f38c8323363f37c37_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Parameter-Efficient Fine-Tuning for HAR: Integrating LoRA and QLoRA into Transformer Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Irina Seregina, Philippe Lalanda, German Vega</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17983" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17983</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/747125a80395e9d95ec80efcc81570ba4ba7205e4e2c8e9b485a5b5a991124d6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/747125a80395e9d95ec80efcc81570ba4ba7205e4e2c8e9b485a5b5a991124d6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Parameter-Efficient Fine-Tuning for HAR: Integrating LoRA and QLoRA into Transformer Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Convolutional-neural-operator-based transfer learning for solving PDEs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Peng Fan, Guofei Pang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17969" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17969</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b48d3312e2a3a21fec95790b71774b617dbbb16d924ea92ea392f72deaedd12_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b48d3312e2a3a21fec95790b71774b617dbbb16d924ea92ea392f72deaedd12_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Convolutional-neural-operator-based transfer learning for solving PDEs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MoE-TransMov: A Transformer-based Model for Next POI Prediction in Familiar &amp; Unfamiliar Movements</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ruichen Tan, Jiawei Xue, Kota Tsubouchi, Takahiro Yabe, Satish V. Ukkusuri</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17985" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17985</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a4ee269c2652d9f53e2d00acd67fb791427f4c088062380a3d842683837aff9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a4ee269c2652d9f53e2d00acd67fb791427f4c088062380a3d842683837aff9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MoE-TransMov: A Transformer-based Model for Next POI Prediction in Familiar &amp; Unfamiliar Movements</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shubham Kumar Nigam, Tanuj Tyagi, Siddharth Shukla, Aditya Kumar Guru, Balaramamahanthi Deepak Patnaik, Danush Khanna, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18014" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18014</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a0d10da8d503938592e2a709885e1a4ad114f85d7b47234084835f143d49cd6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a0d10da8d503938592e2a709885e1a4ad114f85d7b47234084835f143d49cd6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] FedOAED: Federated On-Device Autoencoder Denoiser for Heterogeneous Data under Limited Client Availability</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> S M Ruhul Kabir Howlader, Xiao Chen, Yifei Xie, Lu Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17986" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17986</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1e5e22c4360871dd251f35903ee0f2af43d4bf37326ef9aa7f4dba13f94916b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1e5e22c4360871dd251f35903ee0f2af43d4bf37326ef9aa7f4dba13f94916b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FedOAED: Federated On-Device Autoencoder Denoiser for Heterogeneous Data under Limited Client Availability</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shubham Kumar Nigam, Parjanya Aditya Shukla, Noel Shallum, Arnab Bhattacharya</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18004" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18004</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2ffa85f9a5ddd1bd5f673a211deae55a7bad9087838680e7b143e6daac52df8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2ffa85f9a5ddd1bd5f673a211deae55a7bad9087838680e7b143e6daac52df8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Omar Faruq Shikdar, Fahad Ahammed, B. M. Shahria Alam, Golam Kibria, Tawhidur Rahman, Nishat Tasnim Niloy</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17987" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17987</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e026b2918baadec02fb27d5fb57e2257968a98d72147f8159070f8c9fbb7d1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e026b2918baadec02fb27d5fb57e2257968a98d72147f8159070f8c9fbb7d1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Dataset and Benchmarks for Atrial Fibrillation Detection from Electrocardiograms of Intensive Care Unit Patients</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sarah Nassar, Nooshin Maghsoodi, Sophia Mannina, Shamel Addas, Stephanie Sibley, Gabor Fichtinger, David Pichora, David Maslove, Purang Abolmaesumi, Parvin Mousavi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18031" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18031</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea7240d48041f3aeed8ef43430092d594f522d0dd6c9a8de4d3b6236fccaebc9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea7240d48041f3aeed8ef43430092d594f522d0dd6c9a8de4d3b6236fccaebc9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Dataset and Benchmarks for Atrial Fibrillation Detection from Electrocardiograms of Intensive Care Unit Patients</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Towards Benchmarking Privacy Vulnerabilities in Selective Forgetting with Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wei Qian, Chenxu Zhao, Yangyi Li, Mengdi Huai</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18035" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18035</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3aaf9e957a407bb93f6576826db847a43f0675e8b765d8c6f38b7c40e06953c3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3aaf9e957a407bb93f6576826db847a43f0675e8b765d8c6f38b7c40e06953c3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards Benchmarking Privacy Vulnerabilities in Selective Forgetting with Large Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Hybrid Inductive-Transductive Network for Traffic Flow Imputation on Unsampled Locations</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mohammadmahdi Rahimiasl, Ynte Vanderhoydonc, Siegfried Mercelis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17984" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17984</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a712275d1fc83e7fd5ac0076a27da3a080f91e229b14209ff99a52de68ce9c2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a712275d1fc83e7fd5ac0076a27da3a080f91e229b14209ff99a52de68ce9c2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Hybrid Inductive-Transductive Network for Traffic Flow Imputation on Unsampled Locations</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Fakrul Islam Tushar, Ehsan Samei, Cynthia Rudin, Joseph Y. Lo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18038" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18038</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7823e2e22739636a655f10a5159ed96eedc679b68020943f440020f0658dad87_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7823e2e22739636a655f10a5159ed96eedc679b68020943f440020f0658dad87_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Narrative Consolidation: Formulating a New Task for Unifying Multi-Perspective Accounts</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Roger A. Finger, Eduardo G. Cortes, Sandro J. Rigo, Gabriel de O. Ramos</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18041" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18041</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/405c807df8e2ce95661469b072db6898b3dcf1bb175cd81aa868ecc9d2c06c12_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/405c807df8e2ce95661469b072db6898b3dcf1bb175cd81aa868ecc9d2c06c12_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Narrative Consolidation: Formulating a New Task for Unifying Multi-Perspective Accounts</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Probabilistic Digital Twins of Users: Latent Representation Learning with Statistically Validated Semantics</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Daniel David</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18056" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18056</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12301f0dbb41071430f34a9014134b625a48eecd1e6b2f0d69403405addc6d7d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12301f0dbb41071430f34a9014134b625a48eecd1e6b2f0d69403405addc6d7d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Probabilistic Digital Twins of Users: Latent Representation Learning with Statistically Validated Semantics</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] FOODER: Real-time Facial Authentication and Expression Recognition</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sabri Mustafa Kahya, Muhammet Sami Yavuz, Boran Hamdi Sivrikaya, Eckehard Steinbach</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18057" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18057</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17048a3dede1c165a0f52aab61de33bef5559518cf2996ad61858f9a9d680457_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17048a3dede1c165a0f52aab61de33bef5559518cf2996ad61858f9a9d680457_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FOODER: Real-time Facial Authentication and Expression Recognition</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Approximation and learning with compositional tensor trains</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Martin Eigel, Charles Miranda, Anthony Nouy, David Sommer</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18059" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18059</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91f6bb6272d1b93753642e31936e179de5889b19456b73cd0d2d98f6cb87c979_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91f6bb6272d1b93753642e31936e179de5889b19456b73cd0d2d98f6cb87c979_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Approximation and learning with compositional tensor trains</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Graph-based Nearest Neighbors with Dynamic Updates via Random Walks</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nina Mishra, Yonatan Naamad, Tal Wagner, Lichen Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18060" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18060</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09ac9385682bd6e4b07b769afe016fb50f71be5c2b6656f027de335845a16aed_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09ac9385682bd6e4b07b769afe016fb50f71be5c2b6656f027de335845a16aed_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Graph-based Nearest Neighbors with Dynamic Updates via Random Walks</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ge Yan, Tuomas Oikarinen, Tsui-Wei, Weng</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18092" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18092</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7b2abff42613036ddb63e979f8be79c1123b50e037d39defec5292f1a3eb175_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7b2abff42613036ddb63e979f8be79c1123b50e037d39defec5292f1a3eb175_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] From Coverage to Causes: Data-Centric Fuzzing for JavaScript Engines</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kishan Kumar Ganguly, Tim Menzies</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18102" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18102</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/509a927741960d2c63062790fd8bebe6a6a22769c29f6985c39fb001039d9fab_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/509a927741960d2c63062790fd8bebe6a6a22769c29f6985c39fb001039d9fab_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> From Coverage to Causes: Data-Centric Fuzzing for JavaScript Engines</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Microstructure-based Variational Neural Networks for Robust Uncertainty Quantification in Materials Digital Twins</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Andreas E. Robertson, Samuel B. Inman, Ashley T. Lenau, Ricardo A. Lebensohn, Dongil Shin, Brad L. Boyce, Remi M. Dingreville</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18104" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18104</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd4c8fb93b00e9d3df0d57fa728955a837f9e46dce9a1fe1000acf82c6b0a1f2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd4c8fb93b00e9d3df0d57fa728955a837f9e46dce9a1fe1000acf82c6b0a1f2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Microstructure-based Variational Neural Networks for Robust Uncertainty Quantification in Materials Digital Twins</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] TraCeR: Transformer-Based Competing Risk Analysis with Longitudinal Covariates</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Maxmillan Ries, Sohan Seth</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18129" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18129</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a81b46d02b46e72f2027e054c761a825748caf485414255423ed130f0887b330_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a81b46d02b46e72f2027e054c761a825748caf485414255423ed130f0887b330_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TraCeR: Transformer-Based Competing Risk Analysis with Longitudinal Covariates</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jie Yang, Rui Zhang, Ziyang Cheng, Dawei Cheng, Guang Yang, Bo Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18133" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18133</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b8052788abbad008c8487b789c1968d0448bbef8cdd15ad400f0c6303c23505_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b8052788abbad008c8487b789c1968d0448bbef8cdd15ad400f0c6303c23505_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Learning Generalizable Neural Operators for Inverse Problems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Adam J. Thorpe, Stepan Tretiakov, Dibakar Roy Sarkar, Krishna Kumar, Ufuk Topcu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18120" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18120</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e045cf5e1b4d1841db31e3e3cbb272846f23e43ba557e1455491e138f9aa045_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e045cf5e1b4d1841db31e3e3cbb272846f23e43ba557e1455491e138f9aa045_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning Generalizable Neural Operators for Inverse Problems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rupanshu Soi, Rohan Yadav, Fredrik Kjolstad, Alex Aiken, Maryam Mehri Dehnavi, Michael Garland, Michael Bauer</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18134" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18134</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cfd081b538bd7f4d1c91e06ba3fae36d2a230ad65cf3fc2e5160c3bdd9d98b3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cfd081b538bd7f4d1c91e06ba3fae36d2a230ad65cf3fc2e5160c3bdd9d98b3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Lena Libon, Meghana Bhange, Rushabh Solanki, Elliot Creager, Ulrich Aïvodji</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18174" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18174</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd7baf972d08bf5c53e0a32c68fda68879e3c2febf3407ecf6537a3fcd6d36fd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd7baf972d08bf5c53e0a32c68fda68879e3c2febf3407ecf6537a3fcd6d36fd_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] FairExpand: Individual Fairness on Graphs with Partial Similarity Information</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rebecca Salganik, Yibin Wang, Guillaume Salha-Galvan, Jian Kang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18180" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18180</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/865ccd9cabcf2b700230e9c10d5db5bbd94993cb8f81acdca83db374c99156b8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/865ccd9cabcf2b700230e9c10d5db5bbd94993cb8f81acdca83db374c99156b8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FairExpand: Individual Fairness on Graphs with Partial Similarity Information</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jian Yan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18190" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18190</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebbd501809c39f1f08745f358ebdf2df886f93b7202aeedbd613476ffb27866b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebbd501809c39f1f08745f358ebdf2df886f93b7202aeedbd613476ffb27866b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] When Does Learning Renormalize? Sufficient Conditions for Power Law Spectral Dynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yizhou Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18209" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18209</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5b09e099602878faf6cf44441a1e029c64f15985e3d209f1875434cd54da61a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5b09e099602878faf6cf44441a1e029c64f15985e3d209f1875434cd54da61a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> When Does Learning Renormalize? Sufficient Conditions for Power Law Spectral Dynamics</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Stable and Efficient Single-Rollout RL for Multimodal Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rui Liu, Dian Yu, Lei Ke, Haolin Liu, Yujun Zhou, Zhenwen Liang, Haitao Mi, Pratap Tokekar, Dong Yu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18215" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18215</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81e83852cf5de78a73c53dc039a80d4328420c326e71217ed656de7348457003_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81e83852cf5de78a73c53dc039a80d4328420c326e71217ed656de7348457003_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Stable and Efficient Single-Rollout RL for Multimodal Reasoning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Toward Efficient Testing of Graph Neural Networks via Test Input Prioritization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Lichen Yang, Qiang Wang, Zhonghao Yang, Daojing He, Yu Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18228" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18228</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9987a54aec669e601a71ba8b0ee7d5434ba0c73d08a66e20538cf347a09669cd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9987a54aec669e601a71ba8b0ee7d5434ba0c73d08a66e20538cf347a09669cd_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Toward Efficient Testing of Graph Neural Networks via Test Input Prioritization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] AutoSchA: Automatic Hierarchical Music Representations via Multi-Relational Node Isolation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Stephen Ni-Hahn, Rico Zhu, Jerry Yin, Yue Jiang, Cynthia Rudin, Simon Mak</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18232" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18232</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea758b7b2303802fc5c9547c032b22177a2accdeac3b36caed4230425fda6efa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea758b7b2303802fc5c9547c032b22177a2accdeac3b36caed4230425fda6efa_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AutoSchA: Automatic Hierarchical Music Representations via Multi-Relational Node Isolation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Offline Behavioral Data Selection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shiye Lei, Zhihao Cheng, Dacheng Tao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18246" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18246</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02b464fb04b52640bf81b61d9a535a31cd920c00fd8a2dd34cd4b261366adac1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02b464fb04b52640bf81b61d9a535a31cd920c00fd8a2dd34cd4b261366adac1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Offline Behavioral Data Selection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Dimensionality Reduction Considered Harmful (Some of the Time)</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hyeon Jeon</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18230" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18230</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6c2d73af1d47933ab3fe41a060a175bd40e436f32af5added065941e07d0688_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6c2d73af1d47933ab3fe41a060a175bd40e436f32af5added065941e07d0688_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dimensionality Reduction Considered Harmful (Some of the Time)</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] On the Convergence Rate of LoRA Gradient Descent</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Siqiao Mu, Diego Klabjan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18248" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18248</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f8cfafc1ac3ad3a56d892c47d02b17599c178471d68de77c5fdb73874a8fc10_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f8cfafc1ac3ad3a56d892c47d02b17599c178471d68de77c5fdb73874a8fc10_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> On the Convergence Rate of LoRA Gradient Descent</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] LeJOT: An Intelligent Job Cost Orchestration Solution for Databricks Platform</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Lizhi Ma, Yi-Xiang Hu, Yuke Wang, Yifang Zhao, Yihui Ren, Jian-Xiang Liao, Feng Wu, Xiang-Yang Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18266" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18266</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de0debec5ee9990221746ff49e0aa9313a4766307624a21ded5312ce8127844d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de0debec5ee9990221746ff49e0aa9313a4766307624a21ded5312ce8127844d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LeJOT: An Intelligent Job Cost Orchestration Solution for Databricks Platform</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] FedSUM Family: Efficient Federated Learning Methods under Arbitrary Client Participation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Runze You, Shi Pu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18275" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18275</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/075e8765f18ed2a89f21c4b5c0db9ea968dad78f6fb6aca6d0ea6c6135aadcde_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/075e8765f18ed2a89f21c4b5c0db9ea968dad78f6fb6aca6d0ea6c6135aadcde_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FedSUM Family: Efficient Federated Learning Methods under Arbitrary Client Participation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] AL-GNN: Privacy-Preserving and Replay-Free Continual Graph Learning via Analytic Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xuling Zhang, Jindong Li, Yifei Zhang, Menglin Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18295" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18295</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d219b11c5c859da88d8f68475d0f7f0705331024e0e77eb5124bfa1124849df9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d219b11c5c859da88d8f68475d0f7f0705331024e0e77eb5124bfa1124849df9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AL-GNN: Privacy-Preserving and Replay-Free Continual Graph Learning via Analytic Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Harsh Rathva, Ojas Srivastava, Pruthwik Mishra</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18309" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18309</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c69f269f400e588bb27a40eea78dc7e63a0f8f1b86f9ab29abf255d7a91b0308_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c69f269f400e588bb27a40eea78dc7e63a0f8f1b86f9ab29abf255d7a91b0308_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Vincent Bezold, Patrick Wagner, Jakob Hofmann, Marco Huber, Alexander Sauer</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18317" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18317</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c40f961221ca3df8f5c9388e76d344938990a3d405e09580d23abf68a4da0f57_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c40f961221ca3df8f5c9388e76d344938990a3d405e09580d23abf68a4da0f57_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Youssef Mahran, Zeyad Gamal, Ayman El-Badawy</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18333" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18333</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f5c0778f6eace42568ad8fc221a69e1cf4360df09bb1bb286e34299351febe0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f5c0778f6eace42568ad8fc221a69e1cf4360df09bb1bb286e34299351febe0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A two-stream network with global-local feature fusion for bone age assessment</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Qiong Lou, Han Yang, Fang Lu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18331" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18331</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d26c3d7fded2a10e84ebad25bc6bb1810428c48d125c9ad4b3a401fe14ca66d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d26c3d7fded2a10e84ebad25bc6bb1810428c48d125c9ad4b3a401fe14ca66d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A two-stream network with global-local feature fusion for bone age assessment</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Youssef Mahran, Zeyad Gamal, Ayman El-Badawy</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18336" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18336</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67d046b180bb4bec9247b6bb525bd30b5814a0c5e4673bd03aea3eb64b9797e7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67d046b180bb4bec9247b6bb525bd30b5814a0c5e4673bd03aea3eb64b9797e7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Towards Guided Descent: Optimization Algorithms for Training Neural Networks At Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ansh Nagwekar</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18373" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18373</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a52d6d3b760cd69f9655054febc798829d4e703cf29f84c34244775c0311631_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a52d6d3b760cd69f9655054febc798829d4e703cf29f84c34244775c0311631_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards Guided Descent: Optimization Algorithms for Training Neural Networks At Scale</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Neural Proofs for Sound Verification and Control of Complex Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Alessandro Abate</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18389" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18389</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d4271cbf46eee03fa72db8edd09dea5b0753448d820460e4c09f1171c7bc8f8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d4271cbf46eee03fa72db8edd09dea5b0753448d820460e4c09f1171c7bc8f8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Neural Proofs for Sound Verification and Control of Complex Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] The Challenger: When Do New Data Sources Justify Switching Machine Learning Models?</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Vassilis Digalakis Jr, Christophe Pérignon, Sébastien Saurin, Flore Sentenac</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18390" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18390</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97c5a36d8f48f4ca89c4a34600dea33569d95e61a50916599a5cc161e183185a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97c5a36d8f48f4ca89c4a34600dea33569d95e61a50916599a5cc161e183185a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The Challenger: When Do New Data Sources Justify Switching Machine Learning Models?</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Automated Mosaic Tesserae Segmentation via Deep Learning Techniques</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Charilaos Kapelonis, Marios Antonakakis, Konstantinos Politof, Aristomenis Antoniadis, Michalis Zervakis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18406" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18406</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6893130bacdfd82f19e38a48fe6c1a981e8a6e139face8bc5ca4379fb98ab0c8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6893130bacdfd82f19e38a48fe6c1a981e8a6e139face8bc5ca4379fb98ab0c8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Automated Mosaic Tesserae Segmentation via Deep Learning Techniques</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Why Most Optimism Bandit Algorithms Have the Same Regret Analysis: A Simple Unifying Theorem</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Vikram Krishnamurthy</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18409" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18409</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb98e65517a8f8628860576c4228301198ae634fca16d11c5f0a252aee09a842_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb98e65517a8f8628860576c4228301198ae634fca16d11c5f0a252aee09a842_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Why Most Optimism Bandit Algorithms Have the Same Regret Analysis: A Simple Unifying Theorem</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Badr Moufad, Navid Bagheri Shouraki, Alain Oliviero Durmus, Thomas Hirtz, Eric Moulines, Jimmy Olsson, Yazid Janati</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18365" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18365</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb10f5246f2791c1a5c5d83727bb85643a1c89da44e041633e646e724bfa1d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb10f5246f2791c1a5c5d83727bb85643a1c89da44e041633e646e724bfa1d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MoE Pathfinder: Trajectory-driven Expert Pruning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xican Yang, Yuanhe Tian, Yan Song</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18425" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18425</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c828ca8ec3754ef48c892048aa8de845b7afe7c78f324073b3d3bb8afcdfc08_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c828ca8ec3754ef48c892048aa8de845b7afe7c78f324073b3d3bb8afcdfc08_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MoE Pathfinder: Trajectory-driven Expert Pruning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] On the Universality of Transformer Architectures; How Much Attention Is Enough?</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Amirreza Abbasi, Mohsen Hooshmand</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18445" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18445</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e732525ad7eeb9bd777c68adaa24b4e1209e8441252397e0784b8ce41a3a00d7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e732525ad7eeb9bd777c68adaa24b4e1209e8441252397e0784b8ce41a3a00d7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> On the Universality of Transformer Architectures; How Much Attention Is Enough?</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jayant Lohia</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18453" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18453</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e4686d2a1c44f6a0f1d346c2e5401709f3d974dfd9062781f00cc8eb5d71952_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e4686d2a1c44f6a0f1d346c2e5401709f3d974dfd9062781f00cc8eb5d71952_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Secret mixtures of experts inside your LLM</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Enric Boix-Adsera</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18452" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18452</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bef90d8febf87f4438ed38a790cb9675b92bf541f58daace4d8c67f4e7b28a1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bef90d8febf87f4438ed38a790cb9675b92bf541f58daace4d8c67f4e7b28a1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Secret mixtures of experts inside your LLM</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Out-of-Distribution Detection in Molecular Complexes via Diffusion Models for Irregular Graphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> David Graber, Victor Armegioiu, Rebecca Buller, Siddhartha Mishra</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18454" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18454</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a2eb80d49254700f260ff3967d335e1d51ecb254bd1a0020cef4b670cd5e329_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a2eb80d49254700f260ff3967d335e1d51ecb254bd1a0020cef4b670cd5e329_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Out-of-Distribution Detection in Molecular Complexes via Diffusion Models for Irregular Graphs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Christopher Román Jaimes</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18462" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18462</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27cc701f0cd5020bdc7452f202a07cee2cdb12b9d80e26c528184ec53ea76847_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27cc701f0cd5020bdc7452f202a07cee2cdb12b9d80e26c528184ec53ea76847_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Self-organizing maps for water quality assessment in reservoirs and lakes: A systematic literature review</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Oraib Almegdadi, João Marcelino, Sarah Fakhreddine, João Manso, Nuno C. Marques</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18466" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18466</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4bfdf888d3e80d2b65bbc818e6d4aa3a319033086b834ed685478049ddb17232_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4bfdf888d3e80d2b65bbc818e6d4aa3a319033086b834ed685478049ddb17232_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Self-organizing maps for water quality assessment in reservoirs and lakes: A systematic literature review</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] The Geometry of Abstraction: Continual Learning via Recursive Quotienting</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xin Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18471" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18471</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/946bc88c1851af1d701b7d19272c910b4e964e6d83ae04fa6fbdbf0f215f6e75_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/946bc88c1851af1d701b7d19272c910b4e964e6d83ae04fa6fbdbf0f215f6e75_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The Geometry of Abstraction: Continual Learning via Recursive Quotienting</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Research on a hybrid LSTM-CNN-Attention model for text-based web content classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mykola Kuz, Ihor Lazarovych, Mykola Kozlenko, Mykola Pikuliak, Andrii Kvasniuk</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18475" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18475</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6d6cee54c1b292cf87f190d497421a3a323bc276532b71bc07f75f60e88c9a5d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6d6cee54c1b292cf87f190d497421a3a323bc276532b71bc07f75f60e88c9a5d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Research on a hybrid LSTM-CNN-Attention model for text-based web content classification</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] APC-GNN++: An Adaptive Patient-Centric GNN with Context-Aware Attention and Mini-Graph Explainability for Diabetes Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Khaled Berkani</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18473" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18473</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/917648447e1450eccba6ff12938452166cbb307d588b3036ddd72e80601da793_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/917648447e1450eccba6ff12938452166cbb307d588b3036ddd72e80601da793_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> APC-GNN++: An Adaptive Patient-Centric GNN with Context-Aware Attention and Mini-Graph Explainability for Diabetes Classification</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Santwana Sagnika, Manav Malhotra, Ishtaj Kaur Deol, Soumyajit Roy, Swarnav Kumar</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18500" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18500</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3d3024bb6cf729a0d0827d13bba185e065be8eb34bd4dffa40d58782bc4e5ab_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3d3024bb6cf729a0d0827d13bba185e065be8eb34bd4dffa40d58782bc4e5ab_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] NASTaR: NovaSAR Automated Ship Target Recognition Dataset</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Benyamin Hosseiny, Kamirul Kamirul, Odysseas Pappas, Alin Achim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18503" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18503</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4ac29e833bc5eaed0a9881e81dd877d55f600f6bd8722c27c1466b58afc1378_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4ac29e833bc5eaed0a9881e81dd877d55f600f6bd8722c27c1466b58afc1378_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NASTaR: NovaSAR Automated Ship Target Recognition Dataset</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Prediction and Forecast of Short-Term Drought Impacts Using Machine Learning to Support Mitigation and Adaptation Efforts</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hatim M. E. Geli, Islam Omar, Mona Y. Elshinawy, David W. DuBios, Lara Prehodko, Kelly H Smith, Abdel-Hameed A. Badawy</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18522" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18522</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f61621a97ff1a65e5c71fb89cbb1d94dcdecafcb5694c0379f084f31d850ee0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f61621a97ff1a65e5c71fb89cbb1d94dcdecafcb5694c0379f084f31d850ee0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Prediction and Forecast of Short-Term Drought Impacts Using Machine Learning to Support Mitigation and Adaptation Efforts</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Feature-Enhanced Graph Neural Networks for Classification of Synthetic Graph Generative Models: A Benchmarking Study</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Janek Dyer, Jagdeep Ahluwalia, Javad Zarrin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18524" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18524</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08031a1b54db55f30489d768a136dda2ef1e6ae4c24a0e92ea56eefd2fef4532_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08031a1b54db55f30489d768a136dda2ef1e6ae4c24a0e92ea56eefd2fef4532_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Feature-Enhanced Graph Neural Networks for Classification of Synthetic Graph Generative Models: A Benchmarking Study</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Generalization Gaps in Political Fake News Detection: An Empirical Study on the LIAR Dataset</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> S Mahmudul Hasan, Shaily Roy, Akib Jawad Nafis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18533" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18533</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ae98cacefd250f660449f7354216ce0a7f9d0395ed396c825595eada053e771_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ae98cacefd250f660449f7354216ce0a7f9d0395ed396c825595eada053e771_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generalization Gaps in Political Fake News Detection: An Empirical Study on the LIAR Dataset</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Scaling up Stability: Reinforcement Learning for Distributed Control of Networked Systems in the Space of Stabilizing Policies</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> John Cao, Luca Furieri</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18540" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18540</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43fe9d2dae8cd7dd8a3ae293cabf5bd434385dd8877da1630a601a47c5dc1a7f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43fe9d2dae8cd7dd8a3ae293cabf5bd434385dd8877da1630a601a47c5dc1a7f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Scaling up Stability: Reinforcement Learning for Distributed Control of Networked Systems in the Space of Stabilizing Policies</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Scott Thornton</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18542" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18542</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2afb4d99182c71c26adf649cc513b4f7ffee3c07f215e3f4067f2ce9fa660fa0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2afb4d99182c71c26adf649cc513b4f7ffee3c07f215e3f4067f2ce9fa660fa0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Toward Training Superintelligent Software Agents through Self-Play SWE-RL</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuxiang Wei, Zhiqing Sun, Emily McMilin, Jonas Gehring, David Zhang, Gabriel Synnaeve, Daniel Fried, Lingming Zhang, Sida Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18552" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18552</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0bcd15393eed2ab163719da9a9e1954f5b23176404f9ad291a7ddc746dc5dd6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0bcd15393eed2ab163719da9a9e1954f5b23176404f9ad291a7ddc746dc5dd6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Toward Training Superintelligent Software Agents through Self-Play SWE-RL</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Effiong Blessing, Chiung-Yi Tseng, Somshubhra Roy, Junaid Rehman, Isaac Nkrumah</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18575" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18575</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab099f7e2d96fe21b9b811feaa87d815fe23feb7bea213071f724ebc69a87414_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab099f7e2d96fe21b9b811feaa87d815fe23feb7bea213071f724ebc69a87414_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sumaiya Ali, Areej Alhothali, Ohoud Alzamzami, Sameera Albasri, Ahmed Abduljabbar, Muhammad Alwazzan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18573" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18573</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/387c39b67caa5e84daf0327dfb4c7a948d6d72c292a3404e6c7e8a2bcd6db7df_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/387c39b67caa5e84daf0327dfb4c7a948d6d72c292a3404e6c7e8a2bcd6db7df_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pengcheng Li, Qiang Fang, Tong Zhao, Yixing Lan, Xin Xu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18583" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18583</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fdabaf80e15fc440c53464fb3134095f9121f39b7d6d83850d4cb921dec3fda_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fdabaf80e15fc440c53464fb3134095f9121f39b7d6d83850d4cb921dec3fda_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Comparing Dynamical Models Through Diffeomorphic Vector Field Alignment</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ruiqi Chen, Giacomo Vedovati, Todd Braver, ShiNung Ching</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18566" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18566</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17e1231f344e9051e9a9124b095324f59b3035dd2a93ac7e3bdf19d86c6eec28_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17e1231f344e9051e9a9124b095324f59b3035dd2a93ac7e3bdf19d86c6eec28_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Comparing Dynamical Models Through Diffeomorphic Vector Field Alignment</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] EIA-SEC: Improved Actor-Critic Framework for Multi-UAV Collaborative Control in Smart Agriculture</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Quanxi Zhou, Wencan Mao, Yilei Liang, Manabu Tsukada, Yunling Liu, Jon Crowcroft</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18596" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18596</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/856f913f2cb31f98d005287343fc242bd36ba508c56e6d8fbbbb68c073df2a30_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/856f913f2cb31f98d005287343fc242bd36ba508c56e6d8fbbbb68c073df2a30_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EIA-SEC: Improved Actor-Critic Framework for Multi-UAV Collaborative Control in Smart Agriculture</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Benchmarking neural surrogates on realistic spatiotemporal multiphysics flows</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Runze Mao, Rui Zhang, Xuan Bai, Tianhao Wu, Teng Zhang, Zhenyi Chen, Minqi Lin, Bocheng Zeng, Yangchen Xu, Yingxuan Xiang, Haoze Zhang, Shubham Goswami, Pierre A. Dawe, Yifan Xu, Zhenhua An, Mengtao Yan, Xiaoyi Lu, Yi Wang, Rongbo Bai, Haobu Gao, Xiaohang Fang, Han Li, Hao Sun, Zhi X. Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18595" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18595</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54fa7968484f2312fe5e5490bcb221fdffbad35b3d30834c98660d094a86bdd2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54fa7968484f2312fe5e5490bcb221fdffbad35b3d30834c98660d094a86bdd2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Benchmarking neural surrogates on realistic spatiotemporal multiphysics flows</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wencan Mao, Quanxi Zhou, Tomas Couso Coddou, Manabu Tsukada, Yunling Liu, Yusheng Ji</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18604" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18604</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9df5c14f10e3a4031cb92513653314edaf2d17ccb1f345c4f9e818b04c5c9203_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9df5c14f10e3a4031cb92513653314edaf2d17ccb1f345c4f9e818b04c5c9203_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rongyao Cai, Yuxi Wan, Kexin Zhang, Ming Jin, Hao Wang, Zhiqiang Ge, Daoyi Dong, Yong Liu, Qingsong Wen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18610" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18610</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a15caebdaadc9b61bcc30ac1fa7534d1efe4cc5ec0d9274e6b2bd1e89b75d13_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a15caebdaadc9b61bcc30ac1fa7534d1efe4cc5ec0d9274e6b2bd1e89b75d13_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] The Interaction Bottleneck of Deep Neural Networks: Discovery, Proof, and Modulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Huiqi Deng, Qihan Ren, Zhuofan Chen, Zhenyuan Cui, Wen Shen, Peng Zhang, Hongbin Pei, Quanshi Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18607" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18607</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30ab53152f4d22782382d142896ab1eaf1182b02d53770517901f257b002cc1f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30ab53152f4d22782382d142896ab1eaf1182b02d53770517901f257b002cc1f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The Interaction Bottleneck of Deep Neural Networks: Discovery, Proof, and Modulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ARC: Leveraging Compositional Representations for Cross-Problem Learning on VRPs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Han-Seul Jeong, Youngjoon Park, Hyungseok Song, Woohyung Lim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18633" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18633</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abc675e475f6d219e02688fea9461fecf46d82b6729bd20d29accd9c95cc967f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abc675e475f6d219e02688fea9461fecf46d82b6729bd20d29accd9c95cc967f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ARC: Leveraging Compositional Representations for Cross-Problem Learning on VRPs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] From Shortcut to Induction Head: How Data Diversity Shapes Algorithm Selection in Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ryotaro Kawata, Yujin Song, Alberto Bietti, Naoki Nishikawa, Taiji Suzuki, Samuel Vaiter, Denny Wu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18634" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18634</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e54fb36cb8dc15a87cb11d651f2fd0a36426f91acc811072a8e23c04544cd8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e54fb36cb8dc15a87cb11d651f2fd0a36426f91acc811072a8e23c04544cd8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> From Shortcut to Induction Head: How Data Diversity Shapes Algorithm Selection in Transformers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pengxiang Ouyang, Qing Ma, Zheng Wang, Cong Bai</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18660" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18660</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06fe8e37dc2d1d1d6a5d48298128004f86b1be4d6ecbeca32f5c72786b1a535c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06fe8e37dc2d1d1d6a5d48298128004f86b1be4d6ecbeca32f5c72786b1a535c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Improving Pattern Recognition of Scheduling Anomalies through Structure-Aware and Semantically-Enhanced Graphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ning Lyu, Junjie Jiang, Lu Chang, Chihui Shao, Feng Chen, Chong Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18673" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18673</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c16637936bf2055d30084813c6afcafc16bc9cf66165235e01a57ad34447eb0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c16637936bf2055d30084813c6afcafc16bc9cf66165235e01a57ad34447eb0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Improving Pattern Recognition of Scheduling Anomalies through Structure-Aware and Semantically-Enhanced Graphs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Task Vector in TTS: Toward Emotionally Expressive Dialectal Speech Synthesis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pengchao Feng, Yao Xiao, Ziyang Ma, Zhikang Niu, Shuai Fan, Yao Li, Sheng Wang, Xie Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18699" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18699</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6001c759cfdab0e5242e7ed2d8eff42f898cd26dbf0e8845df8e26a3c8936e15_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6001c759cfdab0e5242e7ed2d8eff42f898cd26dbf0e8845df8e26a3c8936e15_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Task Vector in TTS: Toward Emotionally Expressive Dialectal Speech Synthesis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Generating Risky Samples with Conformity Constraints via Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Han Yu, Hao Zou, Xingxuan Zhang, Zhengyi Wang, Yue He, Kehan Li, Peng Cui</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18722" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18722</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccd2af87861ffba1a1a1172552bec8a5a2c9fb4db73b173801ff6c5db0f1734b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccd2af87861ffba1a1a1172552bec8a5a2c9fb4db73b173801ff6c5db0f1734b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generating Risky Samples with Conformity Constraints via Diffusion Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xue Yang, Michael Schukat, Junlin Lu, Patrick Mannion, Karl Mason, Enda Howley</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18670" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18670</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f001f825ba30968846f540ea46b7c77510d728bd232f6fe2aedc626451956364_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f001f825ba30968846f540ea46b7c77510d728bd232f6fe2aedc626451956364_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhiquan Tan, Yinrong Hong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18730" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18730</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726f00d4aa0c339e8cdaf688c05c28499bf469ac19b055e49f249d532aa40b2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726f00d4aa0c339e8cdaf688c05c28499bf469ac19b055e49f249d532aa40b2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ML Inference Scheduling with Predictable Latency</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Haidong Zhao, Nikolaos Georgantas</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18725" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18725</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a8d1548e228be441a72232fe96db7a46d1c28ad7f5a73b47e37580f3b42dadf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a8d1548e228be441a72232fe96db7a46d1c28ad7f5a73b47e37580f3b42dadf_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ML Inference Scheduling with Predictable Latency</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiangrui Cai, Shaocheng Ma, Lei Cao, Jie Li, Tianyu Liu, Yilin Dong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18689" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18689</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2376b865ddc12cc20f97bb00013197494f3e12cba34f9079248457bb11fb7eab_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2376b865ddc12cc20f97bb00013197494f3e12cba34f9079248457bb11fb7eab_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zichuan Lin, Xiaokai Huang, Jiate Liu, Yuxuan Han, Jia Chen, Xiapeng Wu, Deheng Ye</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18737" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18737</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1c6978b6e1b267d6a0dd6bed5b258ad165849282cb8c0a6f4f89c449c2dfc2a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1c6978b6e1b267d6a0dd6bed5b258ad165849282cb8c0a6f4f89c449c2dfc2a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chainarong Amornbunchornvej</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18732" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18732</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/971d2a8c016b462ec6480b43e3bb4defeb91225b526df8895e9702f727c64232_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/971d2a8c016b462ec6480b43e3bb4defeb91225b526df8895e9702f727c64232_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Is Your Conditional Diffusion Model Actually Denoising?</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Daniel Pfrommer, Zehao Dou, Christopher Scarvelis, Max Simchowitz, Ali Jadbabaie</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18736" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18736</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/856ef1fcb2e79aa2ab4b2a022b1c3d13580d56e426a05e9b3d88850160ea2eac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/856ef1fcb2e79aa2ab4b2a022b1c3d13580d56e426a05e9b3d88850160ea2eac_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Is Your Conditional Diffusion Model Actually Denoising?</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Minh Vu, Konstantinos Slavakis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18763" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18763</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e19f67ca81a26c636db40a8c5fd0bedfcf549bc2e97dbcd90530ca1de4a7f861_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e19f67ca81a26c636db40a8c5fd0bedfcf549bc2e97dbcd90530ca1de4a7f861_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kaican Li, Lewei Yao, Jiannan Wu, Tiezheng Yu, Jierun Chen, Haoli Bai, Lu Hou, Lanqing Hong, Wei Zhang, Nevin L. Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18745" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18745</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87556fdc09b55b65c0d472d205a384cc42453256afeb9e7a28db98178fe1d145_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87556fdc09b55b65c0d472d205a384cc42453256afeb9e7a28db98178fe1d145_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Fanis Mathioulakis, Gorjan Radevski, Tinne Tuytelaars</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18784" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18784</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0254e0e9393c18241de33ec503d4cc8d75622e94c6a0d08ec4ee16da3e3b6b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0254e0e9393c18241de33ec503d4cc8d75622e94c6a0d08ec4ee16da3e3b6b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Label-Informed Outlier Detection Based on Granule Density</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Baiyang Chen, Zhong Yuan, Dezhong Peng, Hongmei Chen, Xiaomin Song, Huiming Zheng</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18774" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18774</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f27f829438adaa9d5eb3ec20d666226593c7dc2a412469b585dd40c6517ab3e3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f27f829438adaa9d5eb3ec20d666226593c7dc2a412469b585dd40c6517ab3e3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Label-Informed Outlier Detection Based on Granule Density</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Hyperbolic Graph Embeddings: a Survey and an Evaluation on Anomaly Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Souhail Abdelmouaiz Sadat, Mohamed Yacine Touahria Miliani, Khadidja Hab El Hames, Hamida Seba, Mohammed Haddad</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18826" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18826</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1947d93734df64c6e62ffe5a621cdb9199875dcacb08de1203cadabf9bce52e3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1947d93734df64c6e62ffe5a621cdb9199875dcacb08de1203cadabf9bce52e3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Hyperbolic Graph Embeddings: a Survey and an Evaluation on Anomaly Detection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Controllable Probabilistic Forecasting with Stochastic Decomposition Layers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> John S. Schreck, William E. Chapman, Charlie Becker, David John Gagne II, Dhamma Kimpara, Nihanth Cherukuru, Judith Berner, Kirsten J. Mayer, Negin Sobhani</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18815" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18815</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b40f77d2965a2b21082e13c0dc95074d21866006415db1b08905e24b2234e5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b40f77d2965a2b21082e13c0dc95074d21866006415db1b08905e24b2234e5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Controllable Probabilistic Forecasting with Stochastic Decomposition Layers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Generative Modeling through Spectral Analysis of Koopman Operator</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuanchao Xu, Fengyi Li, Masahiro Fujisawa, Youssef Marzouk, Isao Ishikawa</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18837" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18837</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3838b816a163dddae4907465cfe37d64e1f909b2317422f86a8a2c6ad7e98898_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3838b816a163dddae4907465cfe37d64e1f909b2317422f86a8a2c6ad7e98898_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generative Modeling through Spectral Analysis of Koopman Operator</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zijun Gao, Zhikun Xu, Xiao Ye, Ben Zhou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18857" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18857</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2bf1013f3c193e706d652fa4c8fdadc0c813c8a361e2efb84151a139cef28420_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2bf1013f3c193e706d652fa4c8fdadc0c813c8a361e2efb84151a139cef28420_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Application of deep learning approaches for medieval historical documents transcription</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Maksym Voloshchuk, Bohdana Zarembovska, Mykola Kozlenko</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18865" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18865</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bc697efc3bfd30e66b187f56c365b6a8add07756fb768bc77ba4586f1ab7d205_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bc697efc3bfd30e66b187f56c365b6a8add07756fb768bc77ba4586f1ab7d205_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Application of deep learning approaches for medieval historical documents transcription</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Gökdeniz Gülmez</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18901" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18901</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8736176cf479e84eb193acab53e62edbdc590a96b0d7bb1adc66a60425d42697_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8736176cf479e84eb193acab53e62edbdc590a96b0d7bb1adc66a60425d42697_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] The Ensemble Schr{ö}dinger Bridge filter for Nonlinear Data Assimilation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Feng Bao, Hui Sun</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18928" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18928</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/151b1984f127a0ea77e013fd30f9e55a2d1893fc429cb8128fbd4e89c1fefcb6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/151b1984f127a0ea77e013fd30f9e55a2d1893fc429cb8128fbd4e89c1fefcb6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The Ensemble Schr{ö}dinger Bridge filter for Nonlinear Data Assimilation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Merging of Kolmogorov-Arnold networks trained on disjoint datasets</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Andrew Polar, Michael Poluektov</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18921" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18921</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e53714df09977ebe2db3d05959d679946262bcbddc4ea0acb1d3d3511f211611_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e53714df09977ebe2db3d05959d679946262bcbddc4ea0acb1d3d3511f211611_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Merging of Kolmogorov-Arnold networks trained on disjoint datasets</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] DPSR: Differentially Private Sparse Reconstruction via Multi-Stage Denoising for Recommender Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sarwan Ali</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18932" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18932</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c243588ebf5b62b4dd8e1854c41bcdec7ca5861520351edb4b3b12455148d30_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c243588ebf5b62b4dd8e1854c41bcdec7ca5861520351edb4b3b12455148d30_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DPSR: Differentially Private Sparse Reconstruction via Multi-Stage Denoising for Recommender Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Michael S. Zhang, Rishi A. Ruia, Arnav Kewalram, Saathvik Dharmapuram, Utkarsh Sharma, Kevin Zhu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18934" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18934</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/939264f370dd6741588d1d57c916b73d9735e25a2515f10e5a8042daa9f43a19_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/939264f370dd6741588d1d57c916b73d9735e25a2515f10e5a8042daa9f43a19_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Saman Forouzandeh, Wei Peng, Parham Moradi, Xinghuo Yu, Mahdi Jalili</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18950" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18950</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54c9156f2821c3dd5ccd4cf3168dbf447bac3d5632d167548d6c2ce179747e7d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54c9156f2821c3dd5ccd4cf3168dbf447bac3d5632d167548d6c2ce179747e7d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Debamita Ghosh, George K. Atia, Yue Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18957" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18957</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f848ae68e82b8a23f061ffdf3e4e75811fcab48a03fe5070cce95c8a38a027b7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f848ae68e82b8a23f061ffdf3e4e75811fcab48a03fe5070cce95c8a38a027b7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Learning Through Little Eyes: Attribute Discrimination Beyond Objects</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Patrick Batsell, Tsutsui Satoshi, Bihan Wen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18951" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18951</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e80b7c71db93c26589e4d25cfffe472d3de86a058a86caaabd4d81fb9bc2c38_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e80b7c71db93c26589e4d25cfffe472d3de86a058a86caaabd4d81fb9bc2c38_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning Through Little Eyes: Attribute Discrimination Beyond Objects</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Lag Operator SSMs: A Geometric Framework for Structured State Space Modeling</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sutashu Tomonaga, Kenji Doya, Noboru Murata</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18965" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18965</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49d3230921a0fe478c29d46346dffb80acdac27624660b8dd8316094a5be88aa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49d3230921a0fe478c29d46346dffb80acdac27624660b8dd8316094a5be88aa_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Lag Operator SSMs: A Geometric Framework for Structured State Space Modeling</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yizhi Wang, Linan Yue, Min-Ling Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18956" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18956</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b69c497301f02ef5b3b08cd69d4893f6dad335ed0c12427b7caaaff9655ec68_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b69c497301f02ef5b3b08cd69d4893f6dad335ed0c12427b7caaaff9655ec68_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Consistency-guided semi-supervised outlier detection in heterogeneous data using fuzzy rough sets</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Baiyang Chen, Zhong Yuan, Dezhong Peng, Xiaoliang Chen, Hongmei Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18977" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18977</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f137793e37d26cff00a3715c8212e2ad3fa2487bcba70f9d1f1f23ec3f06ea9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f137793e37d26cff00a3715c8212e2ad3fa2487bcba70f9d1f1f23ec3f06ea9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Consistency-guided semi-supervised outlier detection in heterogeneous data using fuzzy rough sets</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Outlier detection in mixed-attribute data: a semi-supervised approach with fuzzy approximations and relative entropy</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Baiyang Chen, Zhong Yuan, Zheng Liu, Dezhong Peng, Yongxiang Li, Chang Liu, Guiduo Duan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18978" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18978</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58b9c746816307fee96e3cf245632387dbf3ca0eaff52acdf2fdc9465315f8e9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58b9c746816307fee96e3cf245632387dbf3ca0eaff52acdf2fdc9465315f8e9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Outlier detection in mixed-attribute data: a semi-supervised approach with fuzzy approximations and relative entropy</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] OPBO: Order-Preserving Bayesian Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wei Peng, Jianchen Hu, Kang Liu, Qiaozhu Zhai</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18980" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18980</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b75b70c8d757c498d035de7add4278dda4866bb8a7052d89525a499956b1847_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b75b70c8d757c498d035de7add4278dda4866bb8a7052d89525a499956b1847_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> OPBO: Order-Preserving Bayesian Optimization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer&#x27;s Disease Progression</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kun Zhao, Siyuan Dai, Yingying Zhang, Guodong Liu, Pengfei Gu, Chenghua Lin, Paul M. Thompson, Alex Leow, Heng Huang, Lifang He, Liang Zhan, Haoteng Tang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18986" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18986</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5f5fb7daad78ee0192a96724088e699786c824ab6443f02134a66cc416ef822_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5f5fb7daad78ee0192a96724088e699786c824ab6443f02134a66cc416ef822_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer&#x27;s Disease Progression</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Akshaj Prashanth Rao, Advait Singh, Saumya Kumaar Saksena, Dhruv Kumar</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19011" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19011</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddd38197559bfa789648dce3d4d675d0a05e678684e3999b2ba550170a5c8c1e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddd38197559bfa789648dce3d4d675d0a05e678684e3999b2ba550170a5c8c1e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Lingjie Zhao, Xue Yu, Yongzhi Qi, Hao Hu, Jianshen Zhang, Yingzheng Ma, Shuyu Han, Wei Qi, Zuo-Jun Max Shen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19001" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19001</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c013ebec66cbd4e8c385c0036349f79e012b2d06eacaaa9dad9601fe1f892d1a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c013ebec66cbd4e8c385c0036349f79e012b2d06eacaaa9dad9601fe1f892d1a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tongyuan Miao, Gary Huang, Kai Jun Han, Annie Jiang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19004" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19004</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0ec45adefdbba0ff1f29513a557351fab96e8636ad950ecb6008ff575d0f496_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0ec45adefdbba0ff1f29513a557351fab96e8636ad950ecb6008ff575d0f496_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Konstantin Kaulen, Tobias Ladner, Stanley Bak, Christopher Brix, Hai Duong, Thomas Flinkow, Taylor T. Johnson, Lukas Koller, Edoardo Manino, ThanhVu H Nguyen, Haoze Wu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19007" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19007</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2150c6a005dd7455c0dea890d81e19545e163edf743950930238707d6b4b29ea_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2150c6a005dd7455c0dea890d81e19545e163edf743950930238707d6b4b29ea_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Optimizer Dynamics at the Edge of Stability with Differential Privacy</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ayana Hussain, Ricky Fang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19019" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19019</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe869fb966bc8e432b82a891d4042c2fd2899dd38d8367f5952e66e27a733144_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe869fb966bc8e432b82a891d4042c2fd2899dd38d8367f5952e66e27a733144_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Optimizer Dynamics at the Edge of Stability with Differential Privacy</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zelin Zhao, Xinyu Gong, Bangya Liu, Ziyang Song, Jun Zhang, Suhui Wu, Yongxin Chen, Hao Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19020" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19020</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7bd5a2abb18589060985877478ba075c83351c0fe7a3b61f7db28dccf9b3d5b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7bd5a2abb18589060985877478ba075c83351c0fe7a3b61f7db28dccf9b3d5b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hengrui Jia, Taoran Li, Jonas Guan, Varun Chandrasekaran</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19025" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19025</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d72daaf4e0b704bed60ade2228f84dd6c37332a3588377ebc905b92f9db787ee_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d72daaf4e0b704bed60ade2228f84dd6c37332a3588377ebc905b92f9db787ee_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Time-series Forecast for Indoor Zone Air Temperature with Long Horizons: A Case Study with Sensor-based Data from a Smart Building</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Liping Sun, Yucheng Guo, Siliang Lu, Zhenzhen Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19038" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19038</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac72029d8bfa1d8aa93562d8cd7cf4c8a3ea568788647c1533d43f1e70bc9335_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac72029d8bfa1d8aa93562d8cd7cf4c8a3ea568788647c1533d43f1e70bc9335_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Time-series Forecast for Indoor Zone Air Temperature with Long Horizons: A Case Study with Sensor-based Data from a Smart Building</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Elevating Intrusion Detection and Security Fortification in Intelligent Networks through Cutting-Edge Machine Learning Paradigms</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Md Minhazul Islam Munna, Md Mahbubur Rahman, Jaroslav Frnda, Muhammad Shahid Anwar, Alpamis Kutlimuratov</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19037" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19037</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f65bbdc2e7a521d46fdaeebb4c5a652347737b90cb0eb72dc48b2bfbbff2789_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f65bbdc2e7a521d46fdaeebb4c5a652347737b90cb0eb72dc48b2bfbbff2789_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Elevating Intrusion Detection and Security Fortification in Intelligent Networks through Cutting-Edge Machine Learning Paradigms</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Recontextualization Mitigates Specification Gaming without Modifying the Specification</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ariana Azarbal, Victor Gillioz, Vladimir Ivanov, Bryce Woodworth, Jacob Drori, Nevan Wichers, Aram Ebtekar, Alex Cloud, Alexander Matt Turner</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19027" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19027</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a00743ecd04e88f2319e45c57ea03523b4606221385fae51496a2d85825c258f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a00743ecd04e88f2319e45c57ea03523b4606221385fae51496a2d85825c258f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Recontextualization Mitigates Specification Gaming without Modifying the Specification</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] On Cost-Aware Sequential Hypothesis Testing with Random Costs and Action Cancellation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> George Vershinin, Asaf Cohen, Omer Gurewitz</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19067" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19067</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/395cdf02cbfcdfcb8cd487bcc7bb40a760166e8635105ec2f48357e1a0a09bbb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/395cdf02cbfcdfcb8cd487bcc7bb40a760166e8635105ec2f48357e1a0a09bbb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> On Cost-Aware Sequential Hypothesis Testing with Random Costs and Action Cancellation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chi Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19061" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19061</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/734faa80116bce116311ee42eb0ce886bb9c7a99f6aa6642df27946ec8624b39_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/734faa80116bce116311ee42eb0ce886bb9c7a99f6aa6642df27946ec8624b39_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ariel Lubonja, Pedro R. A. S. Bassi, Wenxuan Li, Hualin Qiao, Randal Burns, Alan L. Yuille, Zongwei Zhou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19091" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19091</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8b75e4c6b7e493a0c6d3b57468d0ae3edcb9efaeaaf9076c00744a6a04c7c6a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8b75e4c6b7e493a0c6d3b57468d0ae3edcb9efaeaaf9076c00744a6a04c7c6a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Efficient Personalization of Generative Models via Optimal Experimental Design</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Guy Schacht, Ziyad Sheebaelhamd, Riccardo De Santi, Mojmír Mutný, Andreas Krause</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19057" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19057</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcef3113c71e842238e399994571c9492cf534afd3c06e6241febfcda1354e87_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcef3113c71e842238e399994571c9492cf534afd3c06e6241febfcda1354e87_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Efficient Personalization of Generative Models via Optimal Experimental Design</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Dual Model Deep Learning for Alzheimer Prognostication</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Alireza Moayedikia, Sara Fin, Uffe Kock Wiil</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19099" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19099</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57c6707a8cb9bbf9189cedc3fc04aab23c29bbec6c15b1c12163211decc70869_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57c6707a8cb9bbf9189cedc3fc04aab23c29bbec6c15b1c12163211decc70869_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dual Model Deep Learning for Alzheimer Prognostication</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Timely Parameter Updating in Over-the-Air Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jiaqi Zhu, Zhongyuan Zhao, Xiao Li, Ruihao Du, Shi Jin, Howard H.Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19103" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19103</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fc52e9cecab5d074f9763764f769e7c7bd5aaa186ce1d747f2bbd3fa2caf41_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fc52e9cecab5d074f9763764f769e7c7bd5aaa186ce1d747f2bbd3fa2caf41_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Timely Parameter Updating in Over-the-Air Federated Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Danny Dongyeop Han, Yonghyeon Gwon, Ahhyun Lucy Lee, Taeyang Lee, Seong Jin Lee, Jubin Choi, Sebin Lee, Jihyun Bang, Seungju Lee, David Keetae Park, Shinjae Yoo, Chun Kee Chung, Jiook Cha</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19097" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19097</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c484fa8acc86bbd4f9063aac8ef59f814dfcc288fb23da3663d1be6cbc19ed0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c484fa8acc86bbd4f9063aac8ef59f814dfcc288fb23da3663d1be6cbc19ed0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Surrogate-Augmented Symbolic CFD-Driven Training Framework for Accelerating Multi-objective Physical Model Development</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuan Fang, Fabian Waschkowski, Maximilian Reissmann, Richard D. Sandberg, Takuo Oda, Koichi Tanimoto</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19031" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19031</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6630d808b10ee83956b0b8d3975fc127ddbef17b5014ced381ff234a9441900d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6630d808b10ee83956b0b8d3975fc127ddbef17b5014ced381ff234a9441900d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Surrogate-Augmented Symbolic CFD-Driven Training Framework for Accelerating Multi-objective Physical Model Development</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Composable Channel-Adaptive Architecture for Seizure Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Francesco Carzaniga, Michael Hersche, Kaspar Schindler, Abbas Rahimi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19123" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19123</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80ba764ee64b005cd983a9e6b724f69e3e08628a8e048027385de0b6ea62e65d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80ba764ee64b005cd983a9e6b724f69e3e08628a8e048027385de0b6ea62e65d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Composable Channel-Adaptive Architecture for Seizure Classification</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SAP: Syntactic Attention Pruning for Transformer-based Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tzu-Yun Lee, Ding-Yong Hong, Jan-Jan Wu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19125" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19125</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3141b4a53facc55a384f90382f9ca7bbbdeafdff36d3027e35548bc8ba2ea87_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3141b4a53facc55a384f90382f9ca7bbbdeafdff36d3027e35548bc8ba2ea87_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SAP: Syntactic Attention Pruning for Transformer-based Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Murtaza Rangwala, Richard O. Sinnott, Rajkumar Buyya</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19131" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19131</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5ab0acf932ea7b2d42fac6b935c509aaf0582fb4879eb9fae7e0fe0a8e7f766_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5ab0acf932ea7b2d42fac6b935c509aaf0582fb4879eb9fae7e0fe0a8e7f766_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Haoyu Jiang, Boan Qu, Junjie Zhu, Fanjie Zeng, Xiaojie Lin, Wei Zhong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19114" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19114</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c9e78435f4153aa973c4506803772e51c588c18e59d73dbebc8e9500306a1a5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c9e78435f4153aa973c4506803772e51c588c18e59d73dbebc8e9500306a1a5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Convex Loss Function for Set Prediction with Optimal Trade-offs Between Size and Conditional Coverage</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Francis Bach</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19142" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19142</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4a93fce488552329133c465e13cb55215345c0def3d3e8109e9ab58f1388fad_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4a93fce488552329133c465e13cb55215345c0def3d3e8109e9ab58f1388fad_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Convex Loss Function for Set Prediction with Optimal Trade-offs Between Size and Conditional Coverage</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] RP-CATE: Recurrent Perceptron-based Channel Attention Transformer Encoder for Industrial Hybrid Modeling</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Haoran Yang, Yinan Zhang, Wenjie Zhang, Dongxia Wang, Peiyu Liu, Yuqi Ye, Kexin Chen, Wenhai Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19147" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19147</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6d3125cc08ed130beab248cba1a7473ff408ca14e82e124e77351d681c62aec5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6d3125cc08ed130beab248cba1a7473ff408ca14e82e124e77351d681c62aec5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RP-CATE: Recurrent Perceptron-based Channel Attention Transformer Encoder for Industrial Hybrid Modeling</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Geraud Nangue Tasse, Matthew Riemer, Benjamin Rosman, Tim Klinger</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19154" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19154</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4ae9081ca365a9b3f9fb29e85d33707cb3a2d86edd9ec1d7bbe7736548be8781_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4ae9081ca365a9b3f9fb29e85d33707cb3a2d86edd9ec1d7bbe7736548be8781_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mahdi Mohammadigohari, Giuseppe Di Fatta, Giuseppe Nicosia, Panos M. Pardalos</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19184" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19184</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbd43bd93ed46f19d672704ea24b1558583d71b0931350481c4a5624e10f1e16_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbd43bd93ed46f19d672704ea24b1558583d71b0931350481c4a5624e10f1e16_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Practical Quantum-Classical Feature Fusion for complex data Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Azadeh Alavi, Fatemeh Kouchmeshki, Abdolrahman Alavi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19180" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19180</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b3d96db4938c738e58622382a424d90cd1dd10f2608995abac211c8355b017a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b3d96db4938c738e58622382a424d90cd1dd10f2608995abac211c8355b017a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Practical Quantum-Classical Feature Fusion for complex data Classification</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Causal Heterogeneous Graph Learning Method for Chronic Obstructive Pulmonary Disease Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Leming Zhou, Zuo Wang, Zhigang Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19194" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19194</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6faf26f0cc652e06ad561a1f1b33d9cf5c741015ebbded034f635a578186206_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6faf26f0cc652e06ad561a1f1b33d9cf5c741015ebbded034f635a578186206_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Causal Heterogeneous Graph Learning Method for Chronic Obstructive Pulmonary Disease Prediction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mahdi Mohammadigohari, Giuseppe Di Fatta, Giuseppe Nicosia, Panos M. Pardalos</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19199" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19199</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d39c24b719b177ace6c9761e568136da70723831c6d8b3c90ed420d732d6b409_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d39c24b719b177ace6c9761e568136da70723831c6d8b3c90ed420d732d6b409_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Marios Thoma, Zenonas Theodosiou, Harris Partaourides, Vassilis Vassiliades, Loizos Michael, Andreas Lanitis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19190" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19190</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a96c05d7294228300f6794f5bbda416f7d11e2b7c58157c93485f6f6ba933ade_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a96c05d7294228300f6794f5bbda416f7d11e2b7c58157c93485f6f6ba933ade_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tao Zhang, Ziqian Zeng, Hao Peng, Huiping Zhuang, Cen Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19206" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19206</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c9e9fce94d780d562e939d0aa6d0aa4602e96ed0f980ba45968a1486b745bc8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c9e9fce94d780d562e939d0aa6d0aa4602e96ed0f980ba45968a1486b745bc8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Anna-Maria Gueorguieva, Aylin Caliskan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19238" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19238</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d125f49a56a4f052b1faf9015b89460bff5794de36222547ccabb3b4a08eca86_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d125f49a56a4f052b1faf9015b89460bff5794de36222547ccabb3b4a08eca86_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Phase-space entropy at acquisition reflects downstream learnability</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiu-Cheng Wang, Jun-Jie Zhanga, Nan Cheng, Long-Gang Pang, Taijiao Du, Deyu Meng</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19223" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19223</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6f7f5d98244f4027de001e9a63b027e02247bcd25714fdcf9483af34b3cbf0c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6f7f5d98244f4027de001e9a63b027e02247bcd25714fdcf9483af34b3cbf0c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Phase-space entropy at acquisition reflects downstream learnability</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Moncef Garouani, Ayah Barhrhouj</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19246" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19246</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1edaee8be894e0b2aabf965adc9c2da7a8499fac8b0b1b7437890bac0428efa6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1edaee8be894e0b2aabf965adc9c2da7a8499fac8b0b1b7437890bac0428efa6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction Analysis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Regression generation adversarial network based on dual data evaluation strategy for industrial application</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zesen Wang, Yonggang Li, Lijuan Lan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19232" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19232</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a0ad96ff9f5740075b9f98eecc3bf3e0d3e6ae902f7528d0c14e4cef5572f55_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a0ad96ff9f5740075b9f98eecc3bf3e0d3e6ae902f7528d0c14e4cef5572f55_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Regression generation adversarial network based on dual data evaluation strategy for industrial application</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Prathamesh Devadiga</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19250" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19250</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9992d696f5e075764c08a2b42f4505f7b8d093d1a3975265c2c2a7716a8fbcbc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9992d696f5e075764c08a2b42f4505f7b8d093d1a3975265c2c2a7716a8fbcbc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Carla Crivoi, Radu Tudor Ionescu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19253" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19253</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d70a17b56ecda8937a9bd488550e13c9d9833f836ea7a0e16e024c8b5e950c5b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d70a17b56ecda8937a9bd488550e13c9d9833f836ea7a0e16e024c8b5e950c5b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Translating Flow to Policy via Hindsight Online Imitation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yitian Zheng, Zhangchen Ye, Weijun Dong, Shengjie Wang, Yuyang Liu, Chongjie Zhang, Chuan Wen, Yang Gao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19269" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19269</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d1e38857114c4e6bdd80a03e175ccdec39b489a4a7abb23d37f7f4402ba68fc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d1e38857114c4e6bdd80a03e175ccdec39b489a4a7abb23d37f7f4402ba68fc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Translating Flow to Policy via Hindsight Online Imitation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] GShield: Mitigating Poisoning Attacks in Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sameera K. M., Serena Nicolazzo, Antonino Nocera, Vinod P., Rafidha Rehiman K. A</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19286" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19286</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c43d719deb3b65a2681b1c10fbb4647b0e73aa0efbe80e20c06d9f27f59d1058_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c43d719deb3b65a2681b1c10fbb4647b0e73aa0efbe80e20c06d9f27f59d1058_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GShield: Mitigating Poisoning Attacks in Federated Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Time-Vertex Machine Learning for Optimal Sensor Placement in Temporal Graph Signals: Applications in Structural Health Monitoring</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Keivan Faghih Niresi, Jun Qing, Mengjie Zhao, Olga Fink</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19309" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19309</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e839b4b1adb5610adcd939ece8fb2b9f58ed8b25b96c321b897b8bac816ca350_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e839b4b1adb5610adcd939ece8fb2b9f58ed8b25b96c321b897b8bac816ca350_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Time-Vertex Machine Learning for Optimal Sensor Placement in Temporal Graph Signals: Applications in Structural Health Monitoring</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chang Dong, Jianfeng Tao, Chengliang Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19280" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19280</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23dae36b800575da09218251c1aa3582a2ce5ca30c8c9988566be10e83f43e38_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23dae36b800575da09218251c1aa3582a2ce5ca30c8c9988566be10e83f43e38_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Alternative positional encoding functions for neural transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ezequiel Lopez-Rubio, Macoris Decena-Gimenez, Rafael Marcos Luque-Baena</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19323" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19323</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a8d61d0d13038271b6545ad7d265494ffd5b3cce79e03046ec542b879d17a8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a8d61d0d13038271b6545ad7d265494ffd5b3cce79e03046ec542b879d17a8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Alternative positional encoding functions for neural transformers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MAGIC: Achieving Superior Model Merging via Magnitude Calibration</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yayuan Li, Jian Zhang, Jintao Guo, Zihan Cheng, Lei Qi, Yinghuan Shi, Yang Gao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19320" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19320</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7204250ada52cfa8e70ee24634b9a58aab0085ac9d5854e5c672a585fb92a0a6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7204250ada52cfa8e70ee24634b9a58aab0085ac9d5854e5c672a585fb92a0a6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MAGIC: Achieving Superior Model Merging via Magnitude Calibration</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Logical View of GNN-Style Computation and the Role of Activation Functions</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pablo Barceló, Floris Geerts, Matthias Lanzinger, Klara Pakhomenko, Jan Van den Bussche</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19332" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19332</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d6cc724cbd8fd52ec250ad8e7e8c8c76440bef31c5f1eae060dbb796cd6e916_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d6cc724cbd8fd52ec250ad8e7e8c8c76440bef31c5f1eae060dbb796cd6e916_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Logical View of GNN-Style Computation and the Role of Activation Functions</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kiril Dichev, Filip Pawlowski, Albert-Jan Yzelman</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19342" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19342</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2babd04ddf70f201df2fa1a003998a91a1e266029f2a3a118314f226a7ce88f0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2babd04ddf70f201df2fa1a003998a91a1e266029f2a3a118314f226a7ce88f0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Orthogonal Approximate Message Passing with Optimal Spectral Initializations for Rectangular Spiked Matrix Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Haohua Chen, Songbin Liu, Junjie Ma</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19334" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19334</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72f86b8d15772f627ff898121603174c39243f416963a0e15e5c8ec2eec03c72_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72f86b8d15772f627ff898121603174c39243f416963a0e15e5c8ec2eec03c72_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Orthogonal Approximate Message Passing with Optimal Spectral Initializations for Rectangular Spiked Matrix Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> JiaWei Zhu, ZiHeng Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19349" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19349</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0b686fca34f155b782afa1f7cabe09de9b7bcb967ccfba8215a39405da5ba99_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0b686fca34f155b782afa1f7cabe09de9b7bcb967ccfba8215a39405da5ba99_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Learning General Policies with Policy Gradient Methods</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Simon Ståhlberg, Blai Bonet, Hector Geffner</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19366" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19366</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c0ac6fc8f7779ac0ec6d29400a2e745e9a17133a7ceb22854a52d83825eca4e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c0ac6fc8f7779ac0ec6d29400a2e745e9a17133a7ceb22854a52d83825eca4e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning General Policies with Policy Gradient Methods</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] From Points to Coalitions: Hierarchical Contrastive Shapley Values for Prioritizing Data Samples</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Canran Xiao, Jiabao Dou, Zhiming Lin, Zong Ke, Liwei Hou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19363" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19363</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bc75c95e9b775b7503c7e61fa819d446488d179c11fba8ca58a1b86470bc4a23_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bc75c95e9b775b7503c7e61fa819d446488d179c11fba8ca58a1b86470bc4a23_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> From Points to Coalitions: Hierarchical Contrastive Shapley Values for Prioritizing Data Samples</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Isshaan Singh, Divyansh Chawla, Anshu Garg, Shivin Mangal, Pallavi Gupta, Khushi Agarwal, Nimrat Singh Khalsa, Nandan Patel</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19361" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19361</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2afec297ffe8c1a2ba4e79439bf065a0920537f7decb882056013d6e7740f8e9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2afec297ffe8c1a2ba4e79439bf065a0920537f7decb882056013d6e7740f8e9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Christian Hägg, Kathlén Kohn, Giovanni Luca Marchetti, Boris Shapiro</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19367" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19367</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fa84d344152205d90f463c26b0bb9356b71dde7f412bdd03d7fd7f036a0b845_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fa84d344152205d90f463c26b0bb9356b71dde7f412bdd03d7fd7f036a0b845_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Real-Time Machine Learning for Embedded Anomaly Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Abdelmadjid Benmachiche, Khadija Rais, Hamda Slimi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19383" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19383</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d97ca2384a1b645ebcb761e5a1b2985732f10d8ceeea842160ea8a4c2445fca_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d97ca2384a1b645ebcb761e5a1b2985732f10d8ceeea842160ea8a4c2445fca_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Real-Time Machine Learning for Embedded Anomaly Detection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xueming Yan, Boyan Xu, Yaochu Jin, Lixian Xiao, Wenlong Ye, Runyang Cai, Zeqi Zheng, Jingfa Liu, Aimin Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19379" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19379</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1619f2062f011937d18aa7e18ebf2c490f57bd39c04a7d06493e15df8a15ae19_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1619f2062f011937d18aa7e18ebf2c490f57bd39c04a7d06493e15df8a15ae19_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Brain-Grounded Axes for Reading and Steering LLM States</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sandro Andric</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19399" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19399</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01206bf10e0d62a8473930c796f72a63c7683f7ede540cfc495bca18c3dae148_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01206bf10e0d62a8473930c796f72a63c7683f7ede540cfc495bca18c3dae148_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Brain-Grounded Axes for Reading and Steering LLM States</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Research Program: Theory of Learning in Dynamical Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Elad Hazan, Shai Shalev Shwartz, Nathan Srebro</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19410" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19410</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/879026bb2ccb2b89c867729f9f077e32888b82173c20d950c0ed5feda6519aa9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/879026bb2ccb2b89c867729f9f077e32888b82173c20d950c0ed5feda6519aa9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Research Program: Theory of Learning in Dynamical Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Symplectic Reservoir Representation of Legendre Dynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Robert Simon Fong, Gouhei Tanaka, Kazuyuki Aihara</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19409" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19409</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2e2995c2a2a446a77a09ae653f133649b499ee9a5ae92858d90f48c38a521f4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2e2995c2a2a446a77a09ae653f133649b499ee9a5ae92858d90f48c38a521f4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Symplectic Reservoir Representation of Legendre Dynamics</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Attention Is Not What You Need</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhang Chong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19428" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19428</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbe1f1a463179312610994fd34a110ca4bfc5b56928e49a6749dd43948421e91_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbe1f1a463179312610994fd34a110ca4bfc5b56928e49a6749dd43948421e91_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Attention Is Not What You Need</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] An Inverse Scattering Inspired Fourier Neural Operator for Time-Dependent PDE Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rixin Yu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19439" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19439</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e313ba9b9f91e7572ee520f356be411e0bf445d35d515381b2d04e8c77cfcf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e313ba9b9f91e7572ee520f356be411e0bf445d35d515381b2d04e8c77cfcf_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> An Inverse Scattering Inspired Fourier Neural Operator for Time-Dependent PDE Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent decomposition training algorithm</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Antonio Consolo, Andrea Manno, Edoardo Amaldi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19440" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19440</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7c0b3cd4456cb3f28896dad50578a59f6ee4373475817c1d203ce6b2eda4d60_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7c0b3cd4456cb3f28896dad50578a59f6ee4373475817c1d203ce6b2eda4d60_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent decomposition training algorithm</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] GLUE: Generative Latent Unification of Expertise-Informed Engineering Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tim Aebersold, Soheyl Massoudi, Mark D. Fuge</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19469" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19469</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e870c1241b521f8cd85e2b95b902b04bfcabd5e890393790f05186cc61348532_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e870c1241b521f8cd85e2b95b902b04bfcabd5e890393790f05186cc61348532_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GLUE: Generative Latent Unification of Expertise-Informed Engineering Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Lorenzo Capelli, Leandro de Souza Rosa, Gianluca Setti, Mauro Mangia, Riccardo Rovatti</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19472" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19472</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40e0a2de9bc90f1d512acba9f8178e3caeb11f59b899b803bcea54b876c14e2e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40e0a2de9bc90f1d512acba9f8178e3caeb11f59b899b803bcea54b876c14e2e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nikita Volzhin, Soowhan Yoon</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19494" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19494</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d0c1cdb2dc4e7f555b6151e7dd057ebb961c3137ad65952a50508e3d588bb4a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d0c1cdb2dc4e7f555b6151e7dd057ebb961c3137ad65952a50508e3d588bb4a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Lightweight Intrusion Detection in IoT via SHAP-Guided Feature Pruning and Knowledge-Distilled Kronecker Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hafsa Benaddi, Mohammed Jouhari, Nouha Laamech, Anas Motii, Khalil Ibrahimi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19488" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19488</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a32520398008c395d68627f40b4f8898b363f28acb9b801f7e86c375bececdb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a32520398008c395d68627f40b4f8898b363f28acb9b801f7e86c375bececdb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Lightweight Intrusion Detection in IoT via SHAP-Guided Feature Pruning and Knowledge-Distilled Kronecker Networks</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Toward Scalable and Valid Conditional Independence Testing with Spectral Representations</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Alek Frohlich, Vladimir Kostic, Karim Lounici, Daniel Perazzo, Massimiliano Pontil</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19510" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19510</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67736573115d067bb1bcf8e6be20a765add92537af45beed64b6a3f2b096ef5c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67736573115d067bb1bcf8e6be20a765add92537af45beed64b6a3f2b096ef5c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Toward Scalable and Valid Conditional Independence Testing with Spectral Representations</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Learning from sanctioned government suppliers: A machine learning and network science approach to detecting fraud and corruption in Mexico</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Martí Medina-Hern ández, Janos Kertész, Mihály Fazekas</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19491" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19491</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4c4c038ee2a9e249f2f33259b933475235132c1cbefe5a1ac37223ea8fc2367_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4c4c038ee2a9e249f2f33259b933475235132c1cbefe5a1ac37223ea8fc2367_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning from sanctioned government suppliers: A machine learning and network science approach to detecting fraud and corruption in Mexico</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hongliang Li, Nong Zhang, Zhewen Xu, Xiang Li, Changzheng Liu, Chongbo Zhao, Jie Wu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19506" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19506</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ab91ff5a253b798aeb4b7fa1cee40fb5a1319f0697a2094d06ad95557270ff9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ab91ff5a253b798aeb4b7fa1cee40fb5a1319f0697a2094d06ad95557270ff9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xueming Yan, Bo Yin, Yaochu Jin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19516" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19516</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/848324ae33b6180c9af25fd1e9aa4829e0fdb2a0ac2b46da8810accc276774e5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/848324ae33b6180c9af25fd1e9aa4829e0fdb2a0ac2b46da8810accc276774e5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Deep Learning for Unrelated-Machines Scheduling: Handling Variable Dimensions</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Diego Hitzges, Guillaume Sagnol</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19527" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19527</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9be7dc2a227e30f57b54fae513beec5b2448e4823ad54422005ffd50328df87_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9be7dc2a227e30f57b54fae513beec5b2448e4823ad54422005ffd50328df87_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Deep Learning for Unrelated-Machines Scheduling: Handling Variable Dimensions</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hongsheng Xing, Qiuxin Si</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19530" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19530</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a398a1c7e656e700ae7d2e4b360c6e2a84d8a4f125de94406eb2fcbd1174cabf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a398a1c7e656e700ae7d2e4b360c6e2a84d8a4f125de94406eb2fcbd1174cabf_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Initialization of a Polyharmonic Cascade, Launch and Testing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuriy N. Bakhvalov</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19524" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19524</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8a226c40c413901a8fba890ca183141879ee006dd07d881c31b02c4c795c952_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8a226c40c413901a8fba890ca183141879ee006dd07d881c31b02c4c795c952_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Initialization of a Polyharmonic Cascade, Launch and Testing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yongxin Wang, Zhicheng Yang, Meng Cao, Mingfei Han, Haokun Lin, Yingying Zhu, Xiaojun Chang, Xiaodan Liang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19554" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19554</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67ed5402e057af7c1649865e93cc5d4cb278374f1381f91c80951d25ce4f4c0e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67ed5402e057af7c1649865e93cc5d4cb278374f1381f91c80951d25ce4f4c0e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] DFORD: Directional Feedback based Online Ordinal Regression Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Naresh Manwani, M Elamparithy, Tanish Taneja</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19550" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19550</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbbc721b57e400a0a633cbed7c7bd41446b344d459c761659f224385919cc0b0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbbc721b57e400a0a633cbed7c7bd41446b344d459c761659f224385919cc0b0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DFORD: Directional Feedback based Online Ordinal Regression Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kirill Djebko, Tom Baumann, Erik Dilger, Frank Puppe, Sergio Montenegro</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19576" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19576</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/559c74a4e132b0428b11e1b742ace3b49e9292ec3c666ac9dd536d79ee6c2a1f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/559c74a4e132b0428b11e1b742ace3b49e9292ec3c666ac9dd536d79ee6c2a1f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Eric Zimmermann, Harley Wiltzer, Justin Szeto, David Alvarez-Melis, Lester Mackey</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19605" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19605</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8876f353bd3b6bc215381979aff16556177f0d7ca7e5b9cf3aa366d3fbd3c21d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8876f353bd3b6bc215381979aff16556177f0d7ca7e5b9cf3aa366d3fbd3c21d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rajyasri Roy, Dibyajyoti Nayak, Somdatta Goswami</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19643" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19643</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de5e312b39c5b4c9d5e5ba414dbf1be28f3eefa665a2062af2d2753365d7ba18_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de5e312b39c5b4c9d5e5ba414dbf1be28f3eefa665a2062af2d2753365d7ba18_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Deep Legendre Transform</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Aleksey Minabutdinov, Patrick Cheridito</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19649" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19649</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6556ccc5250396e2d156a9c4ace9606ae9e710cca90dde9ad9a791f5bf821cd7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6556ccc5250396e2d156a9c4ace9606ae9e710cca90dde9ad9a791f5bf821cd7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Deep Legendre Transform</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuqiao Tan, Minzheng Wang, Shizhu He, Huanxuan Liao, Chengfeng Zhao, Qiunan Lu, Tian Liang, Jun Zhao, Kang Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19673" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19673</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a324294583c22f2459c7cd427d13db040bb89f060fd51e26bb284a001119f6d4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a324294583c22f2459c7cd427d13db040bb89f060fd51e26bb284a001119f6d4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Inferring Latent Market Forces: Evaluating LLM Detection of Gamma Exposure Patterns via Obfuscation Testing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Christopher Regan, Ying Xie</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17923" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17923</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0972c0f33f9a898aad22d9d466edb2d113b1f06ae271519a0310fbe4deb8326_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0972c0f33f9a898aad22d9d466edb2d113b1f06ae271519a0310fbe4deb8326_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Inferring Latent Market Forces: Evaluating LLM Detection of Gamma Exposure Patterns via Obfuscation Testing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A curated UK rain radar data set for training and benchmarking nowcasting models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Viv Atureta, Rifki Priansyah Jasin, Stefan Siegert</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17924" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17924</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71fd337b26c3bfd8670295386b9f2b4df184043caa1e2c1b8a39442f6cdb1c15_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71fd337b26c3bfd8670295386b9f2b4df184043caa1e2c1b8a39442f6cdb1c15_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A curated UK rain radar data set for training and benchmarking nowcasting models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Efficient Beamforming Optimization for STAR-RIS-Assisted Communications: A Gradient-Based Meta Learning Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dongdong Yang, Bin Li, Jiguang He, Yicheng Yan, Xiaoyu Zhang, Chongwen Huang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17928" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17928</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52bfb16cf6da30f529d6affbb7e78493e8d297701e513afd78f81efa7c4804bd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52bfb16cf6da30f529d6affbb7e78493e8d297701e513afd78f81efa7c4804bd_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Efficient Beamforming Optimization for STAR-RIS-Assisted Communications: A Gradient-Based Meta Learning Approach</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Apoorv Vyas, Heng-Jui Chang, Cheng-Fu Yang, Po-Yao Huang, Luya Gao, Julius Richter, Sanyuan Chen, Matt Le, Piotr Dollár, Christoph Feichtenhofer, Ann Lee, Wei-Ning Hsu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19687" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19687</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de3d411e351c730f5a4f6bcbb2b3a9ec59b568b77417127cf865aadf04270b18_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de3d411e351c730f5a4f6bcbb2b3a9ec59b568b77417127cf865aadf04270b18_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Risk-Aware Financial Forecasting Enhanced by Machine Learning and Intuitionistic Fuzzy Multi-Criteria Decision-Making</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Safiye Turgay, Serkan Erdoğan, Željko Stević, Orhan Emre Elma, Tevfik Eren, Zhiyuan Wang, Mahmut Baydaş</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17936" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17936</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13f5144fba33f969e0e0ef3789e33db059938702b3b5d79afd93befb1670b02b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13f5144fba33f969e0e0ef3789e33db059938702b3b5d79afd93befb1670b02b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Risk-Aware Financial Forecasting Enhanced by Machine Learning and Intuitionistic Fuzzy Multi-Criteria Decision-Making</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sheryl Chen, Tony Wang, Kyle Feinstein</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17929" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17929</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4501ed870b87583df869b8985dc8410b30a47654c96f943771a6c67d4279480c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4501ed870b87583df869b8985dc8410b30a47654c96f943771a6c67d4279480c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Disentangled representations via score-based variational autoencoders</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Benjamin S. H. Lyo, Eero P. Simoncelli, Cristina Savin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17127" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17127</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c4fff8035bd7179107df39681970450e10d3f516b687330e2caedbac602c68b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c4fff8035bd7179107df39681970450e10d3f516b687330e2caedbac602c68b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Disentangled representations via score-based variational autoencoders</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MEGState: Phoneme Decoding from Magnetoencephalography Signals</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shuntaro Suzuki, Chia-Chun Dan Hsu, Yu Tsao, Komei Sugiura</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17978" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17978</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82b8623a222c1e415238f07bfd00f186b0fd97345fe3a78288d0e4e33530c69c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82b8623a222c1e415238f07bfd00f186b0fd97345fe3a78288d0e4e33530c69c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MEGState: Phoneme Decoding from Magnetoencephalography Signals</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Sampling from multimodal distributions with warm starts: Non-asymptotic bounds for the Reweighted Annealed Leap-Point Sampler</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Holden Lee, Matheau Santana-Gijzen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17977" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17977</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb197c6366f5076ec8d83592cce39cf5b5b65c76bc5090ea04ea14c30cdf6873_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb197c6366f5076ec8d83592cce39cf5b5b65c76bc5090ea04ea14c30cdf6873_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Sampling from multimodal distributions with warm starts: Non-asymptotic bounds for the Reweighted Annealed Leap-Point Sampler</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Shuttling Compiler for Trapped-Ion Quantum Computers Based on Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Fabian Kreppel, Reza Salkhordeh, Ferdinand Schmidt-Kaler, André Brinkmann</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18021" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18021</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0bdf00e2aa0c745b05913e16f268fd8bf9934eeb7b22056c328f85c5e744b2d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0bdf00e2aa0c745b05913e16f268fd8bf9934eeb7b22056c328f85c5e744b2d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Shuttling Compiler for Trapped-Ion Quantum Computers Based on Large Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Long-range electrostatics for machine learning interatomic potentials is easier than we thought</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dongjin Kim, Bingqing Cheng</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18029" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18029</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7547e964d0dd16907fbafcc9b548ddcc192b748a763449bd7c5098fb4f79beb3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7547e964d0dd16907fbafcc9b548ddcc192b748a763449bd7c5098fb4f79beb3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Long-range electrostatics for machine learning interatomic potentials is easier than we thought</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Causal Inference as Distribution Adaptation: Optimizing ATE Risk under Propensity Uncertainty</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ashley Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18083" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18083</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9e0e031bdf1e2542517c9521c36614dd83cab06c75d8cba77aef133d3bbf5e4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9e0e031bdf1e2542517c9521c36614dd83cab06c75d8cba77aef133d3bbf5e4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Causal Inference as Distribution Adaptation: Optimizing ATE Risk under Propensity Uncertainty</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Estimating Solvation Free Energies with Boltzmann Generators</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Maximilian Schebek, Nikolas M. Froböse, Bettina G. Keller, Jutta Rogal</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18147" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18147</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be00925063782c2d33d3147328ee4619cc5235ab4e60dcbb70ee97bf3dcb2f8f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be00925063782c2d33d3147328ee4619cc5235ab4e60dcbb70ee97bf3dcb2f8f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Estimating Solvation Free Energies with Boltzmann Generators</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Exploring polymer classification with a hybrid single-photon quantum approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Alexandrina Stoyanova, Bogdan Penkovsky</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18125" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18125</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6aacb9f15cccaa0ab48dbf4d0906a45d2c5bb03514111402f33ab0cb75853841_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6aacb9f15cccaa0ab48dbf4d0906a45d2c5bb03514111402f33ab0cb75853841_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Exploring polymer classification with a hybrid single-photon quantum approach</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CrystalFormer-CSP: Thinking Fast and Slow for Crystal Structure Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhendong Cao, Shigang Ou, Lei Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18251" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18251</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8cf6c41ccf7d3886a9f73f6faa6ec65ab26be6a067ce7c99f7b8336c5475e658_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8cf6c41ccf7d3886a9f73f6faa6ec65ab26be6a067ce7c99f7b8336c5475e658_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CrystalFormer-CSP: Thinking Fast and Slow for Crystal Structure Prediction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] PSI3D: Plug-and-Play 3D Stochastic Inference with Slice-wise Latent Diffusion Prior</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wenhan Guo, Jinglun Yu, Yaning Wang, Jin U. Kang, Yu Sun</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18367" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18367</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98dc2d7a9538276a946338546d48fae623eb87f67d0a306a36ac1560cd23d715_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98dc2d7a9538276a946338546d48fae623eb87f67d0a306a36ac1560cd23d715_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PSI3D: Plug-and-Play 3D Stochastic Inference with Slice-wise Latent Diffusion Prior</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] TICL+: A Case Study On Speech In-Context Learning for Children&#x27;s Speech Recognition</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Haolong Zheng, Yekaterina Yegorova, Mark Hasegawa-Johnson</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18263" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18263</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a32dd492d99bdbfa3ef2453e70a027129c638aca8cddc500a7ae12d1a4ae23df_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a32dd492d99bdbfa3ef2453e70a027129c638aca8cddc500a7ae12d1a4ae23df_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TICL+: A Case Study On Speech In-Context Learning for Children&#x27;s Speech Recognition</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Pushing the limits of one-dimensional NMR spectroscopy for automated structure elucidation using artificial intelligence</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Frank Hu, Jonathan M. Tubb, Dimitris Argyropoulos, Sergey Golotvin, Mikhail Elyashberg, Grant M. Rotskoff, Matthew W. Kanan, Thomas E. Markland</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18531" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18531</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08aee7a9e45a9f29af6d103a3e940fa95a91c99015126260b7c777d51567ce83_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08aee7a9e45a9f29af6d103a3e940fa95a91c99015126260b7c777d51567ce83_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Pushing the limits of one-dimensional NMR spectroscopy for automated structure elucidation using artificial intelligence</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Unsupervised Feature Selection via Robust Autoencoder and Adaptive Graph Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Feng Yu, MD Saifur Rahman Mazumder, Ying Su, Oscar Contreras Velasco</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18720" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18720</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0032f6c7f335886e0c863fb5416f2765511b0ba1ca06f1073d2eab5626b6c3f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0032f6c7f335886e0c863fb5416f2765511b0ba1ca06f1073d2eab5626b6c3f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Unsupervised Feature Selection via Robust Autoencoder and Adaptive Graph Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] RIS-Enabled Smart Wireless Environments: Fundamentals and Distributed Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> George C. Alexandropoulos, Kostantinos D. Katsanos, George Stamatelis, Ioannis Gavras</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18788" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18788</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02ac819b22b566bf908c19d15c96b16e6d173f4f375e5123bcf427c7b9405730_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02ac819b22b566bf908c19d15c96b16e6d173f4f375e5123bcf427c7b9405730_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RIS-Enabled Smart Wireless Environments: Fundamentals and Distributed Optimization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] On Conditional Stochastic Interpolation for Generative Nonlinear Sufficient Dimension Reduction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shuntuo Xu, Zhou Yu, Jian Huang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18971" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18971</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7664dca196bdcb61dc392090fb8c53996afeb07a245afa8d3b648e0385bd7f5d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7664dca196bdcb61dc392090fb8c53996afeb07a245afa8d3b648e0385bd7f5d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> On Conditional Stochastic Interpolation for Generative Nonlinear Sufficient Dimension Reduction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yucheng Yang, Chiyuan Wang, Andreas Schaab, Benjamin Moll</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18892" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18892</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/203cc3fec3a819cdd9dd9fc028622d35f3a4ba54e87d06f53a4c83240df799a4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/203cc3fec3a819cdd9dd9fc028622d35f3a4ba54e87d06f53a4c83240df799a4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Finite-sample guarantees for data-driven forward-backward operator methods</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Filippo Fabiani, Barbara Franci</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19172" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19172</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8222a9719b756b6dad7e034a173dde65aa924d9d7b0789fd4e5882f3dfe97b3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8222a9719b756b6dad7e034a173dde65aa924d9d7b0789fd4e5882f3dfe97b3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Finite-sample guarantees for data-driven forward-backward operator methods</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Self-Consistent Probability Flow for High-Dimensional Fokker-Planck Equations</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiaolong Wu, Qifeng Liao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19196" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19196</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3780b800013ebe275e2c8ef8b8f7ba281c86d44d100ae51b95eeb7d19999391_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3780b800013ebe275e2c8ef8b8f7ba281c86d44d100ae51b95eeb7d19999391_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Self-Consistent Probability Flow for High-Dimensional Fokker-Planck Equations</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Haishan Ye</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19104" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19104</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e1de9f54709e8dfd7512b0bf65bde1fbf69bbf9510338a82e721bb9ab43307b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e1de9f54709e8dfd7512b0bf65bde1fbf69bbf9510338a82e721bb9ab43307b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Cluster-Based Generalized Additive Models Informed by Random Fourier Features</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xin Huang, Jia Li, Jun Yu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19373" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19373</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f54a5120ec11e3d3fee124d496a72d9ed6944487b104802da2d971b4720063e2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f54a5120ec11e3d3fee124d496a72d9ed6944487b104802da2d971b4720063e2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Cluster-Based Generalized Additive Models Informed by Random Fourier Features</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Critical Assessment of Pattern Comparisons Between POD and Autoencoders in Intraventricular Flows</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Eneko Lazpita, Andrés Bell-Navas, Jesús Garicano-Mena, Petros Koumoutsakos, Soledad Le Clainche</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19376" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19376</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed5b488d452940ef72b893f7e49bf3407f54d6a1ce8a3642724d7bf4ddcad601_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed5b488d452940ef72b893f7e49bf3407f54d6a1ce8a3642724d7bf4ddcad601_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Critical Assessment of Pattern Comparisons Between POD and Autoencoders in Intraventricular Flows</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Real-Time Streamable Generative Speech Restoration with Flow Matching</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Simon Welker, Bunlong Lay, Maris Hillemann, Tal Peer, Timo Gerkmann</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19442" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19442</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50f384e81e75842e398563da395c9a42cafd7dfeb79c90bc7c4489b17d033102_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50f384e81e75842e398563da395c9a42cafd7dfeb79c90bc7c4489b17d033102_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Real-Time Streamable Generative Speech Restoration with Flow Matching</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Active Convolved Illumination with Deep Transfer Learning for Complex Beam Transmission through Atmospheric Turbulence</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Adrian A. Moazzam, Anindya Ghoshroy, Breeanne Heusdens, Durdu O. Guney, Roohollah Askari</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19540" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19540</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aefac8efeaa58dab809884787824024dfad8ec6cfb7ca6a8de7663af53c553bc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aefac8efeaa58dab809884787824024dfad8ec6cfb7ca6a8de7663af53c553bc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Active Convolved Illumination with Deep Transfer Learning for Complex Beam Transmission through Atmospheric Turbulence</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-24">2025-12-24<a href="#2025-12-24" class="hash-link" aria-label="Direct link to 2025-12-24" title="Direct link to 2025-12-24" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251224] QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sebastian Racedo, Brigitte Jaumard, Oscar Delgado, Meysam Masoudi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19696" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19696</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b77d355579fa14e02c66f844a9c1cf1fbc3d68fee2d28b38fc709f013395b1a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b77d355579fa14e02c66f844a9c1cf1fbc3d68fee2d28b38fc709f013395b1a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Md Nahid Hasan Shuvo, Moinul Hossain</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19711" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19711</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d516fcc6c3de6b0e65c078e5e3f3dda23bfd77fbb9c5f4abfe2c509c2cb6dfe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d516fcc6c3de6b0e65c078e5e3f3dda23bfd77fbb9c5f4abfe2c509c2cb6dfe_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Large Language Models for EDA Cloud Job Resource and Lifetime Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuxuan Yin, Shengke Zhou, Yunjie Zhang, Ajay Mohindra, Boxun Xu, Peng Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19701" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19701</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe0c6f8e6d98607a87a162a4a1cada21d732348d823658c9451d5ce5608a7d1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe0c6f8e6d98607a87a162a4a1cada21d732348d823658c9451d5ce5608a7d1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Large Language Models for EDA Cloud Job Resource and Lifetime Prediction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Behrooz Mamandipoor, Chun-Nan Hsu, Martin Krause, Ulrich H. Schmidt, Rodney A. Gabriel</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19716" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19716</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b80fac6c07719f0fdc3b2a60068a2f3820d61d75d5655632ad18fc7fbee5f80_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b80fac6c07719f0fdc3b2a60068a2f3820d61d75d5655632ad18fc7fbee5f80_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> James K Ruffle, Samia Mohinta, Guilherme Pombo, Asthik Biswas, Alan Campbell, Indran Davagnanam, David Doig, Ahmed Hamman, Harpreet Hyare, Farrah Jabeen, Emma Lim, Dermot Mallon, Stephanie Owen, Sophie Wilkinson, Sebastian Brandner, Parashkev Nachev</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19707" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19707</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e098f2b53a5d94739b784dac1a98f71b53ab4d9f759c65700bc9e1f9500bbafd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e098f2b53a5d94739b784dac1a98f71b53ab4d9f759c65700bc9e1f9500bbafd_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Taoran Sheng, Manfred Huber</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19713" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19713</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8069c49480aa4056d903366d9a07ef262001809b60e89caaaab99b1ab6318bb6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8069c49480aa4056d903366d9a07ef262001809b60e89caaaab99b1ab6318bb6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Node-Level Financial Optimization in Demand Forecasting Through Dynamic Cost Asymmetry and Feedback Mechanism</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Alessandro Casadei, Clemens Grupp, Sreyoshi Bhaduri, Lu Guo, Wilson Fung, Rohit Malshe, Raj Ratan, Ankush Pole, Arkajit Rakshit</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19722" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19722</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5fc3669c6e3b3729a8ceeab7556b8b190a71237342736f2d91f0e9e99de3dc0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5fc3669c6e3b3729a8ceeab7556b8b190a71237342736f2d91f0e9e99de3dc0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Node-Level Financial Optimization in Demand Forecasting Through Dynamic Cost Asymmetry and Feedback Mechanism</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Per-Axis Weight Deltas for Frequent Model Updates</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Stefan Kuyumdzhiev, Radostin Cholakov</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19720" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19720</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fced4b263f86d7aa4d12f96b832ddc3447334c926ee67499537f6d38e8e740d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fced4b263f86d7aa4d12f96b832ddc3447334c926ee67499537f6d38e8e740d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Per-Axis Weight Deltas for Frequent Model Updates</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhan Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19717" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19717</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ee62945031af7f6dac3a6d51384974eec1f8f5db6dd103388502d84eb58bc6a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ee62945031af7f6dac3a6d51384974eec1f8f5db6dd103388502d84eb58bc6a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Sign-Aware Multistate Jaccard Kernels and Geometry for Real and Complex-Valued Signals</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Vineet Yadav</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19721" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19721</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/476a71567ad37a78f3007d1e492eb010eed6a7f8bed8a187d46ae8e80465f4f5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/476a71567ad37a78f3007d1e492eb010eed6a7f8bed8a187d46ae8e80465f4f5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Sign-Aware Multistate Jaccard Kernels and Geometry for Real and Complex-Valued Signals</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Multiscale Dual-path Feature Aggregation Network for Remaining Useful Life Prediction of Lithium-Ion Batteries</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zihao Lv, Siqi Ai, Yanbin Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19719" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19719</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6292853f8fb29c3648a6a9e7a018fcb02691dba13e4d6ce37a63f296f046554_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6292853f8fb29c3648a6a9e7a018fcb02691dba13e4d6ce37a63f296f046554_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multiscale Dual-path Feature Aggregation Network for Remaining Useful Life Prediction of Lithium-Ion Batteries</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural, and graph-based evaluation of synthetic tabular data</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Vasileios C. Pezoulas, Nikolaos S. Tachos, Eleni Georga, Kostas Marias, Manolis Tsiknakis, Dimitrios I. Fotiadis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19718" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19718</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/690024688881d156d3131447c4b795c922984c2e3732a32a3b139b183a1be23e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/690024688881d156d3131447c4b795c922984c2e3732a32a3b139b183a1be23e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural, and graph-based evaluation of synthetic tabular data</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Tiny, On-Device Decision Makers with the MiniConv Library</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Carlos Purves</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19726" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19726</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fcebc0e7acd2aa33081184298763fb8df6f0a43f23fb341b0e84da9a69d1bd6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fcebc0e7acd2aa33081184298763fb8df6f0a43f23fb341b0e84da9a69d1bd6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Tiny, On-Device Decision Makers with the MiniConv Library</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Srishti Gupta, Riccardo Balia, Daniele Angioni, Fabio Brau, Maura Pintor, Ambra Demontis, Alessandro Sebastian, Salvatore Mario Carta, Fabio Roli, Battista Biggio</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19725" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19725</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a6b157cb48d9bcd740b085c12288bed5c0e95c01a7146a5e7c2e50b7d77787d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a6b157cb48d9bcd740b085c12288bed5c0e95c01a7146a5e7c2e50b7d77787d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Trend Extrapolation for Technology Forecasting: Leveraging LSTM Neural Networks for Trend Analysis of Space Exploration Vessels</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Peng-Hung Tsai, Daniel Berleant</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19727" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19727</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2c9404cbdd59c5c56c246473b4a6996ca55f6051700faa6c654323d6990a290_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2c9404cbdd59c5c56c246473b4a6996ca55f6051700faa6c654323d6990a290_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Trend Extrapolation for Technology Forecasting: Leveraging LSTM Neural Networks for Trend Analysis of Space Exploration Vessels</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] End-to-End Data Quality-Driven Framework for Machine Learning in Production Environment</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Firas Bayram, Bestoun S. Ahmed, Erik Hallin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19723" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19723</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e87a73b8e332e90e3cd2839f1faa8882cdeef6cb35e274b072b141a0cabb8572_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e87a73b8e332e90e3cd2839f1faa8882cdeef6cb35e274b072b141a0cabb8572_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> End-to-End Data Quality-Driven Framework for Machine Learning in Production Environment</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Hard Negative Sample-Augmented DPO Post-Training for Small Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Haocheng Lu, Minjun Zhu, Henry Yu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19728" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19728</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c371de45b1b11fd08dee5a184a8d1c0b7de0a0ff5ecd8c96c44e8fddf3eabedc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c371de45b1b11fd08dee5a184a8d1c0b7de0a0ff5ecd8c96c44e8fddf3eabedc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Hard Negative Sample-Augmented DPO Post-Training for Small Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhonghao Yang, Cheng Luo, Daojing He, Yiming Li, Yu Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19730" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19730</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66bbe64f6f3cac5df3dfd69a018806ee7d1973ac071d8af57c13752826c622fe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66bbe64f6f3cac5df3dfd69a018806ee7d1973ac071d8af57c13752826c622fe_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] High-Performance Self-Supervised Learning by Joint Training of Flow Matching</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kosuke Ukita, Tsuyoshi Okita</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19729" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19729</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/269f1eff8cf71b204b6147341b992a81faef6468f33e5edd7a5be137a0ac9100_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/269f1eff8cf71b204b6147341b992a81faef6468f33e5edd7a5be137a0ac9100_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> High-Performance Self-Supervised Learning by Joint Training of Flow Matching</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Leakage-Aware Bandgap Prediction on the JARVIS-DFT Dataset: A Phase-Wise Feature Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Gaurav Kumar Sharma</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19732" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19732</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53da428992d5e5cae28642416139ec0d148864a5c060b683ac9e143fe97079ee_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53da428992d5e5cae28642416139ec0d148864a5c060b683ac9e143fe97079ee_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Leakage-Aware Bandgap Prediction on the JARVIS-DFT Dataset: A Phase-Wise Feature Analysis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiangzhong Luo, Weichen Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19731" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19731</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c10d82314834148af959284b60a6d6f0fa9e36928106648626f8c43d4f07b79_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c10d82314834148af959284b60a6d6f0fa9e36928106648626f8c43d4f07b79_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wilson Fung, Lu Guo, Drake Hilliard, Alessandro Casadei, Raj Ratan, Sreyoshi Bhaduri, Adi Surve, Nikhil Agarwal, Rohit Malshe, Pavan Mullapudi, Hungjen Wang, Saurabh Doodhwala, Ankush Pole, Arkajit Rakshit</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19738" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19738</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa995500ed323b0099b96191763cdf14300972d797c5bdf631faa22d483ef8d4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa995500ed323b0099b96191763cdf14300972d797c5bdf631faa22d483ef8d4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] The Deleuzian Representation Hypothesis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Clément Cornet, Romaric Besançon, Hervé Le Borgne</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19734" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19734</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/877f862cfd6b85ad4699167cc9b9bc17de797ef85a7ede6911637da4967d6121_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/877f862cfd6b85ad4699167cc9b9bc17de797ef85a7ede6911637da4967d6121_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The Deleuzian Representation Hypothesis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Simulation-Driven Railway Delay Prediction: An Imitation Learning Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Clément Elliker, Jesse Read, Sonia Vanier, Albert Bifet</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19737" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19737</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37e9a570297730f5200e5c0dcac9576f29dc77d8856f607f480d2a083088332_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37e9a570297730f5200e5c0dcac9576f29dc77d8856f607f480d2a083088332_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Simulation-Driven Railway Delay Prediction: An Imitation Learning Approach</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] CoPHo: Classifier-guided Conditional Topology Generation with Persistent Homology</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Gongli Xi, Ye Tian, Mengyu Yang, Zhenyu Zhao, Yuchao Zhang, Xiangyang Gong, Xirong Que, Wendong Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19736" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19736</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68578352cc68ad1305abf54d27488dc8db7f857ff2484ef8acd9ab80b0db8641_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68578352cc68ad1305abf54d27488dc8db7f857ff2484ef8acd9ab80b0db8641_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CoPHo: Classifier-guided Conditional Topology Generation with Persistent Homology</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Gangxiong Zhang, Yongchao Long</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19735" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19735</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffac075f9a08ac05f63c8e47026ade8aa961ca7bcc7071d314dbbcf27a110f66_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffac075f9a08ac05f63c8e47026ade8aa961ca7bcc7071d314dbbcf27a110f66_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] EdgeFlex-Transformer: Transformer Inference for Edge Devices</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shoaib Mohammad, Guanqun Song, Ting Zhu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19741" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19741</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a472c5a3779ea5b970e030fee19030719f5db1d085d239de2dc5df41dffd7439_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a472c5a3779ea5b970e030fee19030719f5db1d085d239de2dc5df41dffd7439_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EdgeFlex-Transformer: Transformer Inference for Edge Devices</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Asia Cup 2025: A Structured T20 Match-Level Dataset and Exploratory Analysis for Cricket Analytics</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kousar Raza, Faizan Ali</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19740" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19740</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f547c21d7d60b5ce4f92522c83aa7c297fc051a77b6eddf69ef405348fda9d8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f547c21d7d60b5ce4f92522c83aa7c297fc051a77b6eddf69ef405348fda9d8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Asia Cup 2025: A Structured T20 Match-Level Dataset and Exploratory Analysis for Cricket Analytics</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] On-device Large Multi-modal Agent for Human Activity Recognition</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Md Shakhrul Iman Siam, Ishtiaque Ahmed Showmik, Guanqun Song, Ting Zhu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19742" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19742</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f3cc1dde2877d131c87748e2a0e8975b217b5776f2152e724bf632e2fa6532f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f3cc1dde2877d131c87748e2a0e8975b217b5776f2152e724bf632e2fa6532f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> On-device Large Multi-modal Agent for Human Activity Recognition</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sasan Sharifipour, Constantino Álvarez Casado, Manuel Lage Cañellas, Miguel Bordallo López</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19743" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19743</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28b9eb6359f294d9654c6f1fea215bc4726b236c77ba0b6790735d53dbad5ead_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28b9eb6359f294d9654c6f1fea215bc4726b236c77ba0b6790735d53dbad5ead_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian Optimization in TinyML Keyword Spotting</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Soumen Garai, Suman Samui</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19739" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19739</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9778cadae68709b008dcf5db962164fda68a414cd3d9f0a2847361f564c06bad_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9778cadae68709b008dcf5db962164fda68a414cd3d9f0a2847361f564c06bad_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian Optimization in TinyML Keyword Spotting</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] DeepBridge: A Unified and Production-Ready Framework for Multi-Dimensional Machine Learning Validation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Gustavo Coelho Haase, Paulo Henrique Dourado da Silva</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19744" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19744</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0bccd562ab93d21bfa941fe354c60f5ea1f28b2e74751c8d34532405481aeeca_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0bccd562ab93d21bfa941fe354c60f5ea1f28b2e74751c8d34532405481aeeca_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DeepBridge: A Unified and Production-Ready Framework for Multi-Dimensional Machine Learning Validation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] How Many Experts Are Enough? Towards Optimal Semantic Specialization for Mixture-of-Experts</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sumin Park, Noseong Park</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19765" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19765</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/515767751abfd73b2b6370592d087c270228e210ccda8fa867a672db0ae07a01_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/515767751abfd73b2b6370592d087c270228e210ccda8fa867a672db0ae07a01_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> How Many Experts Are Enough? Towards Optimal Semantic Specialization for Mixture-of-Experts</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Learning to Design City-scale Transit Routes</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Bibek Poudel, Weizi Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19767" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19767</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48f5a0e4652c24c99f21521fcb359e848d25f4d63cc6c81bee61cbd2088a47c6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48f5a0e4652c24c99f21521fcb359e848d25f4d63cc6c81bee61cbd2088a47c6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning to Design City-scale Transit Routes</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] A K-Means, Ward and DBSCAN repeatability study</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Anthony Bertrand, Engelbert Mephu Nguifo, Violaine Antoine, David Hill</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19772" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19772</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b606a0690bd58fd61c6e296efa76193ecfe74e0fd82dcf2bd79d638111e3e1d1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b606a0690bd58fd61c6e296efa76193ecfe74e0fd82dcf2bd79d638111e3e1d1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A K-Means, Ward and DBSCAN repeatability study</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Deepit Sapru</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19805" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19805</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb04e43886d4319737a1d917f74a55c539bf9fd5290a700f1e160de279d18cef_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb04e43886d4319737a1d917f74a55c539bf9fd5290a700f1e160de279d18cef_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Reduced Order Modeling for Tsunami Forecasting with Bayesian Hierarchical Pooling</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shane X. Coffing, John Tipton, Arvind T. Mohan, Darren Engwirda</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19804" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19804</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e689d3fa5a81d3d8eba58c25d7cb4a0158b1806b3118990d514118edc1fa566_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e689d3fa5a81d3d8eba58c25d7cb4a0158b1806b3118990d514118edc1fa566_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Reduced Order Modeling for Tsunami Forecasting with Bayesian Hierarchical Pooling</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] UCCL-EP: Portable Expert-Parallel Communication</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ziming Mao, Yihan Zhang, Chihan Cui, Kaichao You, Zhongjie Chen, Zhiying Xu, Scott Shenker, Costin Raiciu, Yang Zhou, Ion Stoica</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19849" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19849</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb2d143c0a9d64bd2c5bdf4142e8e3a096290fd8319372321c00c3c17d53b658_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb2d143c0a9d64bd2c5bdf4142e8e3a096290fd8319372321c00c3c17d53b658_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> UCCL-EP: Portable Expert-Parallel Communication</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Fine-Tuned In-Context Learners for Efficient Adaptation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jorg Bornschein, Clare Lyle, Yazhe Li, Amal Rannen-Triki, Xu Owen He, Razvan Pascanu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19879" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19879</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31980c4d9f6b1c6d6c1f0f41df293fd637d43c4ea2f2de5d26aa825310d8bdbc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31980c4d9f6b1c6d6c1f0f41df293fd637d43c4ea2f2de5d26aa825310d8bdbc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Fine-Tuned In-Context Learners for Efficient Adaptation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Detecting cyberbullying in Spanish texts through deep learning techniques</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Paúl Cumba-Armijos, Diego Riofrío-Luzcando, Verónica Rodríguez-Arboleda, Joe Carrión-Jumbo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19899" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19899</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d2f7ccac215958604ec2bafb628962c08cfada143b59dda8159af7e4af21661_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d2f7ccac215958604ec2bafb628962c08cfada143b59dda8159af7e4af21661_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Detecting cyberbullying in Spanish texts through deep learning techniques</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jiayun Wu, Jiashuo Liu, Zhiyuan Zeng, Tianyang Zhan, Wenhao Huang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19920" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19920</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33255a480cb8654f8d9838cb7a634102f7086c3eaac9fb63148c10866248563a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33255a480cb8654f8d9838cb7a634102f7086c3eaac9fb63148c10866248563a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Indranil Halder, Cengiz Pehlevan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19905" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19905</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1e6db0153f278bc11740f6ab7077ed6a29fff1f323057711a5dc1210d6e99fe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1e6db0153f278bc11740f6ab7077ed6a29fff1f323057711a5dc1210d6e99fe_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Maxime Lacour, Pu Ren, Rie Nakata, Nori Nakata, Michael Mahoney</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19909" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19909</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e8e80ac4736f89a1123273e8c6f77a605a941de3087891673a6d7728a3d0998_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e8e80ac4736f89a1123273e8c6f77a605a941de3087891673a6d7728a3d0998_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] The Seismic Wavefield Common Task Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Alexey Yermakov, Yue Zhao, Marine Denolle, Yiyu Ni, Philippe M. Wyder, Judah Goldfeder, Stefano Riva, Jan Williams, David Zoro, Amy Sara Rude, Matteo Tomasetto, Joe Germany, Joseph Bakarji, Georg Maierhofer, Miles Cranmer, J. Nathan Kutz</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19927" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19927</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b59879c33166ae9f8c44c6fb1768cff1db7963674dc0f8347a443a44df80bf19_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b59879c33166ae9f8c44c6fb1768cff1db7963674dc0f8347a443a44df80bf19_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The Seismic Wavefield Common Task Framework</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Samruddhi Baviskar</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19935" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19935</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f15b3890b1332bf926501d440f418e2e71c3b0c8c5b3dd19380d13e172c25ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f15b3890b1332bf926501d440f418e2e71c3b0c8c5b3dd19380d13e172c25ac_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Vehicle-centric Perception via Multimodal Structured Pre-training</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wentao Wu, Xiao Wang, Chenglong Li, Jin Tang, Bin Luo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19934" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19934</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25393280cf5e09ecc25c375a88de26bef6b6904fd90a6775cf7591ed8a615fe1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25393280cf5e09ecc25c375a88de26bef6b6904fd90a6775cf7591ed8a615fe1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Vehicle-centric Perception via Multimodal Structured Pre-training</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Block-Recurrent Dynamics in Vision Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mozes Jacobs, Thomas Fel, Richard Hakim, Alessandra Brondetta, Demba Ba, T. Andy Keller</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19941" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19941</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e10f9b4ab6d210902b2c5092ebfe1fcc82dd7a3d2032bfc97fbfadd16867c2a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e10f9b4ab6d210902b2c5092ebfe1fcc82dd7a3d2032bfc97fbfadd16867c2a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Block-Recurrent Dynamics in Vision Transformers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Spatio-Temporal Graph Neural Networks for Dairy Farm Sustainability Forecasting and Counterfactual Policy Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Surya Jayakumar, Kieran Sullivan, John McLaughlin, Christine O&#x27;Meara, Indrakshi Dey</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19970" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19970</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04d93c1a47d24166acae7ed5012d760c9a02de04651f40a319ded305b7aaad13_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04d93c1a47d24166acae7ed5012d760c9a02de04651f40a319ded305b7aaad13_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Spatio-Temporal Graph Neural Networks for Dairy Farm Sustainability Forecasting and Counterfactual Policy Analysis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Bloom Filter Encoding for Machine Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> John Cartmell, Mihaela Cardei, Ionut Cardei</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19991" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19991</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e677d5a3d466425f205097a14f528d2211f9cb2642b715b0647694a1db34a243_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e677d5a3d466425f205097a14f528d2211f9cb2642b715b0647694a1db34a243_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Bloom Filter Encoding for Machine Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tamim Ahasan Rijon, Yeasin Arafath</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19989" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19989</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7415dab1086d45b2bb6f7a49ab33be753b3da047de23310de552ead99dc2448_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7415dab1086d45b2bb6f7a49ab33be753b3da047de23310de552ead99dc2448_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Schoenfeld&#x27;s Anatomy of Mathematical Reasoning by Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ming Li, Chenrui Fan, Yize Cheng, Soheil Feizi, Tianyi Zhou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19995" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19995</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf100eeda1c44688829760003993b1a83478a2d2901a0f5a0b9914bc5ef7c3b0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf100eeda1c44688829760003993b1a83478a2d2901a0f5a0b9914bc5ef7c3b0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Schoenfeld&#x27;s Anatomy of Mathematical Reasoning by Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Control Variate Score Matching for Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Khaled Kahouli, Romuald Elie, Klaus-Robert Müller, Quentin Berthet, Oliver T. Unke, Arnaud Doucet</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20003" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20003</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99c1fcd9cfbbb5a241296c106e6f4311b324493691659f27747af21f56a722fe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99c1fcd9cfbbb5a241296c106e6f4311b324493691659f27747af21f56a722fe_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Control Variate Score Matching for Diffusion Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sukumar Kishanthan, Asela Hevapathige</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20006" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20006</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/767f4e4dcb2738c000b0ea66f1109ff56b696feb519040f06df4fdd1a29c343a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/767f4e4dcb2738c000b0ea66f1109ff56b696feb519040f06df4fdd1a29c343a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rahul Yumlembam, Biju Issac, Seibu Mary Jacob, Longzhi Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20004" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20004</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea3f077bcaec1c639c8029881602d31bdf125a8cbefc2e15ec9ba3e07c126ee1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea3f077bcaec1c639c8029881602d31bdf125a8cbefc2e15ec9ba3e07c126ee1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jiacheng You, Jingcheng Yang, Yuhang Xie, Zhongxuan Wu, Xiucheng Li, Feng Li, Pengjie Wang, Jian Xu, Bo Zheng, Xinyang Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20002" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20002</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3eef74ab05c46cb25ce1372769f7b8dc5bc12b1c381a6076e807a4bdde96f36_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3eef74ab05c46cb25ce1372769f7b8dc5bc12b1c381a6076e807a4bdde96f36_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuan Gao, Zhenguo Dong, Xuelong Wang, Zhiqiang Wang, Yong Zhang, Shaofan Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20028" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20028</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a6ada3496efb972d8c3a130ea0af749449dc6804b758999852b9def0e9692a4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a6ada3496efb972d8c3a130ea0af749449dc6804b758999852b9def0e9692a4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Deep Eigenspace Network and Its Application to Parametric Non-selfadjoint Eigenvalue Problems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> H. Li, J. Sun, Z. Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20058" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20058</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f3aee744984f754345f38a1211552354aa6d648480458cd66df06732c2b2624_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f3aee744984f754345f38a1211552354aa6d648480458cd66df06732c2b2624_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Deep Eigenspace Network and Its Application to Parametric Non-selfadjoint Eigenvalue Problems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] An Optimal Policy for Learning Controllable Dynamics by Exploration</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Peter N. Loxley</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20053" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20053</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1955af7668fd6f7c26946b76a3cbf622271164ba70999a4181146562f156fbbc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1955af7668fd6f7c26946b76a3cbf622271164ba70999a4181146562f156fbbc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> An Optimal Policy for Learning Controllable Dynamics by Exploration</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] DS-HGCN: A Dual-Stream Hypergraph Convolutional Network for Predicting Student Engagement via Social Contagion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ziyang Fan, Li Tao, Yi Wang, Jingwei Qu, Ying Wang, Fei Jiang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20059" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20059</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c40788fe60a93cff593d92f44508a7f4d8652aa38e707e2a9892801ec0179a3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c40788fe60a93cff593d92f44508a7f4d8652aa38e707e2a9892801ec0179a3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DS-HGCN: A Dual-Stream Hypergraph Convolutional Network for Predicting Student Engagement via Social Contagion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mingue Park, Jisung Hwang, Seungwoo Yoo, Kyeongmin Yeo, Minhyuk Sung</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20063" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20063</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1f331573ffd5fdda741a1d0056236e66fefa140bd0c2b8c4b173e3519544061_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1f331573ffd5fdda741a1d0056236e66fefa140bd0c2b8c4b173e3519544061_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jeehong Kim, Youngseok Hwang, Minchan Kim, Sungho Bae, Hyunwoo Park</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20086" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20086</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95b1bc86a744c86f68507c2b101c06be33b9804cc84964060682c34e2802c63e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95b1bc86a744c86f68507c2b101c06be33b9804cc84964060682c34e2802c63e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yanjie Li, Jian Xu, Xueqing Chen, Lina Yu, Shiming Xiang, Weijun Li, Cheng-lin Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20084" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20084</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ad5d1a3b18c9e1109d65023aa18a9c4b5eaddb7fbf1b86de532c25025f845a7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ad5d1a3b18c9e1109d65023aa18a9c4b5eaddb7fbf1b86de532c25025f845a7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Jensen-Shannon Divergence Message-Passing for Rich-Text Graph Representation Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zuo Wang, Ye Yuan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20094" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20094</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d8ebe19b950699158b26962ef207ee20be9d3fd1a9e78b063e5d97cd6c18f0a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d8ebe19b950699158b26962ef207ee20be9d3fd1a9e78b063e5d97cd6c18f0a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Jensen-Shannon Divergence Message-Passing for Rich-Text Graph Representation Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Information-directed sampling for bandits: a primer</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Annika Hirling, Giorgio Nicoletti, Antonio Celani</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20096" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20096</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2bda193cb418441da95d0440f5f59e93b002d7bab9c57780520c427e9d3126c5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2bda193cb418441da95d0440f5f59e93b002d7bab9c57780520c427e9d3126c5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Information-directed sampling for bandits: a primer</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Aly Lidayan, Jakob Bjorner, Satvik Golechha, Kartik Goyal, Alane Suhr</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20111" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20111</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d0d34b2d9754cdb266cf6f4c20cff854836475921a3b1dfd5eebd552c7ef69c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d0d34b2d9754cdb266cf6f4c20cff854836475921a3b1dfd5eebd552c7ef69c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuanhao Chen, Qi Liu, Pengbin Chen, Zhongjian Qiao, Yanjie Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20115" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20115</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5c5ab959b1aabba8373388341144fb9d59c759f4a45a536ba853663551ebb84_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5c5ab959b1aabba8373388341144fb9d59c759f4a45a536ba853663551ebb84_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Retrieval-augmented Prompt Learning for Pre-trained Foundation Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiang Chen, Yixin Ou, Quan Feng, Lei Li, Piji Li, Haibo Ye, Sheng-Jun Huang, Shuofei Qiao, Shumin Deng, Huajun Chen, Ningyu Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20145" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20145</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028ab8e7171d7f497cbdc48e136d419e8642bf876111786382aee29b15d0992_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028ab8e7171d7f497cbdc48e136d419e8642bf876111786382aee29b15d0992_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Retrieval-augmented Prompt Learning for Pre-trained Foundation Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Learning to Reason in LLMs by Expectation Maximization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Junghyun Lee, Branislav Kveton, Sunav Choudhary, Subhojyoti Mukherjee, Anup Rao, Ryan A. Rossi, Alexa Siu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20169" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20169</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fc3f6af733f26b26d15d5332cff89ae665d865f5359eb651dbf107c200c4794_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fc3f6af733f26b26d15d5332cff89ae665d865f5359eb651dbf107c200c4794_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning to Reason in LLMs by Expectation Maximization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Songze Li, Jiameng Cheng, Yiming Li, Xiaojun Jia, Dacheng Tao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20168" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20168</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12bfaa681af3521489a5c856a7d28bf207a72778a776d4ae80d7e0271f100e3b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12bfaa681af3521489a5c856a7d28bf207a72778a776d4ae80d7e0271f100e3b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] NeuralCrop: Combining physics and machine learning for improved crop yield predictions</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yunan Lin, Sebastian Bathiany, Maha Badri, Maximilian Gelbrecht, Philipp Hess, Brian Groenke, Jens Heinke, Christoph Müller, Niklas Boers</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20177" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20177</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b11ac7dee2967ebac7497c5b7c4ac412a47aa4080e36cf0d94605e0611bcf487_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b11ac7dee2967ebac7497c5b7c4ac412a47aa4080e36cf0d94605e0611bcf487_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NeuralCrop: Combining physics and machine learning for improved crop yield predictions</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kausthubh Manda, Raghuram Bharadwaj Diddigi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20220" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20220</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96bb0ad288cee461b985922b13e1446ef6b03b8ee1b5f20f1e908e0e81174e2b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96bb0ad288cee461b985922b13e1446ef6b03b8ee1b5f20f1e908e0e81174e2b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Cost-TrustFL: Cost-Aware Hierarchical Federated Learning with Lightweight Reputation Evaluation across Multi-Cloud</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jixiao Yang, Jinyu Chen, Zixiao Huang, Chengda Xu, Chi Zhang, Sijia Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20218" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20218</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfa99879bd1a1c5083fc9e9363cb2903b42568a0726a124b591c9da1ed744c4f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfa99879bd1a1c5083fc9e9363cb2903b42568a0726a124b591c9da1ed744c4f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Cost-TrustFL: Cost-Aware Hierarchical Federated Learning with Lightweight Reputation Evaluation across Multi-Cloud</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Adaptive Multi-task Learning for Probabilistic Load Forecasting</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Onintze Zaballa, Verónica Álvarez, Santiago Mazuelas</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20232" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20232</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f105781b741c5e961d80700aae99a9a5f5491196f3957723a0ad4a9d0ceee4f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f105781b741c5e961d80700aae99a9a5f5491196f3957723a0ad4a9d0ceee4f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Adaptive Multi-task Learning for Probabilistic Load Forecasting</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] How I Met Your Bias: Investigating Bias Amplification in Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nathan Roos, Ekaterina Iakovleva, Ani Gjergji, Vito Paolo Pastore, Enzo Tartaglione</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20233" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20233</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3c43be0c18a78b4304c027f45372c35a663531d4f9536a15c2b4ca0dbb155b2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3c43be0c18a78b4304c027f45372c35a663531d4f9536a15c2b4ca0dbb155b2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> How I Met Your Bias: Investigating Bias Amplification in Diffusion Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xuanyu Hu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20249" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20249</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1c6a732f9a0082b695f01f79b2bcc47f909daa0f9d50a6af4d86a633213b328_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1c6a732f9a0082b695f01f79b2bcc47f909daa0f9d50a6af4d86a633213b328_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] HGAN-SDEs: Learning Neural Stochastic Differential Equations with Hermite-Guided Adversarial Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuanjian Xu, Yuan Shuai, Jianing Hao, Guang Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20272" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20272</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/336f6868f026371e14a0b1bb308e5d34193071d2aae6293400dd9ee01e404c90_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/336f6868f026371e14a0b1bb308e5d34193071d2aae6293400dd9ee01e404c90_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HGAN-SDEs: Learning Neural Stochastic Differential Equations with Hermite-Guided Adversarial Training</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology Pruning for Emergent Modularity</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuxing Gan, Ziyu Lei</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20291" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20291</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00b419d6d6ce75f3399c7716ea75c97138302ae8a2a1e0c9b1547fa29a97bde7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00b419d6d6ce75f3399c7716ea75c97138302ae8a2a1e0c9b1547fa29a97bde7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology Pruning for Emergent Modularity</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] DeepONet-accelerated Bayesian inversion for moving boundary problems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Marco A. Iglesias, Michael. E. Causon, Mikhail Y. Matveev, Andreas Endruweit, Michael .V. Tretyakov</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20268" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20268</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca57b0bdfa7635748d75eae61d7af10d528a81997788a87eb9ab2703231769b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca57b0bdfa7635748d75eae61d7af10d528a81997788a87eb9ab2703231769b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DeepONet-accelerated Bayesian inversion for moving boundary problems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Algorithm for Interpretable Graph Features via Motivic Persistent Cohomology</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yoshihiro Maruyama</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20311" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20311</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862ac961bd55b0b8dc1a2ea1c32580f402c570473c4f0ee0635c937b87e810e8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862ac961bd55b0b8dc1a2ea1c32580f402c570473c4f0ee0635c937b87e810e8_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Algorithm for Interpretable Graph Features via Motivic Persistent Cohomology</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Saisai Yang, Qingyi Huang, Jing Yuan, Liangyu Zha, Kai Tang, Yuhang Yang, Ning Wang, Yucheng Wei, Liyao Li, Wentao Ye, Hao Chen, Tao Zhang, Junlin Zhou, Haobo Wang, Gang Chen, Junbo Zhao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20312" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20312</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9634e9cda6fc809e3b25d9f21b937a2d8630ecb3e8bb9633968f223bb4e2d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9634e9cda6fc809e3b25d9f21b937a2d8630ecb3e8bb9633968f223bb4e2d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mrinmay Sen, Subhrajit Nag</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20329" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20329</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f60b7e61ea92d36b52de81bc3df02c7af567b1cd9eb4626ad016c3ce36765e1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f60b7e61ea92d36b52de81bc3df02c7af567b1cd9eb4626ad016c3ce36765e1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Toward Explaining Large Language Models in Software Engineering Tasks</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Antonio Vitale, Khai-Nguyen Nguyen, Denys Poshyvanyk, Rocco Oliveto, Simone Scalabrino, Antonio Mastropaolo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20328" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20328</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2779c954fdfbaebf8f7d7d236f2c601ab45082cae14229886e2b749d6d8cd669_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2779c954fdfbaebf8f7d7d236f2c601ab45082cae14229886e2b749d6d8cd669_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Toward Explaining Large Language Models in Software Engineering Tasks</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Top-K Exterior Power Persistent Homology: Algorithm, Structure, and Stability</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yoshihiro Maruyama</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20325" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20325</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d819bb4159585b74aeaf35355e16ef7a49b64ef54f670ff861804098262da1c5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d819bb4159585b74aeaf35355e16ef7a49b64ef54f670ff861804098262da1c5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Top-K Exterior Power Persistent Homology: Algorithm, Structure, and Stability</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Emilia Majerz, Witold Dzwinel, Jacek Kitowski</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20346" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20346</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d6aa7726e909753c196217ccc3bdf17d07a9339b0fc1d35361587823848df71_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d6aa7726e909753c196217ccc3bdf17d07a9339b0fc1d35361587823848df71_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Physics-guided Neural Network-based Shaft Power Prediction for Vessels</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dogan Altan, Hamza Haruna Mohammed, Glenn Terje Lines, Dusica Marijan, Arnbjørn Maressa</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20348" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20348</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d2fe8932d1c82e06128e6863fee4baaa3d988167af43502e32d83bf972eb356_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d2fe8932d1c82e06128e6863fee4baaa3d988167af43502e32d83bf972eb356_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Physics-guided Neural Network-based Shaft Power Prediction for Vessels</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Daniel M. Jimenez-Gutierrez, Mehrdad Hassanzadeh, Aris Anagnostopoulos, Ioannis Chatzigiannakis, Andrea Vitaletti</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20363" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20363</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59262adfe1a821db6db6b416af65f0ab7cc67a6943505bf052c9306bf801e87f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59262adfe1a821db6db6b416af65f0ab7cc67a6943505bf052c9306bf801e87f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Field-Space Attention for Structure-Preserving Earth System Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Maximilian Witte, Johannes Meuer, Étienne Plésiat, Christopher Kadow</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20350" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20350</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8623c14b77fd771294e1b936a58143cff6a715df1b4a5026fe2d59708383e6e2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8623c14b77fd771294e1b936a58143cff6a715df1b4a5026fe2d59708383e6e2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Field-Space Attention for Structure-Preserving Earth System Transformers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xuan-An Le, Minh-Nam Tran, Son Nguyen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20403" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20403</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61964964aad2cf8cac29992fe364ccde3a687f3d0e6b42b8148ce3b0f96dee99_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61964964aad2cf8cac29992fe364ccde3a687f3d0e6b42b8148ce3b0f96dee99_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] GeoTransolver: Learning Physics on Irregumar Domains Using Multi-scale Geometry Aware Physics Attention Transformer</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Corey Adams, Rishikesh Ranade, Ram Cherukuri, Sanjay Choudhry</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20399" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20399</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6966b4d62e10f76b2120afc54fcd95e1e717c9aef6840968117642fb0eb9df42_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6966b4d62e10f76b2120afc54fcd95e1e717c9aef6840968117642fb0eb9df42_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GeoTransolver: Learning Physics on Irregumar Domains Using Multi-scale Geometry Aware Physics Attention Transformer</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rajdeep Chatterjee, Sudip Chakrabarty, Trishaani Acharjee, Deepanjali Mishra</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20407" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20407</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e762dea30a9edc887d84deefc367d35dd7c953e636f54a824f1e7f853c9d370f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e762dea30a9edc887d84deefc367d35dd7c953e636f54a824f1e7f853c9d370f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Simplifying Multi-Task Architectures Through Task-Specific Normalization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mihai Suteu, Ovidiu Serban</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20420" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20420</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d24e6af533166dd44855079b97d7233c65878aa61e02267e65f4e306467d04b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d24e6af533166dd44855079b97d7233c65878aa61e02267e65f4e306467d04b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Simplifying Multi-Task Architectures Through Task-Specific Normalization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Machine Learning to Predict Digital Frustration from Clickstream Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jibin Joseph</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20438" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20438</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54b66cbccae694b64cb30bccff552dc3cd9c19b7ad622d4169444f7f1f67fdcb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54b66cbccae694b64cb30bccff552dc3cd9c19b7ad622d4169444f7f1f67fdcb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Machine Learning to Predict Digital Frustration from Clickstream Data</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Explainable time-series forecasting with sampling-free SHAP for Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Matthias Hertel, Sebastian Pütz, Ralf Mikut, Veit Hagenmeyer, Benjamin Schäfer</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20514" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20514</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce0ac2e96e47ae16fe2a238adc470cf7f3968e7b03d41a218d1483b207e0e532_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce0ac2e96e47ae16fe2a238adc470cf7f3968e7b03d41a218d1483b207e0e532_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Explainable time-series forecasting with sampling-free SHAP for Transformers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Recurrent Off-Policy Deep Reinforcement Learning Doesn&#x27;t Have to be Slow</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tyler Clark, Christine Evers, Jonathon Hare</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20513" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20513</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfed8377a526fc8b1686b7063bcf7aa6afc36205aa596ab393544501923d4a4a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfed8377a526fc8b1686b7063bcf7aa6afc36205aa596ab393544501923d4a4a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Recurrent Off-Policy Deep Reinforcement Learning Doesn&#x27;t Have to be Slow</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Long Nguyen, Micha Fauth, Bernhard Jaeger, Daniel Dauner, Maximilian Igl, Andreas Geiger, Kashyap Chitta</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20563" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20563</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f62c50da026de5eec286e01930af394650fb8b9c309ff48cd8f733ee9ca220b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f62c50da026de5eec286e01930af394650fb8b9c309ff48cd8f733ee9ca220b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Improving ML Training Data with Gold-Standard Quality Metrics</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Leslie Barrett, Michael W. Sherman</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20577" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20577</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fcda9dded34420c31f0b16ebfbeec41a79ad04607b8b4862e82f7a5504ae0ad_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fcda9dded34420c31f0b16ebfbeec41a79ad04607b8b4862e82f7a5504ae0ad_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Improving ML Training Data with Gold-Standard Quality Metrics</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Performative Policy Gradient: Optimality in Performative Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Debabrota Basu, Udvas Das, Brahim Driss, Uddalak Mukherjee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20576" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20576</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef3954869c8b30fe77c2caa1356c077d9f6b214d512935393a84011f79c0d20f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef3954869c8b30fe77c2caa1356c077d9f6b214d512935393a84011f79c0d20f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Performative Policy Gradient: Optimality in Performative Reinforcement Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rui Pan, Zhuofu Chen, Ravi Netravali</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20573" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20573</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab6d51f1421d2f929752b850a0d24b6a7af807b50f1d54c1101b257d175a44f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab6d51f1421d2f929752b850a0d24b6a7af807b50f1d54c1101b257d175a44f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Relu and softplus neural nets as zero-sum turn-based games</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Stephane Gaubert, Yiannis Vlassopoulos</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20582" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20582</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab02d27a8e6adea8ae6003cffa24e9fc6b3b0ee9d8f7ab828d024af5b17df0d7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab02d27a8e6adea8ae6003cffa24e9fc6b3b0ee9d8f7ab828d024af5b17df0d7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Relu and softplus neural nets as zero-sum turn-based games</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yedi Zhang, Andrew Saxe, Peter E. Latham</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20607" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20607</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e73c03581900a982dea4784fdd454e704c3d3b120fb84735e5f76640eb67bc86_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e73c03581900a982dea4784fdd454e704c3d3b120fb84735e5f76640eb67bc86_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network Architectures</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] FedPOD: the deployable units of training for federated learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Daewoon Kim, Si Young Yie, Jae Sung Lee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20610" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20610</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e8e1f095bc0447c82283e16e3fb09acd3ae3773107b9096b3a06a1659a978e0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e8e1f095bc0447c82283e16e3fb09acd3ae3773107b9096b3a06a1659a978e0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FedPOD: the deployable units of training for federated learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel, Angelika Steger, Maciej Wolczyk, Johannes von Oswald, Nino Scherre, Kaitlin Maile, Guillaume Lajoie, Blake A. Richards, Rif A. Saurous, James Manyika, Blaise Agüera y Arcas, Alexander Meulemans, João Sacramento</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20605" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20605</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e252cb57b3bace513337e4bc66ce47050af3dedc086216816f29d838d083c5f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e252cb57b3bace513337e4bc66ce47050af3dedc086216816f29d838d083c5f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LongVideoAgent: Multi-Agent Reasoning with Long Videos</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20618" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20618</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e38fba460d45fac945020bc323269f04211978c3e2e544d86072d5b062f40958_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e38fba460d45fac945020bc323269f04211978c3e2e544d86072d5b062f40958_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LongVideoAgent: Multi-Agent Reasoning with Long Videos</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Siyuan Fu, Xuchen Guo, Mingjun Liu, Hongxiang Li, Boyin Tan, Gongxi Zhu, Xianwei Zhuang, Jinghan Ru, Yuxin Xie, Yuguo Yin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19703" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19703</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ac2c582d6058b0a98adaddd2fc52e7fcb4b2f111f05471b7984fd691dc97aed_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ac2c582d6058b0a98adaddd2fc52e7fcb4b2f111f05471b7984fd691dc97aed_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] NMIRacle: Multi-modal Generative Molecular Elucidation from IR and NMR Spectra</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Federico Ottomano, Yingzhen Li, Alex M. Ganose</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19733" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19733</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e1b9a0fd1ed2cc7769e92111592f07cc1a8189724bd03924f8083fce8cf7a5b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e1b9a0fd1ed2cc7769e92111592f07cc1a8189724bd03924f8083fce8cf7a5b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NMIRacle: Multi-modal Generative Molecular Elucidation from IR and NMR Spectra</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Robust Causal Directionality Inference in Quantum Inference under MNAR Observation and High-Dimensional Noise</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Joonsung Kang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19746" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19746</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e5763bddf883e47ffbd39188c2d91d52b8f64bdca5101b8f91c97d4995c41b17_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e5763bddf883e47ffbd39188c2d91d52b8f64bdca5101b8f91c97d4995c41b17_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Robust Causal Directionality Inference in Quantum Inference under MNAR Observation and High-Dimensional Noise</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Fundamentals of quantum Boltzmann machine learning with visible and hidden units</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mark M. Wilde</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19819" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19819</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/109d3ad52a1d76b25a8f6b53d84a7219f99f5f6c4a1f4d3cfefd214d0a937f8f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/109d3ad52a1d76b25a8f6b53d84a7219f99f5f6c4a1f4d3cfefd214d0a937f8f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Fundamentals of quantum Boltzmann machine learning with visible and hidden units</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Chemically-Informed Machine Learning Approach for Prediction of Reactivity Ratios in Radical Copolymerization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Habibollah Safari, Mona Bavarian</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19715" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19715</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db0de246828ee3f9ea683f9e4205d984e45250d6966e0c692c62d8ded1fbecb0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db0de246828ee3f9ea683f9e4205d984e45250d6966e0c692c62d8ded1fbecb0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Chemically-Informed Machine Learning Approach for Prediction of Reactivity Ratios in Radical Copolymerization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Covariance-Aware Simplex Projection for Cardinality-Constrained Portfolio Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nikolaos Iliopoulos</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19986" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19986</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13f49f51d4e93dfaa76b5d85049c4b769868d3a8661c7cc58b3f721255548f39_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13f49f51d4e93dfaa76b5d85049c4b769868d3a8661c7cc58b3f721255548f39_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Covariance-Aware Simplex Projection for Cardinality-Constrained Portfolio Optimization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Efficient Learning of Lattice Gauge Theories with Fermions</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shreya Shukla, Yukari Yamauchi, Andrey Y. Lokhov, Scott Lawrence, Abhijith Jayakumar</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19891" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19891</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/006eec86d07c59f9bdf92e4095e9b3bdfcd0d6d88a3ff992c9ae1bcaa42efa8d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/006eec86d07c59f9bdf92e4095e9b3bdfcd0d6d88a3ff992c9ae1bcaa42efa8d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Efficient Learning of Lattice Gauge Theories with Fermions</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Quasiprobabilistic Density Ratio Estimation with a Reverse Engineered Classification Loss Function</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Matthew Drnevich, Stephen Jiggins, Kyle Cranmer</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19913" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19913</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae42de4f7e707842fef18d1ebed0d4af2b3ff8eda273d229ca4aeaac9e765c90_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae42de4f7e707842fef18d1ebed0d4af2b3ff8eda273d229ca4aeaac9e765c90_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Quasiprobabilistic Density Ratio Estimation with a Reverse Engineered Classification Loss Function</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Semiparametric KSD test: unifying score and distance-based approaches for goodness-of-fit testing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhihan Huang, Ziang Niu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20007" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20007</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ac47ca7745f3ae745307a996bf9615b4c854438f5673b23f72f2db0c17c0052_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ac47ca7745f3ae745307a996bf9615b4c854438f5673b23f72f2db0c17c0052_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Semiparametric KSD test: unifying score and distance-based approaches for goodness-of-fit testing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Reliable LLM-Based Edge-Cloud-Expert Cascades for Telecom Knowledge Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Qiushuo Hou, Sangwoo Park, Matteo Zecchin, Yunlong Cai, Guanding Yu, Osvaldo Simeone, Tommaso Melodia</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20012" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20012</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9dd4c42fc11f6f1f9e179a396845d6d9c90f18f4b2a0968536d1bbe730aaa182_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9dd4c42fc11f6f1f9e179a396845d6d9c90f18f4b2a0968536d1bbe730aaa182_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Reliable LLM-Based Edge-Cloud-Expert Cascades for Telecom Knowledge Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] GIMLET: Generalizable and Interpretable Model Learning through Embedded Thermodynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Suguru Shiratori, Elham Kiyani, Khemraj Shukla, George Em Karniadakis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19936" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19936</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/045354376dff81526b71609d7a09c5a83b94cf3ff400ab3fe060114398f02961_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/045354376dff81526b71609d7a09c5a83b94cf3ff400ab3fe060114398f02961_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GIMLET: Generalizable and Interpretable Model Learning through Embedded Thermodynamics</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Optimal Anytime-Valid Tests for Composite Nulls</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shubhanshu Shekhar</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20039" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20039</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6951c37e90b9cc41d18c9ac7d45b558a0fb6668f2f64b9527e2985cfe68c8bce_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6951c37e90b9cc41d18c9ac7d45b558a0fb6668f2f64b9527e2985cfe68c8bce_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Optimal Anytime-Valid Tests for Composite Nulls</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Gaussian Process Assisted Meta-learning for Image Classification and Object Detection Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Anna R. Flowers, Christopher T. Franck, Robert B. Gramacy, Justin A. Krometis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20021" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20021</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50cf659ea746173ff874243a1ed23ec74a9a603abf5c8d8333c36a4bf579ec63_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50cf659ea746173ff874243a1ed23ec74a9a603abf5c8d8333c36a4bf579ec63_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Gaussian Process Assisted Meta-learning for Image Classification and Object Detection Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Optimality-Informed Neural Networks for Solving Parametric Optimization Problems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Matthias K. Hoffmann, Amine Othmane, Kathrin Flaßkamp</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20270" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20270</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e060736a8b15ff91b7a61c98f5f036e0acad7f1f487053768b143db60163709_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e060736a8b15ff91b7a61c98f5f036e0acad7f1f487053768b143db60163709_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Optimality-Informed Neural Networks for Solving Parametric Optimization Problems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold Networks with Accelerated Failure Time Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mebin Jose, Jisha Francis, Sudheesh Kumar Kattumannil</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20305" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20305</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f5766a6c4e9984e1a7c6234a1bf0776e7a9fad9ef648cc4ca17ad3ba035698_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f5766a6c4e9984e1a7c6234a1bf0776e7a9fad9ef648cc4ca17ad3ba035698_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold Networks with Accelerated Failure Time Analysis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Samya Praharaj, Koulik Khamaru</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20368" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20368</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/912d43c0875266e42e18f18ddc2bcf11aaf3e8607a61856f82ebf3a2932bf5e1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/912d43c0875266e42e18f18ddc2bcf11aaf3e8607a61856f82ebf3a2932bf5e1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] The Aligned Economic Index &amp; The State Switching Model</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ilias Aarab</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20460" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20460</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac7ffd2530ee7539902991cd05061cc86de381735b4620a6937d0821e9cb5403_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac7ffd2530ee7539902991cd05061cc86de381735b4620a6937d0821e9cb5403_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The Aligned Economic Index &amp; The State Switching Model</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] ScoreMatchingRiesz: Auto-DML with Infinitesimal Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Masahiro Kato</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20523" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20523</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ca45318d031a0b7e00633a6c1f048fa9feead63ea3447f2257acc9011d26b6c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ca45318d031a0b7e00633a6c1f048fa9feead63ea3447f2257acc9011d26b6c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ScoreMatchingRiesz: Auto-DML with Infinitesimal Classification</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Over-the-Air Goal-Oriented Communications</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kyriakos Stylianopoulos, Paolo Di Lorenzo, George C. Alexandropoulos</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20533" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20533</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9cf38466c85fbc6d90f4a4b0d12601a4d1aa3d13a53fc9c4c997edc35e26a213_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9cf38466c85fbc6d90f4a4b0d12601a4d1aa3d13a53fc9c4c997edc35e26a213_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Over-the-Air Goal-Oriented Communications</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Shallow Neural Networks Learn Low-Degree Spherical Polynomials with Learnable Channel Attention</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yingzhen Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20562" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20562</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/126073a8d27aee52f27e9654f39d510e34be1f6aaec1e9c80cd8b6d23342281e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/126073a8d27aee52f27e9654f39d510e34be1f6aaec1e9c80cd8b6d23342281e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Shallow Neural Networks Learn Low-Degree Spherical Polynomials with Learnable Channel Attention</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-25">2025-12-25<a href="#2025-12-25" class="hash-link" aria-label="Direct link to 2025-12-25" title="Direct link to 2025-12-25" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251225] Parameter-Efficient Neural CDEs via Implicit Function Jacobians</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series analysis], [Neural Controlled Differential Equations, parameter efficiency, implicit function Jacobians, continuous RNN]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ilya Kuleshov, Alexey Zaytsev</p>
</li>
<li class="">
<p><strong>institution:</strong> Applied AI Institute</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20625" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20625</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel, parameter-efficient formulation of Neural Controlled Differential Equations (NCDEs) that drastically reduces the number of required parameters. 2. Introduces a logical interpretation of the method as a &quot;Continuous RNN,&quot; aligning with the original inspiration of NCDEs. 3. Presents a method leveraging implicit function Jacobians to achieve this efficiency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f69d35dc890877df610e96a1c984c641596f7fcac2c4ff1dbf30f641c90d5d77_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f69d35dc890877df610e96a1c984c641596f7fcac2c4ff1dbf30f641c90d5d77_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high parameter cost of Neural Controlled Differential Equations (NCDEs) for temporal sequence analysis. It proposes a new, parameter-efficient formulation that reinterprets NCDEs as a &quot;Continuous RNN&quot; and uses implicit function Jacobians to reduce the parameter count. The main conclusion is that this approach maintains the modeling power of NCDEs while being significantly more parameter-efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Uncovering Patterns of Brain Activity from EEG Data Consistently Associated with Cybersickness Using Neural Network Interpretability Maps</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [brain-computer interface], [EEG, cybersickness, interpretability maps, convolutional neural networks, event-related potentials]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jacqueline Yau, Katherine J. Mimnaugh, Evan G. Center, Timo Ojala, Steven M. LaValle, Wenzhen Yuan, Nancy Amato, Minje Kim, Kara Federmeier</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Illinois Urbana-Champaign, University of Oulu</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20620" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20620</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a method using CNNs and transformers with interpretability maps (integrated gradients and class activation) to identify EEG features for cybersickness classification. 2. Identified a consistent and surprising pattern: amplitudes near the left prefrontal cortex electrode are important for cybersickness classification. 3. Proposed using the identified scalp location as a tagged feature for better real-time cybersickness classification with EEG.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5601623e3cdfb620242f065ddf726ad49d48b3d131d2e3cc1f6fa48032be9946_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5601623e3cdfb620242f065ddf726ad49d48b3d131d2e3cc1f6fa48032be9946_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of detecting cybersickness from EEG data by using event-related potentials to isolate sickness-related brain activity from visual stimulus confounds. The authors employ trained convolutional neural networks and transformer models with interpretability maps to identify key EEG features. The main finding is that amplitudes recorded near the left prefrontal cortex are consistently important for classification, suggesting this location as a valuable feature for real-time detection.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [multi-agent language systems, latent strategy evolution, reinforcement feedback, external latent vectors, dual-loop architecture]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenlong Tang</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher (No institutional affiliation inferred from provided content)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20629" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20629</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/wltang-dev/Latent-Strategy-RL-Agent" target="_blank" rel="noopener noreferrer" class="">https://github.com/wltang-dev/Latent-Strategy-RL-Agent</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel multi-agent language framework that enables continual strategy evolution without fine-tuning the underlying language model&#x27;s parameters. 2. Introduces a dual-loop architecture (behavior loop and language loop) that updates external latent vectors through environmental interaction and semantic reflection on generated text. 3. Demonstrates that this approach allows agents to develop stable, disentangled strategic styles and shows emergent adaptation capabilities, providing a low-cost, scalable, and interpretable form of abstract strategic representation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c15e8361c02b5e6e0c755d3089af5adddafaad00ffda95b887b8eca526280761_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c15e8361c02b5e6e0c755d3089af5adddafaad00ffda95b887b8eca526280761_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of static semantic representations in language models by proposing a framework where agents evolve strategies without model fine-tuning. The core method uses a dual-loop architecture to update external latent vectors through environmental rewards and reflection on generated text. The results show that this enables agents to develop adaptable and interpretable strategic behaviors, offering a scalable alternative to parameter tuning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Zero-Training Temporal Drift Detection for Transformer Sentiment Models: A Comprehensive Analysis on Authentic Social Media Streams</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [sentiment analysis], [temporal drift, zero-training detection, transformer models, social media streams, model instability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aayam Bansal, Ishaan Gangwani</p>
</li>
<li class="">
<p><strong>institution:</strong> IEEE</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20631" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20631</a></p>
</li>
<li class="">
<p><strong>contributions:</strong>  1. Demonstrated significant temporal drift in transformer sentiment models during real-world events, with accuracy drops up to 23.4% on authentic social media data. 2. Introduced four novel zero-training drift detection metrics that outperform embedding-based baselines and are suitable for production deployment. 3. Provided comprehensive statistical validation on 12,279 authentic social media posts from major events, establishing practical significance exceeding industry monitoring thresholds.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8e40c0a23df847aa858c5e0ce28602f612024103d66ccaea9f9da99a1dded46_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8e40c0a23df847aa858c5e0ce28602f612024103d66ccaea9f9da99a1dded46_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of temporal drift in transformer-based sentiment models during real-world events without requiring model retraining. It proposes a zero-training detection framework using novel inference-time metrics, validated on authentic social media data. The main conclusion is that this method effectively detects significant model instability and enables immediate deployment for real-time monitoring systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Enhancing Lung Cancer Treatment Outcome Prediction through Semantic Feature Engineering Using Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [clinical prediction], [large language models, semantic feature engineering, multi-modal data integration, goal-oriented knowledge curator, treatment outcome prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> MunHwan Lee, Shaika Chowdhury, Xiaodi Li, Sivaraman Rajaganapathy, Eric W Klee, Ping Yang, Terence Sio, Liewei Wang, James Cerhan, Nansu NA Zong</p>
</li>
<li class="">
<p><strong>institution:</strong> Mayo Clinic</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20633" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20633</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel framework using Large Language Models (LLMs) as Goal-oriented Knowledge Curators (GKC) to generate task-aligned semantic features from raw clinical data, 2. Demonstrates that GKC, as an offline preprocessing step, outperforms expert-engineered features, direct embeddings, and end-to-end transformers in predicting lung cancer treatment outcomes, 3. Shows the complementary value of integrating laboratory, genomic, and medication modalities through ablation studies, highlighting semantic representation quality as key for accuracy in sparse data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c8925ebbf05c9b81fd60fa118034a660d2673702659d23d8b6cf7c1d976903_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c8925ebbf05c9b81fd60fa118034a660d2673702659d23d8b6cf7c1d976903_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of predicting lung cancer treatment outcomes from sparse, heterogeneous clinical data by introducing a framework that uses Large Language Models as Goal-oriented Knowledge Curators to engineer semantic, task-specific features. This method outperforms traditional baselines, achieving a mean AUROC of 0.803, and demonstrates that high-quality semantic representation is crucial for predictive accuracy in clinical settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Real Time Detection and Quantitative Analysis of Spurious Forgetting in Continual Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [catastrophic forgetting, spurious forgetting, shallow alignment, deep alignment, task alignment depth]</p>
</li>
<li class="">
<p><strong>authors:</strong> Weiwei Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shenzhen Sunline Tech Co., Ltd.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20634" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20634</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a quantitative framework (shallow vs. deep alignment) to measure task alignment depth across token positions. 2. Developed real-time detection methods and analysis tools for identifying shallow alignment and spurious forgetting during training. 3. Proposed adaptive mitigation strategies that automatically distinguish forgetting types and promote deep alignment to improve model robustness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2920732eeec32977638a62ffbcf4b4b075dbdd77ebc58fa777ff8a10e117219_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2920732eeec32977638a62ffbcf4b4b075dbdd77ebc58fa777ff8a10e117219_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses catastrophic forgetting in continual learning for LLMs by identifying that performance drops are often due to &quot;spurious forgetting&quot; from shallow task alignment. The authors propose a framework to quantitatively measure alignment depth, detect shallow alignment in real-time, and apply mitigation strategies to promote deep alignment. Experiments show their method accurately identifies spurious forgetting and improves model robustness against forgetting by 3.3-7.1% over baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] SHRP: Specialized Head Routing and Pruning for Efficient Encoder Compression</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [structured pruning, attention head, expert attention, dynamic routing, inference latency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zeli Su, Ziyin Zhang, Wenzheng Zhang, Zhou Liu, Guixian Xu, Wentao Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Minzu University of China, Shanghai Jiao Tong University, Peking University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20635" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20635</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SHRP, a novel structured pruning framework that treats attention heads as independent experts and uses a unified Top-1 usage-driven mechanism for dynamic routing and deterministic pruning. 2. Introduces Expert Attention, a modular design with a lightweight shared expander feed-forward network to refine outputs after head selection. 3. Demonstrates significant compression on BERT-base, achieving high parameter/FLOP reduction with minimal accuracy loss, enabling practical deployment for latency-sensitive services.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eee1b632660555a4b466586c0f3f7e064778c9087cde3160efa73cb5e0bf7723_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eee1b632660555a4b466586c0f3f7e064778c9087cde3160efa73cb5e0bf7723_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high inference latency and memory consumption of Transformer encoders by proposing SHRP, a structured pruning framework that identifies and removes redundant attention heads. The method uses an Expert Attention module and a unified routing mechanism to compress the model while preserving accuracy. Experiments show SHRP can reduce BERT-base&#x27;s parameters by 48% with 93% accuracy retained, and achieve a 4.2x throughput gain under extreme compression.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Data-Free Pruning of Self-Attention Layers in LLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [attention pruning, data-free pruning, Gate-Norm, inference acceleration, attention suppression hypothesis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dhananjay Saikumar, Blesson Varghese</p>
</li>
<li class="">
<p><strong>institution:</strong> University of St Andrews</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20636" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20636</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the Attention Suppression Hypothesis to explain the redundancy of deep self-attention layers in LLMs. 2. Introduces Gate-Norm, a one-shot, weight-only criterion for ranking and pruning attention sublayers without requiring data, forward passes, or fine-tuning. 3. Demonstrates that pruning 8-16 attention layers with Gate-Norm yields up to 1.30x higher inference throughput while maintaining accuracy within 2% of the baseline, matching data-driven methods but being ~1000x faster.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9338cbf768451f7709aa625ae03202bc7b84fcaa758ea1d75a6f5eaa4aa228c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9338cbf768451f7709aa625ae03202bc7b84fcaa758ea1d75a6f5eaa4aa228c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high inference cost of LLMs by proposing a data-free method to prune redundant self-attention layers. It introduces Gate-Norm, a fast weight-only criterion based on query-key coupling, which removes layers without needing calibration data or fine-tuning. The method significantly speeds up inference while preserving model accuracy, enabling practical LLM compression.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Uncovering Competency Gaps in Large Language Models and Their Benchmarks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [llm evaluation], [sparse autoencoders, benchmark gaps, model gaps, concept activations, competency gaps]</p>
</li>
<li class="">
<p><strong>authors:</strong> Matyas Bohacek, Nino Scherrer, Nicholas Dufour, Thomas Leung, Christoph Bregler, Stephanie C. Y. Chan</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University, Google DeepMind</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20638" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20638</a></p>
</li>
<li class="">
<p><strong>code:</strong> competency-gaps.github.io</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel method using sparse autoencoders (SAEs) to automatically uncover fine-grained competency gaps in LLMs and benchmarks. 2. Introduces a representation-grounded evaluation approach that computes saliency-weighted performance scores based on model-internal concept activations. 3. Demonstrates the method&#x27;s ability to identify specific model weaknesses (e.g., non-sycophantic behaviors) and benchmark coverage imbalances (e.g., over-representation of obedience concepts) without manual supervision.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6d5ab9e8ede467e65cbc2079e58ecf9b8d8ade8145f7e9e90f1b6f8382e288b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6d5ab9e8ede467e65cbc2079e58ecf9b8d8ade8145f7e9e90f1b6f8382e288b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that aggregated benchmark scores can hide specific weaknesses in LLMs and imbalances in benchmark coverage. The authors propose an automated method using sparse autoencoders to decompose benchmark performance into fine-grained concepts based on the model&#x27;s internal representations. Their analysis of two models and ten benchmarks revealed model gaps in areas like non-sycophancy and safety, and benchmark gaps such as an over-representation of obedience-related concepts.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Forecasting N-Body Dynamics: A Comparative Study of Neural Ordinary Differential Equations and Universal Differential Equations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scientific machine learning], [Neural Ordinary Differential Equations, Universal Differential Equations, forecasting breakdown point, n-body problem, Julia]</p>
</li>
<li class="">
<p><strong>authors:</strong> Suriya R S, Prathamesh Dinesh Joshi, Rajat Dandekar, Raj Dandekar, Sreedath Panat</p>
</li>
<li class="">
<p><strong>institution:</strong> Vizuara AI Labs</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20643" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20643</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a comparative study of Neural ODEs and Universal Differential Equations (UDEs) for forecasting n-body dynamics, a fundamental astrophysics problem. 2. Introduced and determined the &quot;forecasting breakdown point&quot; to quantify the minimal training data required for accurate future predictions. 3. Demonstrated that the UDE model, which blends known physics with neural networks, is significantly more data-efficient, requiring only 20% of data for a correct forecast compared to 90% for a Neural ODE.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06b852f384c602c478fd1ea2166cf9ac3e8442f63a46b1d479333a1e00699c6b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06b852f384c602c478fd1ea2166cf9ac3e8442f63a46b1d479333a1e00699c6b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper compares two Scientific Machine Learning frameworks, Neural ODEs and Universal Differential Equations (UDEs), for forecasting the dynamics of the n-body problem. The study introduces the concept of a &quot;forecasting breakdown point&quot; to measure data efficiency and finds that the UDE model, which incorporates known physical laws, is far more efficient, requiring only 20% of the training data that a Neural ODE needs for accurate predictions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] MaskOpt: A Large-Scale Mask Optimization Dataset to Advance AI in Integrated Circuit Manufacturing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [mask optimization, optical proximity correction, inverse lithography technique, deep learning, benchmark dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuting Hu, Lei Zhuang, Hua Xiang, Jinjun Xiong, Gi-Joon Nam</p>
</li>
<li class="">
<p><strong>institution:</strong> University at Buffalo, IBM Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20655" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20655</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MaskOpt, a large-scale benchmark dataset for AI-driven mask optimization constructed from real 45nm IC designs, addressing limitations of synthetic data. 2. The dataset preserves standard-cell hierarchy and includes varying context window sizes to enable cell- and context-aware model training. 3. Provides comprehensive benchmarks by evaluating state-of-the-art deep learning models, revealing performance trade-offs and validating the importance of contextual and cell-aware inputs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce9bc1ac552258257c450a9bc4d4c4ae0264c958efae291f56fc44af367bf07a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce9bc1ac552258257c450a9bc4d4c4ae0264c958efae291f56fc44af367bf07a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents MaskOpt, a large-scale dataset built from real integrated circuit designs to advance deep learning for mask optimization in semiconductor manufacturing. It addresses the limitations of existing synthetic datasets by including cell hierarchy and surrounding context. Benchmarking results demonstrate the dataset&#x27;s utility and highlight the critical role of context and cell information for accurate mask generation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Q-RUN: Quantum-Inspired Data Re-uploading Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [quantum machine learning], [data re-uploading, Fourier-expressive, quantum-inspired, neural network layer, parameter efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenbo Qiao, Shuaixian Wang, Peng Zhang, Yan Ming, Jiaming Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Tianjin University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20654" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20654</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Q-RUN, a novel quantum-inspired neural network layer that translates the mathematical paradigm of Data Re-uploading Quantum Circuits (DRQC) into a classical model, retaining their Fourier-expressive power without requiring quantum hardware. 2. Demonstrates that Q-RUN significantly outperforms standard fully connected layers and other state-of-the-art layers, reducing error by 1-3 orders of magnitude on certain tasks while using fewer parameters. 3. Shows that Q-RUN can serve as a versatile, drop-in replacement for fully connected layers, improving performance across a wide range of neural architectures and illustrating how quantum ML principles can enhance classical AI design.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0762ab538e009e0b634d9be449e9107606ff0182095b377b357ae3a8796d291b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0762ab538e009e0b634d9be449e9107606ff0182095b377b357ae3a8796d291b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Q-RUN, a quantum-inspired neural network layer based on data re-uploading principles, designed to capture high-frequency functions efficiently without quantum hardware. It demonstrates superior performance and parameter efficiency compared to standard layers across various modeling tasks. The work shows how quantum machine learning concepts can guide the development of more expressive classical AI models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Managing the Stochastic: Foundations of Learning in Neuro-Symbolic Systems for Software Engineering</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [dual-state architecture, atomic action pairs, guard functions, neuro-symbolic systems, code generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Matthew Thompson</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20660" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20660</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a control boundary that treats the LLM as a stochastic environment component, not the decision-making agent, to manage its unpredictability. 2. Formalizes a Dual-State Architecture separating deterministic workflow state from stochastic environment state. 3. Introduces Atomic Action Pairs and Guard Functions to couple generation with verification as indivisible transactions, projecting probabilistic outputs onto observable workflow state.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a73fceac46d6997904de43696e8db407d645c6e4388012a9e24a3b9565e06fb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a73fceac46d6997904de43696e8db407d645c6e4388012a9e24a3b9565e06fb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of stochastic failures in AI coding agents by proposing a neuro-symbolic architectural framework that treats the LLM as part of the environment. The method uses a Dual-State Architecture with Atomic Action Pairs and Guard Functions to separate deterministic control from stochastic generation. The main conclusion is that such architectural constraints can significantly improve task success rates for qualified models, potentially substituting for parameter scale in achieving reliable code generation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Graph Neural Networks for Source Detection: A Review and Benchmark Study</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [Graph Neural Networks, Epidemic Source Detection, Rumor Centrality, Benchmark Study, Single-Source Problems]</p>
</li>
<li class="">
<p><strong>authors:</strong> Martin Sterchi, Nathan Brack, Lorenz Hilfiker</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Applied Sciences and Arts Northwestern Switzerland FHNW, University of Zürich</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20657" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20657</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A comprehensive review of existing GNN-based methods for source detection, clarifying their settings and models. 2. Proposal of a principled GNN architecture specifically tailored for the source detection task. 3. A systematic benchmark study demonstrating that GNNs substantially outperform traditional source detection methods across various network types, and advocating for epidemic source detection as a benchmark task for evaluating GNN architectures.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3b943e72216650945e413f1e6dcd8c0978e12972df42eff52dc2deb20fcbe59_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3b943e72216650945e413f1e6dcd8c0978e12972df42eff52dc2deb20fcbe59_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper reviews Graph Neural Network (GNN) approaches for detecting the source of an epidemic in a network, proposes a new GNN architecture for the task, and conducts a benchmark study. The experiments show that GNNs significantly outperform traditional methods like rumor centrality, establishing them as highly effective for source detection and suggesting the task as a standard benchmark for GNN evaluation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Dominating vs. Dominated: Generative Collapse in Diffusion Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [text-to-image generation], [diffusion models, cross-attention, generative collapse, multi-concept generation, attention dynamics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hayeon Jeong, Jong-Seok Lee</p>
</li>
<li class="">
<p><strong>institution:</strong> Yonsei University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20666" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20666</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and defines the Dominant-vs-Dominated (DvD) phenomenon in multi-concept text-to-image generation, 2. Introduces DominanceBench for systematic analysis of the DvD imbalance, 3. Provides causal analysis from data (limited instance diversity) and architecture (cross-attention saturation &amp; distributed head mechanisms) perspectives.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e3797020eb871e661d9cda59fdbe8a7bdf314a2935eff1ccf97c35a90d39ee_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e3797020eb871e661d9cda59fdbe8a7bdf314a2935eff1ccf97c35a90d39ee_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the &quot;Dominant-vs-Dominated&quot; (DvD) imbalance in diffusion models, where one concept token suppresses others in multi-concept prompts. The authors analyze this using a new benchmark and find causes in limited training data diversity and cross-attention dynamics. Their findings offer insights into generative collapse for more reliable text-to-image generation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Forward Only Learning for Orthogonal Neural Networks of any Depth</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [neural network training algorithms], [forward-only learning, orthogonal neural networks, backpropagation alternative, FOTON, PEPITA]</p>
</li>
<li class="">
<p><strong>authors:</strong> Paul Caillon, Alex Colagrande, Erwan Fagnou, Blaise Delattre, Alexandre Allauzen</p>
</li>
<li class="">
<p><strong>institution:</strong> Université Paris-Dauphine - PSL, ESPCI PSL</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20668" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20668</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/" target="_blank" rel="noopener noreferrer" class="">https://github.com/</a> (URL mentioned as &quot;this https URL&quot; and &quot;open-sourced on github&quot; in the abstract/first page)</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Theoretical analysis of limitations in existing forward-only frameworks like PEPITA, 2. Design of a forward-only algorithm equivalent to backpropagation under linear/orthogonal assumptions, 3. Introduction of FOTON, a practical forward-only training method for orthogonal networks that scales to any depth and works on CNNs&gt;</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14ce0cf521f1d147eae7293cef8a5a53d6a0ca2fda6723bc707498635d351a0b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14ce0cf521f1d147eae7293cef8a5a53d6a0ca2fda6723bc707498635d351a0b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the computational burden of backpropagation by proposing a forward-only training algorithm called FOTON for orthogonal neural networks. The method replaces the backward pass with a modulated forward pass, enabling training of deep networks without backpropagation. Experiments show FOTON outperforms prior forward-only methods and scales to networks of any depth, including convolutional architectures.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Improving Cardiac Risk Prediction Using Data Generation Techniques</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models], [Conditional Variational Autoencoder, synthetic data generation, cardiac risk prediction, data augmentation, clinical records]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alexandre Cabodevila, Pedro Gamallo-Fernandez, Juan C. Vidal, Manuel Lama</p>
</li>
<li class="">
<p><strong>institution:</strong> Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS), Universidade de Santiago de Compostela</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20669" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20669</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel architecture based on a Conditional Variational Autoencoder (CVAE) for generating realistic and coherent synthetic clinical records. 2. Addresses key limitations in medical data analysis such as data scarcity, unsuitability, and high prevalence of missing values. 3. Demonstrates that using the generated synthetic data improves the accuracy of cardiac risk prediction classifiers, outperforming other deep learning data generation approaches.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0661f3f558f42310471788ea2bad662661692287e3137248998355eb91c8470b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0661f3f558f42310471788ea2bad662661692287e3137248998355eb91c8470b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of scarce and incomplete real-world medical data for cardiac risk prediction by proposing a Conditional Variational Autoencoder (CVAE) architecture to generate realistic synthetic clinical records. The generated data is used to augment datasets, which in turn enhances the performance of cardiac risk prediction models. The results show that the proposed method successfully generates coherent data and improves classifier accuracy compared to state-of-the-art alternatives.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Disentangling Fact from Sentiment: A Dynamic Conflict-Consensus Framework for Multimodal Fake News Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal fake news detection], [inconsistency detection, feature disentanglement, conflict-consensus mechanism, physics-inspired dynamics, cross-modal discrepancy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Weilin Zhou, Zonghao Ying, Junjie Mu, Shengwei Tian, Quanchen Zou, Deyue Zhang, Dongdong Yang, Xiangzheng Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Xinjiang University, 360 AI Security Lab, Beihang University, Politecnico di Milano</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20670" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20670</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a paradigm shift from consistency-seeking to inconsistency-seeking for multimodal fake news detection, explicitly amplifying cross-modal contradictions as evidence. 2. Introduces a novel framework (DCCF) that disentangles inputs into independent Fact and Sentiment spaces to separate objective mismatches from emotional dissonance. 3. Employs physics-inspired feature dynamics and a conflict-consensus mechanism to actively polarize and standardize local discrepancies against a global context for robust judgment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c06ff160d4943f1ae5c4648f6e6cd042a0c1232af6212adf0021ba7f0b7c2ab_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c06ff160d4943f1ae5c4648f6e6cd042a0c1232af6212adf0021ba7f0b7c2ab_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a flaw in mainstream multimodal fake news detection, which treats cross-modal discrepancies as noise, and proposes a new Dynamic Conflict-Consensus Framework (DCCF) designed to actively seek and amplify these inconsistencies as evidence of fabrication. The method disentangles fact from sentiment and uses physics-inspired dynamics to extract conflicts. Experiments show DCCF outperforms state-of-the-art baselines with an average accuracy improvement of 3.52%.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Revisiting the Learning Objectives of Vision-Language Reward Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [reward modeling, vision-language models, triplet loss, Meta-World, contrastive learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Simon Roy, Samuel Barbeau, Giovanni Beltrame, Christian Desrosiers, Nicolas Thome</p>
</li>
<li class="">
<p><strong>institution:</strong> Polytechnique Montréal, École de Technologie Supérieure, Sorbonne Université</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20675</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified framework to isolate and evaluate the impact of learning objectives in vision-language reward models, controlling for backbone, data, and evaluation environments. 2. Demonstrates that a simple triplet loss objective can outperform more complex state-of-the-art methods for reward modeling. 3. Suggests that improvements in recent approaches may be attributed more to differences in training data and model architectures rather than the complexity of their learning objectives.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca907344b4dbef770bf1367dee07e4ba6b6f7a2b525ad28c7bf8d0ff11f62075_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca907344b4dbef770bf1367dee07e4ba6b6f7a2b525ad28c7bf8d0ff11f62075_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the impact of different learning objectives for adapting vision-language models into reward functions for embodied intelligence. By comparing methods under a unified framework, the authors find that a simple triplet loss outperforms more complex state-of-the-art objectives. The results suggest that recent improvements in reward modeling may stem from data and architecture differences rather than objective complexity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [Low-Rank Adaptation (LoRA), parameter-efficient fine-tuning, rank adaptation, mobile vision language model, dynamic scheduling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuanhao Xi, Xiaohuan Bing, Ramin Yahyapour</p>
</li>
<li class="">
<p><strong>institution:</strong> Liaoning Technical University, University of Göttingen, Gesellschaft für Wissenschaftliche Datenverarbeitung mbH Göttingen</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20674" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20674</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes HyDRA, a parameter-efficient fine-tuning framework for mobile VLMs that implements hierarchical and dynamic rank scheduling. 2. Introduces hierarchical optimization with coarse-grained (layer-level) and fine-grained (intra-layer) rank assignment. 3. Employs dynamic adjustment via an end-to-end automatic optimization using a lightweight performance model to determine ranks during fine-tuning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9589d36616e78e4826e61d2dbafa4ccc718075f3659352d4d01c1b6f4795a02a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9589d36616e78e4826e61d2dbafa4ccc718075f3659352d4d01c1b6f4795a02a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces HyDRA, a parameter-efficient fine-tuning framework for mobile Vision Language Models (VLMs) that addresses the computational inefficiency of standard LoRA by implementing hierarchical and dynamic rank scheduling. The method uses a two-pronged optimization strategy and a lightweight performance model to adjust ranks automatically. Experiments show HyDRA outperforms baselines, achieving a 4.7% average improvement without extra parameters and sometimes surpassing full fine-tuning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent systems], [Differentiable Price Mechanism, Dominant Strategy Incentive Compatibility, VCG-equivalent incentive, Dec-POMDPs, Bayesian Incentive Compatibility]</p>
</li>
<li class="">
<p><strong>authors:</strong> Stefano Grassi</p>
</li>
<li class="">
<p><strong>institution:</strong> None (No affiliation or email domain provided in the given content)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20688" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20688</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Mechanism-Based Intelligence (MBI), a new paradigm framing intelligence as emergent from the coordination of multiple agents. 2. Introduces the Differentiable Price Mechanism (DPM), which computes exact loss gradients as incentive signals to guarantee Dominant Strategy Incentive Compatibility and convergence. 3. Demonstrates a framework that scales linearly with the number of agents, bypassing Dec-POMDP complexity and showing significant empirical speedup over model-free RL.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfdd6bb51cf65ab558a1d13cbaf0fbdda25f9c06c58be3e201a62b231f808da4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfdd6bb51cf65ab558a1d13cbaf0fbdda25f9c06c58be3e201a62b231f808da4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the fragility of multi-agent systems in coordinating private information and aligning incentives. It proposes Mechanism-Based Intelligence (MBI) and its core Differentiable Price Mechanism (DPM), which uses differentiable incentives to align agent actions with global objectives. The method guarantees incentive compatibility, scales efficiently, and is shown to be much faster than standard reinforcement learning approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [hierarchical autoregressive model, KV-cache optimization, memory-bound inference, multi-resolution context, throughput-quality trade-off]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuma Ichikawa, Naoya Takagi, Takumi Nakagawa, Yuzi Kanazawa, Akira Sakai</p>
</li>
<li class="">
<p><strong>institution:</strong> Fujitsu Limited, RIKEN Center for AIP, Institute of Science Tokyo, Tokai University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20687" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20687</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes PHOTON, a hierarchical autoregressive model that replaces the Transformer&#x27;s flat token-by-token scanning with a vertical, multi-resolution context access pattern. 2. Introduces a persistent hierarchy of latent streams, with a bottom-up encoder compressing tokens and lightweight top-down decoders reconstructing token representations, reducing decode-time KV-cache traffic. 3. Demonstrates significant improvements in throughput per unit memory (up to 10^3x) and advantages in long-context and multi-query tasks compared to Transformer-based models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6824d7ad660d8d52e1568c90187924380b5fd436a69942bfac67084af3298d40_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6824d7ad660d8d52e1568c90187924380b5fd436a69942bfac67084af3298d40_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that Transformer inference becomes memory-bound due to ever-growing KV-cache reads/writes during autoregressive decoding. To solve this, it proposes PHOTON, a hierarchical model that accesses context vertically at multiple resolutions instead of scanning tokens horizontally. This architectural change drastically reduces memory traffic, yielding orders-of-magnitude higher throughput per unit memory while maintaining quality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Real-World Adversarial Attacks on RF-Based Drone Detectors</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [adversarial machine learning], [adversarial attack, drone detection, radio frequency (RF), over-the-air (OTA), I/Q perturbation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Omer Gazit, Yael Itzhakev, Yuval Elovici, Asaf Shabtai</p>
</li>
<li class="">
<p><strong>institution:</strong> Ben-Gurion University of the Negev</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20712" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20712</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. The first physical adversarial attack targeting image-based object detection models for RF-based drone detection. 2. A novel method that optimizes adversarial perturbations directly in the complex baseband (I/Q) domain for over-the-air transmission. 3. Demonstration of the attack&#x27;s effectiveness and hardware compatibility through both digital and physical (OTA) evaluations with multiple drone types.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/384206c86ac50ef9b9d711e77f0b2e691640ea7831fb12c1517a40575d8c07f3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/384206c86ac50ef9b9d711e77f0b2e691640ea7831fb12c1517a40575d8c07f3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes the first physical adversarial attack against RF-based drone detectors. Instead of modifying digital spectrogram images, the method generates and transmits optimized I/Q perturbation waveforms alongside legitimate drone signals. The results show these structured perturbations are compatible with standard RF hardware and reliably reduce target drone detection while maintaining detection of legitimate ones.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [Finite Element Method (FEM), Code Generation, LLM Benchmark, Computational Mechanics, Scientific Machine Learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Saeed Mohammadzadeh, Erfan Hamdi, Joel Shor, Emma Lejeune</p>
</li>
<li class="">
<p><strong>institution:</strong> Boston University, Move37 Labs</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20732" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20732</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces FEM-Bench, a novel benchmark for evaluating LLMs&#x27; ability to generate scientifically valid code for computational mechanics problems. 2. Provides a structured suite of tasks based on finite element methods that enforce physical and numerical constraints for objective evaluation. 3. Presents initial evaluation results showing that state-of-the-art LLMs (e.g., Gemini 3 Pro, GPT-5) still struggle to reliably solve these introductory tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1933a2d33b13b692f95ee8ddec0a65840af091998d38a7f0154837874636590_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1933a2d33b13b692f95ee8ddec0a65840af091998d38a7f0154837874636590_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a lack of benchmarks for evaluating LLMs&#x27; scientific reasoning and code generation for physical modeling. It proposes FEM-Bench, a computational mechanics benchmark based on the Finite Element Method, to fill this gap. Initial evaluations show that even advanced LLMs cannot reliably solve all its tasks, establishing a foundation for tracking progress in AI-generated scientific code.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [tool-augmented agent, agentic reinforcement learning, supervised fine-tuning (SFT), request-level asynchronous rollout, prefix-aware load balancing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haipeng Luo, Huawen Feng, Qingfeng Sun, Can Xu, Kai Zheng, Yufei Wang, Tao Yang, Han Hu, Yansong Tang, Di Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Tencent Hunyuan</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20745" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20745</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An automated method to convert natural language chain-of-thought into structured tool-augmented trajectories for generating high-quality SFT data. 2. A novel agentic reinforcement learning paradigm that dynamically interleaves natural language generation with real-time code execution for learning tool-use strategies. 3. An efficient training system with techniques like asynchronous rollout scheduling and prefix-aware load balancing, achieving 4-5x speedup for RL training on long sequences.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7211416872630b2f7d460fe4b986a1d141827d69b65487826e3374c5e4cce08d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7211416872630b2f7d460fe4b986a1d141827d69b65487826e3374c5e4cce08d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces AgentMath, a framework that combines language model reasoning with code interpreter precision to solve complex math problems. It uses automated SFT data generation, agentic RL for tool-use learning, and an efficient training system, achieving state-of-the-art results on benchmarks like AIME24 and AIME25.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] AI-Driven Green Cognitive Radio Networks for Sustainable 6G Communication</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [wireless networks], [Deep Reinforcement Learning (DRL), Reconfigurable Intelligent Surfaces (RIS), Energy Harvesting (EH)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Anshul Sharma, Shujaatali Badami, Biky Chouhan, Pushpanjali Pandey, Brijeena Rana, Navneet Kaur</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher (USA), Liverpool John Moores University (UK), Chandigarh University (India), Gyancity Research Consultancy (India)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20739" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20739</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A holistic system model integrating PUs/SUs, energy harvesting, and RIS for sustainable CRN operation. 2. A DRL-based controller enhanced with transfer learning and hybrid metaheuristics for dynamic sensing and resource allocation. 3. EH-aware scheduling and RIS-phase co-adaptation algorithms to reduce SU power consumption.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22a82510c37e7a60d88746806bbece62f0d234e13f225f50ad5635a0bb4ae5ee_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22a82510c37e7a60d88746806bbece62f0d234e13f225f50ad5635a0bb4ae5ee_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an AI-driven framework for green Cognitive Radio Networks (CRNs) in 6G. It integrates Deep Reinforcement Learning (DRL) with transfer learning, energy harvesting, and reconfigurable intelligent surfaces (RIS) to optimize spectrum sensing and resource allocation. The framework demonstrates significant energy savings, high sensing accuracy, and improved packet delivery ratio compared to traditional baselines, offering a sustainable path for 6G IoT and vehicular networks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [neural architecture search, hardware-aware search, edge detection, TinyML, waste detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tony Tran, Bin Hu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Houston</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20746" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20746</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an iterative hardware-aware neural architecture search (NAS) framework that alternates between backbone and neck/head optimization for efficient object detection under TinyML constraints. 2. Introduces a population passthrough mechanism and an accuracy predictor to reduce search cost and improve the stability of the evolutionary search. 3. Delivers a family of deployment-ready detectors (TrashDets) that significantly improve efficiency (energy, latency, power) and accuracy on microcontroller hardware compared to existing TinyML baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4dbb2f9a6c26aed837d78c4cfa78db76890d85a7fadbb49f8592bc1ae30894e1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4dbb2f9a6c26aed837d78c4cfa78db76890d85a7fadbb49f8592bc1ae30894e1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses efficient waste detection for edge/IoT devices under TinyML constraints. It proposes an iterative hardware-aware neural architecture search framework to generate a family of efficient detectors called TrashDets. The resulting models achieve higher accuracy with fewer parameters and significantly reduce energy consumption and latency on resource-constrained microcontrollers.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Stabilizing Multimodal Autoencoders: A Theoretical and Empirical Analysis of Fusion Strategies</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [Lipschitz Continuity, Attention Mechanism, Aggregation Methods, Training Stability, Multimodal Autoencoders]</p>
</li>
<li class="">
<p><strong>authors:</strong> Diyar Altinses, Andreas Schwung</p>
</li>
<li class="">
<p><strong>institution:</strong> South Westphalia University of Applied Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20749" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20749</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Derivation of theoretical Lipschitz constants for aggregation methods in multimodal autoencoders. 2. Introduction of a novel regularized attention-based fusion method designed from the theoretical analysis to improve training stability. 3. Empirical validation of the theoretical findings and demonstration of the proposed method&#x27;s superior performance in consistency, convergence speed, and accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3484b58bc84f22d71a010fca63235d2811ea4f720d1103584b13e220d263f42d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3484b58bc84f22d71a010fca63235d2811ea4f720d1103584b13e220d263f42d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes the stability of multimodal autoencoders by theoretically deriving Lipschitz constants for fusion strategies and proposes a new regularized attention-based fusion method. The method is empirically validated and shown to outperform existing strategies, providing a more stable and performant training process for multimodal models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Bridging Efficiency and Safety: Formal Verification of Neural Networks with Early Exits</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [formal verification, neural network robustness, early exits, adversarial perturbations, off-the-shelf solvers]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yizhak Yisrael Elboher, Avraham Raviv, Amihay Elboher, Zhouxing Shi, Omri Azencot, Hillel Kugler, Guy Katz</p>
</li>
<li class="">
<p><strong>institution:</strong> The Hebrew University of Jerusalem, Bar Ilan University, Ben-Gurion University of the Negev, University of California, Riverside</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20755" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20755</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Defined a formal robustness property specifically tailored for neural network architectures with early exits. 2. Presented a baseline verification algorithm for such networks, enhanced with an early stopping strategy and heuristic optimizations that maintain soundness and completeness. 3. Demonstrated empirically that early exits not only accelerate inference but also enhance verifiability, solving more queries in less time compared to standard networks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cb8be1b40cc4cc73a6f6a75d4c206b456d4189c26838e784e46e437ea5a87b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cb8be1b40cc4cc73a6f6a75d4c206b456d4189c26838e784e46e437ea5a87b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of formally verifying the robustness of neural networks that use early exits for efficiency. The authors propose a tailored robustness property and an enhanced verification algorithm using off-the-shelf solvers. Their experiments show that early exits can improve both inference speed and verifiability, helping navigate the trade-off between accuracy and efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [tokenization], [tokenizer, language models, benchmark, subword segmentation, BPE]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gül Sena Altıntaş, Malikeh Ehghaghi, Brian Lester, Fengyuan Liu, Wanru Zhao, Marco Ciccone, Colin Raffel</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Toronto, Vector Institute, Google DeepMind, McGill University, Mila - Quebec AI Institute, University of Cambridge, Hugging Face</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20757" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20757</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/r-three/Tokenizers" target="_blank" rel="noopener noreferrer" class="">https://github.com/r-three/Tokenizers</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces TokSuite, a collection of fourteen language models that are identical except for their tokenizers, enabling isolated study of tokenizer impact. 2. Curates and releases a new benchmark designed to measure model performance under real-world text perturbations that affect tokenization. 3. Provides a robust framework that supports novel findings on the benefits and shortcomings of various popular tokenizers.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48b6cd3c25dd384b02a8b1601f98df302b0d84a5c6e3b043841ea275f5ffdcbd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48b6cd3c25dd384b02a8b1601f98df302b0d84a5c6e3b043841ea275f5ffdcbd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of isolating the impact of tokenizer choice on language model behavior. It proposes TokSuite, a suite of models with different tokenizers but identical other components, along with a specialized benchmark. The work enables systematic analysis and reveals new insights into how different tokenizers affect model performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Generalization of RLVR Using Causal Reasoning as a Testbed</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [RLVR, causal reasoning, generalization, supervised fine-tuning, large language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Brian Lu, Hongyu Zhao, Shuo Sun, Hao Peng, Rui Ding, Hongyuan Mei</p>
</li>
<li class="">
<p><strong>institution:</strong> Johns Hopkins University, University of Maryland, College Park, National University of Singapore, University of Illinois at Urbana-Champaign, Microsoft Research Asia, Toyota Technological Institute at Chicago</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20760" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20760</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides an empirical study of RLVR generalization using causal inference as a structured testbed, examining generalization across query levels and structural complexity. 2. Identifies that RLVR&#x27;s benefits over SFT for generalization are contingent on specific combinations of model size and training query level, and depend on the model&#x27;s initial reasoning competence. 3. Shows that RLVR improves specific causal reasoning subskills, such as marginalization strategy and intermediate probability calculation, leading to accuracy gains on complex queries.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/472c557c79b64b352421bedd952ba76d099165d613e99323a1beb8845a24cf4c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/472c557c79b64b352421bedd952ba76d099165d613e99323a1beb8845a24cf4c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the generalization of Reinforcement Learning with Verifiable Rewards (RLVR) for large language models on causal reasoning tasks. It finds that RLVR can outperform supervised fine-tuning in generalization, but its effectiveness depends on model size, training data, and the model&#x27;s initial competence. The results indicate RLVR improves specific reasoning sub-skills when the model has a sufficient foundational ability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] TS-Arena Technical Report -- A Pre-registered Live Forecasting Platform</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [time series foundation models, live forecasting, pre-registration, information leakage, temporal split]</p>
</li>
<li class="">
<p><strong>authors:</strong> Marcel Meyer, Sascha Kaltenpoth, Kevin Zalipski, Henrik Albers, Oliver Müller</p>
</li>
<li class="">
<p><strong>institution:</strong> Paderborn University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20761" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20761</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://huggingface.co/spaces/DAG-UPB/TS-Arena" target="_blank" rel="noopener noreferrer" class="">https://huggingface.co/spaces/DAG-UPB/TS-Arena</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces TS-Arena, a platform that uses live data streams and a pre-registration mechanism to create a strict global temporal split for evaluation, preventing historical data contamination. 2. Proposes a methodology that treats the genuinely unknown future as the definitive test environment, establishing a moving temporal frontier for authentic assessment of model generalization. 3. Provides a sustainable infrastructure initially applied in the energy sector for comparing Time Series Foundation Models (TSFMs) under real-world constraints, addressing the evaluation crisis caused by data reuse and leakage.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea84e3460e7e5ece43727d2db2e7515fe17057e90ce083ef73f979343188043f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea84e3460e7e5ece43727d2db2e7515fe17057e90ce083ef73f979343188043f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies an evaluation crisis in Time Series Foundation Models (TSFMs) caused by information leakage from overlapping training/test data. To solve this, it proposes TS-Arena, a live forecasting platform that enforces evaluation on future, unseen data via pre-registration, ensuring a valid temporal split. The platform provides a fair and realistic infrastructure for benchmarking TSFMs, with an initial application in the energy sector.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Subgroup Discovery with the Cox Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [survival analysis], [subgroup discovery, Cox proportional hazards model, expected prediction entropy, conditional rank statistics, interpretable machine learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zachary Izzo, Iain Melvin</p>
</li>
<li class="">
<p><strong>institution:</strong> NEC Labs America</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20762" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20762</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced two novel metrics for evaluating survival models in the context of subgroup discovery: the Expected Prediction Entropy (EPE) and the Conditional Rank Statistics (CRS). 2. Proposed eight algorithms for the Cox subgroup discovery problem, with a main algorithm that leverages both EPE and CRS and has theoretical correctness guarantees. 3. Demonstrated the effectiveness of the methods through empirical evaluation on synthetic and real data, including a case study on NASA jet engine simulation data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa075e21de650db93aab9430f1fc0b8df1921ae52ac2d9c0e9c3dadc8ba3c5f7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa075e21de650db93aab9430f1fc0b8df1921ae52ac2d9c0e9c3dadc8ba3c5f7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of subgroup discovery for survival analysis, aiming to find interpretable data subsets where the Cox model is highly accurate. The authors propose two new evaluation metrics (EPE and CRS) and eight algorithms to solve this problem. Their methods successfully recover ground-truth subgroups and improve model fit compared to fitting a Cox model on the entire dataset, as validated on synthetic, real, and NASA case study data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Improving Matrix Exponential for Generative AI Flows: A Taylor-Based Approach Beyond Paterson--Stockmeyer</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [gpu kernels], [matrix exponential, scaling and squaring, Taylor series, generative AI, numerical stability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jorge Sastre, Daniel Faronbi, José Miguel Alonso, Peter Traver, Javier Ibáñez, Nuria Lloret</p>
</li>
<li class="">
<p><strong>institution:</strong> Universitat Politècnica de València, New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20777" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20777</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An optimized Taylor-based algorithm for matrix exponential designed for high-throughput generative AI flows. 2. A rigorous error analysis and a dynamic selection strategy for Taylor order and scaling factor to minimize computation under error tolerance. 3. Extensive numerical experiments demonstrating significant acceleration and high numerical stability compared to state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf8f618b421ab20009271fba403be914454dbd115182614b0f5256ce16218271_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf8f618b421ab20009271fba403be914454dbd115182614b0f5256ce16218271_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an optimized Taylor-based algorithm for computing the matrix exponential, a key operation in generative AI. The method improves upon classical techniques like Paterson-Stockmeyer and includes a dynamic strategy to balance accuracy and speed. Experiments show it offers significant acceleration while maintaining high numerical stability for large-scale generative modeling.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [nullable prompts, mixed-supervision, vision-language models, breast ultrasound segmentation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Raja Mallina, Bryar Shareef</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Nevada, Las Vegas</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20783" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20783</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes NullBUS, a multimodal mixed-supervision framework for BUS segmentation that can learn from images both with and without prompts in a single model. 2. Introduces nullable prompts, implemented as learnable null embeddings with presence masks, to handle missing text metadata by enabling fallback to image-only evidence. 3. Demonstrates state-of-the-art performance on a unified pool of three public BUS datasets under mixed prompt availability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9c68929834a07c86ac43fe9856efa137aa11c30a231a02ea87272f2b689e4f7f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9c68929834a07c86ac43fe9856efa137aa11c30a231a02ea87272f2b689e4f7f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that many public breast ultrasound datasets lack reliable text or spatial prompts, which limits the training of promptable segmentation models. It proposes NullBUS, a framework that uses nullable global-local prompts to learn from both prompted and prompt-free images. The method achieves state-of-the-art segmentation performance on a unified evaluation of three public datasets, showing robustness under mixed prompt availability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Symbolic regression for defect interactions in 2D materials</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [symbolic regression], [Symbolic Regression, SEGVAE, Graph Neural Networks, 2D Materials, Interpretability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mikhail Lazarev, Andrey Ustyuzhanin</p>
</li>
<li class="">
<p><strong>institution:</strong> HSE University, Institute for Functional Intelligent Materials (National University of Singapore), Constructor University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20785" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20785</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Applied the deep symbolic regression algorithm SEGVAE to model defect interactions in 2D materials. 2. Demonstrated that symbolic regression can achieve performance comparable to state-of-the-art graph neural network methods. 3. Discussed the broader applicability and advantages (e.g., interpretability) of symbolic regression methods in natural sciences.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e61aacf1f171adb632fccf66c94fbeaf1d24724489a5e5c0d3c2a23d60bf7d16_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e61aacf1f171adb632fccf66c94fbeaf1d24724489a5e5c0d3c2a23d60bf7d16_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper applies the SEGVAE deep symbolic regression algorithm to discover analytical equations describing defect interactions in 2D materials. The results show that this interpretable method achieves performance comparable to state-of-the-art graph neural networks, highlighting its potential for scientific discovery.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] FedMPDD: Communication-Efficient Federated Learning with Privacy Preservation Attributes via Projected Directional Derivative</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [gradient compression, directional derivative, privacy preservation, communication efficiency, low-rank projection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mohammadreza Rostami, Solmaz S. Kia</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California Irvine</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20814" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20814</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FedMPDD, a novel FL algorithm that compresses high-dimensional gradients into low-dimensional messages via multi-projected directional derivatives, reducing uplink cost from O(d) to O(m)., 2. Provides theoretical convergence analysis showing FedMPDD achieves an O(1/√K) convergence rate, matching FedSGD, by averaging multiple projections to overcome single-projection limitations., 3. Demonstrates the method offers inherent privacy against gradient inversion attacks due to geometric properties of low-rank projections, providing a tunable privacy-utility trade-off.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/026756c87a289335823e9350b3b3cf6b10435dec5873ca0fe832db069efa00b9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/026756c87a289335823e9350b3b3cf6b10435dec5873ca0fe832db069efa00b9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high communication cost and privacy risks in Federated Learning. It proposes FedMPDD, which compresses client gradients by computing their directional derivatives along multiple random vectors, significantly reducing bandwidth usage while providing inherent privacy. Theoretical and experimental results show the method maintains convergence performance comparable to FedSGD and offers a tunable privacy-utility trade-off.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] GraphFire-X: Physics-Informed Graph Attention Networks and Structural Gradient Boosting for Building-Scale Wildfire Preparedness at the Wildland-Urban Interface</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [physics-informed machine learning], [graph neural networks, XGBoost, AlphaEarth embeddings, contagion dynamics, ensemble learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Miguel Esparza, Vamshi Battal, Ali Mostafavi</p>
</li>
<li class="">
<p><strong>institution:</strong> Texas A&amp;M University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20813" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20813</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel dual-specialist ensemble framework that disentangles wildfire vulnerability into distinct environmental contagion and structural fragility vectors. 2. Integration of a physics-informed Graph Neural Network (GNN) for neighborhood-scale contagion modeling with an XGBoost model for asset-level structural resilience. 3. Generation of a diagnostic risk topology enabling targeted mitigation strategies, such as vegetation management for high-connectivity clusters and structural hardening for vulnerable nodes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2b3a0fb230e974a7c6219e056bcb5e5d527990414b399512769ab5745bdc847_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2b3a0fb230e974a7c6219e056bcb5e5d527990414b399512769ab5745bdc847_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes GraphFire-X, a dual-specialist ensemble framework combining a physics-informed Graph Neural Network and XGBoost to model building-scale wildfire risk at the Wildland-Urban Interface. The method disentangles environmental contagion from structural fragility, revealing that neighborhood-scale environmental pressure dominates propagation pathways while eaves are a key micro-scale vulnerability. The ensemble provides a diagnostic risk map to guide precise, proactive mitigation strategies.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Defending against adversarial attacks using mixture of experts</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [Adversarial Machine Learning], [Mixture of Experts, Adversarial Training, Robustness, Ensemble Learning, Evasion Attacks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mohammad Meymani, Roozbeh Razavi-Far</p>
</li>
<li class="">
<p><strong>institution:</strong> University of New Brunswick</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20821" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20821</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a defense system that integrates an adversarial training module within a Mixture of Experts (MoE) architecture. 2. Employs nine pre-trained experts with ResNet-18 backbones and jointly optimizes both the expert parameters and the gating mechanism during end-to-end training. 3. Demonstrates that the proposed system outperforms state-of-the-art defense systems and plain classifiers, even those with more complex architectures.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0952838b3d0745a7f0396318a91c52e0ef0a7e273730cc1cf1a311f7bfecf079_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0952838b3d0745a7f0396318a91c52e0ef0a7e273730cc1cf1a311f7bfecf079_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the vulnerability of machine learning models to adversarial attacks. It proposes a defense system that combines adversarial training with a Mixture of Experts architecture, using nine pre-trained ResNet-18 models. The system is shown to be more robust than existing defenses and standard classifiers.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [parameterized actions, state abstraction, action abstraction, TD(λ), sample efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rashmeet Kaur Nayyar, Naman Shah, Siddharth Srivastava</p>
</li>
<li class="">
<p><strong>institution:</strong> Arizona State University, Brown University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20831" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20831</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/AAIR-lab/PEARL.git" target="_blank" rel="noopener noreferrer" class="">https://github.com/AAIR-lab/PEARL.git</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Enables agents to autonomously learn both state and action abstractions online for RL with parameterized actions., 2. Introduces algorithms that progressively refine these abstractions during learning, focusing detail on critical regions., 3. Extends RL to long-horizon, sparse-reward settings with parameterized actions, achieving higher sample efficiency than baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c74b22dc11c38dfc5a75f48c2cd94c57d0eecaffdac79525f6362b0b32c448b0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c74b22dc11c38dfc5a75f48c2cd94c57d0eecaffdac79525f6362b0b32c448b0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of reinforcement learning in environments with parameterized actions, which combine discrete choices with continuous parameters. It proposes a method where agents autonomously learn and progressively refine state and action abstractions online. The approach enables TD(λ) to achieve significantly higher sample efficiency in continuous-state, parameterized-action domains compared to state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [bioimage analysis], [multi-channel microscopy, cellular morphology, pre-training dataset, channel-adaptive models, heterogeneous data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vidit Agrawal, John Peters, Tyler N. Thompson, Mohammad Vali Sanian, Chau Pham, Nikita Moshkov, Arshad Kazi, Aditya Pillai, Jack Freeman, Byunguk Kang, Samouil L. Farhi, Ernest Fraenkel, Ron Stewart, Lassi Paavolainen, Bryan A. Plummer, Juan C. Caicedo</p>
</li>
<li class="">
<p><strong>institution:</strong> Morgridge Institute for Research, University of Wisconsin-Madison, Institute for Molecular Medicine Finland (FIMM), University of Helsinki, Boston University, Institute of Computational Biology (Helmholtz Munich), Massachusetts Institute of Technology, Broad Institute of MIT and Harvard</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20833" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20833</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces CHAMMI-75, a novel open-access dataset of heterogeneous, multi-channel microscopy images curated from 75 diverse biological studies. 2. Proposes the use of this dataset to investigate and develop channel-adaptive models for cellular morphology that can process any microscopy image type. 3. Demonstrates experimentally that pre-training with the diverse CHAMMI-75 dataset improves performance on multi-channel bioimaging tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3554315333d73be844495c47c136c201cae29e405327a25f6e11fc1488c5f4f9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3554315333d73be844495c47c136c201cae29e405327a25f6e11fc1488c5f4f9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that models for quantifying cell morphology are typically specialized to single microscopy types, limiting their reuse. It proposes CHAMMI-75, a diverse, multi-channel microscopy image dataset, and shows that pre-training with it improves model performance by enabling channel-adaptability and handling heterogeneous modalities. The work aims to pave the way for more generalizable cellular morphology models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [Mixture-of-Experts, Mamba-Transformer, agentic reasoning, sparse activation, long context]</p>
</li>
<li class="">
<p><strong>authors:</strong> NVIDIA, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Faisal Ladhak, Fay Wang, Fei Jia, Felipe Soares, Feng Chen, Ferenc Galko, Frankie Siino, Gal Hubara Agam, Ganesh Ajjanagadde, Gantavya Bhatt</p>
</li>
<li class="">
<p><strong>institution:</strong> NVIDIA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20848" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20848</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Nemotron 3 Nano, a hybrid MoE Mamba-Transformer model that sparsely activates only 3.2B out of 31.6B parameters per forward pass for efficiency. 2. Demonstrates superior inference throughput (up to 3.3x faster) compared to similarly-sized open models while maintaining or improving accuracy on benchmarks. 3. Supports an extended context length of up to 1 million tokens and shows enhanced agentic and reasoning capabilities through post-training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96a4b5c012acd8208519dcb9669276bd8c3c3709f26e7290e2fce500151c1ccc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96a4b5c012acd8208519dcb9669276bd8c3c3709f26e7290e2fce500151c1ccc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents Nemotron 3 Nano, an efficient 30B-parameter language model that combines Mixture-of-Experts with a Mamba-Transformer architecture to achieve sparse activation. It was pre-trained on 25 trillion tokens and post-trained for agentic reasoning, resulting in higher inference throughput and accuracy compared to similar models while supporting up to 1M token contexts.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] NVIDIA Nemotron 3: Efficient and Open Intelligence</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [Mixture-of-Experts, Mamba-Transformer, LatentMoE, NVFP4, multi-environment reinforcement learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> NVIDIA, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Anjulie Agrusa, Ankur Verma, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asit Mishra, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Cyril Meurillon, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Lo, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elad Segal, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Evgeny Tsykunov, Faisal Ladhak, Fay Wang, Fei Jia</p>
</li>
<li class="">
<p><strong>institution:</strong> NVIDIA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20856" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20856</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Nemotron 3 family of models (Nano, Super, Ultra) built on a Mixture-of-Experts hybrid Mamba-Transformer architecture for high throughput and long context (up to 1M tokens). 2. Proposes novel techniques including LatentMoE for improved model quality and MTP layers for faster text generation in the larger models. 3. Employs multi-environment reinforcement learning for post-training, enabling advanced capabilities like reasoning, multi-step tool use, and granular reasoning budget control.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5203ecd520d6e99bc9f0034f05e8945272d4a34a746eeeae01be1cc728049b5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5203ecd520d6e99bc9f0034f05e8945272d4a34a746eeeae01be1cc728049b5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces the Nemotron 3 family of open models designed for efficient and intelligent agentic applications. The models use a novel hybrid Mamba-Transformer architecture and are trained with techniques like LatentMoE and multi-environment RL to achieve strong reasoning, conversational, and tool-use capabilities with high throughput. The conclusion is that these models provide state-of-the-art accuracy and efficiency, with plans for open release of weights, software, and data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [block low-rank (BLR), Triton kernels, memory-bound optimization, Jetson Orin Nano, roofline analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Pierre Abillama, Changwoo Lee, Juechu Dong, David Blaauw, Dennis Sylvester, Hun-Seok Kim</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Michigan</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20861" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20861</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/pabillam/mem-efficient-blr" target="_blank" rel="noopener noreferrer" class="">https://github.com/pabillam/mem-efficient-blr</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identified through roofline analysis that multi-token inference for BLR-compressed models becomes memory-bound, limiting speedups despite compiler optimizations. 2. Introduced custom Triton kernels with partial fusion and memory layout optimizations specifically for Monarch and BLR-AST (BLAST) methods. 3. Demonstrated significant speedups (up to 3.76x) and model compression (3x) on memory-constrained GPUs (e.g., Jetson Orin Nano, A40) across various foundation models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5f95e7768493ecd557f85d3dd08d75532f4bfee4218e02d377351eaf02b4c20_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5f95e7768493ecd557f85d3dd08d75532f4bfee4218e02d377351eaf02b4c20_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the memory bottleneck in multi-token inference for block low-rank (BLR) compressed foundation models. The authors propose custom Triton kernels with fusion and layout optimizations for BLR methods like Monarch and BLAST. Their solution achieves up to 3.76x speedup and 3x model compression on resource-constrained GPUs compared to optimized PyTorch baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Robustness Certificates for Neural Networks against Adversarial Attacks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [adversarial robustness], [barrier certificates, data poisoning, formal verification, scenario convex program, robustness certification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sara Taheri, Mahalakshmi Sabanayagam, Debarghya Ghoshdastidar, Majid Zamani</p>
</li>
<li class="">
<p><strong>institution:</strong> LMU Munich, TUM Munich, University of Colorado Boulder</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20865" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20865</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a formal robustness certification framework by modeling gradient-based training as a discrete-time dynamical system and formulating poisoning robustness as a safety verification problem., 2. Adapts barrier certificates from control theory to derive sufficient conditions for certifying a robust radius against worst-case ℓp-norm poisoning, and makes it practical by parameterizing BCs as neural networks., 3. Derives PAC bounds via a scenario convex program to provide a confidence lower bound on the certified robustness radius, and extends the unified framework to also certify against test-time attacks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5a85af42ea34173637ac88fc6859bba1d4a68d3161f2fcc2567276ca7d37b5b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5a85af42ea34173637ac88fc6859bba1d4a68d3161f2fcc2567276ca7d37b5b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of formal guarantees in defending neural networks against data poisoning attacks. It proposes a certification framework that models training as a dynamical system, uses barrier certificates for verification, and provides PAC-bounded robustness radii. Experiments show the approach certifies non-trivial perturbation budgets without needing prior attack knowledge.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [malware detection], [function call graphs, Android malware, graph-based classification, APK, dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jakir Hossain, Gurvinder Singh, Lukasz Ziarek, Ahmet Erdem Sarıyüce</p>
</li>
<li class="">
<p><strong>institution:</strong> University at Buffalo</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20872" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20872</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://erdemub.github.io/BCG-dataset" target="_blank" rel="noopener noreferrer" class="">https://erdemub.github.io/BCG-dataset</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Better Call Graphs (BCG) dataset, a new large-scale collection of unique and recent Android FCGs for malware classification., 2. Addresses limitations of existing datasets (outdated, redundant, small graphs) to prevent overfitting and enable reliable evaluation., 3. Demonstrates the necessity and value of the BCG dataset through extensive experiments with baseline classifiers.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87c9a8db7055364694bd0257a161b2a24a77226da1ab49504cd20679d8699222_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87c9a8db7055364694bd0257a161b2a24a77226da1ab49504cd20679d8699222_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of high-quality datasets for Android malware classification using function call graphs (FCGs). It introduces the Better Call Graphs (BCG) dataset, which contains large, unique, and recent FCGs from Android APKs. Experiments show BCG is necessary for reliable evaluation and helps overcome overfitting issues present with older datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Architectural Trade-offs in Small Language Models Under Compute Constraints</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [language modeling], [small language models, compute constraints, architectural trade-offs, rotary positional embeddings, transformer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shivraj Singh Bhatti</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Massachusetts Amherst</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20877" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20877</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A systematic empirical study of architectural choices (from linear predictors to transformers) for small language models under strict compute constraints. 2. An analysis showing attention-based models are more FLOP-efficient than MLPs even at small scale, and that increasing depth/context without sufficient optimization can hurt performance. 3. An investigation revealing that techniques like Rotary Positional Embeddings (RoPE), successful in large models, do not necessarily transfer effectively to the small-model regime.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2b54dee144c67bdb877818e8171163f9556734c09b3a1d7d29fe8464aee558b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2b54dee144c67bdb877818e8171163f9556734c09b3a1d7d29fe8464aee558b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper systematically studies how architectural choices affect small language model performance under limited compute. The method involves progressively building from linear predictors to multi-layer transformers and evaluating them on character and word-level datasets. The main conclusion is that attention is more efficient than MLPs per FLOP at small scales, but scaling depth or applying large-model techniques like RoPE can be detrimental without careful optimization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Time-Efficient Evaluation and Enhancement of Adversarial Robustness in Deep Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [adversarial robustness], [adversarial robustness, deep neural networks, time-efficient, red-blue adversarial framework, adversarial evaluation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Runqi Lin</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Sydney</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20893" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20893</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes time-efficient methods for evaluating adversarial robustness in DNNs, 2. Proposes time-efficient methods for enhancing adversarial robustness in DNNs, 3. Aims to overcome the computational intensity limitation of existing approaches for large-scale models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c027984dda35366287528822de7264568bdef6dac0b744de35e5554d0dc77b9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c027984dda35366287528822de7264568bdef6dac0b744de35e5554d0dc77b9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This thesis addresses the computational inefficiency of existing methods for evaluating and improving the adversarial robustness of deep neural networks. It proposes new time-efficient techniques for both identifying vulnerabilities (red team) and mitigating them (blue team). The main goal is to make adversarial robustness assessment and enhancement more applicable to large-scale models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] From GNNs to Symbolic Surrogates via Kolmogorov-Arnold Networks for Delay Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [communication &amp; networking], [Graph Neural Networks, Kolmogorov-Arnold Networks, Symbolic Regression, Attention, Message Passing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sami Marouani, Kamal Singh, Baptiste Jeudy, Amaury Habrard</p>
</li>
<li class="">
<p><strong>institution:</strong> Université Jean Monnet Saint-Étienne, CNRS, Institut d&#x27;Optique Graduate School, Laboratoire Hubert Curien, Institut Universitaire de France (IUF), Inria</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20885" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20885</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A heterogeneous Graph Neural Network with attention-based message passing as a strong baseline for flow delay prediction. 2. FlowKANet, a fully KAN-based GNN architecture that integrates KAN operators into message-passing and attention computation for efficiency and interpretability. 3. A symbolic distillation of FlowKANet via block-wise regression to produce lightweight, transparent closed-form surrogate models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c96bffa46ff987475631fa980227e1a967ebbe523f541ff36441600d02001b3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c96bffa46ff987475631fa980227e1a967ebbe523f541ff36441600d02001b3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles flow delay prediction in communication networks by proposing a progression of models. It introduces FlowKANet, a GNN that replaces standard MLP layers with Kolmogorov-Arnold Networks (KANs) for better efficiency and interpretability, and further distills it into symbolic surrogate models. The results show that KANs offer a good efficiency-accuracy trade-off, and the symbolic surrogates enable lightweight, transparent deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] DiEC: Diffusion Embedded Clustering</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [deep clustering], [diffusion models, representation selection, self-training, graph regularization, denoising consistency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haidong Hu</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly provided in the given content.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20905" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20905</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DiEC, a novel deep clustering method that directly leverages the internal representation trajectory (across layers and timesteps) of a pretrained diffusion U-Net instead of a single fixed embedding. 2. Introduces a two-stage search strategy (CML and OTS) to efficiently identify the most cluster-friendly representation from the diffusion model&#x27;s internal activations. 3. Enhances the clustering training with a DEC-style objective augmented by adaptive graph regularization, entropy regularization, and a denoising-consistency branch to strengthen and stabilize cluster structures.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66551d88d8940c7a650dca6264246e32d115d3596bb088334602fd1943ca8558_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66551d88d8940c7a650dca6264246e32d115d3596bb088334602fd1943ca8558_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of finding cluster-friendly representations in deep clustering by proposing DiEC, which extracts and optimizes features from the internal activations of a pretrained diffusion model. The method uses a two-stage search to select optimal representations and employs a regularized self-training objective with a consistency branch. Experiments show that DiEC achieves competitive clustering performance on standard benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [reversible networks, memory-efficient fine-tuning, mixture-of-experts, full-parameter fine-tuning, activation recomputation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ningyuan Liu, Jing Yang, Kaitong Cai, Keze Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Sun Yat-sen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20920" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20920</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes RevFFN, a novel memory-efficient fine-tuning paradigm for Mixture-of-Experts (MoE) LLMs. 2. Designs reversible Transformer blocks that reconstruct layer inputs from outputs during backpropagation, eliminating the need to store most intermediate activations. 3. Enables efficient full-parameter fine-tuning on a single GPU by drastically reducing peak memory consumption while preserving model capacity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5502accb933d07822fb8b8c8802a3eda6d016d84580bfda3089eae32cc0ea597_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5502accb933d07822fb8b8c8802a3eda6d016d84580bfda3089eae32cc0ea597_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high memory overhead of full-parameter fine-tuning for large language models (LLMs), especially Mixture-of-Experts (MoE) models, caused by caching intermediate activations. It introduces RevFFN, a method using reversible Transformer blocks to recompute activations during backpropagation, significantly reducing memory usage. This allows for efficient full fine-tuning on a single GPU without compromising the model&#x27;s expressive power.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Towards a General Framework for Predicting and Explaining the Hardness of Graph-based Combinatorial Optimization Problems using Machine Learning and Association Rule Mining</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [combinatorial optimization], [graph features, hardness prediction, association rule mining, maximum clique problem, machine learning classification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bharat Sharman, Elkafi Hassini</p>
</li>
<li class="">
<p><strong>institution:</strong> (Inferred from author names and arXiv submission; specific institution not provided in abstract. Could be a university research group.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20915" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20915</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GCO-HPIF, a general two-stage ML framework for predicting and explaining the hardness of graph-based combinatorial optimization problems. 2. Demonstrates the framework&#x27;s effectiveness by applying it to the maximum clique problem using diverse algorithms (exact and GNN-based) and achieving high prediction accuracy with few features. 3. Introduces the use of association rule mining (FP-Growth) to generate human-interpretable explanations for the hardness predictions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d6637eddd49fc539b03c9e2248f2656c238346a34b80db77df78bdd41cc252f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d6637eddd49fc539b03c9e2248f2656c238346a34b80db77df78bdd41cc252f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces GCO-HPIF, a machine learning framework that predicts and explains the computational hardness of graph-based combinatorial optimization problems by first classifying instances using graph features and then explaining the predictions via association rule mining. The framework was validated on a large dataset of maximum clique problem instances, achieving excellent prediction performance and generating interpretable rules. The results show the framework&#x27;s potential for both accurate hardness forecasting and providing insights into problem difficulty.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Guardrailed Elasticity Pricing: A Churn-Aware Forecasting Playbook for Subscription Strategy</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [revenue management], [constrained optimization, Bayesian hierarchical modeling, Monte Carlo simulation, price elasticity, churn prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Deepit Sapru</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Illinois Urbana-Champaign</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20932" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20932</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel framework integrating demand forecasting, segment-level price elasticity, and churn propensity into a single constrained optimization system for subscription pricing. 2. A methodology blending seasonal time-series models with tree-based learners and using Monte Carlo scenario tests to map risk envelopes for pricing decisions. 3. A modular, API-driven system designed for real-time recalibration with model explainability for governance, functioning as a managerial strategy playbook.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0bf97f0a7c7df406f95ecaff29da9d37a9c9851d8013ef82f3f2669083a60ae7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0bf97f0a7c7df406f95ecaff29da9d37a9c9851d8013ef82f3f2669083a60ae7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a dynamic pricing framework for subscription services that combines forecasting, elasticity modeling, and churn prediction within a constrained optimization system to balance revenue and retention. The method uses Monte Carlo simulations and enforces business guardrails on margins and churn. It outperforms static pricing by targeting price changes to high willingness-to-pay segments while protecting sensitive customers.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws for GNN-based Aerodynamic Field Surrogate</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [graph neural network, surrogate model, multi-fidelity dataset, scaling laws, aerodynamic field prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yiren Shen, Juan J. Alonso</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20941" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20941</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Release of an open-source, multi-fidelity aerodynamic dataset for double-delta wings, generated using a nested Saltelli sampling scheme. 2. Conducted an empirical scaling study linking training data size and model size to prediction accuracy for a GNN-based surrogate, revealing a power-law relationship. 3. Derived practical guidelines, estimating an optimal sampling density of approximately eight samples per dimension in a design space.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bb0e5406bfc3665bfcafc6d32aeeb524e6e21ffb9ceb2ac785fe0ddd6b60b3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bb0e5406bfc3665bfcafc6d32aeeb524e6e21ffb9ceb2ac785fe0ddd6b60b3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the relationship between dataset size and model performance for a Graph Neural Network (GNN) surrogate used in aerodynamic field prediction. The authors release a new multi-fidelity dataset for double-delta wings and conduct a scaling study, finding that test error decreases with data size following a power law, which indicates efficient data utilization and informs optimal sampling strategies.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [communication &amp; networking], [4D Gaussian Splatting, video streaming, integer linear programming, pruning, keyframe selection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhe Wang, Jinghang Li, Yifei Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20943" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20943</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a streaming-optimized 4DGS framework that converts Gaussian streams into multi-channel 2D formats and uses intelligent keyframe identification to enhance reconstruction quality and reduce training time. 2. Models the 4DGS delivery problem as an integer linear programming problem and designs a lightweight pruning algorithm to adaptively prune Gaussian updates for bandwidth-efficient transmission. 3. Demonstrates significant improvements in quality stability (reducing PSNR deviation by &gt;20%), training speed (6x acceleration), and transmission efficiency (50% size reduction) compared to state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67fcf9423d5e160d4a5d4e949518213bf3a4f37a910a1c4fe209190f990922dc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67fcf9423d5e160d4a5d4e949518213bf3a4f37a910a1c4fe209190f990922dc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents AirGS, a framework that optimizes the training and delivery pipeline for 4D Gaussian Splatting to enable real-time free-viewpoint video streaming. It addresses quality degradation and high bandwidth overhead by introducing a 2D representation format, keyframe selection, and an adaptive pruning algorithm for transmission. Experiments show AirGS significantly improves quality stability, accelerates training, and reduces transmission size.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [crosslingual information retrieval], [dual-encoder, contrastive learning, hard negative sampling, data augmentation, multi-source alignment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mohammad Mahdi Abootorabi, Alireza Ghahramani Kure, Mohammadali Mohammadkhani, Sina Elahimanesh, Mohammad Ali Ali Panah</p>
</li>
<li class="">
<p><strong>institution:</strong> Based on the provided email domains (gmail.com), no specific institution can be reliably inferred. The team name is &quot;MultiMind&quot;.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20950" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20950</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces TriAligner, a novel dual-encoder architecture with contrastive learning for crosslingual claim retrieval. 2. Proposes a method to learn the relative importance of different information sources (e.g., native text, English translations) for alignment. 3. Enhances robustness through LLM-based data preprocessing/augmentation and hard negative sampling strategies.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccb6e1762a9617f640573a86ea65e0e68afff48d53006aa74213e0a557970889_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccb6e1762a9617f640573a86ea65e0e68afff48d53006aa74213e0a557970889_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of retrieving fact-checked claims across multiple languages to combat misinformation. The proposed TriAligner system uses a dual-encoder with contrastive learning and multi-source alignment, enhanced by LLM-based data processing. The method shows significant improvements in retrieval accuracy on monolingual and crosslingual benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Solving Functional PDEs with Gaussian Processes and Applications to Functional Renormalization Group Equations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [operator learning], [Gaussian processes, functional renormalization group, functional PDEs, operator learning, non-perturbative methods]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xianjin Yang, Matthieu Darcy, Matthew Hudes, Francis J. Alexander, Gregory Eyink, Houman Owhadi</p>
</li>
<li class="">
<p><strong>institution:</strong> Caltech</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20956" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20956</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Gaussian process-based operator learning framework for solving functional PDEs directly in function space, independent of specific discretizations. 2. Demonstrates the method&#x27;s application to non-perturbative functional renormalization group equations (e.g., Wetterich, Wilson-Polchinski), achieving performance equal to or better than traditional approximations like the local-potential approximation. 3. Shows the framework&#x27;s flexibility in handling non-constant fields and incorporating physical priors, making it promising for studying complex field configurations like instantons.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18c810a016d418e9edb4adddfcb5d19e76d457adf7e311d4c5ca1b064e1fa145_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18c810a016d418e9edb4adddfcb5d19e76d457adf7e311d4c5ca1b064e1fa145_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a Gaussian process operator learning framework to solve functional partial differential equations, specifically targeting non-perturbative functional renormalization group equations. The method directly represents functionals in function space, offering flexibility and independence from equation-specific discretizations. The results demonstrate that it matches or outperforms traditional approximations like the local-potential approximation and can handle complex, non-constant field configurations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Proximal Policy Optimization (PPO), ChemBERTa, ESM-2, reaction-template, de novo drug design]</p>
</li>
<li class="">
<p><strong>authors:</strong> R Yadunandan, Nimisha Ghosh</p>
</li>
<li class="">
<p><strong>institution:</strong> Department of Computer Science and Engineering, Shiv Nadar University Chennai</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20958" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20958</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/YadunandanRaman/ReACT-Drug/" target="_blank" rel="noopener noreferrer" class="">https://github.com/YadunandanRaman/ReACT-Drug/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A target-agnostic RL framework (ReACT-Drug) that uses protein embeddings to find similar proteins and initialize a biologically relevant fragment search space. 2. A PPO agent that guides molecular generation through a dynamic action space defined by chemically valid, reaction-template-based transformations. 3. Ensures 100% chemical validity and novelty while generating candidates with competitive binding affinity and high synthetic accessibility.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/021dd36ada6572d99e5bbd07493acfe6d2766f9ca2c6bf611ba28e410f041efc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/021dd36ada6572d99e5bbd07493acfe6d2766f9ca2c6bf611ba28e410f041efc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces ReACT-Drug, a reinforcement learning framework for de novo drug design. It uses ESM-2 protein embeddings to find similar proteins and their ligands, decomposes them into fragments to guide a PPO agent, which then builds new molecules using reaction-template-based actions encoded by ChemBERTa. The method generates novel, synthetically accessible drug candidates with high binding affinity and guaranteed chemical validity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Can Agentic AI Match the Performance of Human Data Scientists?</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [automated data science], [agentic AI, domain knowledge, synthetic data, large language models, human-AI teaming]</p>
</li>
<li class="">
<p><strong>authors:</strong> An Luo, Jin Du, Fangqiao Tian, Xun Xian, Robert Specht, Ganghua Wang, Xuan Bi, Charles Fleming, Jayanth Srinivasa, Ashish Kundu, Mingyi Hong, Jie Ding</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Minnesota, University of Chicago, Cisco Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20959" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20959</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Designed a novel prediction task where a critical latent variable is hidden in image data to test the limitations of generic agentic AI workflows. 2. Demonstrated through experiments that current agentic AI systems, which rely on generic code generation, fail to match human data scientists who can leverage domain-specific insights. 3. Highlighted a key limitation of current LLM-driven data science automation and underscored the need for future research to develop AI systems that can better incorporate domain knowledge.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79fb0613736763ea339bd898e4056c45f61f97d7ed163ac22b97fef63af1c01a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79fb0613736763ea339bd898e4056c45f61f97d7ed163ac22b97fef63af1c01a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper investigates whether agentic AI can match human data scientists by designing a property insurance prediction task where a crucial variable is hidden in image data. Experiments show that AI relying on generic workflows performs poorly compared to methods using domain-specific insights. The study concludes that current agentic AI has a key limitation in incorporating domain knowledge, highlighting a need for future research in this direction.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Generalization of Diffusion Models Arises with a Balanced Representation Space</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [representation learning, denoising autoencoder, memorization detection, representation steering, generalization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zekai Zhang, Xiao Li, Xiang Li, Lianghe Shi, Meng Wu, Molei Tao, Qing Qu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Michigan, Georgia Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20963" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20963</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://deepthink-umich.github.io" target="_blank" rel="noopener noreferrer" class="">https://deepthink-umich.github.io</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a theoretical analysis linking memorization and generalization in diffusion models to specific representation structures (&quot;spiky&quot; vs. &quot;balanced&quot;) using a two-layer ReLU DAE model. 2. Validates the theoretical findings on real-world unconditional and text-to-image diffusion models, showing the practical emergence of these representation regimes. 3. Proposes a representation-based method for detecting memorization and a training-free editing technique for precise control via representation steering.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58c26c58c94d92dfa7924dfd1c702b8c3239c0a63a4e6ef6f0eaaf42d7dc410a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58c26c58c94d92dfa7924dfd1c702b8c3239c0a63a4e6ef6f0eaaf42d7dc410a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes the distinction between memorization and generalization in diffusion models through representation learning. It theoretically proves that memorization leads to &quot;spiky&quot; representations while generalization yields &quot;balanced&quot; ones, and validates this on real models. Based on these insights, the authors propose methods for memorization detection and training-free image editing, highlighting the central role of good representations for meaningful generation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [spot instance, online scheduling, deadline-aware, LoRA, integer programming]</p>
</li>
<li class="">
<p><strong>authors:</strong> Linggao Kong, Yuedong Xu, Lei Jiao, Chuan Xu</p>
</li>
<li class="">
<p><strong>institution:</strong> Fudan University, University of Oregon, Inria</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20967" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20967</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulated an integer programming problem for deadline-aware LLM fine-tuning using a mix of volatile spot and reliable on-demand GPU instances. 2. Proposed a prediction-based online allocation algorithm and a complementary algorithm without predictions, with a policy selection algorithm that learns the best policy from a parameterized pool. 3. Provided theoretical analysis showing the prediction-based algorithm&#x27;s performance improves with prediction accuracy and that the policy selection algorithm has a sublinear regret bound, with experiments showing up to 54.8% utility improvement.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6addc088c50bc798ccb2da0947c0b05adb32b7751a390fde44b4c5be3c5b85f3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6addc088c50bc798ccb2da0947c0b05adb32b7751a390fde44b4c5be3c5b85f3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the challenge of cost-effective, deadline-aware scheduling for fine-tuning large language models (LLMs) on volatile GPU spot instances. It proposes an online framework that uses a mix of spot and on-demand instances, featuring a prediction-based algorithm, a non-prediction algorithm, and a policy selection mechanism. The framework adapts to market dynamics, is theoretically grounded, and significantly outperforms baselines in experiments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Bayesian Reinforcement Learning, Meta-Reinforcement Learning, Generalised Linear Models, Learnable Basis Functions, Variational Inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jingyang You, Hanna Kurniawati</p>
</li>
<li class="">
<p><strong>institution:</strong> Australian National University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20974" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20974</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GLiBRL, a novel deep Bayesian RL method using Generalised Linear Models with learnable basis functions for efficient and accurate model learning. 2. Enables fully tractable marginal likelihood and Bayesian inference on task parameters and model noises, avoiding the need to optimize the difficult Evidence Lower Bound (ELBO). 3. Demonstrates significant performance improvements on MetaWorld benchmarks, outperforming state-of-the-art methods like VariBAD and showing low-variance, consistent results.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d35dd58d9d51b22dbce9eb7fc7a54a60d532c573648a3a29596e023ac63db13_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d35dd58d9d51b22dbce9eb7fc7a54a60d532c573648a3a29596e023ac63db13_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of inefficient and unstable model learning in deep Bayesian Reinforcement Learning (BRL), which traditionally relies on optimizing the difficult Evidence Lower Bound (ELBO). The authors propose a new method called GLiBRL, which uses Generalised Linear Models with learnable basis functions to enable tractable marginal likelihood and Bayesian inference. The method significantly improves success rates on challenging MetaWorld benchmarks compared to existing deep BRL and meta-RL approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Automatic Replication of LLM Mistakes in Medical Conversations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [llm evaluation], [medical conversation, mistake replication, benchmark creation, llm judges, single-shot qa]</p>
</li>
<li class="">
<p><strong>authors:</strong> Oleksii Proniakin, Diego Fajardo, Ruslan Nazarenko, Razvan Marinescu</p>
</li>
<li class="">
<p><strong>institution:</strong> Lumos AI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20983" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20983</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MedMistake, an automatic pipeline for extracting and replicating LLM mistakes from complex medical conversations into a benchmark format. 2. Releases MedMistake-All, a dataset of 3,390 single-shot QA pairs derived from identified mistakes, and a validated subset, MedMistake-Bench. 3. Provides a comprehensive evaluation of 12 frontier LLMs using the validated benchmark, revealing performance trends among top models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7799ccba99ce08a1cee1bd87ba7b9e986a373df0f78a25479ad9fec7478ca0e9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7799ccba99ce08a1cee1bd87ba7b9e986a373df0f78a25479ad9fec7478ca0e9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the difficulty of replicating specific mistakes made by LLMs in clinical conversations. It proposes MedMistake, an automated pipeline that generates conversational data, uses LLM judges to identify errors, and distills them into single-shot QA pairs to create a benchmark. The resulting benchmark was used to evaluate 12 LLMs, finding that GPT, Claude, and Grok models performed best.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] CoSeNet: A Novel Approach for Optimal Segmentation of Correlation Matrices</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [correlation matrix segmentation], [correlation matrices, segmentation algorithms, metaheuristic optimization, overlapping technique, window difference metric]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alberto. Palomo-Alonso, David Casillas-Perez, Silvia Jimenez-Fernandez, Antonio Portilla-Figueras, Sancho Salcedo-Sanz</p>
</li>
<li class="">
<p><strong>institution:</strong> Department of Signal Processing and Communications, Universidad de Alcalá (Spain)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21000" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21000</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CoSeNet, a novel four-layer algorithmic architecture for optimal segmentation of noisy correlation matrices. 2. Introduces a method using a heuristic algorithm to optimize the re-scaling layer parameters based on a Window Difference-based fitness metric. 3. Utilizes an overlapping technique and pre-trained ML algorithms to enhance robustness and generalizability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a9a31a49fc5fbd3e09151d607fb89a66d2e15d7c89ecd0f67b8be73bebac208_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a9a31a49fc5fbd3e09151d607fb89a66d2e15d7c89ecd0f67b8be73bebac208_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces CoSeNet, a novel four-layer model for optimally segmenting correlated groups within noisy correlation matrices. The method uses an overlapping technique, pre-trained ML algorithms, and a heuristic to optimize its re-scaling parameters. The output is a clean, binary segmentation matrix, offering a balanced solution in terms of efficiency, memory, and speed.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [llm evaluation], [Competitive Swiss-System Dynamics, Expected Win Score, Failure Sensitivity Analysis, Monte Carlo Simulation, risk appetite]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiashuo Liu, Jiayun Wu, Chunjie Wu, Jingkai Liu, Zaiyuan Wang, Huan Zhou, Wenhao Huang, Hongseok Namkoong</p>
</li>
<li class="">
<p><strong>institution:</strong> ByteDance Seed, Carnegie Mellon University, Columbia University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21010" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21010</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Competitive Swiss-System Dynamics (CSD) framework, a novel sequential contest simulation for holistic LLM ranking across multiple benchmarks, 2. Proposes the Expected Win Score via Monte Carlo Simulation to provide a statistically robust ranking that reduces noise from random pairing, 3. Implements Failure Sensitivity Analysis to profile models by risk appetite, distinguishing between robust generalists and aggressive specialists.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c6a4e260d9e6100733ae95995348a1b30295905871b863cf4afa821492e22eb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c6a4e260d9e6100733ae95995348a1b30295905871b863cf4afa821492e22eb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the limitations of static, fragmented LLM evaluation by proposing the Competitive Swiss-System Dynamics (CSD) framework, which simulates a multi-round sequential contest to aggregate performance across benchmarks dynamically. It uses Monte Carlo simulation to compute a robust Expected Win Score and includes a Failure Sensitivity Analysis to assess model risk profiles. The authors demonstrate that CSD provides a more nuanced and context-aware ranking than traditional methods, advancing risk-informed LLM evaluation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Towards Better Search with Domain-Aware Text Embeddings for C2C Marketplaces</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [text embeddings], [Matryoshka Representation Learning, role-specific prefixes, purchase-driven fine-tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Andre Rusli, Miao Cao, Shoma Ishimoto, Sho Akiyama, Max Frenzel</p>
</li>
<li class="">
<p><strong>institution:</strong> Mercari, Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21021" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21021</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a domain-aware Japanese text-embedding model fine-tuned on purchase-driven query-title pairs for C2C marketplace search. 2. Introduced the use of role-specific prefixes to model the query-item asymmetry inherent in search tasks. 3. Applied Matryoshka Representation Learning to create compact, truncation-robust embeddings that meet production latency and throughput constraints.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f78c022541a8ea7125744d1404efda5d54f335700045da22b53146a30bf9632_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f78c022541a8ea7125744d1404efda5d54f335700045da22b53146a30bf9632_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of improving search relevance in noisy, user-generated C2C marketplaces by fine-tuning a Japanese text-embedding model with role-specific prefixes and Matryoshka Representation Learning. The method produces compact embeddings that are robust to truncation for efficiency. Offline and online evaluations show significant improvements in retrieval quality and business metrics, providing a practical foundation for enhanced search experiences.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Agentic Multi-Persona Framework for Evidence-Aware Fake News Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [misinformation detection], [multi-persona agent, LLM-SLM synergy, evidence-grounded, multimodal fusion, credibility fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Roopa Bukke, Soumya Pandey, Suraj Kumar, Soumi Chattopadhyay, Chandranath Adak</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology (Indore, Patna)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21039" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21039</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed AMPEND-LS, an agentic multi-persona framework that integrates textual, visual, and contextual evidence through a structured LLM reasoning pipeline. 2. Introduced a credibility fusion mechanism combining semantic similarity, domain trustworthiness, and temporal context to improve reliability. 3. Designed a complementary SLM classifier to mitigate LLM uncertainty and hallucinations, enhancing robustness and explainability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e8f4708c11ed1d6f5c25e0cab8a4e68e02e81ca73057ce067c85265f0213b46_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e8f4708c11ed1d6f5c25e0cab8a4e68e02e81ca73057ce067c85265f0213b46_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of multimodal fake news detection by proposing AMPEND-LS, a framework that synergizes LLMs and SLMs within a multi-persona agent structure to reason over diverse evidence. It demonstrates superior performance over state-of-the-art baselines in accuracy and robustness across multiple datasets. The work contributes to developing more adaptive and explainable systems for combating online misinformation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [zero-knowledge proofs, trusted execution environments, blockchain, medical AI, verifiable aggregation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Savvy Sharma, George Petrovic, Sarthak Kaushik</p>
</li>
<li class="">
<p><strong>institution:</strong> George Brown Polytechnic</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21048" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21048</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel architecture (zkFL-Health) that integrates Federated Learning with Zero-Knowledge Proofs and Trusted Execution Environments to ensure privacy and verifiable correctness in medical AI training. 2. Introduces a protocol where the aggregator, operating within a TEE, generates a succinct ZK proof to attest it used the correct inputs and aggregation rule, without revealing client updates. 3. Leverages a blockchain to provide an immutable audit trail of cryptographic commitments and proof verification, removing the need to trust a single party and enhancing regulatory compliance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaf3da543259b835ceee63f63b6675f8ca4fdd248035da93b3582ae546b60093_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaf3da543259b835ceee63f63b6675f8ca4fdd248035da93b3582ae546b60093_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses privacy leakage and trust issues in federated learning for healthcare by proposing zkFL-Health, a framework that combines FL with zero-knowledge proofs and TEEs to enable verifiable and private model aggregation. The method ensures the aggregator&#x27;s computations are provably correct and recorded on a blockchain for auditability. The conclusion is that this approach provides strong confidentiality, integrity, and auditability, which are crucial for clinical adoption.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D human pose estimation], [3D sign language reconstruction, biomechanical accuracy, hand and body pose priors, monocular video, SMPL-X]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kaustubh Kundu, Hrishav Bakul Barua, Lucy Robertson-Bell, Zhixi Cai, Kalin Stefanov</p>
</li>
<li class="">
<p><strong>institution:</strong> Monash University, TCS Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21054" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21054</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/kaustesseract/DexAvatar" target="_blank" rel="noopener noreferrer" class="">https://github.com/kaustesseract/DexAvatar</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel framework (DexAvatar) for reconstructing biomechanically accurate 3D hand and body poses from monocular sign language videos. 2. The use of learned 3D hand and body pose priors to guide the reconstruction and overcome challenges like self-occlusion and motion blur. 3. Demonstrating strong performance on the SGNify benchmark, achieving a 35.11% improvement over the state-of-the-art.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/594bef871fe9a00d58a9f3f12c9a0b4bf4f66d3d738bd3f02dedcbad04bdcd25_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/594bef871fe9a00d58a9f3f12c9a0b4bf4f66d3d738bd3f02dedcbad04bdcd25_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces DexAvatar, a framework that uses learned 3D hand and body pose priors to reconstruct accurate 3D sign language poses from monocular videos. It addresses the limitations of existing 2D datasets and noisy 3D estimations. The method significantly outperforms prior work on the SGNify motion capture benchmark.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Understanding Scaling Laws in Deep Neural Networks via Feature Learning Dynamics</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [deep learning theory], [scaling laws, feature learning, infinite-depth limit, ResNets, hyperparameter transfer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zihan Yao, Ruoyu Wu, Tianxiang Gao</p>
</li>
<li class="">
<p><strong>institution:</strong> DePaul University, Iowa State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21075" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21075</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Derives Neural Feature Dynamics (NFD), a theoretical framework characterizing feature learning in ResNets in the joint infinite-width and infinite-depth limit. 2. Identifies a vanishing mechanism induced by 1/√depth scaling that explains feature-learning collapse in deep networks and the failure of depth-µP. 3. Proposes a practical depth-aware learning-rate correction to counteract the collapse and restore depth-wise hyperparameter transfer for improved performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06065072ce8d20b2298a45760b95c3f905a6aff3d726e09d4ddf1ecd2e9cc359_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06065072ce8d20b2298a45760b95c3f905a6aff3d726e09d4ddf1ecd2e9cc359_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of theoretical understanding behind scaling laws in deep learning by analyzing feature learning dynamics in deep ResNets. It proposes the Neural Feature Dynamics (NFD) framework in the infinite-width and depth limit, which explains when scaling succeeds and identifies a cause of feature collapse. Based on this insight, the authors propose a simple learning-rate correction that improves training stability and performance in deeper networks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Blurb-Refined Inference from Crowdsourced Book Reviews using Hierarchical Genre Mining with Dual-Path Graph Convolutions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [text classification], [hierarchical genre classification, zero-shot semantic alignment, dual-path graph convolution, label co-occurrence graph, blurb-refined inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Suraj Kumar, Utsav Kumar Nareti, Soumi Chattopadhyay, Chandranath Adak, Prolay Mallick</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology Indore, Indian Institute of Technology Patna</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21076" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21076</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a two-phase hierarchical genre mining framework (HiGeMine) that integrates noisy user reviews with authoritative blurbs using a zero-shot semantic alignment strategy to filter reviews. 2. Introduces a dual-path, two-level graph-based classification architecture that models inter-genre dependencies via a label co-occurrence graph for coarse and fine-grained prediction. 3. Curates a new hierarchical book genre dataset to facilitate systematic evaluation of the proposed method.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3afc3f4ebea9058118426cd2d72f37e4c09ae5bcc05f191e73c452f78fc501af_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3afc3f4ebea9058118426cd2d72f37e4c09ae5bcc05f191e73c452f78fc501af_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of noisy and unreliable book genre classification by proposing HiGeMine, a framework that filters user reviews using semantic alignment with book blurbs and then performs hierarchical classification using a dual-path graph-based model. The method outperforms baselines on a newly curated dataset, demonstrating an effective solution for leveraging structured and unstructured text in hierarchical genre analysis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] LLM Personas as a Substitute for Field Experiments in Method Benchmarking</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [algorithmic fairness &amp; evaluation], [field experiments, A/B testing, LLM personas, algorithmic benchmarking, information-theoretic bounds]</p>
</li>
<li class="">
<p><strong>authors:</strong> Enoch Hyunwook Kang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Washington</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21080" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21080</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a formal, if-and-only-if characterization of the conditions (aggregate-only observation, algorithm-blind evaluation) under which swapping humans for LLM personas is a valid benchmark substitution, equivalent to changing the evaluation panel. 2. Moves from validity to usefulness by defining an information-theoretic measure of discriminability for the aggregate channel induced by persona simulation. 3. Derives explicit sample-size bounds on the number of independent persona evaluations required to make persona benchmarking as decision-relevant as a field experiment for distinguishing between methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f1e74907f157cdb694f3120aed988d1affab03b63adb6bf73297f43733c1b8ba_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f1e74907f157cdb694f3120aed988d1affab03b63adb6bf73297f43733c1b8ba_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high cost and latency of field experiments (A/B tests) for benchmarking methods in societal systems by proposing LLM-based persona simulation as a synthetic alternative. It formally proves the conditions under which this substitution is valid and provides information-theoretic bounds on the required number of persona evaluations to make the benchmark useful. The main conclusion is that persona benchmarking can be a viable, efficient substitute for field experiments under specific, well-defined conditions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Hierarchical Modeling Approach to Fast and Accurate Table Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [document analysis], [table recognition, multi-task learning, non-causal attention, parallel inference, hierarchical modeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Takaya Kawakatsu</p>
</li>
<li class="">
<p><strong>institution:</strong> Preferred Networks, Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21083" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21083</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel multi-task model utilizing non-causal attention to capture the entire table structure. 2. A parallel inference algorithm for cell content recognition that significantly speeds up inference. 3. A hierarchical modeling approach that extends the performance of multi-task models while addressing their speed and explainability limitations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd3e215f24ffc72ff43528e39e86c65289bbfb5de75955d48f0284f277f0089e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd3e215f24ffc72ff43528e39e86c65289bbfb5de75955d48f0284f277f0089e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the slow inference speed and lack of explainability in existing table recognition models. It proposes a new multi-task model with non-causal attention for global structure understanding and a parallel inference algorithm to decode cell contents simultaneously, achieving both superior accuracy and a 10x+ speedup on large public datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Dyna-Style Reinforcement Learning Modeling and Control of Non-linear Dynamics</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Dyna-Style RL, SINDy, TD3, Model-based RL, Bi-rotor Control]</p>
</li>
<li class="">
<p><strong>authors:</strong> Karim Abdelsalam, Zeyad Gamal, Ayman El-Badawy</p>
</li>
<li class="">
<p><strong>institution:</strong> German University in Cairo</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21081" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21081</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Dyna-Style RL framework that integrates SINDy for data-driven dynamics modeling with TD3 for policy learning., 2. Introduces a method to periodically inject synthetic rollouts from the learned SINDy model into the RL replay buffer to improve sample efficiency., 3. Demonstrates the framework&#x27;s effectiveness on a bi-rotor system, showing superior accuracy and robustness in stabilization and trajectory tracking compared to direct model-free RL.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/508875b78acef042533866808253222b6799135bb65f28295e2b374106b10052_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/508875b78acef042533866808253222b6799135bb65f28295e2b374106b10052_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a hybrid control framework combining Sparse Identification of Nonlinear Dynamics (SINDy) and Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning to efficiently control nonlinear systems. The SINDy model generates synthetic data to augment real-world training, improving sample efficiency. The method is validated on a bi-rotor system, showing better performance than direct model-free RL.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Shared Representation Learning for High-Dimensional Multi-Task Forecasting under Resource Contention in Cloud-Native Backends</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [multi-task time series forecasting, shared representation learning, cloud-native systems, resource contention, dynamic adjustment mechanism]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zixiao Huang, Jixiao Yang, Sijia Li, Chi Zhang, Jinyu Chen, Chengda Xu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Washington, Westcliff University, University of Michigan, Northeastern University, University of Virginia</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21102" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21102</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A unified forecasting framework with a shared encoding structure for high-dimensional, multi-task time series in cloud-native backends. 2. A cross-task structural propagation module to model complex dependencies among nodes caused by resource contention and service topology changes. 3. A dynamic adjustment mechanism that regulates internal feature flows to adapt to non-stationary behaviors like sudden load shifts.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9838b10e22d58d1ccca2c68a210133b154701b0c5ff14deadba418ccdbdb92f2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9838b10e22d58d1ccca2c68a210133b154701b0c5ff14deadba418ccdbdb92f2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a unified forecasting framework for high-dimensional multi-task time series in cloud-native backend systems. The method uses shared representation learning, a structural propagation module, and a dynamic adjustment mechanism to model complex dependencies and adapt to non-stationary behaviors. Experimental results show the framework achieves superior performance and provides reliable predictive capability for dynamic cloud environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Semi-Supervised Learning for Large Language Models Safety and Content Moderation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [content moderation], [semi-supervised learning, data augmentation, safety classifiers, LLM safety, prompt harmfulness]</p>
</li>
<li class="">
<p><strong>authors:</strong> Eduard Stefan Dinuta, Iustin Sirbu, Traian Rebedea</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Science and Technology Politehnica Bucharest, Renius Technologies, NVIDIA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21107" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21107</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Analysis of state-of-the-art semi-supervised learning algorithms for LLM safety, focusing on both prompt and response harmfulness. 2. Introduction of a new, task-specific augmentation technique for safety tasks. 3. Demonstration that task-specific augmentations significantly outperform general-purpose methods like backtranslation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/035c08c88d89969ce37594942a40aa577a3c0c7c7743cd71bdf84366a9dfa5f2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/035c08c88d89969ce37594942a40aa577a3c0c7c7743cd71bdf84366a9dfa5f2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of acquiring high-quality labeled data for training safety classifiers for Large Language Models. It proposes using semi-supervised learning techniques that leverage both labeled and unlabeled data, and introduces a task-specific data augmentation method. The key finding is that this approach, particularly with custom augmentations, significantly improves performance on safety tasks compared to using general-purpose techniques.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Semantic Refinement with LLMs for Graph Representations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph representation learning], [graph neural network, large language model, semantic refinement, structure-semantics heterogeneity, data-centric adaptation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Safal Thapaliya, Zehong Wang, Jiazheng Li, Ziming Li, Yanfang Ye, Chuxu Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Connecticut, University of Notre Dame</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21106" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21106</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a data-centric perspective to address structure-semantics heterogeneity in graphs by treating node semantics as a task-adaptive variable, shifting focus from model-centric inductive bias injection. 2. Introduces the Data-Adaptive Semantic Refinement (DAS) framework, which couples a fixed GNN and an LLM in a closed feedback loop for iterative semantic refinement and graph learning. 3. Demonstrates the framework&#x27;s effectiveness on diverse graphs, showing consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfae73c72759ca834d898f3c6ca5f0824bada06285918fca678e0f809fce9afd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfae73c72759ca834d898f3c6ca5f0824bada06285918fca678e0f809fce9afd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of structure-semantics heterogeneity in graph data, where predictive signals vary across domains. It proposes a Data-Adaptive Semantic Refinement (DAS) framework that uses a closed feedback loop between a GNN and an LLM to iteratively refine node semantics for the learning task. The method shows strong performance on structure-dominated graphs and remains competitive on semantics-rich graphs, validating the data-centric adaptation approach.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [precipitation nowcasting, latent diffusion model, spatio-temporal prediction, variational autoencoder, conditioning network]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shi Quan Foo, Chi-Ho Wong, Zhihan Gao, Dit-Yan Yeung, Ka-Hing Wong, Wai-Kin Wong</p>
</li>
<li class="">
<p><strong>institution:</strong> The Hong Kong University of Science and Technology, Hong Kong Observatory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21118" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21118</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/sqfoo/stldm_official" target="_blank" rel="noopener noreferrer" class="">https://github.com/sqfoo/stldm_official</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes STLDM, a novel two-stage diffusion-based architecture for precipitation nowcasting that combines deterministic forecasting with generative enhancement. 2. Introduces an end-to-end learning framework that jointly trains a Variational Autoencoder, a conditioning network, and a latent diffusion model. 3. Demonstrates superior performance and improved inference efficiency compared to state-of-the-art methods on multiple radar datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3f04a94a2a07676690c2f108f7a602a6b052c1463701d217a319cd81e05ecce_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3f04a94a2a07676690c2f108f7a602a6b052c1463701d217a319cd81e05ecce_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of precipitation nowcasting, where deterministic models produce blurry predictions and generative models suffer from poor accuracy. It proposes STLDM, a spatio-temporal latent diffusion model that decomposes the task into a deterministic forecasting stage and a generative enhancement stage. Experiments show STLDM outperforms state-of-the-art methods while being more efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] A Mechanistic Analysis of Transformers for Dynamical Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [dynamical systems modeling], [transformers, attention mechanism, dynamical systems, representational analysis, delay embedding]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gregory Duthé, Nikolaos Evangelou, Wei Liu, Ioannis G. Kevrekidis, Eleni Chatzi</p>
</li>
<li class="">
<p><strong>institution:</strong> ETH Zürich, Johns Hopkins University, Singapore-ETH Centre</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21113" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21113</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a mechanistic interpretation of causal self-attention in Transformers as a linear, history-dependent recurrence from a dynamical systems perspective. 2. Demonstrates that the softmax attention&#x27;s convexity constraint fundamentally limits the representation of certain linear dynamics, leading to oversmoothing. 3. Shows that for nonlinear partially observable systems, attention acts as an adaptive delay-embedding mechanism for state reconstruction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ad3a459d499a20015cb02bd1e25bf788476d675d70043dd137b07a4b0f35b55_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ad3a459d499a20015cb02bd1e25bf788476d675d70043dd137b07a4b0f35b55_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the representational capabilities of single-layer Transformers for modeling dynamical systems, interpreting attention as a history-dependent recurrence. Through linear and nonlinear case studies, it finds that softmax attention restricts representable linear dynamics but can enable state reconstruction in nonlinear systems via adaptive delay embedding. The work bridges empirical Transformer performance with classical dynamical systems theory.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] AutoBaxBuilder: Bootstrapping Code Security Benchmarking</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [code security evaluation], [automated benchmarking, LLM-generated code, security vulnerabilities, exploit generation, plausibility checks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tobias von Arx, Niels Mündler, Mark Vero, Maximilian Baader, Martin Vechev</p>
</li>
<li class="">
<p><strong>institution:</strong> ETH Zurich, Snyk, INSAIT (Sofia University &quot;St. Kliment Ohridski&quot;)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21132" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21132</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/eth-sri/autobaxbuilder" target="_blank" rel="noopener noreferrer" class="">https://github.com/eth-sri/autobaxbuilder</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces AutoBaxBuilder, a framework for generating code security benchmarking tasks and tests from scratch, addressing the limitations of manual benchmarks. 2. Proposes a robust pipeline with fine-grained plausibility checks that leverages LLMs to construct functionality tests and end-to-end security exploits. 3. Releases AutoBaxBench, a new benchmark of generated tasks, and demonstrates the framework&#x27;s efficiency (under 2 hours and $10 per task) and quality through comparison with human-crafted tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385abd6729afb970eba2217cc3d408efe70ab80f9d7aa0cbe7c2e0254f48b74c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385abd6729afb970eba2217cc3d408efe70ab80f9d7aa0cbe7c2e0254f48b74c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents AutoBaxBuilder, a framework that automatically generates tasks and tests for benchmarking the security of code produced by large language models (LLMs). It uses an LLM-powered pipeline to create functional tests and security exploits, ensuring benchmark quality and scalability. The authors show the method is efficient and release a new benchmark, AutoBaxBench, to evaluate LLM security capabilities.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] MODE: Multi-Objective Adaptive Coreset Selection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [coreset selection, submodular maximization, data efficiency, adaptive weighting, multi-objective optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tanmoy Mukherjee, Pierre Marquis, Zied Bouraoui</p>
</li>
<li class="">
<p><strong>institution:</strong> CRIL, Université d&#x27;Artois</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21152" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21152</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes MODE, a dynamic framework that adaptively combines multiple coreset selection strategies based on their real-time contribution to model performance across different training phases. 2. Provides theoretical guarantees, achieving a (1-1/e)-approximation for the coreset selection problem with O(n log n) complexity and convergence bounds for strategy weights. 3. Demonstrates practical benefits including reduced memory requirements and provides interpretable insights into the evolution of data utility during training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8cefebbb0215d55d5050eb92783788b0acf7386684074cdf20b76685dfef159_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8cefebbb0215d55d5050eb92783788b0acf7386684074cdf20b76685dfef159_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of selecting small, representative data subsets (coresets) for efficient deep learning by proposing MODE, a framework that dynamically adapts selection criteria (like class balance, diversity, and uncertainty) to different training phases. It shows that MODE achieves strong theoretical approximation guarantees and competitive model accuracy while reducing computational and memory costs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] ElfCore: A 28nm Neural Processor Enabling Dynamic Structured Sparse Training and Online Self-Supervised Learning with Activity-Dependent Weight Update</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [spiking neural network, self-supervised learning, structured sparsity, activity-dependent update, event-driven processing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhe Su, Giacomo Indiveri</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Neuroinformatics, University of Zurich and ETH Zurich</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21153" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21153</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A local online self-supervised learning engine enabling multi-layer temporal learning without labeled data. 2. A dynamic structured sparse training engine supporting high-accuracy sparse-to-sparse learning. 3. An activity-dependent sparse weight update mechanism that gates updates based on input activity and network dynamics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d2912d0f31a3824ddb49ba116a841b201d285560ef2177125f8dac188572270_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d2912d0f31a3824ddb49ba116a841b201d285560ef2177125f8dac188572270_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces ElfCore, a 28nm digital spiking neural network processor designed for event-driven sensory processing. It uniquely integrates online self-supervised learning, dynamic structured sparse training, and activity-dependent weight updates. The chip demonstrates significant improvements in power consumption, memory efficiency, and network capacity across various tasks like gesture and speech recognition.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [distributed consensus], [Raft, election timeout, contextual bandits, LinUCB, fault tolerance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qizhi Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> PingCAP, Data &amp; AI-Innovation Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21165" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21165</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. BALLAST, a lightweight contextual-bandit framework for Raft election timeouts with safe exploration and non-stationary adaptation. 2. A reproducible evaluation methodology (discrete-event simulation, fault injection, protocol-level logging, CI-based aggregation) to study election stability under tail latency and recovery turbulence. 3. Demonstration that BALLAST substantially reduces recovery time and unwritable time compared to standard heuristics in challenging WAN regimes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75bff5f2f039d6eaeb1861fcd564ebda78e1b3bd0f91a85b3fc977c335d273e6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75bff5f2f039d6eaeb1861fcd564ebda78e1b3bd0f91a85b3fc977c335d273e6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of leader-election instability in the Raft consensus protocol under variable network conditions like long-tail latency. It proposes BALLAST, a method that uses online linear contextual bandits to adaptively select election timeouts, augmented with safe exploration. The evaluation shows that BALLAST significantly improves recovery performance in unstable WAN environments while remaining competitive in stable settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] A Community-Enhanced Graph Representation Model for Link Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph representation learning], [Graph Neural Networks, Link Prediction, Community Detection, Structure Enhancement]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lei Wang, Darong Lai</p>
</li>
<li class="">
<p><strong>institution:</strong> Southeast University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21166" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21166</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Community-Enhanced Link Prediction (CELP) framework that integrates community structure to jointly model local and global graph topology. 2. Introduces a graph enhancement technique via community-aware, confidence-guided edge completion and pruning. 3. Integrates multi-scale structural features to improve link prediction accuracy, with experimental validation showing superior performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/154686865f18f6411728ca7ec41c4f9a507cd6a00a6b336c6fbfca553149d9b1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/154686865f18f6411728ca7ec41c4f9a507cd6a00a6b336c6fbfca553149d9b1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the limitation of Graph Neural Networks (GNNs) in link prediction, where they often underperform traditional heuristic methods due to over-reliance on local information. It proposes the CELP framework, which incorporates community structure to enhance the graph and capture multi-scale features. Experimental results show CELP achieves superior performance, validating the importance of community structure for accurate link prediction.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] A Unified Framework for EEG Seizure Detection Using Universum-Integrated Generalized Eigenvalues Proximal Support Vector Machine</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [machine learning for healthcare], [Universum learning, GEPSVM, EEG classification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yogesh Kumar, Vrushank Ahire, M. A. Ganaie</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology Ropar</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21170" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21170</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes U-GEPSVM, a novel classifier integrating Universum constraints into the GEPSVM framework via a ratio-based objective function. 2. Introduces IU-GEPSVM, an improved variant with a weighted difference-based formulation for enhanced stability and independent control over class separation and Universum alignment. 3. Demonstrates the effectiveness of the proposed models for EEG seizure detection, achieving superior accuracy on the Bonn University dataset and validating improvements with rigorous statistical tests.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ebfba0923a4428be5bd05b82efd0f6ff72c162a8ee710189c4ccb146c2374f3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ebfba0923a4428be5bd05b82efd0f6ff72c162a8ee710189c4ccb146c2374f3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes two novel classifiers, U-GEPSVM and IU-GEPSVM, which combine the computational efficiency of generalized eigenvalue decomposition with Universum learning to improve EEG seizure detection. The models are designed to handle challenges like non-stationarity and limited labeled data in EEG signals. Evaluation on the Bonn University dataset shows that IU-GEPSVM outperforms baseline methods, providing an efficient and reliable solution for neurological diagnosis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Analytic and Variational Stability of Deep Learning Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [learning theory], [Learning Stability Profile, Lyapunov methods, Clarke generalized derivatives, stability exponents, energy-dissipative systems]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ronald Katende</p>
</li>
<li class="">
<p><strong>institution:</strong> Kabale University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21208" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21208</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified analytic and variational framework for studying stability in deep learning systems via a central object called the Learning Stability Profile (LSP). 2. Proves a Fundamental Analytic Stability Theorem linking uniform boundedness of stability signatures to the existence of a dissipative Lyapunov-type energy. 3. Extends the theory to non-smooth learning systems (e.g., ReLU networks, proximal updates) using Clarke generalized derivatives and variational Lyapunov functionals.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43ea5a1a8d6262f5e2056565c57706a69bbe8f558d9c82e401fa498ff92d5daa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43ea5a1a8d6262f5e2056565c57706a69bbe8f558d9c82e401fa498ff92d5daa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a unified framework to analyze the stability of deep learning systems by viewing them as coupled representation-parameter dynamics. The core method introduces a Learning Stability Profile and proves a theorem linking bounded stability signatures to a dissipative Lyapunov energy, which is extended to non-smooth cases using generalized derivatives. The main conclusion is that this provides a single, foundational dynamical description of stability that unifies and explains robustness across various architectures and optimization methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] MiST: Understanding the Role of Mid-Stage Scientific Training in Developing Chemical Reasoning Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [latent solvability, mid-stage scientific training, chemical reasoning, rule-based rewards, symbolic competence]</p>
</li>
<li class="">
<p><strong>authors:</strong> Andres M Bran, Tong Xie, Shai Pranesh, Jeffrey Meng, Xuan Vu Nguyen, Jeremy Goumaz, David Ming Segura, Ruizhi Xu, Dongzhan Zhou, Wenjie Zhang, Bram Hoex, Philippe Schwaller</p>
</li>
<li class="">
<p><strong>institution:</strong> École Polytechnique Fédérale de Lausanne (EPFL), University of New South Wales (UNSW), Green Dynamics, Shanghai Artificial Intelligence Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21231" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21231</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies two necessary conditions for RL-based chemical reasoning: symbolic competence and latent chemical knowledge. 2. Proposes MiST (mid-stage scientific training), a set of techniques including data-mixing with SMILES/CIF-aware pre-processing and continued pre-training. 3. Demonstrates that MiST significantly improves latent solvability and enables RL to achieve large accuracy gains on challenging chemical tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/734a7266913201c1c5abeaa4d54fa794bfab81e86b7cffa89c8c4dc72702a8fa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/734a7266913201c1c5abeaa4d54fa794bfab81e86b7cffa89c8c4dc72702a8fa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that reinforcement learning for chemical reasoning fails unless the base model already has some latent solvability. The authors propose MiST, a mid-stage training pipeline involving data pre-processing and continued pre-training, to build the necessary prerequisites. Their method substantially boosts model performance on tasks like organic reaction naming and inorganic material generation, establishing clear prerequisites for training chemical reasoning models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [adversarial attacks], [hard-label black-box attacks, query efficiency, ray search optimization, Nesterov&#x27;s Accelerated Gradient, momentum-based optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xinjie Xu, Shuyu Cheng, Dongwei Xu, Qi Xuan, Chen Ma</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University of Technology, Binjiang Institute of Artificial Intelligence, ZJUT, JQ Investments</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21241" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21241</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/machanic/hard_label_attacks" target="_blank" rel="noopener noreferrer" class="">https://github.com/machanic/hard_label_attacks</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed ARS-OPT, a momentum-based algorithm inspired by Nesterov&#x27;s Accelerated Gradient to accelerate the convergence of ray search in hard-label attacks. 2. Introduced PARS-OPT, which further enhances ARS-OPT by incorporating surrogate-model priors into the gradient estimation. 3. Provided theoretical convergence guarantees for the proposed methods and demonstrated superior query efficiency over 13 state-of-the-art approaches on ImageNet and CIFAR-10.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/302b574932ba227c13854e5c9c2cab3d7cf8f1ed29ee145fbc73062632304cd4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/302b574932ba227c13854e5c9c2cab3d7cf8f1ed29ee145fbc73062632304cd4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high query cost of hard-label black-box adversarial attacks by optimizing ray search. The authors propose ARS-OPT, a momentum-based algorithm, and its enhanced version PARS-OPT, which uses surrogate-model priors, to accelerate convergence. Experiments show the methods outperform 13 existing approaches in query efficiency on standard datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [embodied ai], [scene graph, vision language model, dynamic planning, memory graph, graph augmentation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Anatoly O. Onishchenko, Alexey K. Kovalev, Aleksandr I. Panov</p>
</li>
<li class="">
<p><strong>institution:</strong> MIRAI, Cognitive AI Systems Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21243" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21243</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://lookplangraph.github.io/" target="_blank" rel="noopener noreferrer" class="">https://lookplangraph.github.io/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LookPlanGraph, a method for embodied instruction following that dynamically updates a scene graph during execution using a Vision Language Model to verify object priors and discover new entities. 2. Introduces the GraSIF (Graph Scenes for Instruction Following) dataset with an automated validation framework, comprising 514 tasks from existing benchmarks. 3. Demonstrates superior performance over static scene graph methods in simulated environments with changed object positions and shows practical applicability in real-world experiments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39706723670e257f6d0916c7c37badacde760a1f6d3061d011d8c22fa4f29bea_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39706723670e257f6d0916c7c37badacde760a1f6d3061d011d8c22fa4f29bea_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of LLM-based embodied agents failing in dynamic environments due to reliance on pre-built, static scene graphs. It proposes LookPlanGraph, a method that continuously augments a memory graph with real-time visual observations from a VLM to verify and discover objects during plan execution. Experiments show it outperforms static graph methods in simulated and real-world settings, and a new dataset (GraSIF) is introduced for evaluation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Assessing the Software Security Comprehension of Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [software security assessment], [Bloom&#x27;s Taxonomy, knowledge boundary, misconception patterns]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mohammed Latif Siddiq, Natalie Sekerak, Antonio Karam, Maria Leal, Arvin Islam-Gomes, Joanna C. S. Santos</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Notre Dame</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21238" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21238</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a systematic evaluation framework using Bloom&#x27;s Taxonomy to assess LLMs&#x27; software security comprehension across six cognitive levels. 2. Proposed the concept of a &quot;software security knowledge boundary&quot; to identify the highest reliable cognitive performance level for an LLM. 3. Identified and documented 51 recurring misconception patterns made by LLMs in software security tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b3809be1ff4cae9ad7acb47676829cd58ca3ea614efa57d9121857f85d97fc7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b3809be1ff4cae9ad7acb47676829cd58ca3ea614efa57d9121857f85d97fc7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper systematically evaluates the software security comprehension of five leading LLMs using Bloom&#x27;s Taxonomy as a framework across diverse datasets. The results show that while LLMs perform well on lower-level cognitive tasks like recalling facts, their performance significantly degrades on higher-order tasks requiring reasoning and secure system creation. The study introduces a knowledge boundary to quantify reliable performance limits and identifies common misconception patterns.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Model Merging via Multi-Teacher Knowledge Distillation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [model merging], [model merging, knowledge distillation, PAC-Bayes, sharpness-aware minimization, multi-task learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Seyed Arshan Dalili, Mehrdad Mahdavi</p>
</li>
<li class="">
<p><strong>institution:</strong> The Pennsylvania State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21288" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21288</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/arshandalili/SAMerging" target="_blank" rel="noopener noreferrer" class="">https://github.com/arshandalili/SAMerging</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Establishes a novel flatness-aware PAC-Bayes generalization bound for model merging, introducing a &quot;cross-task heterogeneity&quot; term. 2. Frames model merging as multi-teacher knowledge distillation on scarce unlabeled data, showing minimizing student-teacher KL divergence tightens the risk bound. 3. Proposes SAMerging, a method that operationalizes the objective using Sharpness-Aware Minimization (SAM) to find flat minima.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1edfb41aaef41e719d5724fa70ca9022ddcf1e1c683791b3bd1d929286795c62_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1edfb41aaef41e719d5724fa70ca9022ddcf1e1c683791b3bd1d929286795c62_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of theoretical understanding in model merging by framing it as multi-teacher knowledge distillation and deriving a PAC-Bayes generalization bound. It proposes SAMerging, a method that uses Sharpness-Aware Minimization to optimize the merging process based on this theory. The method achieves state-of-the-art performance on vision and NLP benchmarks with high data efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Transcriptome-Conditioned Personalized De Novo Drug Generation for AML Using Metaheuristic Assembly and Target-Driven Filtering</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [computational drug design], [metaheuristic assembly, de novo drug generation, transcriptomics, molecular docking, multi-objective optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abdullah G. Elafifi, Basma Mamdouh, Mariam Hanafy, Muhammed Alaa Eldin, Yosef Khaled, Nesma Mohamed El-Gelany, Tarek H.M. Abou-El-Enien</p>
</li>
<li class="">
<p><strong>institution:</strong> Faculty of Computers and Artificial Intelligence, Cairo University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21301" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21301</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel end-to-end computational framework that integrates patient transcriptomics with de novo drug generation for personalized AML therapy. 2. Development of a reaction-first evolutionary metaheuristic algorithm for assembling novel ligands guided by target-specific structural hotspots. 3. Validation of generated drug candidates through in-silico ADMET profiling and molecular docking, demonstrating pharmacologically viable leads.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8733d98ab11f787e5d221faf5b5b5383a6af0c23974a0e3febf13cb0aa80b4f3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8733d98ab11f787e5d221faf5b5b5383a6af0c23974a0e3febf13cb0aa80b4f3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a computational framework for personalized drug discovery in Acute Myeloid Leukemia (AML). It uses patient transcriptome data to identify biomarkers, models their 3D structures, and employs a novel metaheuristic algorithm to generate new drug-like molecules targeting these biomarkers. The results show the framework can produce viable drug candidates, offering a scalable approach for precision oncology.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Learning to Solve PDEs on Neural Shape Representations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [geometry processing], [neural shape representations, mesh-free PDE solver, geometry-conditioned operator, surface PDEs, differentiable pipeline]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lilian Welschinger, Yilin Liu, Zican Wang, Niloy Mitra</p>
</li>
<li class="">
<p><strong>institution:</strong> University College London, Adobe Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21311" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21311</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel, mesh-free formulation that learns a local update operator conditioned on neural shape attributes to solve surface PDEs directly on neural representations. 2. A method that is trained once on a single shape and generalizes across shape and topology variations, enabling fast inference without per-instance optimization. 3. The first end-to-end pipeline that solves surface PDEs on both neural and classical surface representations, preserving differentiability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed9eac0c574821f5dc10188c61bf1bd40ede7fca32ee8f98accf90b52993315f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed9eac0c574821f5dc10188c61bf1bd40ede7fca32ee8f98accf90b52993315f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the mismatch between traditional mesh-based PDE solvers and modern neural shape representations by proposing a learned, mesh-free operator. This operator, trained once, solves surface PDEs directly on neural data, generalizing across shapes and preserving differentiability. The method performs comparably to classical solvers and enables the first end-to-end PDE-solving pipeline for neural surfaces.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Does the Data Processing Inequality Reflect Practice? On the Utility of Low-Level Tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [information theory, statistical learning], [data processing inequality, Bayes classifier, low-level processing, classification accuracy, finite sample analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Roy Turgeman, Tom Tirer</p>
</li>
<li class="">
<p><strong>institution:</strong> Bar-Ilan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21315" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21315</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A theoretical proof that for any finite number of training samples, there exists a pre-classification processing that improves classification accuracy, even for a classifier converging to the optimal Bayes classifier. 2. An analysis of how class separation, training set size, and class balance affect the relative gain from low-level pre-processing. 3. Empirical validation of the theory on a synthetic setup and on practical deep classifiers with denoising/encoding tasks on benchmark datasets, showing consistent trends.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59be928ca6eaa4e86b8cd3fb9b4f9e66ff3aed7a604e05c3f63b00a3ca0c56bd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59be928ca6eaa4e86b8cd3fb9b4f9e66ff3aed7a604e05c3f63b00a3ca0c56bd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the practical utility of performing low-level signal processing tasks (like denoising) before classification, which seemingly contradicts the information-theoretic Data Processing Inequality. Through a theoretical binary classification analysis and empirical studies, it demonstrates that for finite training samples, such pre-processing can improve accuracy, with the benefit influenced by dataset characteristics like size and noise level.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Parallel Token Prediction for Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [parallel token prediction, speculative decoding, autoregressive decoding, transformer inference, latency optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Felix Draxler, Justus Will, Farrin Marouf Sofian, Theofanis Karaletsos, Sameer Singh, Stephan Mandt</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Irvine, Chan-Zuckerberg Initiative, Pyramidal AI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21323" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21323</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Parallel Token Prediction (PTP), a universal framework for parallel sequence generation that jointly predicts multiple dependent tokens in a single transformer call. 2. Proves that PTP can represent arbitrary autoregressive sequence distributions, avoiding the restrictive independence assumptions of prior multi-token prediction methods. 3. Demonstrates state-of-the-art speculative decoding performance, accepting over four tokens per step on Spec-Bench with Vicuna-7B, showing parallel long-sequence generation is feasible without losing modeling power.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d5681ed12f339fba2f3eb1e012a75a0b58e77b2c9c9aa20352d3b12fcba4f3c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d5681ed12f339fba2f3eb1e012a75a0b58e77b2c9c9aa20352d3b12fcba4f3c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high latency of autoregressive decoding in large language models by proposing Parallel Token Prediction (PTP), a framework that predicts multiple dependent tokens in parallel within a single transformer call. It proves PTP&#x27;s universality in representing autoregressive distributions and shows it achieves superior speculative decoding performance, enabling faster text generation without sacrificing quality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Variationally correct operator learning: Reduced basis neural operator with a posteriori error estimation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [scientific computing], [first-order system least squares, reduced basis neural operator, a posteriori error estimation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuan Qiu, Wolfgang Dahmen, Peng Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Georgia Institute of Technology, University of South Carolina</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21319" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21319</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a variationally correct operator learning framework using First-Order System Least-Squares (FOSLS) objectives that are provably equivalent to solution error in PDE-induced norms. 2. Introduces a Reduced Basis Neural Operator (RBNO) that predicts coefficients for a pre-computed, conforming reduced basis to ensure function space conformity and variational stability. 3. Provides a rigorous convergence analysis that decomposes the total error into discretization bias, basis truncation error, neural network approximation error, and statistical estimation errors, with the residual serving as a reliable a posteriori error estimator.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00a56bc0fe5da5167f8e01f84491e1f4cb6dfb3c8d8926f4a22fe5ff594010bf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00a56bc0fe5da5167f8e01f84491e1f4cb6dfb3c8d8926f4a22fe5ff594010bf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the issue of &quot;variational correctness&quot; in neural operators for solving PDEs, where standard residual losses do not guarantee small solution errors. The authors propose a new framework using a First-Order System Least-Squares (FOSLS) loss and a Reduced Basis Neural Operator (RBNO) to ensure stability and norm equivalence. The method provides a rigorous error bound and demonstrates superior accuracy in PDE-compliant norms, with the residual acting as a reliable error estimator.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Measuring all the noises of LLM Evals</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [LLM evaluation, statistical noise, paired analysis, prediction variance, data variance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sida Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> FAIR at Meta</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21326" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21326</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Clearly defines and measures three types of noise (prediction, data, total) in LLM evaluations using the law of total variance. 2. Proposes the &quot;all-pairs paired method&quot; to apply paired statistical analysis across all model pairs for increased statistical power. 3. Empirically reveals that total noise is predictable per evaluation and that prediction noise typically dominates data noise, enabling more effective significance testing.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa15912febac5bc93aec8d1b8870feaf16ae89016c37e24c26550d053d396fec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa15912febac5bc93aec8d1b8870feaf16ae89016c37e24c26550d053d396fec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of statistical noise in Large Language Model (LLM) evaluations. It proposes an &quot;all-pairs paired method&quot; to measure prediction, data, and total noise across model pairs. The key findings are that each evaluation benchmark has a characteristic noise level and that reducing prediction noise through averaging can significantly improve the detection of performance differences.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [Denoising Entropy, Masked Diffusion Models, decoding path optimization, predictive uncertainty, non-autoregressive generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ziyu Chen, Xinbei Jiang, Peng Sun, Tao Lin</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Westlake University, University of Chicago</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21336" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21336</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/LINs-lab/DenoisingEntropy" target="_blank" rel="noopener noreferrer" class="">https://github.com/LINs-lab/DenoisingEntropy</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formalized the problem of decoding path sensitivity in Masked Diffusion Models (MDMs) by introducing the concept of cumulative Path Uncertainty. 2. Proposed Denoising Entropy, a novel, computable metric to quantify predictive uncertainty along a generative path. 3. Developed two entropy-guided algorithms (post-hoc selection and real-time guidance) to optimize the decoding path and improve generation quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4480eb4fa3d14900373effb4e74dd207b42c650b50de1044a2cad8b4036e465f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4480eb4fa3d14900373effb4e74dd207b42c650b50de1044a2cad8b4036e465f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that the flexible generation of Masked Diffusion Models (MDMs) leads to variable output quality due to the chosen decoding order. To address this, it introduces Denoising Entropy to measure path uncertainty and proposes two algorithms that use this metric to guide the decoding process. Experiments show these methods significantly improve generation accuracy on reasoning, planning, and code tasks, turning uncertainty into an advantage.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Fast and Exact Least Absolute Deviations Line Fitting via Piecewise Affine Lower-Bounding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [optimization], [least absolute deviations, piecewise affine lower-bounding, linear programming, robust regression, exact algorithm]</p>
</li>
<li class="">
<p><strong>authors:</strong> Stefan Volz, Martin Storath, Andreas Weinmann</p>
</li>
<li class="">
<p><strong>institution:</strong> Technische Hochschule Würzburg-Schweinfurt</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20682" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20682</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the Piecewise Affine Lower-Bounding (PALB) method, a novel exact algorithm for LAD line fitting. 2. Provides theoretical proof of correctness and bounds on the number of iterations for the algorithm. 3. Demonstrates empirical log-linear scaling and superior speed compared to existing LP-based and IRLS-based solvers on synthetic and real-world data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b21c32d0f735b0e43e7203101aa47a6f9a1a7a2ca1eed65c3391f51147bd5e6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b21c32d0f735b0e43e7203101aa47a6f9a1a7a2ca1eed65c3391f51147bd5e6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the computational challenge of exact Least Absolute Deviations (LAD) line fitting, a robust regression method. It proposes the PALB algorithm, which uses piecewise-affine lower bounds and a subdivision scheme to find the exact solution efficiently. The method is proven correct, shows log-linear empirical scaling, and is faster than existing practical solvers like linear programming and iteratively reweighted least squares.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Diffusion Models in Simulation-Based Inference: A Tutorial Review</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [simulation-based inference, score-based models, parameter estimation, generative modeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jonas Arruda, Niels Bracher, Ullrich Köthe, Jan Hasenauer, Stefan T. Radev</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Bonn, Rensselaer Polytechnic Institute, Heidelberg University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20685" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20685</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a comprehensive tutorial review synthesizing recent developments on diffusion models for Simulation-Based Inference (SBI)., 2. Highlights and discusses key design choices and concepts affecting SBI performance, such as guidance, score composition, flow matching, and noise schedules., 3. Illustrates the application of diffusion models in SBI through case studies and outlines open questions for future research.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/024277637e6ce2f35e35619e320a14456fb6904af22070de1a30be768a5a2475_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/024277637e6ce2f35e35619e320a14456fb6904af22070de1a30be768a5a2475_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This tutorial review synthesizes how diffusion models, through their score-based formulation, are used for fast and accurate parameter estimation in Simulation-Based Inference (SBI). It covers key design choices in training and inference, discusses factors affecting efficiency and accuracy, and presents case studies to illustrate the concepts.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Weighted MCC: A Robust Measure of Multiclass Classifier Performance for Observations with Individual Weights</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [classification evaluation], [Matthews Correlation Coefficient, weighted performance measure, multiclass classification, robustness analysis, confusion matrix]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rommel Cortez, Bala Krishnamoorthy</p>
</li>
<li class="">
<p><strong>institution:</strong> Washington State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20811" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20811</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new weighted version of the Pearson-Matthews Correlation Coefficient (MCC) for binary and multiclass classification that is sensitive to individual observation weights. 2. Proves the robustness of the proposed weighted measures, showing that changes in weights by at most ε lead to bounded changes in the measure value (by a factor of ε for binary and ε² for multiclass). 3. Demonstrates empirically that the weighted measures can effectively distinguish classifiers based on their performance on highly weighted observations, unlike standard unweighted measures.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/277843f9180deb3e0b651c6e34535bdaba884d63d88920a3f85e89a8483c52d1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/277843f9180deb3e0b651c6e34535bdaba884d63d88920a3f85e89a8483c52d1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the lack of performance measures for classification that account for individually weighted observations. It proposes a weighted version of the Matthews Correlation Coefficient (MCC) for binary and multiclass tasks, which is proven to be robust to weight perturbations. The results show that this new measure successfully identifies classifiers that perform well on more important, highly weighted data points.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] A Physics Informed Neural Network For Deriving MHD State Vectors From Global Active Regions Observations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [astro-physics, solar physics, magnetohydrodynamics], [Physics-Informed Neural Network (PINN), MagnetoHydroDynamic Shallow-Water Tachocline (MHD-SWT), solar active regions (ARs), toroidal bands (toroids), state-vector reconstruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Subhamoy Chatterjee, Mausumi Dikpati</p>
</li>
<li class="">
<p><strong>institution:</strong> Southwest Research Institute, High Altitude Observatory (NSF-NCAR)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20747" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20747</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes PINNBARDS, a novel Physics-Informed Neural Network framework to derive the initial MHD state-vector for solar tachocline models from surface observations of active region distributions. 2. Demonstrates the method&#x27;s ability to converge to physically consistent solutions that match observed toroidal band patterns, specifically using data from the Feb-14-2024 SDO/HMI synoptic map. 3. Explores the parameter space to constrain key physical properties, finding optimal agreement with observations for toroidal field strengths of 20–30 kG and a bandwidth of ~10 degrees, which is consistent with low-order longitudinal mode excitation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64164e9d078622b42b01e54f9efaeb57ab61d047c881403541551b016e24827e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64164e9d078622b42b01e54f9efaeb57ab61d047c881403541551b016e24827e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of initializing solar magnetohydrodynamic models for predicting flare-producing active regions, which requires a full state-vector not provided by surface observations. The authors develop PINNBARDS, a Physics-Informed Neural Network that uses observed toroidal band patterns and the governing MHD equations to reconstruct the necessary initial state-vector for the tachocline. Their analysis identifies optimal physical parameters (20-30 kG field strength) that best match observations, providing a novel pathway for weeks-ahead solar activity prediction.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] GenTSE: Enhancing Target Speaker Extraction via a Coarse-to-Fine Generative Language Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [speech separation], [target speaker extraction, generative language model, coarse-to-fine, exposure bias, direct preference optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haoyang Li, Xuyi Zhuang, Azmat Adnan, Ye Ni, Wei Rao, Shreyas Gopal, Eng Siong Chng</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanyang Technological University, Southeast University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20978" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20978</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GenTSE, a fully generative two-stage decoder-only language model architecture for target speaker extraction, separating coarse semantic token prediction from fine acoustic token generation. 2. Introduces a Frozen-LM Conditioning training strategy to mitigate exposure bias by conditioning models on their own past predictions from earlier checkpoints. 3. Employs Direct Preference Optimization to better align the model&#x27;s outputs with human perceptual preferences.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2e6714f10d4ca9cf7e6cc6ddc5eea2048c365e1e206f97988dcc13bab4d72ef_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2e6714f10d4ca9cf7e6cc6ddc5eea2048c365e1e206f97988dcc13bab4d72ef_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces GenTSE, a novel generative language model approach for target speaker extraction that uses a two-stage, coarse-to-fine process to generate speech. The method addresses exposure bias with a specific training strategy and aligns outputs with human preferences using DPO. Experiments show it outperforms previous LM-based systems in speech quality, intelligibility, and speaker consistency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Enhancing diffusion models with Gaussianization preprocessing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [Gaussianization, bifurcation, sampling efficiency, generative models, data preprocessing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Li Cunzhi, Louis Kang, Hideaki Shimazaki</p>
</li>
<li class="">
<p><strong>institution:</strong> Kyoto University, RIKEN Center for Brain Science</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21020" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21020</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Gaussianization preprocessing step for training data to align the target distribution with the initial Gaussian noise in diffusion models. 2. Aims to mitigate the bifurcation-related sampling inefficiency, particularly improving early-stage reconstruction quality. 3. Enables more stable and efficient sampling, especially beneficial for small-scale network architectures.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1994287172f7f096232171fd21b57b4b1d23f670ef89d86fdbadc32dceabbf46_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1994287172f7f096232171fd21b57b4b1d23f670ef89d86fdbadc32dceabbf46_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the slow sampling problem in diffusion models, which is caused by a delay before trajectory bifurcation. The authors propose applying Gaussianization preprocessing to the training data to make it more closely resemble the initial Gaussian noise, simplifying the model&#x27;s learning task. This method improves generation quality in the early reconstruction stages and enables more efficient sampling.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Critical Points of Degenerate Metrics on Algebraic Varieties: A Tale of Overparametrization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [optimization theory], [Euclidean distance degree, overparametrization, algebraic geometry, degenerate metrics, critical points]</p>
</li>
<li class="">
<p><strong>authors:</strong> Giovanni Luca Marchetti, Erin Connelly, Paul Breiding, Kathlén Kohn</p>
</li>
<li class="">
<p><strong>institution:</strong> KTH Royal Institute of Technology, University of Osnabrück, Digital Futures</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21029" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21029</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Relates degenerate quadratic optimization problems to nondegenerate ones via a projection, revealing the role of the projection&#x27;s ramification locus. 2. Provides tools for counting the number of critical points over projective varieties in the degenerate setting. 3. Extends the theory of Euclidean distance degree (EDD) to the overparametrized regime, bridging algebraic geometry with machine learning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a41be3cd2e9b4a085f8c60d76c30d977b51dcb4e0c2ab2a8f0cb584bc2b6a827_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a41be3cd2e9b4a085f8c60d76c30d977b51dcb4e0c2ab2a8f0cb584bc2b6a827_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the critical points of a degenerate quadratic objective over an algebraic variety, a scenario arising in overparametrized machine learning. The main method connects the degenerate problem to a nondegenerate one via a projection, highlighting the importance of the projection&#x27;s ramification locus. The work extends the Euclidean distance degree framework to the degenerate setting and provides tools for analyzing optimization landscapes in overparametrized models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Learning from Neighbors with PHIBP: Predicting Infectious Disease Dynamics in Data-Sparse Environments</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [Bayesian nonparametric models], [Poisson Hierarchical Indian Buffet Process, sparse count data, infectious disease prediction, Bayesian machine learning, microbiome unseen species problems]</p>
</li>
<li class="">
<p><strong>authors:</strong> Edwin Fong, Lancelot F. James, Juho Lee</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Hong Kong (HKU), The Hong Kong University of Science and Technology (HKUST), Korea Advanced Institute of Science and Technology (KAIST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21005" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21005</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the PHIBP, a Bayesian nonparametric model, for robustly modeling sparse count data in epidemiology. 2. Demonstrates the model&#x27;s ability to borrow statistical strength from related regions to predict outbreaks in areas with zero historical cases. 3. Provides a unified framework that yields accurate predictions and meaningful epidemiological insights (e.g., alpha/beta diversity) in data-sparse environments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71507d2ce74cd2738ff8811f5abfd9a141d92f2c2fbc07f24c76785e9d864c9e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71507d2ce74cd2738ff8811f5abfd9a141d92f2c2fbc07f24c76785e9d864c9e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of predicting infectious disease outbreaks in regions with historically zero reported cases. It proposes using the Poisson Hierarchical Indian Buffet Process (PHIBP), a Bayesian model that leverages information from neighboring areas to handle sparse count data. The experiments show this approach provides accurate outbreak forecasts and useful epidemiological insights in data-scarce settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Clever Hans in Chemistry: Chemist Style Signals Confound Activity Prediction on Public Benchmarks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [cheminformatics], [Clever Hans, shortcut learning, activity prediction, author-disjoint splits, CHEMBL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Andrew D. Blevins, Ian K. Quigley</p>
</li>
<li class="">
<p><strong>institution:</strong> (Institution not explicitly stated in the provided content. Based on the authors&#x27; names and arXiv submission, it cannot be reliably inferred without full paper affiliations.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20924" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20924</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that machine learning models can predict the author of a molecule from its structure alone with high accuracy, revealing distinctive &quot;chemist style&quot; signals in public datasets. 2. Shows that an activity prediction model using only inferred author probabilities (and a protein ID) performs comparably to a structure-based baseline, exposing a &quot;Clever Hans&quot; failure mode where models exploit chemist intent rather than learning causal chemistry. 3. Analyzes the sources of this data leakage and proposes author-disjoint dataset splits as a mitigation strategy to decouple chemist intent from biological outcomes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0714bbaf21a9c4a2f8221248ca78fbc74ff052dffe780f1607a9187bc7047174_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0714bbaf21a9c4a2f8221248ca78fbc74ff052dffe780f1607a9187bc7047174_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper investigates whether machine learning models for molecular activity prediction exploit &quot;chemist style&quot; signals rather than learning causal chemistry. By training a classifier to predict molecule authors from structure and then using only the author probabilities to predict bioactivity, the authors show that models can achieve competitive performance without direct structural input. This reveals a shortcut learning problem in cheminformatics benchmarks, prompting recommendations for author-disjoint data splits to mitigate intent leakage.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Causal-driven attribution (CDA): Estimating channel influence without user-level data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [causal inference], [PCMCI, Structural Causal Model, privacy-first analytics, marketing attribution, temporal causal discovery]</p>
</li>
<li class="">
<p><strong>authors:</strong> Georgios Filippou, Boi Mai Quach, Diana Lenghel, Arthur White, Ashish Kumar Jha</p>
</li>
<li class="">
<p><strong>institution:</strong> Trinity College Dublin</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21211" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21211</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Causal-Driven Attribution (CDA) framework that estimates channel influence using only aggregated impression-level data, eliminating the need for user-level path data. 2. Integrates temporal causal discovery (PCMCI) with causal effect estimation via a Structural Causal Model to recover directional channel relationships and quantify their contributions. 3. Demonstrates the framework&#x27;s accuracy and robustness on synthetic data, showing it captures cross-channel interdependencies while providing a privacy-preserving, scalable alternative to traditional models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/375f379a2c16450357d40be6e36b5fe98344711a4e78be19f7e0ad3d106a71cf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/375f379a2c16450357d40be6e36b5fe98344711a4e78be19f7e0ad3d106a71cf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Causal-Driven Attribution (CDA), a framework that uses aggregated impression data and causal inference techniques to estimate marketing channel influence without user-level tracking. It combines temporal causal discovery (PCMCI) with structural causal modeling to quantify channel contributions to conversions. The method shows strong accuracy in synthetic experiments and offers a privacy-preserving, scalable solution for attribution modeling.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Autonomous Uncertainty Quantification for Computational Point-of-care Sensors</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [Monte Carlo dropout, uncertainty quantification, computational point-of-care sensors, vertical flow assays, neural networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Artem Goncharov, Rajesh Ghosh, Hyou-Arm Joung, Dino Di Carlo, Aydogan Ozcan</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Los Angeles</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21335" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21335</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed an autonomous uncertainty quantification technique for computational point-of-care diagnostics to mitigate erroneous neural network predictions. 2. Integrated a Monte Carlo dropout-based method into a diagnostic pipeline to identify and exclude high-uncertainty predictions without needing ground truth. 3. Demonstrated the method&#x27;s effectiveness on a Lyme disease diagnostic platform, significantly improving sensitivity from 88.2% to 95.7% in blinded testing.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5869aed4bf3e962c5ac24490891076a87e2bc2b992a51fbf1ce76fcce3ce32b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5869aed4bf3e962c5ac24490891076a87e2bc2b992a51fbf1ce76fcce3ce32b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of erroneous predictions from neural networks in computational point-of-care diagnostic sensors. The authors propose an autonomous uncertainty quantification method using Monte Carlo dropout to identify and exclude unreliable predictions. Their approach, tested on a Lyme disease diagnostic platform, significantly improved diagnostic sensitivity, demonstrating enhanced robustness for neural network-driven sensing systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-27T11:22:39.000Z" itemprop="dateModified">Dec 27, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/cs_LG/20251215-20251221"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251215-20251221 (cs.LG)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/category/cslo"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">cs.LO</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-22" class="table-of-contents__link toc-highlight">2025-12-22</a></li><li><a href="#2025-12-23" class="table-of-contents__link toc-highlight">2025-12-23</a></li><li><a href="#2025-12-24" class="table-of-contents__link toc-highlight">2025-12-24</a></li><li><a href="#2025-12-25" class="table-of-contents__link toc-highlight">2025-12-25</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>