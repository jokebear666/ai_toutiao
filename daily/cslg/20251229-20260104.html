<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_LG/20251229-20260104" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251229-20260104 (cs.LG) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/cslg/20251229-20260104"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251229-20260104 (cs.LG) | AI头条"><meta data-rh="true" name="description" content="2025-12-29"><meta data-rh="true" property="og:description" content="2025-12-29"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/cslg/20251229-20260104"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cslg/20251229-20260104" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cslg/20251229-20260104" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.LG","item":"https://jokebear666.github.io/ai_toutiao/daily/cslg"},{"@type":"ListItem","position":3,"name":"20251229-20260104 (cs.LG)","item":"https://jokebear666.github.io/ai_toutiao/daily/cslg/20251229-20260104"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.647bd1ff.css">
<script src="/ai_toutiao/assets/js/runtime~main.a5a9d6f2.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.8703b74f.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/arxiv-daily"><span title="Arxiv每日论文" class="linkLabel_WmDU">Arxiv每日论文</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Collapse sidebar category &#x27;cs.LG&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cs_LG/20251215-20251221"><span title="20251215-20251221 (cs.LG)" class="linkLabel_WmDU">20251215-20251221 (cs.LG)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cslg/20251222-20251228"><span title="20251222-20251228 (cs.LG)" class="linkLabel_WmDU">20251222-20251228 (cs.LG)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/cslg/20251229-20260104"><span title="20251229-20260104 (cs.LG)" class="linkLabel_WmDU">20251229-20260104 (cs.LG)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cslg/20260105-20260111"><span title="20260105-20260111 (cs.LG)" class="linkLabel_WmDU">20260105-20260111 (cs.LG)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csoh"><span title="cs.OH" class="categoryLinkLabel_W154">cs.OH</span></a><button aria-label="Expand sidebar category &#x27;cs.OH&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20260105-20260111"><span title="20260105-20260111" class="linkLabel_WmDU">20260105-20260111</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/daily/cslg"><span>cs.LG</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251229-20260104 (cs.LG)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251229-20260104 (cs.LG)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-29">2025-12-29<a href="#2025-12-29" class="hash-link" aria-label="Direct link to 2025-12-29" title="Direct link to 2025-12-29" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251229] Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [data spaces, cloud-edge continuum, containerized microservices, edge AI, intelligent infrastructure monitoring]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dimitrios Amaxilatis, Themistoklis Sarantakos, Nikolaos Tsironis, Souvik Sengupta, Kostas Ramantas, Jhofre Ojeda</p>
</li>
<li class="">
<p><strong>institution:</strong> Spark Works Ltd., IONOS SE, Iquadrat Informática S.L.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21340" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21340</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A real-world implementation of intelligent infrastructure monitoring within a data space-enabled cloud-edge framework, demonstrating practical integration. 2. Leveraging edge computing, containerized microservices, and interoperable data sharing to address challenges like sensor integration, data privacy, and scalability. 3. Showcasing the training and deployment of AI/ML services directly at the edge for optimized resource use and timely decision-making in smart city applications.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ae37cae2dac445e3682b661f116dd30e5d680105ac5070eb7b48c049ab9aff3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ae37cae2dac445e3682b661f116dd30e5d680105ac5070eb7b48c049ab9aff3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a real-world use case for smart cities, implementing an intelligent climate monitoring system within a cloud-edge continuum framework enabled by data spaces. The method combines edge computing, containerized microservices, and secure data sharing to facilitate localized analytics and AI deployment. The conclusion highlights the transformative potential of integrating AI, edge computing, and data spaces for building efficient and resilient smart city infrastructures.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Physics-Informed Neural Solvers for Periodic Quantum Eigenproblems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [physics-informed machine learning], [physics-informed neural networks, Floquet-Bloch eigenvalue problem, honeycomb lattice, band structure, transfer learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haaris Mian</p>
</li>
<li class="">
<p><strong>institution:</strong> Columbia University (Program in Applied Mathematics, Department of Applied Physics and Applied Mathematics)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21349" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21349</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A physics-informed machine learning framework for solving the Floquet-Bloch eigenvalue problem for particles in 2D periodic potentials, using neural networks to learn Bloch functions and eigenvalues simultaneously. 2. A mesh-free solver that enforces the Schrödinger equation, Bloch periodicity, and normalization via a composite loss function without supervision. 3. Demonstration of transfer learning to adapt the solver from nearly-free to strongly varying potentials, capturing changes in band topology.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18928753702626417cc700c2238d86a4711adfe422a6c1995a68fbcbf3401d4f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18928753702626417cc700c2238d86a4711adfe422a6c1995a68fbcbf3401d4f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This thesis proposes a physics-informed neural network framework to solve quantum eigenproblems for particles in periodic potentials, specifically targeting the honeycomb lattice. The method learns Bloch wavefunctions and their energies by training over the Brillouin zone with a loss function that encodes the governing physics. It is validated against traditional methods and shows promise for exploring band structure topology through transfer learning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Reinforcement Learning Approach to Synthetic Data Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [synthetic data generation, reinforcement learning, proximal policy optimization, privacy, biomedical data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Natalia Espinosa-Dice, Nicholas J. Jackson, Chao Yan, Aaron Lee, Bradley A. Malin</p>
</li>
<li class="">
<p><strong>institution:</strong> Princeton University, Vanderbilt University Medical Center, Washington University in St. Louis</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21395" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21395</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Reframes synthetic data generation (SDG) as a reinforcement learning problem, introducing a novel perspective. 2. Proposes RLSyn, a framework that models the data generator as a stochastic policy optimized via Proximal Policy Optimization with discriminator-derived rewards for stable, data-efficient training. 3. Demonstrates the effectiveness of the RL approach, showing it performs comparably to or better than GANs and diffusion models, especially in data-scarce regimes on biomedical datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75d9e0c3d2cba88654d16f9652042680f0037508065d6d94fba27208a6199969_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75d9e0c3d2cba88654d16f9652042680f0037508065d6d94fba27208a6199969_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes RLSyn, a reinforcement learning framework for generating synthetic biomedical data by modeling the generator as a policy optimized with PPO. It shows that this approach achieves performance comparable to or better than GANs and diffusion models, particularly when training data is limited, offering a stable and data-efficient alternative for privacy-preserving data sharing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Learning to Reconfigure: Using Device Status to Select the Right Constrained Coding Scheme</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [storage systems], [constrained coding, LOCO codes, linear programming, code reconfiguration, two-dimensional magnetic recording (TDMR)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Doğukan Özbayrak, Ahmed Hareedy</p>
</li>
<li class="">
<p><strong>institution:</strong> Middle East Technical University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21396" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21396</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes offline and online learning methods to reconfigure constrained coding schemes based on actual device status, moving beyond predetermined time stamps. 2. Models the reconfiguration problem as a linear programming optimization to maximize storage capacity and/or minimize decoding complexity, proving global optimality. 3. Demonstrates the effectiveness of the approach through experimental results in Two-Dimensional Magnetic Recording (TDMR) systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602c83173fa9054dd0b0d4fea2c1b1c7d235aa475323c43a09847363ee851c20_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602c83173fa9054dd0b0d4fea2c1b1c7d235aa475323c43a09847363ee851c20_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of optimally reconfiguring constrained coding schemes in storage devices as they age. The authors propose offline and online learning methods that fit device status data to polynomial models and formulate the reconfiguration decision as a linear programming problem to maximize capacity or minimize complexity. Experimental results show the proposed method is effective for TDMR systems and can be extended to other storage or transmission systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] kooplearn: A Scikit-Learn Compatible Library of Algorithms for Evolution Operator Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [dynamical systems learning], [Koopman operator, transfer operator, spectral decomposition, scikit-learn API, reduced-order models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Giacomo Turri, Grégoire Pacreau, Giacomo Meanti, Timothée Devergne, Daniel Ordonez, Erfan Mirzaei, Bruno Belucci, Karim Lounici, Vladimir Kostic, Massimiliano Pontil, Pietro Novelli</p>
</li>
<li class="">
<p><strong>institution:</strong> Italian Institute of Technology, École Polytechnique, Inria, University College London</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21409" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21409</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Machine-Learning-Dynamical-Systems/kooplearn" target="_blank" rel="noopener noreferrer" class="">https://github.com/Machine-Learning-Dynamical-Systems/kooplearn</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a unified library implementing linear, kernel, and deep-learning estimators for both discrete-time (Koopman/Transfer) and continuous-time evolution operators. 2. Offers a scikit-learn compatible API for easy integration into existing ML workflows. 3. Includes curated benchmark datasets to support experimentation, reproducibility, and fair algorithm comparison.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6abf4d9f1d18bb64fbf627b5a4cc8d5b0c12b9f7fda20d8f6811df3f96602779_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6abf4d9f1d18bb64fbf627b5a4cc8d5b0c12b9f7fda20d8f6811df3f96602779_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces kooplearn, a Python library for learning evolution operators (like Koopman and transfer operators) from dynamical systems data. It provides a suite of estimators and a scikit-learn compatible interface to enable spectral analysis, reduced-order modeling, and forecasting. The library aims to standardize and facilitate research in data-driven dynamical systems modeling.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [tool-use framework, vision-language model, interpretability, data-efficiency, tool bottleneck model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Christina Liu, Alan Q. Wang, Joy Hsu, Jiajun Wu, Ehsan Adeli</p>
</li>
<li class="">
<p><strong>institution:</strong> California Institute of Technology, Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21414" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21414</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/christinaliu2020/tool-bottleneck-framework" target="_blank" rel="noopener noreferrer" class="">https://github.com/christinaliu2020/tool-bottleneck-framework</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed the Tool Bottleneck Framework (TBF) for medical image understanding, which uses a learned Tool Bottleneck Model (TBM) to compose tool outputs instead of text-based composition. 2. Introduced a strategy for the TBM to handle arbitrary VLM tool selections, enabling flexible and robust prediction. 3. Demonstrated that the framework improves performance, interpretability, and data-efficiency, matching or surpassing deep learning classifiers and other tool-use frameworks, especially in data-limited scenarios.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that existing tool-use frameworks for image understanding perform poorly on medical images due to their reliance on text to compose specialized tools. The authors propose the Tool Bottleneck Framework (TBF), which uses a vision-language model to select tools and a learned neural network (the Tool Bottleneck Model) to fuse their outputs. Evaluations on histopathology and dermatology tasks show TBF performs on par with or better than existing methods, offering gains in data-limited settings and more interpretable predictions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [wireless networking], [Age of Information (AoI), reinforcement learning, freshness optimization, wireless networks, multi-agent systems]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alimu Alibotaiken, Suyang Wang, Oluwaseun T. Ajayi, Yu Cheng</p>
</li>
<li class="">
<p><strong>institution:</strong> Illinois Institute of Technology, California State University, San Bernardino</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21412" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21412</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel taxonomy for Age of Information (AoI) and its variants, categorizing them into native, function-based, and application-oriented families to clarify freshness modeling for B5G/6G systems. 2. Introduces a policy-centric taxonomy for reinforcement learning in freshness-aware networks, consisting of update-control RL, medium-access RL, risk-sensitive RL, and multi-agent RL. 3. Synthesizes recent RL-driven freshness control progress and highlights open challenges like delayed decision processes and cross-layer design to establish a unified foundation for learning-based freshness optimization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b58e6fe193d4299ab0beca4eb949d8b13e21e8198c223c3dd6c0ca64896bbf7d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b58e6fe193d4299ab0beca4eb949d8b13e21e8198c223c3dd6c0ca64896bbf7d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This survey addresses the gap between classical Age of Information (AoI) studies and broad reinforcement learning (RL) discussions in wireless networks by examining RL specifically for freshness optimization. It organizes AoI variants and introduces a policy-centric RL taxonomy to provide a coherent framework for freshness-aware decision-making in next-generation wireless systems. The paper aims to establish a unified foundation for learning-based freshness control and highlights key open challenges for future research.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Scalable Deep Subspace Clustering Network</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [subspace clustering], [landmark-based approximation, self-expression, spectral clustering, linear complexity, convolutional auto-encoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nairouz Mrabah, Mohamed Bouguessa, Sihem Sami</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Quebec at Montreal</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21434" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21434</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a deep subspace clustering framework (SDSNet) that achieves linear O(n) computational complexity, breaking the traditional O(n^3) bottleneck. 2. Introduces a landmark-based approximation to avoid constructing the full n×n affinity matrix, combined with joint optimization of auto-encoder reconstruction and self-expression objectives. 3. Enables direct spectral clustering on factorized representations, integrating convolutional auto-encoders with subspace-preserving constraints for efficient and accurate clustering.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high computational cost (O(n^3)) of traditional subspace clustering methods. It proposes SDSNet, a scalable deep learning framework that uses landmark approximation and joint optimization to achieve linear O(n) complexity. Experiments show SDSNet maintains clustering quality comparable to state-of-the-art methods while being significantly more efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [se], [software testing], [runtime error detection, coverage-guided testing, multi-agent reasoning, large language models, static analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hridya Dhulipala, Xiaokai Rong, Tien N. Nguyen</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Texas at Dallas</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21431" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21431</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Cerberus, a novel predictive, execution-free coverage-guided testing framework that uses LLMs for input generation, coverage prediction, and error detection without code execution. 2. Introduces a two-phase feedback loop that first maximizes code coverage and detects errors, then focuses solely on error detection after coverage is maximized, improving performance over single-phase prompting. 3. Empirically demonstrates that Cerberus outperforms conventional and learning-based testing frameworks for both complete and incomplete code snippets by generating high-coverage test cases more efficiently and discovering more runtime errors.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d41379a4c8476f7ed1a8a02b193c5fe427e6a274d56beccd85313ce47ba5e76_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d41379a4c8476f7ed1a8a02b193c5fe427e6a274d56beccd85313ce47ba5e76_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes Cerberus, a framework that uses Large Language Models (LLMs) to statically detect runtime errors in code snippets without execution. It employs a multi-agent reasoning approach with a two-phase, coverage-guided feedback loop to generate test inputs and predict errors. The evaluation shows Cerberus is more efficient and effective at finding runtime errors than existing testing methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [lossy compression, quality prediction, deep-surrogate, mixture-of-experts, feature-extraction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Khondoker Mirazul Mumenin, Robert Underwood, Dong Dai, Jinzhen Wang, Sheng Di, Zarija Lukić, Franck Cappello</p>
</li>
<li class="">
<p><strong>institution:</strong> University of North Carolina at Charlotte, Argonne National Laboratory, University of Delaware, Lawrence Berkeley National Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21433" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21433</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1) A generalizable surrogate model for predicting compression quality across different compressors, quality metrics, and datasets. 2) A novel two-stage design that decouples expensive feature extraction from lightweight prediction for efficient training and modular inference. 3) A mixture-of-experts design to optimize performance and robustness for time-evolving scientific data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06e831f83754144a92a117a184fdcc51da0584d4163390cf94b34d4175b77a1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06e831f83754144a92a117a184fdcc51da0584d4163390cf94b34d4175b77a1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes DeepCQ, a deep-surrogate framework to efficiently predict the quality of data after lossy compression, which is traditionally computationally expensive to assess. The method uses a two-stage and mixture-of-experts design for generalizability and robustness across different compressors, metrics, and time-evolving datasets. The framework achieves high predictive accuracy (errors under 10%) on real-world applications, enabling informed compression decisions and reducing I/O and computational overhead.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [computational ethics], [moral context, probabilistic clustering, LLM semantics, interpretable prediction, human judgment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Geoffroy Morlat, Marceau Nahon, Augustin Chartouny, Raja Chatila, Ismael T. Freire, Mehdi Khamassi</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Intelligent Systems and Robotics, Sorbonne University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21439" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21439</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An empirically grounded dataset of 300 moral scenarios with human ternary judgments. 2. A reproducible pipeline (COMETH) combining human judgments, probabilistic context learning, and LLM-based semantic abstraction. 3. An interpretable, context-sensitive moral prediction model that outperforms end-to-end LLM prompting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f4abc9f666c2d29dadd77869bddf3f159d0bbc8839c7c0f65bbdb4c29ad40c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f4abc9f666c2d29dadd77869bddf3f159d0bbc8839c7c0f65bbdb4c29ad40c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that moral judgments depend heavily on context. It proposes the COMETH framework, which uses probabilistic clustering on human judgment data and LLM-based semantic abstraction to learn and explain action-specific moral contexts. The main conclusion is that COMETH significantly outperforms direct LLM prompting in aligning with human majority judgments while providing interpretable predictions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [se], [fuzz testing], [initial corpus generation, large language models, multi-agent framework, predictive code coverage, mutation-based fuzzing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hridya Dhulipala, Xiaokai Rong, Aashish Yadavally, Tien N. Nguyen</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Texas at Dallas</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21440" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21440</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FuzzWise, a novel method that integrates initial corpus generation and minimization into a single, streamlined process using an LLM-based multi-agent framework., 2. Introduces a predictive code coverage module (an LLM agent) that assesses new test cases without requiring actual program execution, saving computational resources., 3. Demonstrates empirically that FuzzWise generates a smaller, higher-quality initial corpus that achieves higher code coverage and triggers more runtime errors more efficiently than baseline methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcb8eafec283e39ae04e0274dc4688aced924346193560581e8469f1151507f6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcb8eafec283e39ae04e0274dc4688aced924346193560581e8469f1151507f6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of generating a high-quality initial seed corpus for mutation-based fuzzing. It proposes FuzzWise, a method that uses a multi-agent LLM framework to generate and intelligently select test cases based on predicted coverage without execution. The evaluation shows FuzzWise produces a smaller, more effective corpus that achieves higher coverage and finds more bugs efficiently.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [masked diffusion language models, reinforcement learning, parallel decoding, on-policy optimization, unmasking planner]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shirui Chen, Jiantao Jiao, Lillian J. Ratliff, Banghua Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Washington, University of California, Berkeley</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21446" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21446</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes dUltra, an on-policy RL framework (GRPO-based) for learning efficient unmasking strategies in MDLMs. 2. Introduces a joint optimization scheme for the base diffusion model and a new unmasking planner head using a composite reward. 3. Demonstrates improved accuracy-efficiency trade-off over heuristic and distillation baselines in reasoning and code generation tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f1e67e2dde4b6b9e98e4e3c5574326f0e2d63114afe47049e17c2ae04bb41b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f1e67e2dde4b6b9e98e4e3c5574326f0e2d63114afe47049e17c2ae04bb41b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the slow sampling speed of masked diffusion language models (MDLMs) by proposing dUltra, a reinforcement learning framework that learns an optimal strategy for parallel token unmasking. The method jointly optimizes the diffusion model and a planner head using rewards for correctness, distillation, and step count. The results show dUltra achieves a better trade-off between accuracy and efficiency than existing methods, advancing towards &quot;diffusion supremacy&quot; over autoregressive models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] An Equivariance Toolbox for Learning Dynamics</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [learning theory], [equivariance, Noether&#x27;s theorem, Hessian constraints, learning dynamics, symmetry]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yongyi Yang, Liu Ziyin</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Michigan, Massachusetts Institute of Technology, NTT Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21447" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21447</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a general equivariance framework that unifies first-order constraints (conservation laws, implicit bias) into a single identity. 2. Extended the analysis to second-order, providing structural predictions about curvature, flat/sharp directions, and gradient-Hessian alignment. 3. Generalized classical symmetry analyses from continuous transformations to include general equivariance and discrete transformations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69f03ddf87a64feb67cd904eadbbfc83a393f0b282be28c74e5bfeac489db50b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69f03ddf87a64feb67cd904eadbbfc83a393f0b282be28c74e5bfeac489db50b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper develops a unified equivariance toolbox for analyzing learning dynamics in neural networks. It extends classical symmetry-based analyses to derive coupled first- and second-order constraints, connecting transformation structure to loss landscape geometry. The framework recovers known results and provides new characterizations linking equivariance to modern optimization phenomena.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] RLLaVA: An RL-central Framework for Language and Vision Assistants</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [reinforcement learning, vision-language models, Markov decision process, resource-efficient training, modular framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lei Zhao, Zihao Ma, Boyu Lin, Yuhe Liu, Wenjun Wu, Lei Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Beihang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21450" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21450</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/TinyLoopX/RLLaVA" target="_blank" rel="noopener noreferrer" class="">https://github.com/TinyLoopX/RLLaVA</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a modular RL framework (RLLaVA) that decouples algorithmic logic from model architecture and distributed execution, enabling easy implementation of new RL algorithms. 2. Formulates the joint visual-textual sequential decision of VLMs as a unified Markov Decision Process (MDP), providing a principled foundation for multi-modal RL. 3. Achieves resource-efficient training, enabling end-to-end full-parameter updates for up to 4B-scale models on a single 24GB GPU.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69c1815245107c8b5c717a57c722eeb64e0b9b4b77c5a989f0d2b9f4353b36df_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69c1815245107c8b5c717a57c722eeb64e0b9b4b77c5a989f0d2b9f4353b36df_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces RLLaVA, a lightweight and modular reinforcement learning framework specifically designed for training vision-language models. It decouples RL logic from system components to simplify algorithm development and enables efficient training of large models on common GPUs. Experiments show that models trained with RLLaVA improve over base models and are competitive with other specialized RL frameworks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] CCAD: Compressed Global Feature Conditioned Anomaly Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [anomaly detection], [global feature conditioning, adaptive compression, reconstruction-based anomaly detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiao Jin, Liang Diao, Qixin Xiao, Yifan Hu, Ziqi Zhang, Yuchen Liu, Haisong Gu</p>
</li>
<li class="">
<p><strong>institution:</strong> Columbia University, University of Michigan, University of California at Berkeley, Stevens Institute of Technology, VisionX LLC</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21459" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21459</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/chloeqxq/CCAD" target="_blank" rel="noopener noreferrer" class="">https://github.com/chloeqxq/CCAD</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CCAD, a novel method that synergizes reconstruction-based and representation-based anomaly detection by using compressed global features as a conditioning modality for the reconstruction model. 2. Introduces an adaptive compression mechanism to enhance model generalization and training efficiency. 3. Contributes a reorganized and re-annotated version of the DAGM 2007 dataset to validate the method&#x27;s effectiveness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of anomaly detection in industrial settings with limited anomalous data by proposing CCAD, a method that combines the strengths of reconstruction-based and representation-based approaches. CCAD conditions a reconstruction model on adaptively compressed global features, leading to improved generalization and training efficiency. Experiments show that CCAD outperforms state-of-the-art methods in AUC and achieves faster convergence.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Statistical vs. Deep Learning Models for Estimating Substance Overdose Excess Mortality in the US</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [LSTM, SARIMA, conformal prediction, counterfactual estimation, uncertainty quantification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sukanya Krishna, Marie-Laure Charpignon, Maimuna Majumder</p>
</li>
<li class="">
<p><strong>institution:</strong> Harvard University, Boston Children&#x27;s Hospital, Broad Institute of MIT and Harvard, Massachusetts Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21456" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21456</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a systematic empirical comparison showing LSTM outperforms SARIMA in point estimation and uncertainty calibration for counterfactual mortality projection under regime change. 2. Provided analysis that attention-based models (Seq2Seq, Transformer) underperform in this task due to overfitting to historical means. 3. Developed a reproducible pipeline incorporating conformal prediction intervals and extensive convergence analysis, providing an open-source framework deployable for public health planning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f7ae15bb7d75d1d53ce8df2d4924f6311b8bfce998ef81244521cd5679c4225_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f7ae15bb7d75d1d53ce8df2d4924f6311b8bfce998ef81244521cd5679c4225_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper compares traditional statistical (SARIMA) and deep learning models (LSTM, Seq2Seq, Transformer) for estimating substance overdose excess mortality in the US during the COVID-19 pandemic. The study finds that LSTM provides more accurate and better-calibrated counterfactual estimates than SARIMA, establishing that carefully validated deep learning models can be more reliable for public health planning under structural disruptions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] When Bayesian Tensor Completion Meets Multioutput Gaussian Processes: Functional Universality and Rank Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [tensor decomposition], [Bayesian tensor completion, multioutput Gaussian processes, variational inference, rank learning, functional universality]</p>
</li>
<li class="">
<p><strong>authors:</strong> Siyuan Li, Shikai Fang, Lei Cheng, Feng Yin, Yik-Chung Wu, Peter Gerstoft, Sergios Theodoridis</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, The Chinese University of Hong Kong, Shenzhen, The University of Hong Kong, Technical University of Denmark, University of California, San Diego, Athena R.C.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21486" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21486</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/OceanSTARLab/RR-FBTC" target="_blank" rel="noopener noreferrer" class="">https://github.com/OceanSTARLab/RR-FBTC</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a rank-revealing functional Bayesian tensor completion (RR-FBTC) method that handles tensors with real-valued indices and automatically determines the tensor rank during inference. 2. Establishes the universal approximation property of the model, demonstrating its expressive power for continuous multi-dimensional signals. 3. Derives an efficient variational inference algorithm with closed-form updates for learning the model.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac46f14d99db0300a24117fcc6a1f29a54c4ad4ae1586354da1e156d0b048995_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac46f14d99db0300a24117fcc6a1f29a54c4ad4ae1586354da1e156d0b048995_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of determining the optimal rank in functional tensor decomposition, which is NP-hard. It proposes a new method called RR-FBTC that uses multioutput Gaussian processes to model latent functions, enabling automatic rank learning and functional universality. Experiments show the method is effective and superior to state-of-the-art approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [wearable sensing, actigraphy encoder, projection module, frozen LLM, behavioral summarization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aiwei Zhang, Arvind Pillai, Andrew Campbell, Nicholas C. Jacobson</p>
</li>
<li class="">
<p><strong>institution:</strong> Dartmouth College</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21506" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21506</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs) for free-text generation of daily behavioral summaries. 2. Constructs a novel, large-scale dataset of 54,383 (actigraphy, text) pairs derived from real-world NHANES recordings. 3. Demonstrates superior performance over prompt-based baselines in semantic fidelity and lexical accuracy, with qualitative analysis showing the model captures circadian structure and behavioral transitions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of generating natural language summaries from raw physiological signals like actigraphy. It proposes MotionTeller, a framework that integrates a pretrained actigraphy encoder with a frozen LLM via a projection module. The model, trained on a novel dataset, outperforms baselines in generating fluent, human-centered descriptions of daily behavior.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-view clustering], [incomplete multi-view clustering, missing pattern tree, decision ensemble, knowledge distillation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenyuan Yang, Jie Xu, Hongqing He, Jiangzhang Gan, Xiaofeng Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China, Hainan University, Singapore University of Technology and Design, Guangxi Normal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21510" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21510</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a missing-pattern tree model to group data into decision sets for fully utilizing available multi-view pairs, addressing the pair under-utilization issue. 2. Introduces a multi-view decision ensemble module that uses uncertainty-based weighting to aggregate robust clustering decisions from different sets. 3. Designs an ensemble-to-individual knowledge distillation module to transfer ensemble knowledge to view-specific models, promoting mutual optimization between ensemble and individual modules.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of incomplete multi-view clustering (IMVC) where inconsistent missing patterns hinder performance. It proposes a framework called TreeEIC that groups data by missing pattern, performs clustering within each group, ensembles the results with uncertainty weighting, and uses knowledge distillation to refine individual models. Experiments show TreeEIC achieves state-of-the-art performance and robustness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] First Provable Guarantees for Practical Private FL: Beyond Restrictive Assumptions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [differential privacy, convergence guarantees, partial client participation, local updates, clipping bias]</p>
</li>
<li class="">
<p><strong>authors:</strong> Egor Shulgin, Grigory Malinovsky, Sarit Khirirat, Peter Richtárik</p>
</li>
<li class="">
<p><strong>institution:</strong> King Abdullah University of Science and Technology (KAUST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21521" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21521</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Fed-α-NormEC, the first DP FL framework with provable convergence under standard assumptions (without bounded gradients or heterogeneity). 2. Fully supports practical FL features like multiple local updates and, crucially, partial client participation. 3. Provides theoretical analysis showing how partial client participation is essential for real-world deployment and vital for privacy amplification.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffad28b47b2302842557d1ba4a799ee36a241e4ab2529611ee61b38dd671f76d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffad28b47b2302842557d1ba4a799ee36a241e4ab2529611ee61b38dd671f76d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the gap between theoretical private federated learning methods, which rely on unrealistic assumptions, and practical deployment needs. It proposes Fed-α-NormEC, a new framework that provides provable differential privacy and convergence guarantees while supporting key practical features like local updates and partial client participation. Experiments on private deep learning tasks validate the theoretical findings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Perplexity-Aware Data Scaling Law: Perplexity Landscapes Predict Performance for Continual Pre-training</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [continual pre-training, scaling laws, perplexity, data selection, knowledge gap]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lei Liu, Hao Zhu, Yue Shen, Zhixuan Chu, Jian Wang, Jinjie Gu, Kui Ren</p>
</li>
<li class="">
<p><strong>institution:</strong> Ant Group, Zhejiang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21515" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21515</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel perplexity-aware data scaling law that predicts model test loss from the perplexity landscape of domain data, moving beyond dataset size. 2. Introduces the concept of &quot;perplexity landscapes&quot; to quantify the informational value and knowledge gap of candidate training samples. 3. Enables adaptive selection of high-utility data subsets for Continual Pre-training, improving efficiency and performance by prioritizing informative content and reducing redundancy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d01b1cad6e908309c7e813cb1d98f2c41345aa1e4d78cbdaf163996c5d111b4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d01b1cad6e908309c7e813cb1d98f2c41345aa1e4d78cbdaf163996c5d111b4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the inefficiency of scaling data for Continual Pre-training (CPT) of LLMs, where simply adding more data yields diminishing returns. The authors propose a new scaling law that uses the model&#x27;s perplexity on domain data as a proxy for the knowledge gap, allowing for the predictive selection of optimal training subsets. Experiments show this method consistently identifies high-utility data, leading to superior performance on domain-specific benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-view clustering], [contrastive learning, incomplete multi-view data, noise-robust clustering, graph-guided learning, imputation-free]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hongqing He, Jie Xu, Wenyuan Yang, Yonghua Zhu, Guoqiu Wen, Xiaofeng Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> Guangxi Normal University, University of Electronic Science and Technology of China, Singapore University of Technology and Design, Hainan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21516" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21516</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a global-graph guided contrastive learning strategy to address the rare-paired sample issue in incomplete multi-view data by constructing a global affinity graph to form new sample pairs. 2. Introduces a local-graph weighted contrastive learning mechanism to mitigate the mis-paired sample issue caused by noise, using local neighbors to generate adaptive weights for pair-wise contrast. 3. Integrates these strategies into a unified, imputation-free framework for effective clustering on both incomplete and noisy multi-view data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of rare-paired and mis-paired samples in contrastive learning for multi-view clustering on incomplete and noisy data. It proposes a unified framework combining global-graph guided contrastive learning to explore complementary information and local-graph weighted contrastive learning to adaptively handle noisy pairs. Experiments show the method outperforms state-of-the-art approaches on both incomplete and noisy data settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Generative Actor Critic</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [generative modeling, policy evaluation, latent plan, offline-to-online, actor-critic]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aoyang Qin, Deqian Kong, Wei Wang, Ying Nian Wu, Song-Chun Zhu, Sirui Xie</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Beijing Institute of General Artificial Intelligence (BIGAI), UCLA, Peking University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21527" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21527</a></p>
</li>
<li class="">
<p><strong>code:</strong> github.com/qayqaq/Generative-Actor-Critic</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the Generative Actor Critic (GAC) framework that reframes policy evaluation as learning a generative model of the joint distribution over trajectories and returns, decoupling decision-making. 2. Introduces a specific instantiation using a latent variable model with continuous latent plan vectors and novel inference strategies for exploitation and exploration. 3. Demonstrates strong offline performance and significantly enhanced offline-to-online improvement on benchmarks, even without step-wise rewards.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62b697a3a1c4a1b559c19af7d65fd19d2eed0bb097eccecdd9008a509a0456c0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62b697a3a1c4a1b559c19af7d65fd19d2eed0bb097eccecdd9008a509a0456c0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Generative Actor Critic (GAC), a novel reinforcement learning framework that decouples sequential decision-making by learning a generative model of trajectories and returns and then performing inference on it. It shows strong performance in offline learning and significantly improves when fine-tuned online, even in sparse-reward environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] AVP-Fusion: Adaptive Multi-Modal Fusion and Contrastive Learning for Two-Stage Antiviral Peptide Identification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [bioinformatics], [adaptive gating mechanism, contrastive learning, transfer learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xinru Wen, Weizhong Lin, Xuan Xiao</p>
</li>
<li class="">
<p><strong>institution:</strong> JCI (inferred from email domain <code>jci.edu.cn</code>)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21544" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21544</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a two-stage deep learning framework (AVP-Fusion) for antiviral peptide identification and subclass prediction. 2. Introduces an Adaptive Gating Mechanism to dynamically fuse local (CNN) and global (BiLSTM) sequence features. 3. Employs a contrastive learning strategy with OHEM and BLOSUM62-based data augmentation to sharpen decision boundaries and handle hard samples.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72c58b1c8ae5a53950bfc6d9e8fcca6dc27fc849f514d47d4a2cdff795cb10b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72c58b1c8ae5a53950bfc6d9e8fcca6dc27fc849f514d47d4a2cdff795cb10b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes AVP-Fusion, a two-stage deep learning framework that integrates adaptive feature fusion and contrastive learning for identifying antiviral peptides (AVPs). The method dynamically fuses multi-modal sequence features and uses contrastive learning to improve classification, achieving state-of-the-art accuracy and enabling precise prediction of antiviral activity against specific viral families.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Discovering Sparse Recovery Algorithms Using Neural Architecture Search</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [sparse recovery], [neural architecture search, meta-learning, iterative shrinkage thresholding algorithm, sparse optimization, algorithm discovery]</p>
</li>
<li class="">
<p><strong>authors:</strong> Patrick Yubeaton, Sarthak Gupta, M. Salman Asif, Chinmay Hegde</p>
</li>
<li class="">
<p><strong>institution:</strong> New York University, University of California, Riverside</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21563" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21563</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a meta-learning framework using Neural Architecture Search (NAS) for automated discovery of sparse recovery algorithms. 2. Demonstrates the framework&#x27;s capability to rediscover key elements of ISTA and FISTA from a search space of over 50,000 variables. 3. Shows the framework&#x27;s applicability to various data distributions and algorithms beyond ISTA/FISTA.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49f1d5d0a89c3d52b36f1e443675494175442f0930725fc8a763e525ca05243a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49f1d5d0a89c3d52b36f1e443675494175442f0930725fc8a763e525ca05243a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a meta-learning framework that uses Neural Architecture Search (NAS) to automatically discover sparse recovery algorithms. It successfully rediscovers components of ISTA and FISTA from a large search space and demonstrates generalizability to other algorithms and data distributions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] AnchorGK: Anchor-based Incremental and Stratified Graph Learning Framework for Inductive Spatio-Temporal Kriging</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [spatio-temporal kriging], [graph neural networks, incremental learning, data stratification, anchor locations, incomplete features]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiaobin Ren, Kaiqi Zhao, Katerina Taškova, Patricia Riddle</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Auckland, Harbin Institute of Technology, Shenzhen</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21569" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21569</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/xren451/Spatial-interpolation" target="_blank" rel="noopener noreferrer" class="">https://github.com/xren451/Spatial-interpolation</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces an anchor-based stratification framework to handle sparse spatial distributions and heterogeneous feature availability in sensor networks. 2. Proposes a dual-view graph learning layer that jointly aggregates feature-relevant and location-relevant information to learn stratum-specific representations. 3. Designs an incremental representation mechanism that systematically utilizes all available features across different strata to mitigate feature incompleteness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c757748db9cf59d7295a4cdf698c5e194dc4ae79ec767aa6f7c71c0e78243768_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c757748db9cf59d7295a4cdf698c5e194dc4ae79ec767aa6f7c71c0e78243768_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes AnchorGK, a graph learning framework for inductive spatio-temporal kriging that addresses sparse sensor deployment and incomplete features. It uses anchor-based stratification and a dual-view graph learning layer to model correlations and incrementally integrate features. Experiments show it outperforms existing state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [compiler &amp; ir], [e-graph, term rewriting, phase ordering, NUMA abstraction, auto vectorize]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hui Guo, Qihang Zheng, Chenghai Huo, Dongliang Guo, Haoqi Yang, Yang Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Canaan Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21571" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21571</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/kendryte/nncase" target="_blank" rel="noopener noreferrer" class="">https://github.com/kendryte/nncase</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an end-to-end compilation framework (nncase) that unifies LLM deployment across heterogeneous memory architectures using a NUMA abstraction for a &quot;compile once, adapt everywhere&quot; capability. 2. Introduces an e-graph-based term rewriting engine with equality saturation to mitigate the phase ordering problem and enable global optimization of computation and data movement. 3. Integrates three key automated optimization modules: Auto Vectorize for heterogeneous computing units, Auto Distribution for parallel strategies with communication optimization, and Auto Schedule for on-chip cache locality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a555b38ecd80e8c11606362e5402405bbf0053e11754e06f720d50ce213ee0d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a555b38ecd80e8c11606362e5402405bbf0053e11754e06f720d50ce213ee0d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents nncase, an end-to-end compiler framework designed to tackle the challenge of efficiently deploying large language models on heterogeneous memory architectures. Its core innovation is an e-graph-based rewriting engine that avoids the phase ordering problem, enabling unified optimization across diverse hardware targets. Evaluations show nncase outperforms frameworks like MLC LLM and Intel IPEX, achieving performance close to hand-optimized llama.cpp, demonstrating the viability of automated compilation for high-performance LLM deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Unified Definition of Hallucination, Or: It&#x27;s the World Model, Stupid</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [hallucination detection &amp; evaluation], [hallucination, world modeling, knowledge conflict, benchmark, language model evaluation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Emmy Liu, Varun Gangal, Chelsea Zou, Xiaoqi Huang, Michael Yu, Alex Chang, Zhuofu Tao, Sachin Kumar, Steven Y. Feng</p>
</li>
<li class="">
<p><strong>institution:</strong> Carnegie Mellon University, Stanford University, The Ohio State University, Patronus AI, DegenAI Labs, Independent Researchers</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21577" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21577</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified definition of hallucination as inaccurate internal world modeling that is observable to the user, synthesizing prior definitions. 2. Provides a framework for analyzing hallucinations by varying the reference world model and knowledge conflict policy, clarifying what constitutes a hallucination versus other error types. 3. Outlines plans for a family of benchmarks based on synthetic, fully-specified world models to stress-test and improve the world modeling components of language models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e2cc31121f769cca7464239d4aa27b26c8c4bc903970a4833fbebac56dc9b85_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e2cc31121f769cca7464239d4aa27b26c8c4bc903970a4833fbebac56dc9b85_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper argues that the persistent problem of hallucination in language models stems from inaccurate internal world modeling. It unifies various historical definitions under this core concept and proposes a framework for clearer evaluation. The authors conclude by sketching plans for new benchmarks to rigorously test and improve language models&#x27; world modeling capabilities.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [Schrödinger Bridge, Low-rank Adaptation, Time Series Foundation Models, Financial Forecasting, Generative Refinement]</p>
</li>
<li class="">
<p><strong>authors:</strong> Anthony Bolton, Wuyang Zhou, Zehua Chen, Giorgos Iacovides, Danilo Mandic</p>
</li>
<li class="">
<p><strong>institution:</strong> Imperial College London</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21572" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21572</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes RefineBridge, a novel post-processing module built on a tractable Schrödinger Bridge framework to refine TSFM forecasts. 2. Introduces a paradigm that treats TSFM predictions as a generative prior and learns context-conditioned stochastic transport maps to iteratively improve them. 3. Demonstrates consistent performance improvements over state-of-the-art TSFMs on multiple financial benchmarks, addressing challenges like non-stationarity and noise.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d984f91104f06a2169d1d04626078f4bd0698ade16094c7642b5b47c5a1a6198_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d984f91104f06a2169d1d04626078f4bd0698ade16094c7642b5b47c5a1a6198_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of applying Time Series Foundation Models (TSFMs) to financial forecasting, where data properties like non-stationarity degrade performance. It proposes RefineBridge, a generative refinement module based on Schrödinger Bridges that iteratively transports TSFM predictions toward ground truths. Experiments show RefineBridge consistently enhances TSFM forecasts across various financial benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [imitation learning], [behavior cloning, latent representation, self-supervised learning, sample efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xin Liu, Haoran Li, Dongbin Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21586" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21586</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel, unsupervised framework (BCV-LR) for imitation learning from videos (ILV) that learns latent actions from visual inputs. 2. Introduces an iterative policy improvement loop that aligns pre-trained latent actions with the real action space online, enabling highly sample-efficient learning. 3. Demonstrates state-of-the-art sample efficiency, outperforming existing ILV and RL methods on a wide range of visual control tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c96d71be701998ebf55ef6ed2a0c0a001a306b44f3555239559ced527b17559_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c96d71be701998ebf55ef6ed2a0c0a001a306b44f3555239559ced527b17559_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes BCV-LR, a framework for learning policies from videos without action labels. It uses self-supervised learning to extract latent actions and an iterative alignment process for sample-efficient behavior cloning. The method achieves expert-level performance on many tasks with minimal interaction, showing videos can be highly effective supervision for policy learning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Quantitative Verification of Omega-regular Properties in Probabilistic Programming</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [probabilistic programming and verification], [temporal posterior inference, omega-regular properties, stochastic barrier certificates, Rabin automata, quantitative verification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Peixin Wang, Jianhao Bai, Min Zhang, C.-H. Luke Ong</p>
</li>
<li class="">
<p><strong>institution:</strong> East China Normal University, Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21596" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21596</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Temporal Posterior Inference (TPI), a new framework unifying probabilistic programming with temporal logic to compute posterior distributions over execution traces satisfying omega-regular properties. 2. Develops a novel method for computing rigorous upper and lower bounds on satisfaction probabilities by decomposing Rabin acceptance conditions and constructing sound stochastic barrier certificates. 3. Implements the approach in a prototype tool named TPInfer and demonstrates its effectiveness and efficiency on a suite of benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c111a01f9d5e96a85d9b5c62645dae0f5bb40053d723e34cb57dc7f31554dcda_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c111a01f9d5e96a85d9b5c62645dae0f5bb40053d723e34cb57dc7f31554dcda_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of standard probabilistic program inference, which fails to capture temporal behavior, by proposing Temporal Posterior Inference (TPI). TPI computes posterior distributions over program traces that satisfy omega-regular temporal specifications, using a method based on stochastic barrier certificates to provide quantitative verification bounds. The approach is implemented in the TPInfer tool and shown to be effective for inference over rich temporal properties.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data in Emergency and Critical Care</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [imbalanced classification], [XGBoost, TabNet, TabResNet, Bayesian hyperparameter search, class imbalance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yusuf Brima, Marcellin Atemkeng</p>
</li>
<li class="">
<p><strong>institution:</strong> Osnabrück University, Rhodes University, National Institute for Theoretical and Computational Sciences (NITheCS)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21602" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21602</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Systematic evaluation of robustness and scalability for classical ML and deep learning models on imbalanced clinical tabular data. 2. Introduction of TabResNet, a custom lightweight residual network designed as a computationally efficient alternative to TabNet for real-time clinical use. 3. Evidence-based finding that tree-based ensemble methods (especially XGBoost) offer the most stable and scalable performance for high-stakes, time-sensitive clinical environments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper evaluates the robustness and scalability of machine learning models on imbalanced clinical data from emergency and critical care settings. It proposes TabResNet, a lightweight deep learning alternative to TabNet, and compares it against tree-based methods like XGBoost. The main conclusion is that tree-based ensemble models, particularly XGBoost, provide the most stable performance and computational scalability for practical clinical deployment, outperforming more complex deep learning architectures in these environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Data-Driven Multi-Objective Approach for Predicting Mechanical Performance, Flowability, and Porosity in Ultra-High-Performance Concrete (UHPC)</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [predictive modeling], [XGBoost, SHAP analysis, K-Fold Cross-Validation, Isolation Forest, hyperparameter tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jagaran Chakma, Zhiguang Zhou, Jyoti Chakma, Cao YuSen</p>
</li>
<li class="">
<p><strong>institution:</strong> Tongji University, University of Chittagong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21610" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21610</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a two-stage data-driven framework for UHPC property prediction, integrating model selection, data cleaning (multicollinearity removal, outlier detection), and feature importance analysis. 2. Demonstrated the superior performance of the XGBoost model after rigorous hyperparameter tuning and validation, achieving high accuracy across multiple objectives (mechanical performance, flowability, porosity). 3. Created a practical graphical user interface (GUI) to facilitate the application of the predictive model by material designers, reducing reliance on extensive experimental testing.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/988c9f68c148fb1ef37ff8e292bf9a2cab1878ea08a2f5baee1a1a47f095be23_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/988c9f68c148fb1ef37ff8e292bf9a2cab1878ea08a2f5baee1a1a47f095be23_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a data-driven, multi-objective machine learning framework to predict the mechanical performance, flowability, and porosity of Ultra-High-Performance Concrete (UHPC). The method involves testing 21 algorithms, selecting and tuning XGBoost, and refining the dataset through multicollinearity removal, outlier detection, and SHAP-based feature selection. The final model achieves high prediction accuracy, and a supporting GUI is developed to aid in UHPC mix design, reducing the need for experimental tests.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] MAD-NG: Meta-Auto-Decoder Neural Galerkin Method for Solving Parametric Partial Differential Equations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scientific machine learning], [Neural Galerkin Method, meta-learning, parametric PDEs, space-time decoupling, randomized sparse updates]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qiuqi Li, Yiting Liu, Jin Zhao, Wencan Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> Hunan University, Capital Normal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21633" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21633</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel framework that enhances the Neural Galerkin Method by integrating the Meta-Auto-Decoder paradigm for solving parametric PDEs. 2. Introduces space-time decoupling to enable more stable and efficient time integration compared to full space-time approximations. 3. Employs meta-learning for rapid adaptation to new parameters and randomized sparse updates to reduce computational cost without sacrificing accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8574d601c121a7a53ef19693b53d019c5139f87d5ae2dd641aec204463aa59c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8574d601c121a7a53ef19693b53d019c5139f87d5ae2dd641aec204463aa59c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenges of generalization and efficiency in neural network-based solvers for parametric PDEs. It proposes MAD-NG, a framework that combines the Neural Galerkin Method with meta-learning and space-time decoupling to achieve stable, efficient, and accurate long-term predictions. Numerical experiments show the method performs well in accuracy, robustness, and adaptability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Mechanical Strength Prediction of Steel-Polypropylene Fiber-based High-Performance Concrete Using Hybrid Machine Learning Algorithms</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [materials informatics], [hybrid machine learning, SHAP analysis, uncertainty quantification, strength prediction, high-performance concrete]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jagaran Chakma, Zhiguang Zhou, Badhan Chakma</p>
</li>
<li class="">
<p><strong>institution:</strong> Tongji University, Chongqing Jiaotong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21638" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21638</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed and evaluated three novel hybrid machine learning model families (ET-XGB, RF-LGBM, Transformer-XGB) for predicting the mechanical properties of fiber-reinforced concrete. 2. Conducted a comprehensive evaluation including k-fold cross-validation, hyperparameter optimization, SHAP analysis for interpretability, and uncertainty quantification for robustness assessment. 3. Identified key influential material parameters (fiber aspect ratios, silica fume, steel fiber content) and negative factors (water content, water-binder ratio) on concrete strength through SHAP analysis.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76eefc8193c0d98df6f997d7ae3134d34a448f8aba255f28a39044233c1f3bb8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76eefc8193c0d98df6f997d7ae3134d34a448f8aba255f28a39044233c1f3bb8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This research develops hybrid machine learning models to predict the compressive, flexural, and tensile strength of steel-polypropylene fiber-reinforced high-performance concrete. The study compares three hybrid models (ET-XGB, RF-LGBM, Transformer-XGB) and finds that the ET-XGB model achieved the highest overall accuracy, while SHAP analysis reveals the most influential material factors. The findings confirm that these ML models provide accurate and interpretable tools for optimizing concrete mix design in engineering applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Monte Carlo Tree Search, Upper Confidence Bound, Variance-Aware, Prior-Based Tree Policy, Inverse-RPO]</p>
</li>
<li class="">
<p><strong>authors:</strong> Maximilian Weichart</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Regensburg</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21648" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21648</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Max-We/inverse-rpo" target="_blank" rel="noopener noreferrer" class="">https://github.com/Max-We/inverse-rpo</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Inverse-RPO, a general methodology to systematically derive prior-based UCTs from any prior-free UCB., 2. Applies Inverse-RPO to UCB-V to create two new variance-aware prior-based tree policies., 3. Provides an extension to the mctx library for variance-aware UCTs, showing minimal code changes and improved performance over PUCT in benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c9098504a8a9013ab805fffa4a23f04af76e28f5d8b8a0353e1e7d5583f589_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c9098504a8a9013ab805fffa4a23f04af76e28f5d8b8a0353e1e7d5583f589_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of extending prior-based tree policies in Monte Carlo Tree Search beyond the empirically derived PUCT. The authors propose Inverse-RPO, a principled method to derive prior-based UCTs from any prior-free UCB, and apply it to create variance-aware policies. Their new policies outperform the standard PUCT across multiple benchmarks without added computational cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Causal-HM: Restoring Physical Generative Logic in Multimodal Anomaly Detection via Hierarchical Modulation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [anomaly detection], [multimodal fusion, causal modeling, hierarchical modulation, sensor guidance, unsupervised learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiao Liu, Junchen Jin, Yanjie Zhao, Zhixuan Xing</p>
</li>
<li class="">
<p><strong>institution:</strong> Chongqing University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21650" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21650</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified multimodal UAD framework (Causal-HM) that explicitly models the physical Process→Result dependency to address causal blindness. 2. Introduces a Sensor-Guided CHM Modulation mechanism that uses low-dimensional sensor signals as context to guide high-dimensional audio-visual feature extraction. 3. Designs a Causal-Hierarchical Architecture that enforces a unidirectional generative mapping to identify anomalies violating physical consistency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f0b7760ac76038dd08b0ccd2890cb2628906dcf233282246d08ee584093193_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f0b7760ac76038dd08b0ccd2890cb2628906dcf233282246d08ee584093193_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of causal blindness and the heterogeneity gap in multimodal anomaly detection for industrial processes like welding. It proposes the Causal-HM framework, which models the physical generative logic from process to result modalities using sensor-guided modulation and a causal-hierarchical architecture. The method achieves state-of-the-art performance on a new Weld-4M benchmark, demonstrating its effectiveness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Semantic Codebooks as Effective Priors for Neural Speech Compression</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [speech compression], [semantic codebooks, residual vector quantization (RVQ), HuBERT, FiLM-conditioned decoder, neural audio codec]</p>
</li>
<li class="">
<p><strong>authors:</strong> Liuyang Bai, Weiyi Lu, Li Guo</p>
</li>
<li class="">
<p><strong>institution:</strong> NYU Shanghai</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21653" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21653</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SemDAC, a semantic-aware neural audio codec that uses semantic codebooks as priors for compression., 2. Introduces a design where the first RVQ quantizer is distilled from HuBERT to capture phonetic content, and a FiLM-conditioned decoder uses these semantic tokens., 3. Demonstrates superior performance over baseline DAC in perceptual metrics and ASR (Whisper) WER at significantly lower bitrates.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a14c953c6c23139c4473b8d6e59b36c7615f79f4316119e168deb19de30eced_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a14c953c6c23139c4473b8d6e59b36c7615f79f4316119e168deb19de30eced_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes SemDAC, a neural speech codec that uses semantic codebooks distilled from HuBERT as priors within an RVQ framework to separate phonetic from acoustic information. This method achieves better perceptual quality and lower word error rates for speech recognition at much lower bitrates compared to traditional neural codecs. The results show that semantic priors provide an effective inductive bias for efficient, recognition-friendly speech compression.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Rethinking Output Alignment For 1-bit Post-Training Quantization of Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [1-bit quantization, post-training quantization, output alignment, activation error, large language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dung Anh Hoang, Cuong Pham, Cuong Nguyen, Trung le, Jianfei Cai, Thanh-Toan Do</p>
</li>
<li class="">
<p><strong>institution:</strong> Monash University, University of Surrey</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21651" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21651</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Investigates the failure conditions of naive output-matching in 1-bit LLM quantization, 2. Proposes a novel data-aware PTQ approach that explicitly accounts for activation error accumulation, 3. Demonstrates consistent performance improvements over existing 1-bit PTQ methods with minimal overhead</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0357717d29d733787933b581fbbb292b187b9fbc651fbc0f15bb04ef03e619eb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0357717d29d733787933b581fbbb292b187b9fbc651fbc0f15bb04ef03e619eb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the significant performance degradation in 1-bit post-training quantization (PTQ) of Large Language Models. The authors propose a new data-aware PTQ method that efficiently accounts for activation error accumulation. Their solution is shown to consistently outperform existing 1-bit PTQ approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [deepfake detection], [mechanistic interpretability, sparse autoencoder, forensic manifold analysis, feature selectivity, vision-language model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Subramanyam Sahoo, Jared Junkin</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Berkeley, Johns Hopkins University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21670" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21670</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/SubramanyamSahoo/The-Deepfake-Detective" target="_blank" rel="noopener noreferrer" class="">https://github.com/SubramanyamSahoo/The-Deepfake-Detective</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a novel mechanistic interpretability framework specifically for deepfake detection, combining sparse autoencoder analysis with forensic manifold analysis. 2. Demonstrates that only a small fraction of latent features are actively used in each layer of the model for detection. 3. Shows that geometric properties of the model&#x27;s feature manifold (e.g., intrinsic dimensionality, curvature) vary systematically with different types of deepfake artifacts, linking learned features to specific forensic cues.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the &quot;black box&quot; nature of deepfake detectors by proposing a mechanistic interpretability framework that analyzes a vision-language model&#x27;s internal representations. The method uses sparse autoencoders and a novel forensic manifold analysis to probe how the model&#x27;s features respond to forensic artifacts. The key findings are that detection relies on a sparse set of features and that the geometry of the feature manifold correlates with specific artifact types, providing a step towards more interpretable and robust detectors.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [YOLO-NAS, YOLOv8, perception, autonomous vehicles, custom dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jalal Khan</p>
</li>
<li class="">
<p><strong>institution:</strong> United Arab Emirates University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21673" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21673</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Dictionary-Transform Generative Adversarial Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative adversarial networks], [dictionary learning, transform learning, sparse modeling, adversarial learning, nash equilibrium]</p>
</li>
<li class="">
<p><strong>authors:</strong> Angshul Majumdar</p>
</li>
<li class="">
<p><strong>institution:</strong> Indraprastha Institute of Information Technology Delhi (IIIT-D)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21677" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21677</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces DT-GAN, a fully model-based adversarial framework with a sparse synthesis dictionary generator and an analysis transform discriminator, enabling rigorous theoretical analysis. 2. Proves the adversarial game is well-posed, admits at least one Nash equilibrium, and that equilibrium solutions are identifiable under a sparse generative model. 3. Establishes finite-sample stability and consistency of empirical equilibria, demonstrating reliable convergence and robustness, validated on synthetic data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e368966cab749cb3ef0799abfb9a31d801a73291e7a5d2b4a5b81fd185a9d25_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e368966cab749cb3ef0799abfb9a31d801a73291e7a5d2b4a5b81fd185a9d25_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the theoretical fragility and instability of classical GANs by proposing DT-GAN, a model-based adversarial framework where the generator and discriminator are constrained linear operators (a sparse synthesis dictionary and an analysis transform). The authors prove the framework is well-posed, has identifiable equilibria, and converges reliably, demonstrating that adversarial learning can be made interpretable and provably correct for data with sparse structure.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [spatiotemporal forecasting], [probabilistic forecasting, uncertainty estimation, principal component analysis, road impedance, traffic flow]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haochen Lv, Yan Lin, Shengnan Guo, Xiaowei Mao, Hong Nie, Letian Gong, Youfang Lin, Huaiyu Wan</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Jiaotong University, Aalborg University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21685" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21685</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a dynamic impedance evolution network to model directional traffic transfer patterns driven by congestion and flow variability, revealing causes of uncertainty and enhancing reliability and interpretability. 2. Designs a principal component network to forecast the dominant eigenvectors of future flow covariance, enabling the capture of spatiotemporal uncertainty correlations. 3. Integrates domain-specific transportation theory with spatiotemporal principal component learning for probabilistic traffic flow forecasting, achieving superior performance over existing methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f835b2051524d67481fbf9f4479086a541b29307c2876046f2c3ee4eec60f92_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f835b2051524d67481fbf9f4479086a541b29307c2876046f2c3ee4eec60f92_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes RIPCN, a Road Impedance Principal Component Network for probabilistic traffic flow forecasting. It integrates transportation theory with principal component learning to model the causes of uncertainty and capture spatiotemporal uncertainty correlations. Experimental results show it outperforms existing probabilistic forecasting methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [communication &amp; networking], [multiconnectivity, SAGIN, resource allocation, agentic reinforcement learning, heterogeneous networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abd Ullah Khan, Adnan Shahid, Haejoon Jung, Hyundong Shin</p>
</li>
<li class="">
<p><strong>institution:</strong> Kyung Hee University, Ghent University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21717" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21717</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a comprehensive review of current developments and key implementation challenges in SAGIN-enabled multiconnectivity. 2. Highlights the transformative potential of AI-driven approaches, particularly agentic reinforcement learning, for resource optimization in heterogeneous SAGIN environments. 3. Presents a case study demonstrating that learning-based methods can effectively enhance network performance (latency, capacity) with a moderate trade-off in power consumption.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31a130dffac74abb8ad0616817d555bf857333050b690554853596eda30c2fa7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31a130dffac74abb8ad0616817d555bf857333050b690554853596eda30c2fa7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper reviews the challenges of implementing multiconnectivity in heterogeneous Space-Air-Ground Integrated Networks (SAGIN) and proposes AI-driven solutions, specifically agentic reinforcement learning, for optimal resource allocation. A case study shows these methods significantly improve latency and capacity, albeit with a moderate increase in power consumption as a trade-off. The work concludes by outlining open research problems for realizing efficient SAGIN-enabled multiconnectivity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] An Information Theoretic Perspective on Agentic System Design</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [mutual information, noisy channel, compressor-predictor, on-device AI, information-theoretic]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shizhe He, Avanika Narayan, Ishan S. Khare, Scott W. Linderman, Christopher Ré, Dan Biderman</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21720" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21720</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an information-theoretic framework for analyzing agentic LM systems, viewing the compressor as a noisy channel. 2. Introduces a task-independent estimator of mutual information between context and compression to quantify compression quality. 3. Empirically demonstrates that scaling compressor models is more effective than scaling predictors for performance and cost, enabling efficient on-device compression.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124535642e159e8f7a123525ffbf3cb5f163a7ae4a7876a4d1e71e7e6c885ace_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124535642e159e8f7a123525ffbf3cb5f163a7ae4a7876a4d1e71e7e6c885ace_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the ad-hoc design of agentic LM systems that use a compressor LM to summarize context for a predictor LM. It proposes an information-theoretic framework using mutual information to evaluate compressors, finding that larger compressors are more accurate, concise, and information-dense, making scaling compressors more effective than scaling predictors for cost-efficient performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] HELP: Hierarchical Embodied Language Planner for Household Tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [embodied agent, hierarchical planning, large language model, household tasks, open source LLM]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alexandr V. Korchemnyi, Anatoly O. Onishchenko, Eva A. Bakaeva, Alexey K. Kovalev, Aleksandr I. Panov</p>
</li>
<li class="">
<p><strong>institution:</strong> MIRAI, Cognitive AI Systems Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21723" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21723</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Hierarchical Embodied Language Planner (HELP) architecture using multiple LLM-based agents for decomposing and grounding natural language instructions. 2. Demonstrates the approach on a real-world household task using an embodied agent. 3. Focuses on the use of relatively small, open-source LLMs to enable autonomous deployment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of planning for embodied agents following ambiguous natural language instructions in complex environments. It proposes HELP, a hierarchical planner using multiple LLM-based agents to decompose high-level instructions into grounded, executable subtasks. The method is evaluated on a household task with a real robot, showing the feasibility of using smaller, open-source LLMs for autonomous operation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Model of Causal Explanation on Neural Networks for Tabular Data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [explainable ai], [CENNET, structural causal models, entropy, causal explanation, tabular data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Takashi Isozaki, Masahiro Yamamoto, Atsushi Noda</p>
</li>
<li class="">
<p><strong>institution:</strong> Sony Computer Science Laboratories, Inc., Sony Corporation of America</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21746" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21746</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CENNET, a novel causal explanation method for neural network predictions on tabular data. 2. Introduces a new explanation power index based on entropy for evaluating the proposed method. 3. Demonstrates the method&#x27;s effectiveness by combining structural causal models with neural networks for causal explanations, validated on synthetic and quasi-real data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02e8ba8968728eba560984773ed8ba2b124e42c46e3c839dec8a1ca2a5975ce1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02e8ba8968728eba560984773ed8ba2b124e42c46e3c839dec8a1ca2a5975ce1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of providing causal explanations for neural network predictions on tabular data, where pseudo-correlations can mislead. The authors propose a method called CENNET, which integrates structural causal models with neural networks to generate causal explanations and introduces an entropy-based index to measure explanation power. Experiments on synthetic and quasi-real data show that CENNET effectively provides causal explanations compared to existing methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [continual learning], [entropy scaling, catastrophic forgetting, stability-plasticity dilemma, dynamic feedback, layer-wise control]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hengyi Wu, Zhenyi Wang, Heng Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Maryland, College Park, University of Central Florida</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21743" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21743</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel entropy-aware continual learning method that uses a dynamic feedback mechanism to regulate each layer based on its entropy, addressing underfitting and overfitting. 2. Introduces Self-Adaptive Entropy Scaling, which adaptively adjusts regularization strength per layer to encourage convergence to wider local minima for better generalization. 3. Presents a complementary adaptive training mechanism that modulates layer plasticity based on performance, preserving knowledge in high-performing layers while amplifying updates in underperforming ones.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses catastrophic forgetting in continual learning by proposing a dynamic feedback mechanism that regulates each neural network layer based on its entropy. This entropy-aware method reduces entropy in high-entropy layers to prevent underfitting and increases it in low-entropy layers to prevent overfitting, promoting convergence to wider minima for improved generalization. The approach is general and integrates with existing replay- and regularization-based methods, showing substantial performance gains over state-of-the-art baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Approximation Capabilities of Feedforward Neural Networks with GELU Activations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [neural network approximation theory], [GELU activation, feedforward neural networks, approximation error bounds, derivative approximation, constructive approximation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Konstantin Yakovlev, Nikita Puchkin</p>
</li>
<li class="">
<p><strong>institution:</strong> HSE University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21749" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21749</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Derivation of simultaneous approximation error bounds for a function and all its derivatives up to any prescribed order using GELU networks. 2. Constructive approximation guarantees for elementary operations (multiplication, division, exponential) with control over network size, weight magnitudes, and behavior at infinity. 3. Extension of approximation results to multivariate polynomials and other elementary functions, ensuring global boundedness of higher-order derivatives of the approximators.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af63acd2ab9f3e42763901f23d1762f6658fd25b72daf9e1f1f73e7b30ce1173_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af63acd2ab9f3e42763901f23d1762f6658fd25b72daf9e1f1f73e7b30ce1173_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes the approximation capabilities of feedforward neural networks using the GELU activation function. The authors provide constructive methods to approximate elementary functions and their derivatives, proving simultaneous error bounds for the function and all its higher-order derivatives. The main conclusion is that GELU networks can effectively approximate key operations like multiplication, division, and exponentiation with controlled network complexity and globally bounded derivatives.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Assessing the Effectiveness of Membership Inference on Generative Music</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [membership inference attacks], [membership inference attack (MIA), generative music, MuseGAN, privacy, copyright]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kurtis Chow, Omar Samiullah, Vinesh Sridhar, Hewen Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Irvine</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21762" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21762</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducts the first preliminary study on the effectiveness of membership inference attacks (MIAs) specifically on generative music models., 2. Evaluates several existing MIA techniques on the popular MuseGAN model to assess their performance in this domain., 3. Provides empirical evidence suggesting that generative music data is relatively resilient to known membership inference techniques, aligning with prior findings in generative audio.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09b7176fccbb69c4f0a15bfa6bcbb6ff59b09cf6ac02e0f524334f306ce4041f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09b7176fccbb69c4f0a15bfa6bcbb6ff59b09cf6ac02e0f524334f306ce4041f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether membership inference attacks (MIAs) are effective against generative music models. The authors conduct a preliminary study by applying several existing MIA techniques to the MuseGAN model. Their findings indicate that generative music data is fairly resilient to these known attacks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3d medical image analysis], [Masked Autoencoder, Swin Transformer, Self-Supervised Learning, 3D Vision Transformer, Structural Priority Loss]</p>
</li>
<li class="">
<p><strong>authors:</strong> Evgeny Alves Limarenko, Anastasiia Studenikina</p>
</li>
<li class="">
<p><strong>institution:</strong> Moscow Institute of Physics and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21769" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21769</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed BertsWin, a hybrid architecture combining full BERT-style token masking with Swin Transformer windows to preserve 3D spatial topology during SSL pre-training. 2. Introduced a structural priority loss function to enhance learning. 3. Demonstrated significant acceleration in semantic convergence (5.8x) and a 15-fold reduction in training epochs to reach SOTA fidelity when combined with the GradientConductor optimizer, without increasing computational FLOPs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the difficulty of applying standard Masked Autoencoders to 3D medical images, which lose spatial context. It proposes BertsWin, a hybrid architecture that maintains a full 3D token grid using Swin Transformer windows and a structural loss. The method achieves much faster convergence and state-of-the-art reconstruction fidelity for 3D CBCT scans without extra computational cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Accelerating Scientific Discovery with Autonomous Goal-evolving Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scientific discovery agents], [autonomous goal evolution, bi-level optimization, LLM agents, objective function design, scientific discovery]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuanqi Du, Botao Yu, Tianyu Liu, Tony Shen, Junwu Chen, Jan G. Rittig, Kunyang Sun, Yikun Zhang, Zhangde Song, Bo Zhou, Cassandra Masschelein, Yingze Wang, Haorui Wang, Haojun Jia, Chao Zhang, Hongyu Zhao, Martin Ester, Teresa Head-Gordon, Carla P. Gomes, Huan Sun, Chenru Duan, Philippe Schwaller, Wengong Jin</p>
</li>
<li class="">
<p><strong>institution:</strong> Cornell University, The Ohio State University, Yale University, Simon Fraser University, École Polytechnique Fédérale de Lausanne, University of California Berkeley, Northeastern University, Deep Principle, University of Illinois Chicago, Georgia Institute of Technology, Broad Institute of MIT and Harvard</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21782" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21782</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and addresses the unmet requirement of automating objective function design for scientific discovery agents, moving beyond fixed, imperfect proxies. 2. Proposes the SAGA framework, a novel bi-level architecture where an outer loop of LLM agents evolves objectives and an inner loop optimizes solutions under them. 3. Demonstrates the framework&#x27;s effectiveness across diverse scientific domains (antibiotic, materials, DNA, chemical process design), showing improved discovery outcomes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9035dcf8fd16c34c235e39c8960c63fa826c45df283663a01722181f7ab419d8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9035dcf8fd16c34c235e39c8960c63fa826c45df283663a01722181f7ab419d8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces SAGA, a framework for scientific discovery where LLM agents autonomously evolve and refine the objective functions used to guide optimization, rather than relying on fixed human-specified goals. This bi-level architecture enables systematic exploration of objective spaces and their trade-offs. The method is shown to substantially improve the effectiveness of discovery agents across multiple application domains.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Synthetic Financial Data Generation for Enhanced Financial Modelling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [synthetic data generation], [synthetic financial data, TimeGAN, ARIMA-GARCH, VAE, Maximum Mean Discrepancy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Christophe D. Hounwanou, Yae Ulrich Gaba, Pierre Ntakirutimana</p>
</li>
<li class="">
<p><strong>institution:</strong> AIMS Rwanda, Carnegie Mellon University, Sefako Makgatho Health Sciences University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21791" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21791</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified multi-criteria evaluation framework for synthetic financial data. 2. Empirically compares three generative paradigms (ARIMA-GARCH, VAE, TimeGAN) on fidelity, temporal structure, and downstream utility. 3. Provides practical guidelines for model selection and releases a reproducible codebase to standardize benchmarking.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3f9e4bbcf426a2c32c05089e0964e9937c0becca521cac8a83b48c652d0d86c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3f9e4bbcf426a2c32c05089e0964e9937c0becca521cac8a83b48c652d0d86c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses data scarcity in finance by proposing a unified evaluation framework for synthetic financial data generation. It compares ARIMA-GARCH, VAE, and TimeGAN models using S&amp;P 500 data, finding that TimeGAN offers the best trade-off between realism and temporal coherence. The work concludes with practical guidelines and aims to standardize benchmarking in the field.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Multi-agent Adaptive Mechanism Design</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [mechanism design], [distributionally robust optimization, online learning, incentive compatibility, adaptive mechanism, regret analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qiushi Han, David Simchi-Levi, Renfei Tan, Zishuo Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Massachusetts Institute of Technology, University of Illinois Urbana-Champaign</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21794" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21794</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces DRAM, a novel framework combining mechanism design and online learning to handle unknown agent beliefs. 2. Provides theoretical guarantees of high-probability truthfulness and achieves optimal <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mo stretchy="false">{</mo><mo>~</mo></mover><mi>O</mi><mo stretchy="false">}</mo><mo stretchy="false">(</mo><msqrt><mo stretchy="false">{</mo></msqrt><mi>T</mi><mo stretchy="false">}</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tilde\{O\}(\sqrt\{T\})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2919em;vertical-align:-0.305em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9869em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mopen">{</span></span><span style="top:-3.669em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">~</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.25em"><span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mclose">}</span><span class="mopen">(</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.935em"><span class="svg-align" style="top:-3.2em"><span class="pstrut" style="height:3.2em"></span><span class="mopen" style="padding-left:1em">{</span></span><span style="top:-2.895em"><span class="pstrut" style="height:3.2em"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.28em" viewBox="0 0 400000 1296" preserveAspectRatio="xMinYMin slice"><path d="M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.305em"><span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mclose">})</span></span></span></span> cumulative regret with a matching lower bound. 3. Generalizes the framework (DRAM+) to support plug-in estimators, structured priors, and delayed feedback.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ece7d8b60855735e1eee48c51256eacf5f3997d56ced3396434f12a30ad44_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ece7d8b60855735e1eee48c51256eacf5f3997d56ced3396434f12a30ad44_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of designing a truthful mechanism when the principal has no prior knowledge of agents&#x27; beliefs. It proposes the Distributionally Robust Adaptive Mechanism (DRAM), which iteratively learns beliefs and updates a robust optimization problem to minimize cost while ensuring truthfulness. The mechanism is proven to achieve optimal regret, and the framework is the first to maintain truthfulness under these general learning conditions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] VAMP-Net: An Interpretable Multi-Path Framework of Genomic Permutation-Invariant Set Attention and Quality-Aware 1D-CNN for MTB Drug Resistance</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [bioinformatics], [Set Attention Transformer, 1D-CNN, Multi-Path Network, Interpretable Machine Learning, Genomic Variant Analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aicha Boutorh, Kamar Hibatallah Baghdadi, Anais Daoud</p>
</li>
<li class="">
<p><strong>institution:</strong> National School of Artificial Intelligence (ENSIA)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21786" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21786</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel multi-path deep learning architecture (VAMP-Net) that combines a permutation-invariant Set Attention Transformer for capturing epistatic interactions and a 1D-CNN for analyzing sequencing quality metrics. 2. Introduces a comparative evaluation of unmasked vs. padding-masked Set Attention Blocks within the architecture. 3. Provides dual-layer interpretability through attention weight analysis for epistatic networks and gradient-based methods for feature importance on both genetic variants and quality metrics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf587b5249dfee9f3a9866a887e3d791367588c15a447d0c86619ea9c8df8a45_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf587b5249dfee9f3a9866a887e3d791367588c15a447d0c86619ea9c8df8a45_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces VAMP-Net, a multi-path deep learning framework designed to predict drug resistance in Mycobacterium tuberculosis. It combines a Set Attention Transformer to model genetic interactions and a 1D-CNN to assess data quality, achieving high accuracy and AUC. The work concludes that this approach offers superior predictive performance and dual-layer interpretability for clinical genomics.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [cluster infrastructure], [LSTM, Random Forest, MQTT, InfluxDB, Streamlit]</p>
</li>
<li class="">
<p><strong>authors:</strong> Krishna Chaitanya Sunkara, Rambabu Konakanchi</p>
</li>
<li class="">
<p><strong>institution:</strong> Oracle, Charles Schwab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21801" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21801</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Probabilistic LSTM forecasting validated within ±30-minute windows for coolant leaks, 2. 96.5% F1-score Random Forest detection for immediate leak identification, 3. Integrated smart IoT architecture design with MQTT streaming, InfluxDB storage, and Streamlit dashboards for energy-efficient data center operations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa113d59b2dc6ec7a3bf00f9eebc8541689a6235867cf59ce376cdcfa30a2a5c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa113d59b2dc6ec7a3bf00f9eebc8541689a6235867cf59ce376cdcfa30a2a5c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes a smart IoT monitoring system combining LSTM neural networks for probabilistic leak forecasting and Random Forest classifiers for instant detection in liquid-cooled AI data centers. The system, tested on synthetic data, achieves 96.5% detection accuracy and 87% forecasting accuracy, potentially preventing significant energy waste through proactive maintenance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [adversarial attacks], [entropy-guided attacks, vision-language models, adversarial robustness, harmful content generation, transferability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mengqi He, Xinyu Tian, Xin Shen, Jinhong Ni, Shu Zou, Zhaoyuan Yang, Jing Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Australian National University, The University of Queensland, GE Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21815" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21815</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies that only a small fraction (~20%) of high-entropy tokens (critical decision points) disproportionately govern output trajectories in autoregressive VLMs, challenging the prior assumption of equal token importance. 2. Proposes a selective attack strategy that concentrates adversarial perturbations on these high-entropy positions, achieving strong semantic degradation with a smaller perturbation budget and inducing 35-49% of benign outputs to become harmful. 3. Demonstrates that vulnerable high-entropy forks recur across diverse VLMs, enabling feasible transferable attacks (17-26% harmful rates), and proposes the Entropy-bank Guided Adversarial attack (EGA) method which achieves high attack success rates (93-95%) while exposing new safety weaknesses.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper reveals that adversarial attacks on Vision-Language Models (VLMs) can be made more efficient and dangerous by targeting only the critical high-entropy tokens during autoregressive generation, rather than all tokens. The proposed Entropy-bank Guided Adversarial attack (EGA) concentrates perturbations on these positions, achieving high attack success and converting many benign outputs into harmful ones, while also showing significant transferability across models. This exposes a critical and previously overlooked vulnerability in current VLM safety mechanisms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Scalable Class-Incremental Learning Based on Parametric Neural Collapse</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [class-incremental learning], [parametric neural collapse, equiangular tight frame, knowledge distillation, adaptive expansion, feature alignment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chuangxin Zhang, Guangfeng Lin, Enhui Zhao, Kaiyang Liao, Yajun Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in the provided content. Affiliation information is not included.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21845" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21845</a></p>
</li>
<li class="">
<p><strong>code:</strong> this https URLETF2</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a scalable class-incremental learning method (SCL-PNC) based on parametric neural collapse, enabling demand-driven, minimal-cost backbone expansion via an adapt-layer. 2. Introduces a dynamic parametric Equiangular Tight Frame (ETF) classifier to address class misalignment caused by evolving class distributions. 3. Presents a parallel expansion framework with a knowledge distillation algorithm to counteract feature drift and ensure feature consistency across expansion modules.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses challenges in class-incremental learning, such as overfitting and catastrophic forgetting, by proposing SCL-PNC. This method uses a parametric neural collapse framework with an adaptive backbone expansion and a dynamic ETF classifier to efficiently handle growing categories while maintaining feature consistency via distillation. Experiments show the method is effective and efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Comedy of Estimators: On KL Regularization in RL Training of LLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [KL divergence, policy gradient, on-policy sampling, off-policy training, gradient bias]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro, Yoshua Bengio, Nikolay Malkin, Moksh Jain, Siddarth Venkatraman, Aaron Courville</p>
</li>
<li class="">
<p><strong>institution:</strong> Mila – Quebec AI Institute, Université de Montréal, McGill University, LLNL, University of Edinburgh, CIFAR</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21852" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21852</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Systematic analysis of KL divergence estimator configurations in RL for LLMs, revealing how design choices introduce gradient bias. 2. Empirical demonstration that estimator configurations with unbiased gradients lead to better and more stable performance on both in-domain and out-of-domain tasks. 3. Investigation showing KL regularization can stabilize off-policy RL training in asynchronous setups.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96999f081bc421143202d98f560b5d13a6fb0c09613b8c160b121158bce3811a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96999f081bc421143202d98f560b5d13a6fb0c09613b8c160b121158bce3811a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes the use of various estimators for the KL divergence regularization term in RL fine-tuning of LLMs, finding that common practices introduce biased gradients. Through experiments on models like Qwen2.5-7B, the study shows that using estimator configurations with unbiased gradients improves training stability and downstream task performance. The work also finds that KL regularization helps stabilize off-policy RL training.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [feature-level fusion, convolutional neural networks, diabetic retinopathy screening, EfficientNet, DenseNet]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan</p>
</li>
<li class="">
<p><strong>institution:</strong> North South University, Korea Institute of Oriental Medicine, American International University–Bangladesh</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21861" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21861</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [Privacy-preserving machine learning], [dataset distillation, random forest, synthetic data generation, explainable AI, membership-inference attack]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yiming Qian, Thorsten Neumann, Xueyining Huang, David Hardoon, Fei Gao, Yong Liu, Siow Mong Rick Goh</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of High Performance Computing (A*STAR), Standard Chartered Bank, Xi’an Jiaotong–Liverpool University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21866" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21866</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a privacy-preserving dataset distillation framework that converts a trained random forest into transparent rule regions and generates synthetic data by uniform sampling within these regions, creating a compact, auditable surrogate dataset. 2. Enables explainable AI by providing both global pattern summaries (e.g., support, lift) from aggregated rules and local, human-readable rationales with calibrated uncertainty for individual cases based on tree-vote disagreement. 3. Demonstrates strong utility-privacy trade-offs, showing the distilled data maintains competitive model performance (e.g., precision, F1) with significant data reduction (85-93%), improves cross-institution learning, and resists membership-inference attacks (AUC ~0.5), indicating low memorization risk.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6994d363f1e76d7cc78ab02d48685c11199b353aab61b3eb5297064b1df9b722_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6994d363f1e76d7cc78ab02d48685c11199b353aab61b3eb5297064b1df9b722_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a dataset distillation method for financial fraud detection that converts a random forest model into interpretable rule regions to generate synthetic data, preserving privacy and model utility. The approach produces a compact, explainable dataset that supports collaborative learning across institutions while resisting privacy attacks. Experiments show it reduces data volume by over 85% with minimal performance loss and enhances cross-cluster fraud detection.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] MMCTOP: A Multimodal Textualization and Mixture-of-Experts Framework for Clinical Trial Outcome Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [multimodal fusion, sparse mixture-of-experts, schema-guided textualization, clinical trial prediction, temperature scaling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Carolina Aparício, Qi Shi, Bo Wen, Tesfaye Yadete, Qiwei Han</p>
</li>
<li class="">
<p><strong>institution:</strong> Nova School of Business and Economics, Hogarthian Technologies, IBM Research, Cleveland Clinic, Oregon Health &amp; Science University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21897" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21897</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A multimodal framework (MMCTOP) that integrates molecular structures, protocol metadata, eligibility narratives, and disease ontologies for clinical trial outcome prediction. 2. A novel architecture combining schema-guided textualization for data normalization and a drug-disease-conditioned sparse Mixture-of-Experts (SMoE) for context-aware, specialized multimodal fusion. 3. Demonstrates improved prediction performance (precision, F1, AUC) over baselines and incorporates operational safeguards like temperature scaling for calibrated probabilities to enhance auditability and reproducibility.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb552211ea905d5cbf8e190ead9078caf65c5d200327a02c7a6dab156a030645_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb552211ea905d5cbf8e190ead9078caf65c5d200327a02c7a6dab156a030645_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes MMCTOP, a multimodal framework for predicting clinical trial outcomes. It uses schema-guided textualization to normalize heterogeneous data and a sparse Mixture-of-Experts model for specialized fusion, achieving better performance than existing methods and providing calibrated risk estimates.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] GQ-VAE: A gated quantized VAE for learning variable length tokens</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [tokenization], [GQ-VAE, variable-length tokens, VQ-VAE, neural tokenizer, byte-pair encoding]</p>
</li>
<li class="">
<p><strong>authors:</strong> Theo Datta, Kayla Huang, Sham Kakade, David Brandfonbrener</p>
</li>
<li class="">
<p><strong>institution:</strong> Kempner Institute, Harvard University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21913" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21913</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Theo-Datta-115/gq-vae" target="_blank" rel="noopener noreferrer" class="">https://github.com/Theo-Datta-115/gq-vae</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GQ-VAE, a novel gated quantized variational autoencoder architecture for learning discrete, variable-length tokens. 2. Demonstrates the model can serve as a standalone, pre-trained drop-in replacement for existing tokenizers, improving over fixed-chunk baselines. 3. Shows that with equivalent compression, GQ-VAE improves downstream language model learning compared to BPE.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49412452efda1a18023f64f968f3406b217d747381b47a09694b7d37c61c918a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49412452efda1a18023f64f968f3406b217d747381b47a09694b7d37c61c918a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the limitations of traditional tokenizers like BPE and complex neural tokenizers by proposing GQ-VAE, a novel architecture that learns variable-length discrete tokens. GQ-VAE can be independently pre-trained as a drop-in replacement, approaching BPE&#x27;s performance and, under equivalent compression, improving downstream language model learning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [Tabular Data Generation, Large Language Models, Multi-Arm Bandit, Data Diversity, In-context Learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yafeng Tang, Xiaoou Ding, Jianzhuo Du, Zishuo Yan, Zhuang Ma, Zheng Liang, Zekai Qian, Hongzhi Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Harbin Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21915" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21915</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/windblow32/DATE" target="_blank" rel="noopener noreferrer" class="">https://github.com/windblow32/DATE</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces DATE, a framework that partitions heterogeneous tabular data into diverse subsets to prepare high-quality examples for in-context learning. 2. Proves the selection problem in heterogeneous data generation lacks the greedy-choice property and designs a Multi-Arm Bandit-based sampling algorithm to balance diversity and quality. 3. Demonstrates that DATE-generated data improves downstream tasks like Direct Preference Optimization (DPO) and enhances LLM reasoning on target data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d8fa8039f8a5fa0717fc5e4a9c7ba2cf1fd158e44821059f4a767bc88790eaf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d8fa8039f8a5fa0717fc5e4a9c7ba2cf1fd158e44821059f4a767bc88790eaf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of generating high-quality synthetic tabular data from heterogeneous distributions. It proposes DATE, a framework that uses LLMs with decision tree feedback for subset-specific generation and a Multi-Arm Bandit algorithm for data selection. Experiments show DATE outperforms existing methods, reducing error rates and improving downstream model performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning from human feedback (rlhf)], [preference optimization, single-index model, semiparametric, link function, policy learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nathan Kallus</p>
</li>
<li class="">
<p><strong>institution:</strong> Netflix, Cornell University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21917" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21917</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulates policy alignment as a semiparametric single-index model problem, relaxing the need for a known link function between preferences and rewards. 2. Develops novel policy learners based on profiling, orthogonalizing, and link-agnostic ranking objectives, providing theoretical error bounds. 3. Proposes practical first-order optimization implementations that are robust to unknown preference noise and scale, enabling direct policy optimization without explicit reward fitting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/378875cc0ef0eb142c184d960e479724ea3df83c84cf81e185efea5469a4387e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/378875cc0ef0eb142c184d960e479724ea3df83c84cf81e185efea5469a4387e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of bias in aligning language models when the assumed link function between human preferences and latent rewards is misspecified. It proposes a semiparametric framework that treats the link function as unknown, develops several robust policy learning algorithms, and provides theoretical guarantees. The main conclusion is that this approach enables more reliable policy alignment without needing to correctly specify the preference noise distribution.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] AutoPP: Towards Automated Product Poster Generation and Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image generation], [product poster generation, click-through rate optimization, isolated direct preference optimization, AutoPP1M dataset, unified design module]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiahao Fan, Yuxin Qin, Wei Feng, Yanyin Chen, Yaoyu Li, Ao Ma, Yixiu Li, Li Zhuang, Haoyi Bian, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law</p>
</li>
<li class="">
<p><strong>institution:</strong> JD.COM</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21921" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21921</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/JD-GenX/AutoPP" target="_blank" rel="noopener noreferrer" class="">https://github.com/JD-GenX/AutoPP</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An automated pipeline (AutoPP) for end-to-end product poster generation and optimization, requiring only basic product information as input. 2. A novel optimization method that uses systematic element replacement and Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to specific poster elements. 3. The creation and release of AutoPP1M, the largest dataset for product poster generation and optimization, containing one million posters and user feedback.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces AutoPP, an automated pipeline that generates product posters from basic product information and then optimizes them for higher Click-Through Rate (CTR) using online feedback and a novel Isolated Direct Preference Optimization technique. It is supported by a large-scale dataset, AutoPP1M. Experiments show that AutoPP achieves state-of-the-art performance in both offline and online evaluations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Data relativistic uncertainty framework for low-illumination anime scenery image enhancement</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [low-light image enhancement], [Data Relativistic Uncertainty, Unsupervised Learning, EnlightenGAN, Anime Scenery, Domain Gap]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yiquan Gao, John See</p>
</li>
<li class="">
<p><strong>institution:</strong> Heriot-Watt University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21944" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21944</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Hybrid Combinatorial Multi-armed Bandits with Probabilistically Triggered Arms</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-armed bandits], [combinatorial multi-armed bandits, probabilistically triggered arms, hybrid learning, offline data, online interaction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kongchang Zhou, Tingyu Zhang, Wei Chen, Fang Kong</p>
</li>
<li class="">
<p><strong>institution:</strong> Southern University of Science and Technology, Microsoft Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21925" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21925</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new hybrid CMAB-T framework that integrates offline data with online interaction to address the complementary weaknesses of purely online or offline methods. 2. Introduces the hybrid CUCB algorithm, which leverages offline data to guide exploration and strategically uses online interactions to correct dataset bias. 3. Provides theoretical regret guarantees and empirical results demonstrating the algorithm&#x27;s consistent advantage over purely online or offline baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e97b8cf09a0691d48238a8272fecd32791ddc20b9ba00115a757d778b1e2017f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e97b8cf09a0691d48238a8272fecd32791ddc20b9ba00115a757d778b1e2017f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a hybrid framework for combinatorial multi-armed bandits with probabilistically triggered arms (CMAB-T) that combines offline data with online interaction. The core method is the hybrid CUCB algorithm, which uses offline data to accelerate learning and online interaction to correct for dataset limitations. Theoretical and empirical results show this hybrid approach outperforms purely online or offline methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [hallucination mitigation, adversarial parametric editing, parameter clustering, visual-language models, activation dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji, Zhongshi He</p>
</li>
<li class="">
<p><strong>institution:</strong> Chongqing University, Xinjiang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21999" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21999</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/hujiayu1223/ALEAHallu" target="_blank" rel="noopener noreferrer" class="">https://github.com/hujiayu1223/ALEAHallu</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel adversarial parametric editing framework (ALEAHallu) following an Activate-Locate-Edit Adversarially paradigm for mitigating hallucinations in VLMs. 2. Introduces a method to construct an activation dataset with grounded and hallucinatory response pairs and identifies critical hallucination-prone parameter clusters via differential hidden state analysis. 3. Demonstrates the framework&#x27;s effectiveness by fine-tuning identified parameter clusters with adversarially tuned prefixes to force the model to prioritize visual evidence over linguistic priors.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the hallucination problem in Vision-Language Models (VLMs), where models generate content misaligned with visual inputs due to over-reliance on linguistic priors. The authors propose ALEAHallu, an adversarial parametric editing framework that identifies and fine-tunes hallucination-prone parameter clusters using adversarially optimized prompts to enhance visual grounding. Evaluations show the method significantly reduces hallucinations in both generative and discriminative VLM tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [computational biology], [protein language model, ESM-2, dual-stream architecture, 1D CNN, transformer encoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aicha Boutorh, Soumia Bouyahiaoui, Sara Belhadj, Nour El Yakine Guendouz, Manel Kara Laouar</p>
</li>
<li class="">
<p><strong>institution:</strong> National School of Artificial Intelligence (ENSIA)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22007" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22007</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DuaDeep-SeqAffinity, a novel sequence-only deep learning framework for antigen-antibody affinity prediction using a dual-stream hybrid architecture. 2. Integrates pre-trained ESM-2 embeddings with 1D CNNs for local motifs and Transformer encoders for global context, followed by a fusion module. 3. Demonstrates superior performance over single-branch models and existing SOTA methods, even surpassing some structure-sequence hybrid models, proving the efficacy of sequence-only high-fidelity embeddings.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1985304be62407b10b5e5be53aea80fe4ff1468be03e261847b49774ddd937a8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1985304be62407b10b5e5be53aea80fe4ff1468be03e261847b49774ddd937a8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces DuaDeep-SeqAffinity, a deep learning framework that predicts antigen-antibody binding affinity using only amino acid sequences. It combines ESM-2 embeddings with a dual-stream architecture of 1D CNNs and Transformers to capture local and global features. The model outperforms existing methods, showing that sequence-only models can effectively capture binding patterns and accelerate therapeutic discovery.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] HWL-HIN: A Hypergraph-Level Hypergraph Isomorphism Network as Powerful as the Hypergraph Weisfeiler-Lehman Test with Application to Higher-Order Network Robustness</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [hypergraph isomorphism network, hypergraph weisfeiler-lehman test, higher-order network robustness, hypergraph neural networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chengyu Tian, Wenbin Pei</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in the provided content.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22014" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22014</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hypergraph-level Hypergraph Isomorphism Network (HWL-HIN) framework for hypergraph learning. 2. Theoretically proves the model&#x27;s expressive power is strictly equivalent to the Hypergraph Weisfeiler-Lehman test. 3. Successfully applies the model to predict hypergraph robustness, demonstrating superior performance and efficiency over existing graph-based and conventional HGNN models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3b228ae57b3d7937d8586b0c60419df83b49466ba7ca3b946109dd8186f827c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3b228ae57b3d7937d8586b0c60419df83b49466ba7ca3b946109dd8186f827c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the computational overhead of traditional robustness assessment and the limited expressive power of existing hypergraph neural networks by proposing a hypergraph-level Hypergraph Isomorphism Network (HWL-HIN). The method is proven theoretically to be as powerful as the Hypergraph Weisfeiler-Lehman test and is applied to predict hypergraph robustness. Experiments show it outperforms existing graph-based models and conventional HGNNs in tasks prioritizing topological structure while maintaining high efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models for drug discovery], [hit-like molecule generation, autoregressive models, diffusion models, docking scores, multi-stage filtering]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nagham Osman, Vittorio Lembo, Giovanni Bottegoni, Laura Toni</p>
</li>
<li class="">
<p><strong>institution:</strong> University College London, University of Urbino Carlo Bo</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22031" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22031</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Framing hit-like molecule generation as a standalone task for generative models. 2. Proposing a tailored evaluation framework integrating physicochemical, structural, and bioactivity criteria. 3. Benchmarking autoregressive and diffusion models, with synthesized GSK-3β hits confirmed active in vitro.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0d0d188f2e13e0f8561de5950d5637586c871a0ee36935ebfbd5bb2bad540_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0d0d188f2e13e0f8561de5950d5637586c871a0ee36935ebfbd5bb2bad540_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether generative models can replace the hit identification step in drug discovery. It proposes a multi-stage evaluation framework and benchmarks autoregressive and diffusion models, showing they can generate valid, diverse, and biologically relevant compounds, with some synthesized hits confirmed active in vitro.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Direction Finding with Sparse Arrays Based on Variable Window Size Spatial Smoothing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [signal processing], [DOA estimation, sparse arrays, coarrays, spatial smoothing, MUSIC]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wesley S. Leite, Rodrigo C. de Lamare, Yuriy Zakharov, Wei Liu, Martin Haardt</p>
</li>
<li class="">
<p><strong>institution:</strong> University of York, Pontifical Catholic University of Rio de Janeiro, University of York, University of York, Ilmenau University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22024" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22024</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a variable window size (VWS) spatial smoothing framework for coarray-based DOA estimation with sparse linear arrays. 2. Proposes the VWS-CA-MUSIC and VWS-CA-rMUSIC algorithms that replace perturbed data terms with unperturbed ones to improve signal-noise subspace separation. 3. Derives theoretical bounds for the compression parameter to guarantee identifiability and demonstrates performance improvements and complexity savings over fixed-window methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/20bb39a95f66255cc62bbe7ee6e002de101d9a346c703b72e27fc3d3b08e1d30_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/20bb39a95f66255cc62bbe7ee6e002de101d9a346c703b72e27fc3d3b08e1d30_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of direction-of-arrival (DOA) estimation for correlated sources using sparse linear arrays. It proposes a variable window size spatial smoothing framework and new VWS-CA-MUSIC algorithms that enhance estimation accuracy by compressing the smoothing aperture. Simulations show the method provides significant performance gains and reduced computational complexity compared to standard fixed-window coarray MUSIC.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] LibContinual: A Comprehensive Library towards Realistic Continual Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [catastrophic forgetting, stability-plasticity dilemma, modular architecture, memory budget, online continual learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenbin Li, Shangge Liu, Borui Kang, Yiyang Chen, KaXuan Lew, Yang Chen, Yinghuan Shi, Lei Wang, Yang Gao, Jiebo Luo</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanjing University, University of Wollongong, University of Rochester</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22029" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22029</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/RL-VIG/LibContinual" target="_blank" rel="noopener noreferrer" class="">https://github.com/RL-VIG/LibContinual</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed LibContinual, a unified, modular, and reproducible library for Continual Learning (CL) that integrates 19 representative algorithms across five methodological categories. 2. Systematically identified and investigated three unrealistic implicit assumptions (offline data accessibility, unregulated memory, intra-task semantic homogeneity) prevalent in mainstream CL evaluation. 3. Conducted a comprehensive analysis under stricter, more realistic settings (strict online CL, unified memory budget, category-randomized tasks), revealing significant performance drops in many existing methods and highlighting the need for resource-aware and semantically robust CL strategies.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a733a967f663a216bbd5fb0cee657ed8bb70a4e6f0205d669f983cbf9bb6fd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a733a967f663a216bbd5fb0cee657ed8bb70a4e6f0205d669f983cbf9bb6fd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces LibContinual, a comprehensive library designed to unify and standardize research in Continual Learning (CL). By using this framework to evaluate existing methods under more realistic constraints, the study shows that many current CL algorithms suffer significant performance drops, underscoring the gap between common evaluation practices and real-world applicability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Why Smooth Stability Assumptions Fail for ReLU Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [optimization theory], [ReLU networks, nonsmooth optimization, stability analysis, generalized derivatives, learning dynamics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ronald Katende</p>
</li>
<li class="">
<p><strong>institution:</strong> Kabale University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22055" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22055</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that no uniform smoothness-based stability proxy (e.g., gradient Lipschitzness) can hold globally for ReLU networks, even in simple, empirically stable settings. 2. Provides a concrete counterexample showing the failure of classical stability bounds for ReLU learning dynamics. 3. Identifies a minimal generalized derivative condition under which stability statements can be meaningfully restored for nonsmooth learning systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3150186f6ab289f5e6db87d8ca89851af409290b0b4c37667d493ee4b7e894b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3150186f6ab289f5e6db87d8ca89851af409290b0b4c37667d493ee4b7e894b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper shows that smoothness assumptions like gradient Lipschitzness, commonly used in stability analyses, fail globally for ReLU networks due to their inherent nonsmoothness. It provides a counterexample to classical bounds and proposes a minimal condition based on generalized derivatives to restore meaningful stability analysis. This clarifies the limitations of smooth approximations and motivates nonsmooth-aware frameworks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Scaling Adversarial Training via Data Selection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [adversarial robustness], [adversarial training, PGD, sample selection, gradient matching, margin-based sampling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Youran Ye, Dejin Wang, Ajinkya Bhandare</p>
</li>
<li class="">
<p><strong>institution:</strong> Northeastern University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22069" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22069</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/youranye/Selective-Adversarial-Training" target="_blank" rel="noopener noreferrer" class="">https://github.com/youranye/Selective-Adversarial-Training</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Selective Adversarial Training, which perturbs only a critical subset of samples per minibatch to reduce computational cost., 2. Introduces two novel sample selection criteria: margin-based sampling and gradient-matching sampling., 3. Demonstrates that the method achieves comparable or superior robustness to full PGD adversarial training while reducing adversarial computation by up to 50%.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad822d1db3ef050d2caa84f51828c7b48f93eee442f9a9f954f4f36b07929f44_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad822d1db3ef050d2caa84f51828c7b48f93eee442f9a9f954f4f36b07929f44_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high computational cost of Projected Gradient Descent (PGD) in adversarial training. It proposes Selective Adversarial Training, which uses principled criteria to select and perturb only critical samples in each batch. Experiments show the method maintains robustness while cutting adversarial computation by up to 50%.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [SRAM, frequency scaling, energy-delay product, systolic array, memory bandwidth]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hannah Atmer, Yuan Yao, Thiemo Voigt, Stefanos Kaxiras</p>
</li>
<li class="">
<p><strong>institution:</strong> Uppsala University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22066" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22066</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Quantified the distinct energy and performance impacts of SRAM size and operating frequency on the compute-bound prefill and memory-bound decode phases of LLM inference. 2. Demonstrated a counter-intuitive result: high compute frequency can reduce total energy by shortening execution time and reducing static energy more than the dynamic power increase. 3. Identified an optimal hardware configuration (high frequency, small SRAM buffer) that minimizes the energy-delay product, providing concrete design insights for energy-efficient accelerators.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52c2db009cb44a92a388e1684ea0d1bdb9ae27c0c4a4e683651eb7bd091bbe93_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52c2db009cb44a92a388e1684ea0d1bdb9ae27c0c4a4e683651eb7bd091bbe93_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates how on-chip SRAM size and operating frequency affect the energy efficiency of LLM inference. Using a simulation methodology combining OpenRAM, LLMCompass, and ScaleSIM, it finds that a high-frequency, small-SRAM configuration optimizes the energy-delay product, as memory bandwidth caps decode phase improvements. The analysis provides architectural guidance for designing efficient LLM accelerators.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Unifying Learning Dynamics and Generalization in Transformers Scaling Law</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [learning theory], [scaling law, learning dynamics, generalization error, transformer, stochastic gradient descent]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chiwun Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> Sun Yat-sen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22088" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22088</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formalizes the learning dynamics of transformers as an ODE system and approximates it to kernel behaviors, moving beyond toy models to analyze SGD on multi-layer transformers with arbitrary data distributions. 2. Establishes a theoretical upper bound on excess risk with a distinct phase transition: exponential decay in the optimization phase and a power-law decay of Θ(C^{-1/6}) in the statistical phase. 3. Derives isolated scaling laws for model size, training time, and dataset size, explaining how each variable independently governs generalization bounds.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9b067b56202cd4607e684058e78ac331373bf12bf6848ca444276e2dcafe9f9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9b067b56202cd4607e684058e78ac331373bf12bf6848ca444276e2dcafe9f9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper provides a theoretical foundation for the empirical scaling laws of large language models. It models transformer learning dynamics as an ODE system and analyzes SGD training on realistic data. The main result is a unified theory showing a phase transition in generalization error, from exponential to power-law decay, as computational resources scale.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [multi-agent pipeline, automated data analysis, insight generation, report synthesis, visual analytics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuyu Gan, Renxiang Wang, James Mooney, Dongyeop Kang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Minnesota</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22101" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22101</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A Data Analyzer agent that orchestrates data profiling, generates diverse visualizations, filters low-quality charts, and automatically scores candidate insights for depth, correctness, and actionability. 2. A Presenter agent that sequences topics, composes chart-grounded narratives from top insights, writes transitions, and revises the document to produce a coherent, publication-ready report. 3. An end-to-end Analyzer-to-Presenter (A2P) pipeline that operationalizes co-analysis by coupling quality-assured analysis with narrative synthesis, improving the real-world usefulness of automated data analysis.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92651ded84480402816f8db1df902e28dd62cce1b0958cece60e0f518bdd7e1c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92651ded84480402816f8db1df902e28dd62cce1b0958cece60e0f518bdd7e1c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents A2P-Vis, a two-part multi-agent pipeline designed to automate the generation of data visualization reports. The system uses a Data Analyzer to create and vet visual insights and a Presenter to assemble them into a coherent narrative. The authors claim this end-to-end approach improves the practical utility of automated data analysis for practitioners.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Explainable Multimodal Regression via Information Decomposition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal machine learning], [Partial Information Decomposition (PID), multimodal regression, interpretability, Gaussianity assumption, conditional independence regularizer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhaozhao Ma, Shujian Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Georgia Institute of Technology, Vrije Universiteit Amsterdam, UiT - The Arctic University of Norway</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22102" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22102</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/xxx/PIDReg" target="_blank" rel="noopener noreferrer" class="">https://github.com/xxx/PIDReg</a> (URL placeholder from abstract)</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel multimodal regression framework based on Partial Information Decomposition (PID) to quantify unique, redundant, and synergistic information from modalities. 2. Introduction of a Gaussianity assumption to resolve the underdetermined nature of PID, enabling analytical computation of its terms. 3. Derivation of a closed-form conditional independence regularizer to isolate unique information within each modality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a07cab8b6bb765060f8a015e46e79e686a9acb69bac3aa8045cf96acec9f61e6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a07cab8b6bb765060f8a015e46e79e686a9acb69bac3aa8045cf96acec9f61e6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of interpretability in multimodal regression by proposing a new framework based on Partial Information Decomposition (PID). The method resolves PID&#x27;s underdetermination by enforcing Gaussianity and uses a conditional independence regularizer to isolate unique modality information. Experiments on six datasets, including brain age prediction, show the framework outperforms state-of-the-art methods in both accuracy and interpretability, while aiding modality selection.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Sensitivity Analysis of the Consistency Assumption</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [causal inference], [consistency assumption, sensitivity analysis, hidden versions of treatment, partial identification, stable unit treatment value assumption (SUTVA)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Brian Knaeble, Qinyun Lin, Erich Kummerfeld, Kenneth A. Frank</p>
</li>
<li class="">
<p><strong>institution:</strong> Utah Valley University, University of Gothenburg, University of Minnesota, Michigan State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21379" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21379</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A formal mathematical framework for analyzing violations of the consistency assumption. 2. A novel sensitivity parameter to quantify bias induced by hidden versions of treatment. 3. Methods for specifying bounds on this parameter to enable partial identification of causal effects.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0816c25e8631081977ffb91ecadbb3f527cfd99c7daf15f81c3f18a332125fcc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0816c25e8631081977ffb91ecadbb3f527cfd99c7daf15f81c3f18a332125fcc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses violations of the consistency assumption in causal inference, which posits no hidden versions of treatment. It proposes a new sensitivity analysis method focused on confounding by hidden treatment versions, introducing a formal framework and a novel sensitivity parameter. The work enables researchers to assess and bound the bias from consistency violations when interpreting causal estimates.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Dynamic Attention (DynAttn): Interpretable High-Dimensional Spatio-Temporal Forecasting (with Application to Conflict Fatalities)</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [spatio-temporal forecasting], [dynamic attention, zero-inflated negative binomial, elastic-net gating]</p>
</li>
<li class="">
<p><strong>authors:</strong> Stefano M. Iacus, Haodong Qi, Marcello Carammia, Thomas Juneau</p>
</li>
<li class="">
<p><strong>institution:</strong> Harvard University, Stockholm University, Malmö University, University of Catania, University of Toronto</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21435" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21435</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces DynAttn, an interpretable dynamic-attention framework for high-dimensional spatio-temporal count processes. 2. Combines rolling-window estimation, shared elastic-net feature gating, a compact self-attention encoder, and a ZINB likelihood for calibrated forecasts. 3. Demonstrates superior predictive accuracy and enables structured interpretation of regional conflict dynamics, showing the roles of persistence, diffusion, and climate stress.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad96b82177be36f56ddfe0b4e4d6be51396aa49723cdabef2c85f538301f2e0e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad96b82177be36f56ddfe0b4e4d6be51396aa49723cdabef2c85f538301f2e0e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces DynAttn, an interpretable forecasting framework for sparse, high-dimensional spatio-temporal data like conflict fatalities. It combines dynamic attention, feature gating, and a specialized likelihood to produce accurate, multi-horizon forecasts and probabilities. The method outperforms benchmarks, particularly in sparse settings, and provides interpretable insights into conflict drivers such as persistence, spatial diffusion, and conditional climate effects.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Atomistic Simulation Guided Convolutional Neural Networks for Thermal Modeling of Friction Stir Welding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [physics-informed machine learning], [molecular dynamics, convolutional neural network, friction stir welding, explainable AI, LAMMPS]</p>
</li>
<li class="">
<p><strong>authors:</strong> Akshansh Mishra</p>
</li>
<li class="">
<p><strong>institution:</strong> Politecnico di Milano, AI Fab Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21344" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21344</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a novel method to transform atomistic simulation data (atomic positions and velocities) into physics-based 2D spatial grids for deep learning input. 2. Created and optimized a 2D CNN model to directly predict temperature evolution from spatially resolved atomistic data, achieving high accuracy (R²=0.94). 3. Used Class Activation Map analysis to provide explainability, showing the model&#x27;s focus aligns with physical mechanisms (e.g., tool-material interface).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34d6f71500319f52aecac1e95e1951a537fdc504134a8ba43851f31acc2e41c6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34d6f71500319f52aecac1e95e1951a537fdc504134a8ba43851f31acc2e41c6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a method that combines molecular dynamics simulations with convolutional neural networks for thermal modeling in friction stir welding. The method transforms atomic-scale simulation data into spatial grids and uses a CNN to accurately predict temperature, with results validated against physical mechanisms. The approach demonstrates that deep learning can effectively learn from atomistic data to model complex thermomechanical processes.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] An approach to Fisher-Rao metric for infinite dimensional non-parametric information geometry</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [information geometry], [Fisher-Rao metric, non-parametric, G-entropy, Covariate Fisher Information Matrix (cFIM), intrinsic dimensionality]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bing Cheng, Howell Tong</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in provided content. Affiliation likely inferred from author names or arXiv metadata, but not present in the given text.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21451" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21451</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces an orthogonal decomposition of the tangent space to derive a finite-dimensional, computable Covariate Fisher Information Matrix (cFIM) from the infinite-dimensional Fisher-Rao metric. 2. Establishes the geometric foundation of G-entropy via a Trace Theorem, linking it to total explainable statistical information. 3. Connects the cFIM to the Cramér-Rao Lower Bound and the Manifold Hypothesis, providing a method for estimating intrinsic dimensionality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a8f179358a2d0d118621b506d45d051ad949b1cc0acfe723d3c3268c4390e5a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a8f179358a2d0d118621b506d45d051ad949b1cc0acfe723d3c3268c4390e5a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the intractability of the infinite-dimensional Fisher-Rao metric in non-parametric information geometry by proposing an orthogonal decomposition of the tangent space. This decomposition yields a finite-dimensional Covariate Fisher Information Matrix (cFIM), which serves as a computable geometric object for analyzing statistical information and model efficiency. The framework rigorously grounds G-entropy, links to fundamental statistical bounds, and provides a testable condition for the Manifold Hypothesis, bridging abstract geometry with explainable AI.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Deep learning-enhanced dual-mode multiplexed optical sensor for point-of-care diagnostics of cardiovascular diseases</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [biomedical sensing and diagnostics], [vertical flow assay, dual-mode detection, neural network-based quantification, multiplexed optical sensor, point-of-care diagnostics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gyeo-Re Han, Merve Eryilmaz, Artem Goncharov, Yuzhu Li, Shun Ye, Aoi Tomoeda, Emily Ngo, Margherita Scussat, Xiao Wang, Zixiang Ji, Max Zhang, Jeffrey J. Hsu, Omai B. Garner, Dino Di Carlo, Aydogan Ozcan</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Los Angeles (UCLA)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21389" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21389</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a dual-mode (colorimetric and chemiluminescent) multiplexed vertical flow assay (xVFA) for simultaneous detection of three cardiac biomarkers, achieving a wide dynamic range of ~6 orders of magnitude. 2. Integrated a deep learning-based quantification pipeline with a portable optical reader to automate analysis and enhance accuracy, achieving high correlation (Pearson&#x27;s r &gt; 0.96) with reference assays. 3. Created a compact, cost-effective, and rapid (23 min) point-of-care diagnostic system for cardiovascular diseases using only 50 µL of serum.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6eb23e46dc346744879ce56ec8c667b82f8cab434c54e4a4922a2eb842bf77d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6eb23e46dc346744879ce56ec8c667b82f8cab434c54e4a4922a2eb842bf77d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a deep learning-enhanced, dual-mode optical sensor for point-of-care cardiovascular diagnostics. The system uses a multiplexed vertical flow assay with colorimetric and chemiluminescent detection to simultaneously measure three key biomarkers with high sensitivity across a wide dynamic range, and a neural network pipeline for accurate quantification. The result is a rapid, portable, and automated diagnostic tool validated on patient samples.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Quantum Nondecimated Wavelet Transform: Theory, Circuits, and Applications</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [quantum signal processing], [quantum nondecimated wavelet transform, epsilon decimation, Hadamard test, quantum wavelet shrinkage, shift invariance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Brani Vidakovic</p>
</li>
<li class="">
<p><strong>institution:</strong> Texas A&amp;M University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21478" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21478</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a quantum formulation of the Nondecimated Wavelet Transform (NDWT) based on epsilon-decimation, using a quantum register for shift indices and controlled circular shifts to realize all shifted transforms simultaneously. 2. Proposed an alternative quantum NDWT formulation using the Hadamard test and diagonal phase operators to directly access shift-invariant energy scalograms without full coefficient reconstruction. 3. Demonstrated that quantum redundancy and translation invariance can be exploited for applications like denoising and feature extraction, providing a foundation for multiscale quantum signal processing.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01489d687d6b4e38b42c82e3054e553beab66883e073aec7483d40c87388a47a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01489d687d6b4e38b42c82e3054e553beab66883e073aec7483d40c87388a47a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces two quantum formulations of the classical Nondecimated Wavelet Transform (NDWT) to achieve shift invariance and redundancy in quantum computation. The first uses controlled shifts and a wavelet analysis unitary, while the second employs the Hadamard test with phase operators for direct energy measurement. The work shows these quantum NDWTs enable coherent multiscale signal processing tasks like denoising and feature extraction.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [diffusion models, generative modeling, evidence lower bound, residual learning, two-stage framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Takuro Kutsuna</p>
</li>
<li class="">
<p><strong>institution:</strong> Toyota Central R&amp;D Labs., Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21593" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21593</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Incorporating rank-free coupling and external field via an amplitude-only modulated spatial photonic Ising machine</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [photonic computing], [spatial photonic Ising machine, Hadamard product, amplitude-only modulation, rank-free coupling, incoherent light field]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ze Zheng, Yuegang Li, Hang Xu, Jingzheng Huang, Tailong Xiao, Guihua Zeng</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, Hefei National Laboratory, Shanghai Research Center for Quantum Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21587" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21587</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an amplitude-only modulated, rank-free spatial photonic Ising machine (AR-SPIM) that eliminates the need for repeated/auxiliary operations by reformulating the Ising Hamiltonian as a sum of Hadamard products. 2. Demonstrates direct mapping of a 797-spin Ising model with external fields into an incoherent light field using aligned amplitude modulators, achieving high encoding accuracy (correlation &gt;0.98). 3. Shows the system&#x27;s capability for ground-state search with &lt;0.3% error, complex phase transition observation, and scalable spin counts for sparse problems by removing zero-valued terms.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fe8a02df2acd50923ce3480e4b3fb28b68540ae3331e12dd83853a5b046a5c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fe8a02df2acd50923ce3480e4b3fb28b68540ae3331e12dd83853a5b046a5c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a new spatial photonic Ising machine (AR-SPIM) that uses an amplitude-only modulation scheme to encode arbitrary-rank spin-spin couplings and external fields directly into an incoherent light field. The method reformulates the Ising Hamiltonian as a sum of Hadamard products, enabling efficient and accurate computation. The demonstrated system achieves high-speed iterations, low error rates for optimization problems, and offers scalability for practical applications in optimization and quantum simulations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Tilt Matching for Scalable Sampling and Fine-Tuning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [Tilt Matching, stochastic interpolants, flow matching, unnormalized densities, fine-tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Peter Potaptchik, Cheuk-Kit Lee, Michael S. Albergo</p>
</li>
<li class="">
<p><strong>institution:</strong> Harvard University, University of Oxford, Kempner Institute, IAIFI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21829" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21829</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Tilt Matching, a simple, scalable algorithm for sampling and fine-tuning based on stochastic interpolants and an implicit stochastic optimal control solution., 2. Demonstrates that the method has strictly lower variance than standard flow matching and does not require gradients of the reward or backpropagation through trajectories., 3. Empirically shows state-of-the-art results on sampling under Lennard-Jones potentials and competitive performance on fine-tuning Stable Diffusion without reward multipliers.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2cc2b096f1ad1f69d17a5fbdbac71d5ccac470d3cc86a47d74fed9dba5202c88_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2cc2b096f1ad1f69d17a5fbdbac71d5ccac470d3cc86a47d74fed9dba5202c88_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces Tilt Matching, a new algorithm for sampling from complex distributions and fine-tuning generative models. It works by deriving a velocity field from stochastic interpolants to target a &quot;tilted&quot; distribution, offering lower variance than flow matching and avoiding gradient computations. The method is shown to be scalable and effective, achieving strong results on molecular sampling and image generation tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Modeling high dimensional point clouds with the spherical cluster model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [clustering], [spherical cluster model, high-dimensional median, non-smooth optimization, Clarke gradient, stratified cell complex]</p>
</li>
<li class="">
<p><strong>authors:</strong> Frédéric Cazals, Antoine Commaret, Louis Goldenberg</p>
</li>
<li class="">
<p><strong>institution:</strong> Université Côte d&#x27;Azur, Inria, Ecole Polytechnique</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21960" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21960</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Showed that fitting a spherical cluster model is a strictly convex but non-smooth combinatorial optimization problem. 2. Presented an exact solver using the Clarke gradient on a stratified cell complex derived from an arrangement of hyperspheres. 3. Demonstrated through experiments that the exact solver is significantly faster than BFGS heuristics for many datasets and that the model&#x27;s center behaves as a parameterized high-dimensional median.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c9618adf1a45723b5b136e8c21eaa91dc645be7664e2fae293ae569f2adde20_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c9618adf1a45723b5b136e8c21eaa91dc645be7664e2fae293ae569f2adde20_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces the Spherical Cluster (SC) model for approximating high-dimensional point clouds with a sphere. It proposes an exact solver for this non-smooth optimization problem, which is shown to be much faster than heuristic methods for many datasets, especially in high dimensions. The model&#x27;s center is found to act as a robust, parameterized median.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Frobenius-Optimal Projection for Enforcing Linear Conservation in Learned Dynamical Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [dynamical systems, numerical linear algebra], [linear conservation laws, Frobenius norm, orthogonal projection, matrix correction, data-driven models]</p>
</li>
<li class="">
<p><strong>authors:</strong> John M. Mango, Ronald Katende</p>
</li>
<li class="">
<p><strong>institution:</strong> Makerere University, Kabale University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22084" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22084</a></p>
</li>
<li class="">
<p><strong>contributions:</strong>  1. Provides a closed-form, Frobenius-norm optimal projection to enforce linear conservation laws on any learned linear dynamical operator. 2. Proves that the correction is uniquely defined, low-rank, and fully determined by the constraint violation, reducing to a rank-one update for a single invariant. 3. Demonstrates that the method enforces exact conservation while minimally perturbing the learned dynamics, verified with a numerical example.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff37179b43efd7a7b64ff062f4e49c8aeaa1dc0b65febe24067b7ddd46dba12c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff37179b43efd7a7b64ff062f4e49c8aeaa1dc0b65febe24067b7ddd46dba12c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of learned linear dynamical models violating known linear conservation laws. It proposes a closed-form orthogonal projection that minimally perturbs a learned operator in the Frobenius norm to exactly satisfy the constraints. The method provides a general, optimal post-processing step for embedding invariants into data-driven linear models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-30">2025-12-30<a href="#2025-12-30" class="hash-link" aria-label="Direct link to 2025-12-30" title="Direct link to 2025-12-30" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251230] Pruning Graphs by Adversarial Robustness Evaluation to Strengthen GNN Defenses</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [adversarial robustness, graph pruning, message passing, spurious connections, graph defense]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yongyu Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Michigan Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22128" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22128</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a novel graph pruning framework that uses adversarial robustness evaluation to identify fragile graph components. 2. Proposes a method that selectively prunes edges based on robustness scores to improve model reliability and resilience. 3. Validates the framework by instantiating it on three representative GNN architectures and demonstrating significant defense enhancement in high-perturbation regimes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d658f9bc62a6d2a57e4602f69b9d0c69056551601abd2ba0336e97d592bae1dc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d658f9bc62a6d2a57e4602f69b9d0c69056551601abd2ba0336e97d592bae1dc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the vulnerability of Graph Neural Networks (GNNs) to adversarial attacks and noise in graph structure. It proposes a pruning framework that uses adversarial robustness scores to identify and remove detrimental edges, resulting in cleaner and more resilient graph representations. Experiments show this method significantly strengthens GNN defenses against high levels of perturbation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent reinforcement learning], [ad-hoc teamwork, retrieval-augmented generation, teammate modeling, Overcooked]</p>
</li>
<li class="">
<p><strong>authors:</strong> Conor Wallace, Umer Siddique, Yongcan Cao</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Texas at San Antonio</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22129" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22129</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces COLLAB, a novel language-based framework that uses LLMs as behavioral world models to classify unseen teammate types in ad-hoc teamwork. 2. Extends COLLAB to RECOLLAB by incorporating retrieval-augmented generation (RAG) with exemplar trajectories to stabilize inference and improve adaptation. 3. Demonstrates empirically in the Overcooked environment that RECOLLAB achieves Pareto-optimal trade-offs between classification accuracy and episodic return, highlighting the value of retrieval grounding.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/078fe396118022eaa0d391e86072d08c5b6617a143aba1478029ca3185af6472_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/078fe396118022eaa0d391e86072d08c5b6617a143aba1478029ca3185af6472_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of ad-hoc teamwork, where an agent must collaborate with unseen teammates. It proposes RECOLLAB, a framework that uses retrieval-augmented LLMs to model and classify teammate behavior from short interaction traces. The method is shown to effectively improve adaptation and coordination in the cooperative Overcooked environment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On Harnessing Idle Compute at the Edge for Foundation Model Training</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [edge computing, tensor parallelism, parameter server, device heterogeneity, fault-tolerance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Leyang Xue, Meghana Madhyastha, Myungjin Lee, Amos Storkey, Randal Burns, Mahesh K. Marina</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Edinburgh, Johns Hopkins University, Cisco Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22142" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22142</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel selective hybrid tensor parallelism method to finely partition training operations for edge devices. 2. A parameter server-centric training framework to cope with device memory limits and avoid communication bottlenecks. 3. A cost optimization model to guide device selection and workload distribution, effectively handling device heterogeneity and churn.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fefe0f72fa70ae7be2cad54f15e74f96fd08cc506630060214e298521278148_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fefe0f72fa70ae7be2cad54f15e74f96fd08cc506630060214e298521278148_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of decentralized foundation model training on edge devices, which is hindered by memory limits, communication overhead, and device heterogeneity. It proposes Cleave, a new paradigm that uses selective hybrid tensor parallelism and a parameter server framework to partition training efficiently. The evaluation shows Cleave matches cloud-based training performance, scales to thousands of devices, and handles failures with much faster recovery than prior methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [hpc], [gpu kernels], [Minimal Executable Program (MEP), Automatic Error Repair, Performance Pattern Inheritance, iterative optimization, cross-platform]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ruifan Chu, Anbang Wang, Xiuxiu Bai, Shuai Liu, Xiaoshe Dong</p>
</li>
<li class="">
<p><strong>institution:</strong> School of Software Engineering, Xi’an Jiaotong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22147" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22147</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an end-to-end LLM framework that optimizes GPU kernels by constructing Minimal Executable Programs (MEPs) to avoid expensive full application builds and executions. 2. Introduces Automatic Error Repair and Performance Pattern Inheritance to automatically fix faults and reuse effective optimization strategies, reducing search cost. 3. Demonstrates cross-platform portability and effectiveness on NVIDIA GPUs and the Haiguang DCU platform, achieving significant speedups over direct LLM optimization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd593bd3569f30bdbf11d361054f51142863fb91e592b76bc4eb2f600850c5e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd593bd3569f30bdbf11d361054f51142863fb91e592b76bc4eb2f600850c5e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high cost of full builds for GPU kernel optimization in large HPC applications by proposing an LLM framework that uses Minimal Executable Programs (MEPs) for iterative optimization. The method integrates automatic error repair and performance pattern inheritance to maintain correctness and reuse strategies. It achieves substantial speedups across different hardware platforms without requiring full-source dependencies.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Towards Unsupervised Causal Representation Learning via Latent Additive Noise Model Causal Autoencoders</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [causal representation learning], [additive noise model, Wasserstein auto-encoder, identifiability, unsupervised learning, causal discovery]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hans Jarett J. Ong, Brian Godwin S. Lim, Dominic Dayta, Renzo Roel P. Tan, Kazushi Ikeda</p>
</li>
<li class="">
<p><strong>institution:</strong> Nara Institute of Science and Technology, Kyoto University, Ateneo de Manila University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22150" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22150</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed LANCA, a novel autoencoder framework that operationalizes the Additive Noise Model (ANM) as an inductive bias for unsupervised causal representation learning., 2. Provided a theoretical analysis showing that the ANM constraint restricts admissible transformations to the affine class, resolving component-wise indeterminacy., 3. Introduced a deterministic WAE architecture with a differentiable ANM layer to explicitly optimize for residual independence, overcoming limitations of stochastic VAEs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/383ef9534443c860dced6678ba459335afb022e411bf183cc7f2e3dbad2bb385_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/383ef9534443c860dced6678ba459335afb022e411bf183cc7f2e3dbad2bb385_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of unsupervised causal representation learning by proposing LANCA, a method that uses the Additive Noise Model as an inductive bias within a deterministic autoencoder framework. Theoretically, it shows this constraint narrows down the solution space, and empirically, LANCA outperforms existing methods on synthetic and photorealistic benchmarks by being more robust to spurious correlations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning from human feedback (rlhf)], [reward model, video generation, reward hacking, bradley-terry loss, hierarchical attention]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiesong Lian, Ruizhe Zhong, Zixiang Zhou, Xiaoyue Mi, Yixue Hao, Yuan Zhou, Qinglin Lu, Long Hu, Junchi Yan</p>
</li>
<li class="">
<p><strong>institution:</strong> Huazhong University of Science and Technology, Shanghai Jiao Tong University, Tencent Hunyuan, University of Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22170" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22170</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A data collection and pairing strategy using single-item binary annotations and cross-prompt pairing to reduce labeling noise. 2. A Hierarchical Progressive Query Attention mechanism for better feature aggregation in the reward model architecture. 3. A modified Bradley-Terry loss function that accommodates win-tie scenarios to regularize the score distribution and mitigate reward hacking.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04ea37599d144feb85d3d1e8303d5d59adfabb579e172f06aef976d3783ee4ef_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04ea37599d144feb85d3d1e8303d5d59adfabb579e172f06aef976d3783ee4ef_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes SoliReward, a systematic framework to improve reward models for video generation alignment. It addresses data noise and reward hacking through a new data annotation strategy, a hierarchical attention architecture, and a modified loss function. The approach shows improved performance on benchmarks for video quality and enhances the effectiveness of post-training video models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Wireless Traffic Prediction with Large Language Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [spatio-temporal forecasting], [large language model, wireless traffic prediction, spatial-temporal correlation, prompt engineering, fine-tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chuanting Zhang, Haixia Zhang, Jingping Qiao, Zongzhang Li, Mohamed-Slim Alouini</p>
</li>
<li class="">
<p><strong>institution:</strong> Shandong University, Shandong Normal University, China Mobile Communications Group Shandong Co., Ltd, King Abdullah University of Science and Technology (KAUST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22178" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22178</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes TIDES, an LLM-based framework that captures spatial-temporal correlations for urban wireless traffic prediction. 2. Introduces a prompt engineering scheme to bridge the domain gap between numerical traffic data and language models by embedding statistical features as structured inputs. 3. Designs a DeepSeek module enabling spatial alignment via cross-domain attention, allowing the LLM to leverage information from related regions, and employs efficient fine-tuning of lightweight components.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/402a3d6681d9c203daf7e8ef09e0d0af8998f3eba780abc404c945025563d6a8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/402a3d6681d9c203daf7e8ef09e0d0af8998f3eba780abc404c945025563d6a8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes TIDES, a novel framework that uses a large language model (LLM) enhanced with spatial awareness for urban wireless traffic prediction. It addresses the lack of spatial modeling in existing LLM-based predictors through region clustering, prompt engineering, and a spatial alignment module, achieving superior accuracy and robustness on real-world datasets, which is key for intelligent 6G network management.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Latent Sculpting for Zero-Shot Generalization: A Manifold Learning Approach to Out-of-Distribution Anomaly Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [anomaly detection], [manifold learning, normalizing flows, dual-centroid compactness loss, out-of-distribution detection, zero-shot generalization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rajeeb Thapa Chhetri, Zhixiong Chen, Saurab Thapa</p>
</li>
<li class="">
<p><strong>institution:</strong> Mercy University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22179" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22179</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Latent Sculpting, a novel two-stage framework that decouples manifold structure learning from density estimation for OOD anomaly detection. 2. Introduces the Dual-Centroid Compactness Loss (DCCL) to actively sculpt a compact, low-entropy latent manifold for benign data. 3. Demonstrates superior zero-shot generalization on the CIC-IDS-2017 benchmark, significantly outperforming supervised and unsupervised baselines on complex distribution shifts like &quot;Infiltration&quot;.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfcac5687161ad2a09f74eab3c1b7f91a350a2435fb71a0c791f2e305dff108c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfcac5687161ad2a09f74eab3c1b7f91a350a2435fb71a0c791f2e305dff108c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of &quot;Generalization Collapse&quot; in supervised models when detecting Out-of-Distribution (OOD) anomalies. It proposes Latent Sculpting, a two-stage method that first uses a novel loss to sculpt a compact latent manifold for benign data and then applies a normalizing flow for density estimation. The results show this approach enables robust zero-shot anomaly detection, significantly outperforming existing methods on unseen attack scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [dimensionality reduction], [Locally Linear Embedding (LLE), AI-enhanced LLE, medical data analysis, medical billing, transcription]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hassan Khalid, Muhammad Mahad Khaliq, Muhammad Jawad Bashir</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Science and Technology (NUST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22182" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22182</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an innovative integration of AI with Locally Linear Embedding (LLE) to handle high-dimensional medical data. 2. Develops a comprehensive mathematical model for the AI-enhanced LLE technique. 3. Demonstrates the model&#x27;s application in real-world healthcare scenarios, showing significant improvements in data processing accuracy and operational efficiency for medical billing and transcription.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces an AI-enhanced Locally Linear Embedding (LLE) model to improve the analysis of high-dimensional medical data. The method is applied to automate and enhance medical billing and transcription services. The results show significant improvements in processing accuracy and operational efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Dueling Double Deep Q-Network, curriculum learning, tennis simulation, sequential decision-making, sports analytics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vishnu Mohan</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22186" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22186</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a custom tennis simulation environment that models hierarchical scoring, tactical decisions, fatigue, and opponent skill. 2. Integrated a Dueling Double Deep Q-Network (DDQN) with curriculum learning to enable stable and effective strategy learning in a long-horizon, stochastic domain. 3. Identified a key limitation of win-rate optimization, revealing a learned defensive bias and highlighting challenges in reward design for sports RL.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/543e2f9f07244abac63adfdbdefd7fccfefed9147b55aed87016c10657f91bae_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/543e2f9f07244abac63adfdbdefd7fccfefed9147b55aed87016c10657f91bae_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a reinforcement learning framework using a Dueling Double Deep Q-Network trained with curriculum learning to optimize tennis strategy in a custom simulation. The method achieves high win rates and demonstrates stable convergence, but analysis reveals the learned policy is overly defensive, pointing to a fundamental issue with reward design in sports simulations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part I: Basic Concepts, Neural Networks, and Variants</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [prognostics &amp; health management (phm)], [Neural Networks, Convolutional Neural Networks, Reinforcement Learning, Uncertainty Quantification, Physics-Informed Machine Learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jose I. Aizpurua</p>
</li>
<li class="">
<p><strong>institution:</strong> University of the Basque Country (UPV/EHU)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22190" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22190</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the application of Neural Networks (NNs) and their variants, specifically Convolutional Neural Networks (CNNs), for transformer condition monitoring using diverse data modalities. 2. Discusses the integration of NN concepts within the Reinforcement Learning (RL) paradigm for decision-making and control in transformer health management. 3. Provides perspectives on emerging research directions at the intersection of physics-informed machine learning and transformer Prognostics &amp; Health Management (PHM).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73f2611de029a059f651f47ffd6b707f684cdbc0c0f865e4c8568c2765f5fede_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73f2611de029a059f651f47ffd6b707f684cdbc0c0f865e4c8568c2765f5fede_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of traditional, rule-based transformer condition monitoring by proposing the use of machine learning, particularly Neural Networks and their variants. It explores Convolutional Neural Networks for processing diverse sensor data and discusses Reinforcement Learning for control, concluding that physics-informed ML provides a powerful framework for more accurate diagnostics, prognostics, and decision-making in power transformer health management.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part II: Physics-Informed Neural Networks and Uncertainty Quantification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [physics-informed machine learning], [Physics-Informed Neural Networks (PINNs), Bayesian PINNs, uncertainty quantification, Prognostics &amp; Health Management (PHM), transformer condition monitoring]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jose I. Aizpurua</p>
</li>
<li class="">
<p><strong>institution:</strong> University of the Basque Country (UPV/EHU)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22189" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22189</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Physics-Informed Neural Networks (PINNs) for integrating physics into neural network training for transformer applications like thermal modeling and insulation ageing. 2. Presents Bayesian PINNs as a framework to quantify epistemic uncertainty, enabling robust predictions under sparse data conditions. 3. Outlines emerging research directions for physics-aware and trustworthy machine learning in the management of critical power assets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d981ba5912467b9788cda82e843171a8af166b1ec965beb37ff22f102f1e02c8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d981ba5912467b9788cda82e843171a8af166b1ec965beb37ff22f102f1e02c8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes using Physics-Informed Neural Networks (PINNs) and their Bayesian extension to improve transformer condition monitoring. The method integrates physical laws directly into the learning process and quantifies model uncertainty, aiming to deliver more reliable predictions with limited data. The work highlights the potential of physics-aware machine learning for robust prognostics and health management of critical power infrastructure.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Frequency Regularization: Unveiling the Spectral Inductive Bias of Deep Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [regularization theory], [Spectral Bias, L2 Regularization, Frequency Principle, Spectral Suppression Ratio, Inductive Bias]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiahao Lu</p>
</li>
<li class="">
<p><strong>institution:</strong> The Chinese University of Hong Kong, Shenzhen</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22192" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22192</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/lujiahao760/FrequencyRegularization" target="_blank" rel="noopener noreferrer" class="">https://github.com/lujiahao760/FrequencyRegularization</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a Visual Diagnostic Framework to track the dynamic evolution of weight frequencies during CNN training. 2. Proposed a novel metric, the Spectral Suppression Ratio (SSR), to quantify the &quot;low-pass filtering&quot; intensity of different regularizers. 3. Empirically revealed a critical Accuracy-Robustness Trade-off in L2-regularized models, showing their sensitivity to broadband noise but superior robustness to high-frequency information loss.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7443604e19a6c972d6ac6ad332d345bd15a272d6f58a489ef07c494950a3d274_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7443604e19a6c972d6ac6ad332d345bd15a272d6f58a489ef07c494950a3d274_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the spectral inductive bias of deep neural networks, focusing on how regularization techniques like L2 affect feature frequency selection. The authors propose a visual diagnostic framework and a new metric to quantify spectral suppression, demonstrating that L2 regularization strongly suppresses high-frequency energy and leads to a trade-off between accuracy and robustness. The work confirms that regularization enforces a strong spectral bias towards low-frequency structures, providing a signal-processing perspective on generalization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MatKV: Trading Compute for Flash Storage in LLM Inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [retrieval-augmented generation, key-value cache, flash storage, prefill optimization, power efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kun-Woo Shin, Jay H. Park, Moonwook Oh, Yohan Jo, Jaeyoung Do, Sang-Won Lee</p>
</li>
<li class="">
<p><strong>institution:</strong> Seoul National University, Samsung Electronics</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22195" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22195</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/kunwooshin/MatKV" target="_blank" rel="noopener noreferrer" class="">https://github.com/kunwooshin/MatKV</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes MatKV, a scheme to precompute and materialize KV vectors of RAG documents in flash storage to avoid recomputation during inference. 2. Demonstrates that MatKV reduces inference time and power consumption by half for RAG workloads with minimal accuracy impact. 3. Shows MatKV enables additional optimizations like overlapping KV loading with decoding and enabling the use of low-end GPUs for decoding.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d47111ab2d615579a09c00ff5a99f391f035b974cc23e324cddfbabf4d23cec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d47111ab2d615579a09c00ff5a99f391f035b974cc23e324cddfbabf4d23cec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high compute and energy cost of the prefill phase in RAG-based LLM inference. It proposes MatKV, which precomputes and stores key-value vectors of documents in flash storage for reuse, trading compute for storage. Experiments show this approach halves inference time and power consumption while maintaining accuracy and enabling further hardware optimizations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AETAS: Analysis of Evolving Temporal Affect and Semantics for Legal History</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [semantic change detection], [diachronic embeddings, orthogonal Procrustes, lexical drift]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qizhi Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> PingCAP, Data &amp; AI-Innovation Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22196" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22196</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A reproducible, expert-system style pipeline for quantifying and visualizing lexical drift in historical corpora. 2. A method coupling interpretable semantic trajectories with legally meaningful axes (e.g., mercy-versus-retribution). 3. The application of the pipeline to the Old Bailey Corpus, exposing the evolution of legal concepts like justice and crime alongside historical events.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df36f3ea25509e1c01d661a7893c2b940f15cfeb346560d105c4488a5fba4140_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df36f3ea25509e1c01d661a7893c2b940f15cfeb346560d105c4488a5fba4140_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a reproducible pipeline for analyzing semantic drift in historical legal texts. The method involves training and aligning diachronic word embeddings to quantify and visualize lexical change. The analysis of the Old Bailey Corpus reveals how concepts of justice and crime evolved with penal reforms and societal debates.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [dynamic routing, residual networks, cosine incompatibility, Gumbel-Softmax, FLOPs regularization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yogeswar Reddy Thota</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Texas at Dallas</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22206" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22206</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces CosineGate, an end-to-end differentiable architecture for dynamic routing in residual networks using cosine incompatibility as a self-supervised skip signal. 2. Proposes the Cosine Incompatibility Ratio (CIR) to measure semantic redundancy and employs Gumbel-Softmax relaxation for per-sample, per-block gating during training. 3. Incorporates a progressive FLOPs regularization term to control average computational usage without destabilizing the optimization process.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed7e3fa1c114a73795152210d2b55ffe2541fede331ce04d228e11ca599688fa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed7e3fa1c114a73795152210d2b55ffe2541fede331ce04d228e11ca599688fa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the computational inefficiency in residual networks, where all blocks are evaluated for every input. It proposes CosineGate, a method that uses the cosine incompatibility between identity and residual features to dynamically skip redundant blocks, achieving significant FLOPs savings on CIFAR-10 while maintaining or improving accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Emotion-Inspired Learning Signals (EILS): A Homeostatic Framework for Adaptive Autonomous Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [intrinsic motivation, homeostatic control, adaptive optimization, non-stationary learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dhruv Tiwari</p>
</li>
<li class="">
<p><strong>institution:</strong> Lovely Professional University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22200" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22200</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel framework, Emotion-Inspired Learning Signals (EILS), that models emotions as continuous, homeostatic appraisal signals (e.g., Curiosity, Stress, Confidence) for adaptive control. 2. Formalizes these signals as vector-valued internal states derived from interaction history to dynamically modulate the agent&#x27;s optimization landscape in real-time. 3. Hypothesizes that this closed-loop homeostatic regulation enables superior sample efficiency and adaptation to non-stationary environments compared to standard baselines like PPO.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a45d2a56af1becf3eaddb05dbaeff3cf5453d19d41771b6d8ce1c1a70d3825c2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a45d2a56af1becf3eaddb05dbaeff3cf5453d19d41771b6d8ce1c1a70d3825c2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies the fragility of standard AI agents that rely on static, external rewards in open-ended environments. It proposes the Emotion-Inspired Learning Signals (EILS) framework, which uses bio-inspired internal signals like curiosity and stress to dynamically control learning. The authors hypothesize this approach will lead to more robust, adaptive, and sample-efficient autonomous agents.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [multimodal large language models], [Moxin, open-source LLM, vision-language-action, Model Openness Framework, multimodal models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Arash Akbari, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Northeastern University, Harvard University, Cornell University, Tulane University, University of Washington, Roboraction.ai, Futurewei, AIBAO LLC</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22208" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22208</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/moxin-org/Moxin-LLM" target="_blank" rel="noopener noreferrer" class="">https://github.com/moxin-org/Moxin-LLM</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Moxin 7B, a fully open-source LLM developed under the Model Openness Framework, promoting transparency in training, datasets, and implementation. 2. Develops three specialized variants of Moxin: Moxin-VLM for vision-language tasks, Moxin-VLA for vision-language-action tasks, and Moxin-Chinese for Chinese language capabilities. 3. Demonstrates superior performance of the proposed models in various evaluations using open-source frameworks and data, with all models, code, and data publicly released.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79873d0c3143fc2b06f1be1be9b8bc80555fa0aefd4a6f2b8d6bda39bce5f227_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79873d0c3143fc2b06f1be1be9b8bc80555fa0aefd4a6f2b8d6bda39bce5f227_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Moxin 7B, a fully transparent open-source large language model, and extends it into three multimodal variants for vision-language, vision-language-action, and Chinese tasks. The models are trained using open-source frameworks and data. The authors release the models, code, and data, reporting superior performance in evaluations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Transformer Reconstructed with Dynamic Value Attention</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [transformer architecture], [dynamic value attention, single-head attention, feed forward network removal]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiaowei Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> College of Artificial Intelligence, China University of Petroleum (Beijing)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22212" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22212</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Dynamic Value Attention (DVA), a method to dynamically decide a value for each query, addressing the limitation of static values in standard attention heads. 2. Enabled the removal of redundant multi-head attention, reducing the architecture to a single-head attention mechanism. 3. Demonstrated that the subsequent feed-forward network can be entirely removed, as the revised embeddings already capture sufficient contextual information.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79cd017ace993f77d704944ed5eeaa739c2f83168d5d9f241b0355966c04ea5f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79cd017ace993f77d704944ed5eeaa739c2f83168d5d9f241b0355966c04ea5f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a limitation in the standard Transformer where static values are used for all queries within an attention head. It proposes Dynamic Value Attention (DVA), which computes a dynamic value per query, allowing the model to use only a single attention head and remove the feed-forward network entirely. Experiments show DVA reduces training time by 37.6% while improving learning capability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On the Existence and Behaviour of Secondary Attention Sinks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [attention sinks, transformer, mlp, attention mechanism, large language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jeffrey T.H. Wong, Cheng Zhang, Louis Mahon, Wayne Luk, Anton Isopoussu, Yiren Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Imperial College London, UnlikelyAI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22213" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22213</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and characterizes a new class of &quot;secondary attention sinks&quot; that arise in middle layers, have variable lifetimes, and draw moderate attention, differing from persistent primary sinks like BOS. 2. Shows that secondary sinks are formed by specific middle-layer MLP modules that map token representations to align with the primary sink&#x27;s direction, with their L2-norm determining sink strength and lifetime. 3. Observes that in larger models, these sink patterns (sink levels) become more deterministic and frequent, with distinct levels identified in models like QwQ-32B and Qwen3-14B.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec1462ca646134f445eac98192ff5189abb63f37682802d695aadace9f83b0d3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec1462ca646134f445eac98192ff5189abb63f37682802d695aadace9f83b0d3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies a new phenomenon called &quot;secondary attention sinks&quot; in transformer LLMs, which are distinct from the known primary sinks (like BOS). The authors show these secondary sinks are created by middle-layer MLPs aligning tokens with the primary sink direction, and their properties (strength, lifetime) become more structured in larger models. This provides new insights into the internal mechanics of attention in large language models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Interpretable and Adaptive Node Classification on Heterophilic Graphs via Combinatorial Scoring and Hybrid Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [heterophily, interpretability, combinatorial inference, hybrid learning, node classification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Soroush Vahidi</p>
</li>
<li class="">
<p><strong>institution:</strong> New Jersey Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22221" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22221</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an interpretable, adaptive framework for node classification using explicit combinatorial inference instead of deep message passing, with a tunable scoring function. 2. Introduces a validation-gated hybrid strategy that optionally refines combinatorial predictions with a lightweight neural model only when beneficial. 3. Ensures a leakage-free evaluation protocol by computing all adaptation signals strictly from training data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31111cc9c8d13c529d7f271adf29c47c440b43adfaa3481dde88da7f87b76463_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31111cc9c8d13c529d7f271adf29c47c440b43adfaa3481dde88da7f87b76463_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of node classification on heterophilic graphs where standard GNNs struggle. It proposes an interpretable framework based on combinatorial scoring and a conditional hybrid learning strategy, achieving competitive performance with modern GNNs while offering better interpretability, tunability, and efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [megakernel, kernel fusion, SM-level graph, software pipelining, CUDA]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xinhao Cheng, Zhihao Zhang, Yu Zhou, Jianan Ji, Jinchen Jiang, Zepeng Zhao, Ziruo Xiao, Zihao Ye, Yingyi Huang, Ruihang Lai, Hongyi Jin, Bohan Hou, Mengdi Wu, Yixin Dong, Anthony Yip, Zihao Ye, Songting Wang, Wenqin Yang, Xupeng Miao, Tianqi Chen, Zhihao Jia</p>
</li>
<li class="">
<p><strong>institution:</strong> Carnegie Mellon University, Tsinghua University, NVIDIA, University of Michigan, Purdue University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22219" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22219</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/mirage-project/mirage" target="_blank" rel="noopener noreferrer" class="">https://github.com/mirage-project/mirage</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces an SM-level graph representation for capturing fine-grained data dependencies across GPU streaming multiprocessors. 2. Develops a compiler and an in-kernel parallel runtime that automatically transforms multi-operator inference into a single, high-performance mega-kernel. 3. Enables previously infeasible GPU optimizations like cross-operator software pipelining and fine-grained kernel overlap, significantly reducing inference latency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f886cd06ce6c0f773c062fa38aae0fa982d862cc66ef54da9fbbfd6cf62dd86a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f886cd06ce6c0f773c062fa38aae0fa982d862cc66ef54da9fbbfd6cf62dd86a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces Mirage Persistent Kernel (MPK), a compiler and runtime system that automatically fuses multiple GPU kernels for model inference into a single, optimized mega-kernel. It achieves this by using a novel SM-level graph representation and decentralized scheduling to enable fine-grained optimizations like software pipelining. Evaluation shows MPK reduces LLM inference latency by up to 1.7x, pushing performance close to hardware limits.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [neural network architecture], [Müntz-Szász Networks, fractional power bases, physics-informed neural networks, universal approximation, singular function approximation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gnankan Landry Regis N&#x27;guessan</p>
</li>
<li class="">
<p><strong>institution:</strong> Axiom Research Group, The Nelson Mandela African Institution of Science and Technology (NM-AIST), African Institute for Mathematical Sciences (AIMS) Research and Innovation Centre</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22222" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22222</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Müntz-Szász Networks (MSN), a novel neural architecture with learnable fractional power bases to approximate functions with singular or fractional power behavior. 2. Provides theoretical analysis proving MSN inherits universal approximation from the Müntz-Szász theorem and establishes superior approximation rates compared to standard MLPs for singular functions. 3. Demonstrates empirical superiority, achieving significantly lower error with fewer parameters in supervised regression and 3-6x improvement in physics-informed neural network benchmarks, while learning interpretable exponents.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66c3ba4917449496481593b1432346dc5ef9301c3c66f4713e327bbb234969d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66c3ba4917449496481593b1432346dc5ef9301c3c66f4713e327bbb234969d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces Müntz-Szász Networks (MSN), a neural architecture that replaces fixed activation functions with learnable fractional power bases to better approximate singular functions common in physics. It proves MSN&#x27;s universal approximation capability and shows it achieves much lower error with fewer parameters than standard MLPs on regression and physics-informed tasks, demonstrating the value of theory-guided design.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video understanding], [streaming video, multimodal large language models, event segmentation, hierarchical representation, elastic-scale]</p>
</li>
<li class="">
<p><strong>authors:</strong> Naishan Zheng, Jie Huang, Qingpei Guo, Feng Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology of China, Ant Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22226" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22226</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/zheng980629/VideoScaffold" target="_blank" rel="noopener noreferrer" class="">https://github.com/zheng980629/VideoScaffold</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes VideoScaffold, a dynamic representation framework for streaming video understanding in MLLMs that adaptively adjusts event granularity. 2. Introduces Elastic-Scale Event Segmentation (EES) for prediction-guided, dynamic boundary refinement. 3. Introduces Hierarchical Event Consolidation (HEC) for progressively aggregating segments into multi-level abstractions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of understanding long, streaming videos with MLLMs by proposing VideoScaffold, a framework that dynamically segments and hierarchically consolidates video events to adapt granularity and preserve semantics. It achieves state-of-the-art performance on benchmarks and can extend image-based MLLMs to video comprehension.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [retrieval-augmented generation (RAG), network traffic analysis, large language models (LLMs), hierarchical retrieval, explainable AI]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shaghayegh Shajarian, Kennedy Marsh, James Benson, Sajad Khorsandroo, Mahmoud Abdelsalam</p>
</li>
<li class="">
<p><strong>institution:</strong> North Carolina A&amp;T State University, University of Texas at San Antonio</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22223" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22223</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/270771/llm-traffictraffic" target="_blank" rel="noopener noreferrer" class="">https://github.com/270771/llm-traffictraffic</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ReGAIN, a multi-stage framework combining traffic summarization, RAG, and LLM reasoning for transparent network traffic analysis. 2. Introduces a hierarchical retrieval pipeline with metadata filtering, MMR sampling, cross-encoder reranking, and an abstention mechanism to ground responses and reduce hallucinations. 3. Demonstrates high accuracy (95.95%-98.82%) on real-world attack traces and outperforms traditional baselines while providing explainable, evidence-cited outputs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56c5ba03e3d4a510212509143bfecf0fa8b76f9171aa38dd765dadecc7b1ab32_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56c5ba03e3d4a510212509143bfecf0fa8b76f9171aa38dd765dadecc7b1ab32_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents ReGAIN, a framework that uses retrieval-augmented generation (RAG) and LLMs to analyze network traffic. It converts traffic into summaries, retrieves relevant evidence from a vector database, and generates interpretable, grounded analyses. The method achieves high accuracy on attack detection and provides explainable results, outperforming traditional rule-based and ML approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [representation analysis], [sentence embeddings, probing, hierarchical geometry, transformer models, cognitive states]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sophie Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Georgia Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22227" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22227</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Constructed a novel dataset of 480 sentences annotated with continuous energy scores and discrete tier labels for seven ordered cognitive categories. 2. Demonstrated that both continuous scores and discrete tier labels are reliably decodable from fixed transformer sentence embeddings using linear and nonlinear probes, with nonlinear probes providing consistent gains. 3. Provided statistical and qualitative evidence (via permutation tests, UMAP visualizations, and confusion matrices) that the embedding space exhibits a hierarchical geometric organization aligned with human-defined cognitive attributes, beyond surface word statistics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fec0b31a0f9f75593cbc3cdadecae63f4a1a7b7b6d910165a358bc72dde0f1d7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fec0b31a0f9f75593cbc3cdadecae63f4a1a7b7b6d910165a358bc72dde0f1d7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether transformer-based sentence embeddings encode a hierarchical structure aligned with cognitive states. The authors construct an annotated dataset and use linear and nonlinear probes to decode continuous scores and discrete labels from embeddings, finding reliable recoverability and a structured geometric gradient. The results show that transformer embedding spaces exhibit a systematic organization corresponding to interpretable cognitive attributes.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] We are not able to identify AI-generated images</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image forensics], [AI-generated images, human evaluation, MidJourney, CC12M, synthetic media detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Adrien Pavão</p>
</li>
<li class="">
<p><strong>institution:</strong> (Institution not explicitly stated in provided content. Author name is Adrien Pavão; no affiliation or email domain is given. Therefore, institution cannot be reliably inferred.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22236" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22236</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a controlled web-based experiment to empirically test human ability to distinguish real photographs from AI-generated portraits, finding performance near random chance (54% accuracy). 2. Created and released a curated, challenging dataset of 120 images (real from CC12M and AI-generated counterparts from MidJourney) designed to be difficult for humans. 3. Demonstrated that human judgment is insufficient for reliable detection of synthetic media, highlighting the need for greater public awareness and ethical guidelines as AI-generated content becomes more realistic.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b100d4e4882d297b51639fb736da62f90b11f1d810da4b6665f9da69ef3f31_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b100d4e4882d297b51639fb736da62f90b11f1d810da4b6665f9da69ef3f31_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper tests the assumption that humans can easily identify AI-generated images through an interactive web experiment where participants classified 20 images as real or AI-generated. Using a carefully curated dataset of 120 difficult portrait images (real from CC12M and AI-generated from MidJourney), the study found an average human accuracy of only 54%, barely above random guessing. The results show that humans struggle to reliably detect AI-generated content, indicating that human judgment alone is becoming insufficient and underscoring the need for awareness and ethical guidelines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] DiRL: An Efficient Post-Training Framework for Diffusion Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [Diffusion Language Models, FlexAttention, Group Relative Policy Optimization, LMDeploy, blockwise training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ying Zhu, Jiaxin Wan, Xiaoran Liu, Siyanag He, Qiqi Wang, Xu Guo, Tianyi Liang, Zengfeng Huang, Ziwei He, Xipeng Qiu</p>
</li>
<li class="">
<p><strong>institution:</strong> Fudan University, Shanghai Innovation Institute, OpenMoss Team</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22234" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22234</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/OpenMOSS/DiRL" target="_blank" rel="noopener noreferrer" class="">https://github.com/OpenMOSS/DiRL</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DiRL, an efficient post-training framework for Diffusion Language Models (dLLMs) that integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. 2. Introduces DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation specifically designed for dLLMs. 3. Demonstrates state-of-the-art math reasoning performance for dLLMs by training DiRL-8B-Instruct, surpassing comparable models like Qwen2.5 series on benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0ae2d009d9099214203b1dcca9a8b460cf0609d952e240f981b2689f247e17d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0ae2d009d9099214203b1dcca9a8b460cf0609d952e240f981b2689f247e17d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the underdeveloped and inefficient post-training landscape for Diffusion Language Models (dLLMs). It proposes DiRL, an efficient framework combining accelerated training and optimized inference, and introduces DiPO, a tailored reinforcement learning method. The resulting model, DiRL-8B-Instruct, achieves state-of-the-art math performance among dLLMs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Enhanced geometry prediction in laser directed energy deposition using meta-learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [meta-learning], [meta-learning, model-agnostic meta-learning, reptile, laser-directed energy deposition, bead geometry prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abdul Malik Al Mardhouf Al Saadi, Amrita Basak</p>
</li>
<li class="">
<p><strong>institution:</strong> The Pennsylvania State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22241" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22241</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a cross-dataset knowledge transfer model for L-DED bead geometry prediction using meta-learning to address data scarcity and heterogeneity. 2. Investigated and applied two gradient-based meta-learning algorithms (MAML and Reptile) for rapid adaptation to new deposition conditions with limited data. 3. Demonstrated strong generalization performance of the meta-learning models across diverse L-DED processes (powder-fed, wire-fed, hybrid) using minimal training examples, outperforming conventional neural networks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4239294fb44dd0fe97ebc3a123162ba7aaa82e51c2805d4460072daeffe6de9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4239294fb44dd0fe97ebc3a123162ba7aaa82e51c2805d4460072daeffe6de9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of predicting bead geometry in laser-directed energy deposition (L-DED) where experimental data is scarce and heterogeneous. It proposes using meta-learning algorithms, specifically MAML and Reptile, to enable rapid model adaptation to new printing conditions with very few training examples. The results show that this approach achieves accurate predictions and outperforms traditional neural networks under similar data constraints, demonstrating effective knowledge transfer across different L-DED settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical imaging], [algorithmic fairness, subgroup performance analysis, JustEFAB framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shaurya Gaur, Michel Vitale, Alessa Hering, Johan Kwisthout, Colin Jacobs, Lena Philipp, Fennie van der Graaf</p>
</li>
<li class="">
<p><strong>institution:</strong> Radboud University Medical Center, Radboud University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22242" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22242</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a fairness evaluation of three lung cancer risk estimation models (Sybil, Venkadesh21, PanCan2b) using the JustEFAB framework to assess ethically significant biases. 2. Identified and quantified statistically significant performance disparities across demographic subgroups (e.g., gender, race) that were not explained by available clinical confounders. 3. Highlighted the critical need for monitoring and improving model fairness in lung cancer screening AI to ensure equitable clinical application.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study evaluates the fairness of AI models for lung cancer risk estimation from CT scans. Using the JustEFAB framework, it assessed performance disparities across demographic groups and found significant, unexplained biases in two deep learning models. The findings underscore the importance of algorithmic fairness in medical AI to ensure equitable screening outcomes.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Predicting Mycotoxin Contamination in Irish Oats Using Deep and Transfer Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [transfer learning], [TabPFN, FT-Transformer, permutation-based variable importance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alan Inglis, Fiona Doohan, Subramani Natarajan, Breige McNulty, Chris Elliott, Anne Nugent, Julie Meneely, Brett Greer, Stephen Kildea, Diana Bucur, Martin Danaher, Melissa Di Rocco, Lisa Black, Adam Gauley, Naoise McKenna, Andrew Parnell</p>
</li>
<li class="">
<p><strong>institution:</strong> Maynooth University, University College Dublin, Queen&#x27;s University Belfast, Teagasc, Agri-Food and Biosciences Institute (AFBI)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22243" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22243</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Evaluated and compared the performance of multiple deep learning and transfer learning models (MLP, TabPFN, TabNet, FT-Transformer) for predicting mycotoxin contamination in oats. 2. Applied these models to a multi-response prediction task using a dataset of environmental, agronomic, and geographical predictors from Irish oat samples. 3. Conducted a permutation-based variable importance analysis, identifying weather history in the 90-day pre-harvest period and seed moisture content as the most influential predictors.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64407e4c1b4c79c19f30f80502744b2e2eadb71d078c9e48b84cad3e1286130e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64407e4c1b4c79c19f30f80502744b2e2eadb71d078c9e48b84cad3e1286130e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study uses neural networks and transfer learning to predict mycotoxin contamination in Irish oat crops. The models, including TabPFN, TabNet, and FT-Transformer, were evaluated on a multi-response task, with TabPFN achieving the best overall performance. The analysis found that weather patterns before harvest and seed moisture are the most critical factors for prediction.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Masking Teacher and Reinforcing Student for Distilling Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [knowledge distillation, reinforcement learning, vision-language models, progressive masking, offline RL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Byung-Kwan Lee, Yu-Chiang Frank Wang, Ryo Hachiuma</p>
</li>
<li class="">
<p><strong>institution:</strong> NVIDIA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22238" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22238</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Masters, a mask-progressive RL distillation framework that first masks non-dominant teacher weights to reduce complexity and then progressively restores them for stable student learning. 2. Introduces an offline RL stage with complementary accuracy and distillation rewards, leveraging pre-generated responses from masked teachers for efficient guidance. 3. Demonstrates that progressive teacher scaling (e.g., from 14B to 38B) yields smoother convergence and stronger generalization than one-shot distillation, providing a scalable path to efficient VLMs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of distilling large vision-language models (VLMs) into compact ones by proposing Masters, a framework that uses progressive masking of the teacher model and offline reinforcement learning. This method enables stable knowledge transfer and efficient training, resulting in small VLMs that achieve strong performance, sometimes surpassing larger models, while being far more efficient for deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [interpretability], [mechanistic multiplicity, explanatory stability, stochastic optimization, model explanations, diagnostic framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chama Bensmail</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Hertfordshire, Omics Data Solutions LTD</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22240" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22240</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/bensmailchama-boop/EvoXplain" target="_blank" rel="noopener noreferrer" class="">https://github.com/bensmailchama-boop/EvoXplain</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces EvoXplain, a diagnostic framework for measuring the stability of model explanations across repeated training runs, treating explanations as samples from the optimization process. 2. Demonstrates that high-accuracy models (e.g., Logistic Regression, Random Forests) can rely on multiple distinct internal mechanisms, revealing explanatory multimodality not captured by single-model or averaged explanations. 3. Reframes interpretability as a property of a model class under repeated instantiation, challenging the assumption that a single high-accuracy model yields a unique or trustworthy explanation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a39d3b0c4608e98a5ffde3a753078019f45b0c48774cb02ee158633c35e823d2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a39d3b0c4608e98a5ffde3a753078019f45b0c48774cb02ee158633c35e823d2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces EvoXplain, a framework to diagnose if models achieving similar high accuracy do so via the same or different internal mechanisms by analyzing explanation stability across training runs. It finds that even simple, stable models like Logistic Regression can exhibit multiple distinct explanatory modes on datasets like Breast Cancer and COMPAS, showing that single-model explanations can be misleading. This work highlights explanatory instability as a measurable property and reframes interpretability as a characteristic of a model class rather than a single trained instance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [uncertainty estimation, calibration, linear probe, brier score, llm-as-judge]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bhaktipriya Radharapu, Eshika Saxena, Kenneth Li, Chenxi Whitehouse, Adina Williams, Nicola Cancedda</p>
</li>
<li class="">
<p><strong>institution:</strong> Meta (FAIR at Meta, Meta Superintelligence Labs)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22245" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22245</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a method using linear probes on LLM hidden states to provide calibrated uncertainty estimates for LLM judges, requiring no additional model training. 2. Demonstrates superior calibration and ≈10x computational savings compared to baseline methods like verbalized confidence. 3. Shows the method generalizes robustly across different model architectures, training paradigms, and unseen evaluation domains.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33ea018b43295c51d076b3840537c861c2e2c7f22ca2bdd183164d1f1feed91d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33ea018b43295c51d076b3840537c861c2e2c7f22ca2bdd183164d1f1feed91d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of obtaining efficient and well-calibrated uncertainty estimates for LLM-based judges. It proposes using linear probes trained with a Brier score loss on the model&#x27;s hidden states. The method achieves better calibration with significant computational savings and provides a practical plug-and-play solution for production deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Amortized Inference for Model Rocket Aerodynamics: Learning to Estimate Physical Parameters from Simulation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [physics-informed machine learning], [amortized inference, sim-to-real transfer, model rocketry, neural network, parameter estimation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rohit Pandey, Rohan Pandey</p>
</li>
<li class="">
<p><strong>institution:</strong> Bellevue High School, University of Washington</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22248" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22248</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulates model rocket parameter estimation as an amortized inference problem and demonstrates neural networks can invert physics simulations from sparse observations. 2. Proposes a simulation-based amortized inference approach that enables zero-shot sim-to-real transfer for aerodynamic parameter estimation. 3. Provides quantitative analysis of the sim-to-real gap and shows the learned model reduces apogee prediction error compared to a traditional baseline (OpenRocket).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59b12172a46941853e291b33e21d912e272992d414baf27e4495c7137fe4266c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59b12172a46941853e291b33e21d912e272992d414baf27e4495c7137fe4266c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a simulation-based amortized inference method that trains a neural network on synthetic flight data to predict aerodynamic parameters like drag coefficient from a single apogee measurement. The trained model is applied directly to real flights without fine-tuning, achieving promising zero-shot sim-to-real transfer and reducing apogee prediction error compared to traditional tools.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Affine Divergence: Aligning Activation Updates Beyond Normalisation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [optimization theory], [activation updates, gradient descent, normalisation, PatchNorm, affine divergence]</p>
</li>
<li class="">
<p><strong>authors:</strong> George Bird</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Manchester</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22247" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22247</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies a systematic mismatch between mathematically ideal and effective activation updates during gradient descent, providing a new theoretical framework for understanding optimization. 2. Derives normalisation from first principles as a solution to this mismatch and proposes a functionally distinct, non-scale-invariant alternative that outperforms conventional normalisers. 3. Introduces &quot;PatchNorm&quot;, a new compositionally inseparable normaliser for convolutional layers, and argues for reframing normalisers as activation-function-like maps with parameterised scaling.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48ed42bcb2d9358f1931e9e0ea31246380c7ef78409f6004933a0fb2461eb9d1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48ed42bcb2d9358f1931e9e0ea31246380c7ef78409f6004933a0fb2461eb9d1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a mismatch between the ideal steepest-descent direction for activations and their effective updates during gradient descent. It proposes a theoretical framework that derives normalisation as a solution and introduces a new alternative, including &quot;PatchNorm&quot; for convolutions, which empirically outperforms standard normalisers. This reframes normalisation&#x27;s role and questions the standard affine+nonlinear model-building approach.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Analyzing Skill Element in Online Fantasy Cricket</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [game theory and decision making], [statistical framework, team selection strategies, dynamic tournament model, softmax reweighting, IPL dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sarthak Sarkar, Supratim Das, Purushottam Saha, Diganta Mukherjee, Tridib Mukherjee</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Statistical Institute, Kolkata</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22254" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22254</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Development of a statistical framework to assess the role of skill in online fantasy cricket. 2. Construction and analysis of a range of deterministic and stochastic team selection strategies. 3. Introduction of a dynamic tournament model with agent populations evolving via a softmax reweighting mechanism.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e92b7b67260659549974352e85a2e375a90fd015a5561a1d909fb586dcfd635_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e92b7b67260659549974352e85a2e375a90fd015a5561a1d909fb586dcfd635_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper develops a statistical framework to analyze whether success in online fantasy cricket is driven by skill or chance. It constructs various team selection strategies and a dynamic tournament model, testing them on IPL 2024 data. The results provide quantitative evidence supporting the presence of a skill element in these platforms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Graph Attention-based Adaptive Transfer Learning for Link Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [graph attention network, link prediction, transfer learning, graph transformer, contrastive loss]</p>
</li>
<li class="">
<p><strong>authors:</strong> Huashen Lu, Wensheng Gan, Guoting Chen, Zhichao Huang, Philip S. Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> Jinan University, Great Bay University, JD Technology, University of Illinois Chicago</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22252" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22252</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/DSI-Lab1/GAATNet" target="_blank" rel="noopener noreferrer" class="">https://github.com/DSI-Lab1/GAATNet</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GAATNet, a novel graph attention adaptive transfer network combining pre-training and fine-tuning for cross-dataset knowledge transfer in link prediction. 2. Incorporates distant neighbor embeddings as biases in self-attention to capture global node features. 3. Introduces a lightweight self-adapter module during fine-tuning to improve training efficiency and generalization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e9ea1091686509af1da226ce7553577b4005c5646524a5c6718473e451d3c39_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e9ea1091686509af1da226ce7553577b4005c5646524a5c6718473e451d3c39_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses challenges in link prediction on large-scale sparse graphs and cross-dataset transfer learning by proposing GAATNet, which integrates graph attention with adaptive transfer strategies. The method uses distant neighbor embeddings and a self-adapter module to enhance global feature capture and training efficiency. Experiments on seven datasets show state-of-the-art performance, offering a scalable solution for integrating GNNs with transfer learning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human motion segmentation], [temporal vision semantics, subspace clustering, large language model, temporal regularizer, feedback framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zheng Xing, Weibing Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Shenzhen University, Shenzhen MSU-BIT University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22249" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22249</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel method to learn Temporal Vision Semantics (TVS) from human motion sequences by querying a Large Language Model (LLM) to determine motion consistency between consecutive frames. 2. Develops a TVS-integrated subspace clustering framework that incorporates a temporal regularizer and constraint to enforce similarity among temporal neighbors in the subspace embedding and segmentation. 3. Introduces a feedback-enabled optimization framework that iteratively refines the subspace embedding based on the segmentation output to improve performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fce87220cc17109bbd9138b27d5aa353003bb134fb0924f68b7fc59fdfd3ee0d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fce87220cc17109bbd9138b27d5aa353003bb134fb0924f68b7fc59fdfd3ee0d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of unsupervised human motion segmentation by integrating temporal semantic information into subspace clustering. The proposed method uses a Large Language Model to extract textual motion descriptions from consecutive frames and incorporates this learned temporal neighbor information as constraints within the clustering framework. Experimental results show the method outperforms state-of-the-art approaches on four benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [biomedical knowledge graph, graph attention network, gene perturbation, multimodal embeddings, PrimeKG++]</p>
</li>
<li class="">
<p><strong>authors:</strong> Pascal Passigan, Kevin zhu, Angelina Ning</p>
</li>
<li class="">
<p><strong>institution:</strong> Massachusetts Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22251" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22251</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a novel framework for gene perturbation prediction by merging PrimeKG++ with LINCS L1000 data into a heterogeneous biomedical knowledge graph, moving beyond binary drug-disease association tasks. 2. Demonstrates the application of a Graph Attention Network (GAT) to predict delta gene expression profiles for drug-cell pairs, outperforming MLP baselines. 3. Provides interpretability through ablation studies (edge shuffling, node feature randomization) showing the critical role of biomedical KG edges in enhancing perturbation-level prediction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a8b52522e1bdfc53ae9978a144c2011810eca2532cb9819ad999bf9b2b6cbb6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a8b52522e1bdfc53ae9978a144c2011810eca2532cb9819ad999bf9b2b6cbb6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the gap in predicting detailed gene expression changes (perturbations) caused by drugs by constructing a merged biomedical knowledge graph from PrimeKG++ and LINCS L1000 data. The proposed method uses a Graph Attention Network to predict delta expression profiles for drug-cell pairs, which outperforms baseline models and demonstrates the value of graph structure for mechanistic understanding.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [prompt engineering], [Logic Sketch Prompting, deterministic prompting, interpretability, rule adherence, clinical decision support]</p>
</li>
<li class="">
<p><strong>authors:</strong> Satvik Tripathi</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Pennsylvania</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22258" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22258</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/satviktri/LSP" target="_blank" rel="noopener noreferrer" class="">https://github.com/satviktri/LSP</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Logic Sketch Prompting (LSP), a lightweight prompting framework that introduces typed variables and deterministic condition evaluators for structured reasoning., 2. Incorporates a rule-based validator to produce traceable and repeatable outputs, enhancing auditability., 3. Demonstrates significant performance gains over standard prompting methods (zero-shot, chain-of-thought, concise) on pharmacologic logic-compliance tasks across multiple open-weight LLMs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b5d49d54027e4f7e6b110a48568a0255a45010197e26e8bc344e0cd3e1785a9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b5d49d54027e4f7e6b110a48568a0255a45010197e26e8bc344e0cd3e1785a9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the unreliability of LLMs on tasks requiring strict rule adherence and determinism. It proposes Logic Sketch Prompting (LSP), a framework using typed variables and rule-based validation to produce traceable outputs. Evaluations on clinical tasks show LSP significantly outperforms standard prompting methods in accuracy and F1 score, making it suitable for safety-critical systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [reasoning], [chain-of-thought, synthetic data, distribution shift, fine-tuning, reasoning robustness]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abhranil Chandra, Ayush Agrawal, Arian Hosseini, Sebastian Fischmeister, Rishabh Agarwal, Navin Goyal, Aaron Courville</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Waterloo, University of Massachusetts Amherst, MILA - Quebec AI Institute, Université de Montréal, Microsoft Research India, Google DeepMind, Periodic Labs</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22255" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22255</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that training on synthetic chain-of-thought traces from more capable models, even when they lead to incorrect final answers, can improve a language model&#x27;s reasoning performance more than training on human-annotated datasets. 2. Proposes and validates two hypotheses for this phenomenon: the distributional alignment of synthetic data with the model, and the partial validity of reasoning steps within flawed traces. 3. Shows that paraphrasing human traces to align with the model&#x27;s distribution improves performance, and investigates model tolerance to increasingly flawed reasoning steps.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf038d101bb93ad9f8c253c481419dec618655c74a6b867d0128b6358a3fa331_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf038d101bb93ad9f8c253c481419dec618655c74a6b867d0128b6358a3fa331_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper challenges the assumption that correctness is the primary determinant of data quality for training language models on reasoning tasks. It shows that fine-tuning on synthetic, incorrect chain-of-thought traces from stronger models can outperform training on correct human-annotated data, primarily because the synthetic data&#x27;s distribution is closer to the model&#x27;s own. The key conclusion is that aligning the training data distribution with the model&#x27;s is more critical for performance than the correctness of the final answers.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Cardiac mortality prediction in patients undergoing PCI based on real and synthetic data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [clinical prediction], [synthetic data, class imbalance, permutation feature importance, tabular data, mortality prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Daniil Burakov, Ivan Petrov, Dmitrii Khelimskii, Ivan Bessonov, Mikhail Lazarev</p>
</li>
<li class="">
<p><strong>institution:</strong> HSE University, Meshalkin National Medical Research Center, Tyumen Cardiology Research Center</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22259" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22259</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed and evaluated machine learning models for predicting 3-year cardiac mortality after PCI using a dataset of patients with bifurcation lesions. 2. Demonstrated that augmenting the training set with synthetic samples effectively addresses class imbalance, improving minority-class recall and probability quality with minimal impact on AUROC. 3. Identified key clinical predictors (Age, Ejection Fraction, Peripheral Artery Disease, Cerebrovascular Disease) through feature importance analysis and highlighted the brittleness of models on external validation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8124ba060c0a4a10502266c7255b089103b4cb860b15006e80971c9e28cfdc68_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8124ba060c0a4a10502266c7255b089103b4cb860b15006e80971c9e28cfdc68_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study developed machine learning models to predict cardiac death within three years for patients undergoing percutaneous coronary intervention (PCI). To handle class imbalance, the authors augmented the real patient data with synthetic samples, which improved the models&#x27; ability to identify high-risk patients. The analysis identified key risk factors and demonstrated that data augmentation can reduce model brittleness in imbalanced clinical prediction tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Physics Constraint Paradox: When Removing Explicit Constraints Improves Physics-Informed Data for Machine Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [physics-informed machine learning], [physics-constrained data generation, ablation study, grating coupler, Fabry-Perot oscillations, energy conservation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rahul D Ray</p>
</li>
<li class="">
<p><strong>institution:</strong> BITS Pilani, Hyderabad Campus</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22261" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22261</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies the &quot;physics constraint paradox,&quot; demonstrating that explicit energy conservation enforcement can be mathematically redundant in physically consistent models. 2. Shows that removing Fabry-Perot oscillations significantly reduces bandwidth variability and improves downstream ML model accuracy for bandwidth prediction. 3. Reveals a subtle pitfall where standard noise-addition-and-renormalization pipelines can introduce unphysical negative absorption values.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c510a23d89c2ae438eca78e6b993f8ce6b30f94405a1290b3509d74226bafc1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c510a23d89c2ae438eca78e6b993f8ce6b30f94405a1290b3509d74226bafc1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates physics-constrained data generation for machine learning through an ablation study of a grating coupler spectrum generator. It finds that explicitly enforcing certain physical constraints, like energy conservation, can be redundant, while others, like Fabry-Perot oscillations, can hinder machine learning performance for specific prediction tasks. The main conclusion is that increased physical realism in data generation does not always improve ML learnability, and ML performance can be used to diagnose the relevance of physical constraints.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph representation learning], [temporal motifs, dynamic graphs, llm agent, structure-aware dispatcher, prompting techniques]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bing Hao, Minglai Shao, Zengyi Wo, Yunlong Chu, Yuhang Liu, Ruijie Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tianjin University, Beihang University, Guangxi Normal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22266" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22266</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Wjerry5/LLMTM" target="_blank" rel="noopener noreferrer" class="">https://github.com/Wjerry5/LLMTM</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LLMTM, a comprehensive benchmark for evaluating LLMs on six tasks across nine types of temporal motifs in dynamic graphs. 2. Develops a tool-augmented LLM agent that uses engineered prompts to achieve high accuracy on temporal motif analysis tasks. 3. Introduces a structure-aware dispatcher that intelligently routes queries between standard LLM prompting and the more powerful agent to balance accuracy and cost.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ebbb05fef920a296d5863029d0c69e932a75230132aa0658a09e6bd8d04010c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ebbb05fef920a296d5863029d0c69e932a75230132aa0658a09e6bd8d04010c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the use of Large Language Models (LLMs) for analyzing temporal motifs in dynamic graphs, an area that is relatively unexplored. The authors propose a new benchmark (LLMTM), develop a high-accuracy but costly tool-augmented LLM agent, and then introduce a structure-aware dispatcher to reduce cost while maintaining performance. Their experiments show the dispatcher effectively maintains high accuracy while reducing cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [RGB-LWIR fusion, multispectral imagery, YOLO, adaptive framework, illumination conditions]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aahan Sachdeva, Dhanvinkumar Ganeshkumar, James E. Gallagher, Tyler Treat, Edward J. Oughton</p>
</li>
<li class="">
<p><strong>institution:</strong> George Mason University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22263" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22263</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An adaptive framework that dynamically selects the optimal RGB-LWIR fusion ratio and detection model based on real-time illumination conditions. 2. Creation of a comprehensive dataset and model set, training 33 YOLO models on over 22,000 annotated images across three light levels with eleven fusion ratios. 3. Demonstrated significant performance improvements over RGB-only and thermal-only baselines, particularly in full-light and dim-light conditions, enhancing detection reliability for autonomous systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of RGB and thermal-only object detection in variable lighting by proposing an adaptive framework that fuses RGB and LWIR video streams at multiple ratios and selects the best model for the current illumination. The method, evaluated on a large dataset, showed that optimized fusion models significantly outperformed baseline models in full and dim light, improving detection confidence and reliability for autonomous robotic vision.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LuxIA: A Lightweight Unitary matriX-based Framework Built on an Iterative Algorithm for Photonic Neural Network Training</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [photonic neural networks, transfer matrix, Slicing method, back-propagation, simulation framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tzamn Melendez Carmona, Federico Marchesin, Marco P. Abrate, Peter Bienstman, Stefano Di Carlo, Alessandro Savino Senior</p>
</li>
<li class="">
<p><strong>institution:</strong> Politecnico di Torino, Ghent University - imec, University College London</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22264" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22264</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the Slicing method, an efficient transfer matrix computation approach compatible with back-propagation for training Photonic Neural Networks (PNNs). 2. Introduces LuxIA, a unified simulation and training framework that integrates the Slicing method to enable scalable PNN training. 3. Demonstrates through experiments that LuxIA surpasses existing tools in speed and scalability for training large-scale PNNs on standard datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/362a4190a58349fa96aae555a8a4643a7fdd50b962ff88817d9c12e4678fbe98_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/362a4190a58349fa96aae555a8a4643a7fdd50b962ff88817d9c12e4678fbe98_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the scalability challenges in simulating and training large-scale Photonic Neural Networks (PNNs) by introducing the Slicing method for efficient transfer matrix computation. The method is integrated into the LuxIA framework, which significantly reduces memory usage and training time. Experimental results show LuxIA outperforms existing tools, enabling the exploration of larger and more complex photonic architectures.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Hierarchical Stacking Optimization Using Dirichlet&#x27;s Process (SoDip): Towards Accelerated Design for Graft Polymerization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [bayesian optimization], [Dirichlet Process Mixture Model, Gaussian Process Regression, Transformer, TabNet, XGBoost]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amgad Ahmed Ali Ibrahim, Hein Htet, Ryoji Asahi</p>
</li>
<li class="">
<p><strong>institution:</strong> Nagoya University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22279" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22279</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a hierarchical stacking optimization framework (SoDip) integrating a Transformer for text, TabNet/XGBoost for multimodal features, and GPR with DPMM for uncertainty. 2. Curated a diverse dataset for radiation-induced grafting using automated tools to handle numerical and textual variables. 3. Demonstrated ~33% performance improvement over standard GPR with calibrated confidence intervals for identifying low-reproducibility regimes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e1ff3feaf3b0e588ef295bb1595e516fbcdb32563ea40c93095217ac66e1fc5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e1ff3feaf3b0e588ef295bb1595e516fbcdb32563ea40c93095217ac66e1fc5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses reproducibility issues in radiation-induced graft polymerization by proposing SoDip, a hierarchical data-driven framework that combines Transformer encoders, multimodal feature models, and Bayesian optimization with uncertainty quantification. The method integrates sparse textual and numerical data, showing a 33% improvement over Gaussian Process Regression and providing reliable confidence estimates. This establishes a foundation for morphology-aware, reproducible design in polymer research.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Valori: A Deterministic Memory Substrate for AI Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [memory &amp; caching], [deterministic memory, fixed-point arithmetic, vector embeddings, approximate nearest neighbor search, state machine]</p>
</li>
<li class="">
<p><strong>authors:</strong> Varshith Gudur</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher (Valori Kernel Project)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22280" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22280</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/varshith-Git/Valori-Kernel" target="_blank" rel="noopener noreferrer" class="">https://github.com/varshith-Git/Valori-Kernel</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and characterizes the fundamental non-determinism in AI memory systems caused by hardware-dependent floating-point arithmetic, which leads to divergent memory states and retrieval results. 2. Proposes Valori, a deterministic AI memory substrate that replaces floating-point operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. 3. Demonstrates that Valori guarantees bit-identical memory states, snapshots, and search results across different hardware platforms (e.g., x86 vs. ARM), establishing deterministic memory as a necessary primitive for trustworthy AI.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb7ed17a8a06e6a2b8760f08fa9345391995aee74a3542cacf55dd051b383f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb7ed17a8a06e6a2b8760f08fa9345391995aee74a3542cacf55dd051b383f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies non-determinism in AI memory systems due to hardware-dependent floating-point arithmetic, which compromises replayability and auditability. It proposes Valori, a memory substrate using fixed-point arithmetic and a state machine model to guarantee bit-identical behavior across platforms. The work concludes that deterministic memory is essential for building trustworthy AI systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Cluster Aggregated GAN (CAG): A Cluster-Based Hybrid Model for Appliance Pattern Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models], [Generative Adversarial Networks, Non-Intrusive Load Monitoring, Clustering, LSTM, Pattern Generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zikun Guoa, Adeyinka.P. Adedigbaa, Rammohan Mallipeddi</p>
</li>
<li class="">
<p><strong>institution:</strong> Kyungpook National University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22287" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22287</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hybrid GAN framework that routes appliances to specialized branches based on behavioral characteristics (intermittent vs. continuous). 2. Introduces a clustering module for intermittent appliances to group similar activation patterns and allocate dedicated generators, improving modeling of both common and rare modes. 3. Employs a separate LSTM-based generator branch for continuous appliances to capture gradual temporal evolution while maintaining training stability through sequence compression.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37f2c4c207022049ee869567d36df9e77e5a04d1beb0cc584dc57ee6ad1145b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37f2c4c207022049ee869567d36df9e77e5a04d1beb0cc584dc57ee6ad1145b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes the Cluster Aggregated GAN (CAG), a hybrid generative model that synthesizes appliance load patterns by separating intermittent and continuous devices into specialized branches, using clustering for the former and an LSTM for the latter. Experiments on the UVIC dataset show it outperforms baselines in realism, diversity, and training stability. The integration of clustering as an active component also enhances the model&#x27;s interpretability and scalability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [causal inference], [Double Machine Learning, Moderated Mediation, Algorithmic Control, Nonmonotonic Effects, Gig Economy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Arunkumar V, Nivethitha S, Sharan Srinivas, Gangadharan G.R</p>
</li>
<li class="">
<p><strong>institution:</strong> Anna University, National Institute of Technology Tiruchirappalli, University of Missouri</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22290" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22290</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Applied a Double Machine Learning framework to estimate a moderated mediation model without restrictive linear assumptions in organizational research. 2. Uncovered a nonmonotonic relationship between algorithmic oversight, worker wellbeing, and performance, highlighting a &quot;murky middle&quot; of confusing oversight. 3. Demonstrated that simple linear models can be misleading and provided practical insights for designing transparent and explainable algorithmic management systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ffeb8d364908888226033cff39cbb354772ae1044ba1a51632db140d81dca17_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ffeb8d364908888226033cff39cbb354772ae1044ba1a51632db140d81dca17_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the nonlinear effects of algorithmic control on gig workers. Using a Double Machine Learning approach on survey data, it finds that the link between supportive HR practices and worker performance weakens under opaque algorithmic oversight but strengthens again when the oversight is transparent and explainable.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [Masked Diffusion Models, Markov Decision Process, Group Relative Policy Optimization, inference schedule optimization, trajectory-level training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Renping Zhou, Zanlin Ni, Tianyi Chen, Zeyu Liu, Yang Yue, Yulin Wang, Yuxuan Wang, Jingshu Liu, Gao Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University (Leap Lab), Anyverse Dynamics</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22288" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22288</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://co-grpo.github.io" target="_blank" rel="noopener noreferrer" class="">https://co-grpo.github.io</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and addresses the discrepancy between the single-step training and multi-step inference of Masked Diffusion Models (MDMs). 2. Proposes Co-GRPO, a method that reformulates MDM generation as a unified Markov Decision Process to jointly optimize model parameters and inference schedule parameters. 3. Introduces a trajectory-level optimization using Group Relative Policy Optimization that avoids costly backpropagation through the multi-step generation process.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the misalignment between the training and inference procedures of Masked Diffusion Models (MDMs). It proposes Co-GRPO, a method that jointly optimizes the MDM and its inference schedule as a unified Markov Decision Process using Group Relative Policy Optimization. The approach improves generation quality across multiple benchmarks without requiring expensive backpropagation through the full generation trajectory.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scientific machine learning], [Physics-informed neural networks, Kolmogorov-Arnold networks, Adaptive weighting, B-splines, Partial differential equations]</p>
</li>
<li class="">
<p><strong>authors:</strong> Guokan Chen, Yao Xiao</p>
</li>
<li class="">
<p><strong>institution:</strong> Fujian University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22283" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22283</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DBAW-PIKAN, a novel architecture combining a Kolmogorov-Arnold Network (KAN) with learnable B-splines for enhanced function representation in solving PDEs., 2. Introduces an adaptive weighting strategy with a dynamic decay upper bound to mitigate gradient flow stiffness and spectral bias, addressing key failure modes of PINNs., 3. Demonstrates significant improvements in convergence speed and solution accuracy (at least an order of magnitude) on benchmarks like Klein-Gordon, Burgers, and Helmholtz equations without added computational cost.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb9e65232c0c340c6072237e2ad1388255462aafca80cf91d4b7f3054eb9fe8c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb9e65232c0c340c6072237e2ad1388255462aafca80cf91d4b7f3054eb9fe8c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes DBAW-PIKAN, a novel neural network that integrates a Kolmogorov-Arnold architecture with an adaptive weighting strategy to overcome the stiffness and spectral bias challenges faced by Physics-Informed Neural Networks (PINNs) when solving multi-scale PDEs. The method accelerates convergence and improves solution accuracy by at least an order of magnitude on standard benchmarks without increasing computational complexity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [language model safety], [sparse autoencoder, feature orthogonalization, stealth slip, pragmatic interpretation, statistical co-occurrence]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tsogt-Ochir Enkhbayar</p>
</li>
<li class="">
<p><strong>institution:</strong> Mongol-AI (inferred from email domain)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22293" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22293</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Empirically demonstrates that warning-framed training data fails to teach language models to avoid warned-against behaviors, showing generation rates similar to direct exposure. 2. Provides a mechanistic interpretation using sparse autoencoders, identifying a failure of feature orthogonalization where &quot;describing&quot; and &quot;performing&quot; an action activate overlapping latent features. 3. Identifies and names the &quot;stealth slip&quot; phenomenon, where conversational preambles can rotate activations into subspaces undetectable by linear probes, and shows that training-time feature ablation, not prompting, is required to address the issue.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fddb803a434c3d49e04dd722184b33f238c4b81254540d6bb0d1961a3d09e1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fddb803a434c3d49e04dd722184b33f238c4b81254540d6bb0d1961a3d09e1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates why language models trained on warning-framed examples (e.g., &quot;DO NOT USE&quot;) still learn to generate the warned-against content. Through behavioral experiments and sparse autoencoder analysis, it finds that models learn statistical co-occurrences rather than pragmatic intent, due to overlapping latent features for description and action. The core conclusion is that current architectures prioritize pattern completion over understanding speaker intent, requiring training-time interventions like feature ablation for correction.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multi-Head Spectral-Adaptive Graph Anomaly Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph anomaly detection], [spectral graph neural network, hypernetwork, Chebyshev filter, teacher-student contrastive learning, Barlow Twins loss]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qingyue Cao, Bo Jin, Changwei Gong, Xin Tong, Wenzheng Li, Xiaodong Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> People&#x27;s Public Security University of China, Third Research Institute of the Ministry of Public Security, Shanghai Police College</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22291" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22291</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Multi-Head Spectral-Adaptive Graph Neural Network (MHSA-GNN) that uses a lightweight hypernetwork to dynamically generate instance-specific Chebyshev filter parameters based on a &#x27;spectral fingerprint&#x27;. 2. Introduces a novel dual regularization strategy combining teacher-student contrastive learning (TSC) and Barlow Twins diversity loss (BTD) to prevent mode collapse and ensure representation accuracy and head orthogonality in the multi-head mechanism. 3. Demonstrates through extensive experiments that the method effectively preserves high-frequency anomaly signals and outperforms state-of-the-art methods, especially on highly heterogeneous datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ad6dee88128393dc0036e3430c2763e19882412f9750f09622c52d9ae28dc6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ad6dee88128393dc0036e3430c2763e19882412f9750f09622c52d9ae28dc6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of graph anomaly detection where fixed filters in spectral GNNs cause over-smoothing and fail to adapt to varying graph structures. It proposes MHSA-GNN, which uses a hypernetwork to generate adaptive filters per instance and a dual regularization strategy to stabilize multi-head learning. Experiments show the method preserves critical high-frequency signals and achieves superior performance, particularly on heterogeneous graphs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Hybrid Quantum-Classical Mixture of Experts: Unlocking Topological Advantage via Interference-Based Routing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [Quantum Machine Learning, Mixture-of-Experts, Parameterized Quantum Circuits, Quantum Router, Interference Hypothesis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Reda Heddad, Lamiae Bouanane</p>
</li>
<li class="">
<p><strong>institution:</strong> Al Akhawayn University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22296" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22296</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Design of a Hybrid Quantum-Classical Mixture of Experts (QMoE) architecture with a Quantum Router for an ablation study to isolate the source of quantum advantage. 2. Validation of the Interference Hypothesis, demonstrating the Quantum Router&#x27;s topological advantage and superior parameter efficiency on non-linearly separable data. 3. Empirical analysis of the architecture&#x27;s robustness against quantum noise, confirming its feasibility for near-term (NISQ) hardware.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407a17903b2360f889e7afbc3b5f776273cc33f5cf3daf0fa26caf69bc1ed179_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407a17903b2360f889e7afbc3b5f776273cc33f5cf3daf0fa26caf69bc1ed179_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a Hybrid Quantum-Classical Mixture of Experts (QMoE) architecture that uses a Quantum Router to address limitations like expert imbalance in classical MoE systems. The core finding is that the Quantum Router, leveraging quantum interference, provides a topological advantage for routing complex data more efficiently than classical routers. The method is shown to be robust to noise and feasible for near-term quantum hardware.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human activity recognition], [driver monitoring systems, edge AI, quantization, temporal decision head, confounder-aware labeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vesal Ahsani, Babak Hossein Khalaj</p>
</li>
<li class="">
<p><strong>institution:</strong> Sharif University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22298" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22298</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A deployable single-camera driver behavior recognition pipeline optimized for low-cost edge hardware (Raspberry Pi 5 and Google Coral Edge TPU). 2. A confounder-aware label design to reduce false positives from visually similar actions. 3. A temporal decision head that generates stable alerts based on sustained, confident predictions rather than noisy per-frame outputs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bd52e06dc77080ab346fc3d6fe46b900bf06b5c1eaeb6ae89801ac125ee7c51_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bd52e06dc77080ab346fc3d6fe46b900bf06b5c1eaeb6ae89801ac125ee7c51_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a real-time driver behavior recognition system designed for low-cost edge hardware to address the challenges of compute, power, and cost constraints in vehicles. The method combines a compact vision model, confounder-aware labeling, and a temporal decision head to recognize 17 distraction and drowsiness-related behaviors. The optimized system achieves 16 FPS on a Raspberry Pi 5 and 25 FPS on a Coral Edge TPU, enabling practical deployment for in-cabin monitoring.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Statistical and Machine Learning Analysis of Traffic Accidents on US 158 in Currituck County: A Comparison with HSM Predictions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [transportation safety analytics], [Random Forest, Kernel Density Estimation (KDE), Moran&#x27;s I, Highway Safety Manual (HSM), Negative Binomial Regression]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jennifer Sawyer, Julian Allagan</p>
</li>
<li class="">
<p><strong>institution:</strong> Elizabeth City State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22302" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22302</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Applied and compared advanced statistical and machine learning methods (Random Forest, KDE, Negative Binomial Regression) to rural highway crash data, demonstrating a methodological advancement beyond basic techniques. 2. Validated spatial clustering of accidents using Moran&#x27;s I test and identified specific crash hotspots via KDE, extending previous hotspot analysis. 3. Showed that a Random Forest classifier for injury severity prediction outperformed the standard Highway Safety Manual (HSM) Safety Performance Function (SPF) model.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87320e45eec3961a5ef58f2548cb06120fd2e4baee855440998207874d568dbe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87320e45eec3961a5ef58f2548cb06120fd2e4baee855440998207874d568dbe_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study analyzes traffic accident data from a rural highway using advanced statistical and machine learning techniques, including Random Forest and spatial analysis. The proposed Random Forest model for predicting injury severity achieved 67% accuracy, outperforming the standard HSM model, and spatial analysis confirmed crash clustering near intersections. The results provide actionable insights for targeted safety interventions on US 158.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PDx -- Adaptive Credit Risk Forecasting Model in Digital Lending using Machine Learning Operations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [MLOps, champion-challenger framework, out-of-time validation, probability of default, data drift]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sultan Amed, Chan Yu Hang, Sayantan Banerjee</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Management Indore, National University of Singapore</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22305" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22305</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes PDx, an adaptive, MLOps-driven decision system for end-to-end lifecycle management of credit risk models. 2. Introduces a dynamic champion-challenger framework with regular model updates and out-of-time validation to combat data drift. 3. Empirically demonstrates that decision tree-based ensemble models perform best for default classification but require frequent retraining, and validates PDx&#x27;s effectiveness across multiple lending datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c779eabc4c2fa3f75801b8b61c3eed3c105bd748ce6aa84ff37317acd929db3a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c779eabc4c2fa3f75801b8b61c3eed3c105bd748ce6aa84ff37317acd929db3a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes PDx, an adaptive credit risk forecasting system that uses an MLOps pipeline and a champion-challenger framework to continuously monitor, retrain, and validate models against data drift. The study finds decision tree ensembles are most effective but degrade without updates, and shows PDx mitigates value erosion in digital lending, especially for short-term loans.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LLMBoost: Make Large Language Models Stronger with Boosting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [ensemble learning, boosting, cross-model attention, chain training, near-parallel inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zehao Chen, Tianxiang Ai, Yifei Li, Gongxun Li, Yuyang Wei, Wang Zhou, Guanghui Li, Bin Yu, Zhijun Chen, Hailong Sun, Fuzhen Zhuang, Jianxin Li, Deqing Wang, Yikun Ban</p>
</li>
<li class="">
<p><strong>institution:</strong> Beihang University, China Telecom eSurfing Cloud</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22309" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22309</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A cross-model attention mechanism that allows successor models to access and fuse hidden states from predecessors for hierarchical error correction and knowledge transfer. 2. A chain training paradigm that progressively fine-tunes connected models with an error-suppression objective to rectify predecessor mispredictions efficiently. 3. A near-parallel inference paradigm that pipelines hidden states across models layer by layer, achieving inference efficiency close to single-model decoding.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb6a601c8d3bba7ec992d00772421c31e3e03d00d8a89a06f09ad3fe3c1b6ce1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb6a601c8d3bba7ec992d00772421c31e3e03d00d8a89a06f09ad3fe3c1b6ce1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes LLMBoost, a novel ensemble fine-tuning framework for LLMs that leverages intermediate hidden states across models. Inspired by boosting, it introduces cross-model attention, chain training, and a near-parallel inference pipeline to improve accuracy and reduce latency. Experiments on reasoning tasks show it consistently boosts performance while maintaining efficient inference.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [hardware security, model protection], [logic locking, intellectual property protection, hardware accelerator, model theft, supply chain security]</p>
</li>
<li class="">
<p><strong>authors:</strong> You Li, Guannan Zhao, Yuhao Ju, Yunqi He, Jie Gu, Hai Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> Northwestern University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22307" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22307</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LLA, a hardware-software co-design scheme for protecting generative AI models by embedding key bits into neurons and using invariance transformations to obscure them. 2. Integrates a lightweight, dataflow-compatible locking module into the AI accelerator, using the accelerator with a secret key as a license for model access. 3. Demonstrates that the approach is resilient against oracle-guided key optimization attacks while adding minimal computational overhead (&lt;0.1% for 7,168 key bits).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df323cc56f8d048243dce9e6af97041fca6264165872c422e5f81437cb03ef0d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df323cc56f8d048243dce9e6af97041fca6264165872c422e5f81437cb03ef0d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces LLA, a method to protect generative AI models from supply chain threats like theft and corruption by combining software-based key embedding in neurons with a hardware locking module in the accelerator. This approach uses the accelerator as a license key, ensuring only authorized hardware can run the model correctly. Evaluation shows it effectively resists attacks with negligible performance overhead.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Optimistic Feasible Search for Closed-Loop Fair Threshold Decision-Making</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [optimistic feasible search, closed-loop decision-making, demographic parity, bandit feedback, threshold policy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenzhang Du</p>
</li>
<li class="">
<p><strong>institution:</strong> Mahanakorn University of Technology, International College (MUTIC)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22313" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22313</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Optimistic Feasible Search (OFS), a simple grid-based method for constrained closed-loop threshold learning using optimism under uncertainty. 2. Introduced synthetic and semi-synthetic closed-loop benchmarks with stable contraction dynamics and real-world datasets (German Credit, COMPAS) to evaluate feedback effects. 3. Demonstrated that OFS achieves near-oracle performance with higher reward and lower cumulative constraint violation compared to unconstrained and primal-dual bandit baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/baad942bfd56e4e5f21dcd74fbfaad4a484372898d0862f9bc3c15ebb2b11958_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/baad942bfd56e4e5f21dcd74fbfaad4a484372898d0862f9bc3c15ebb2b11958_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses online learning of a threshold policy under fairness and service constraints in closed-loop decision systems with bandit feedback. It proposes Optimistic Feasible Search (OFS), which selects thresholds based on optimistic confidence bounds to maximize reward while minimizing constraint violations. Experiments show OFS outperforms baselines and achieves near-oracle performance across synthetic and real-world benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LangPrecip: Language-Aware Multimodal Precipitation Nowcasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [weather forecasting], [multimodal nowcasting, rectified flow, semantic motion constraint, latent space integration, large-scale dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xudong Ling, Tianxi Huang, Qian Dong, Tao He, Chaorong Li, Guiduo Duan</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China (UESTC), Chengdu Textile College, Yibin University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22317" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22317</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed LangPrecip, a language-aware multimodal nowcasting framework that uses meteorological text as a semantic motion constraint to guide precipitation evolution. 2. Introduced LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. 3. Formulated nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm for efficient and physically consistent multimodal integration.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes LangPrecip, a novel framework that integrates meteorological text descriptions with radar data to constrain precipitation nowcasting. By formulating the problem as semantically constrained trajectory generation using Rectified Flow, it achieves significant performance gains, especially for heavy rainfall at long lead times, as demonstrated on Swedish and MRMS datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Decomposing Uncertainty in Probabilistic Knowledge Graph Embeddings: Why Entity Variance Is Not Enough</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [knowledge graph embeddings], [probabilistic embeddings, uncertainty quantification, out-of-distribution detection, semantic uncertainty, structural uncertainty]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chorok Lee</p>
</li>
<li class="">
<p><strong>institution:</strong> Korea Advanced Institute of Science and Technology (KAIST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22318" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22318</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and formalizes the fundamental limitation of relation-agnostic uncertainty in probabilistic KG embeddings, proving an impossibility result for detecting novel relational contexts using only entity-level statistics. 2. Proposes a novel decomposition of uncertainty into complementary semantic (entity variance) and structural (entity-relation co-occurrence) components, proving their non-redundancy and the superiority of their combination. 3. Introduces the CAGP method that combines semantic and structural uncertainty with learned weights, achieving significant improvements (60-80% relative gain) in temporal OOD detection and selective prediction performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5efb28f1949226a86993ed6d92d592762282d32746964ad0c85ab5a6de90ee36_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5efb28f1949226a86993ed6d92d592762282d32746964ad0c85ab5a6de90ee36_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that probabilistic knowledge graph embeddings use relation-agnostic entity variances, which conflates two distinct types of out-of-distribution data: emerging entities and novel relational contexts. To address this, the authors propose decomposing uncertainty into semantic and structural components and introduce the CAGP method to combine them. This approach achieves a 60-80% relative improvement in temporal OOD detection performance over existing baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [self-verifying agent, proactive evidence seeking, LLM-as-a-Judge, 3C Principles, agentic reinforcement learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University, Tencent</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22322" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22322</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://huggingface.co/collections/yolay/smartsnap" target="_blank" rel="noopener noreferrer" class="">https://huggingface.co/collections/yolay/smartsnap</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed SmartSnap, a paradigm shift from passive, post-hoc task verification to proactive, in-situ self-verification by the agent itself. 2. Introduced the Self-Verifying Agent, a new agent type with dual missions to complete tasks and prove accomplishment via curated snapshot evidences guided by 3C Principles (Completeness, Conciseness, Creativity). 3. Demonstrated that the SmartSnap paradigm enables scalable training of LLM-driven agents, achieving significant performance gains (up to 26.08% and 16.66%) on mobile tasks and competitive results against larger models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the scalability bottleneck in agentic RL caused by costly and unreliable post-hoc task verification. It proposes SmartSnap, a paradigm where agents proactively seek minimal, decisive snapshot evidence to prove task completion during execution, guided by 3C Principles. Experiments show this approach significantly improves agent performance and enables scalable training, achieving competitive results with much larger models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [TimeXer, Global M2 Liquidity, exogenous variable, long-horizon forecasting]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sravan Karthick T</p>
</li>
<li class="">
<p><strong>institution:</strong> RV College of Engineering (RVCE), Bengaluru, India</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22326" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22326</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the integration of Global M2 Liquidity as a leading exogenous variable with a 12-week lag for Bitcoin price forecasting. 2. Proposes a liquidity-conditioned forecasting model (TimeXer-Exog) based on the TimeXer architecture. 3. Demonstrates that explicit macroeconomic conditioning significantly stabilizes and improves long-horizon forecasts, outperforming a univariate baseline by over 89% at a 70-day horizon.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38de8480bbb274bd2bcfff3317dfee4cd7813e42e3af4cc61efcd6d928a0ae36_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38de8480bbb274bd2bcfff3317dfee4cd7813e42e3af4cc61efcd6d928a0ae36_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of long-horizon Bitcoin price forecasting by proposing a model that integrates Global M2 Liquidity as an exogenous variable into the TimeXer transformer architecture. The proposed TimeXer-Exog model significantly outperforms univariate benchmarks, showing that conditioning on global macroeconomic factors substantially improves forecast stability and accuracy over long horizons.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Emotion classification using EEG headset signals and Random Forest</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [affective computing], [EEG, Random Forest, emotion classification, brain-computer interface, real-time prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ricardo Vasquez, Diego Riofrío-Luzcando, Joe Carrion-Jumbo, Cesar Guevara</p>
</li>
<li class="">
<p><strong>institution:</strong> Universidad Internacional SEK, Universidad Indoamérica, The Institute of Mathematical Sciences (ICMAT-CSIC)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22333" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22333</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a model for classifying human emotions (happiness, sadness, relaxation) using EEG signals from a consumer-grade headset (EMOTIV EPOC). 2. Applied the Random Forest algorithm to achieve high accuracy, particularly for happiness (97.21%). 3. Implemented a real-time emotion prediction system that captures EEG signals, processes them, and visually displays the predicted emotion.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e19385b22eca0965c66f7006e387468776c757a86a9b784693bc7b77b3c7533_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e19385b22eca0965c66f7006e387468776c757a86a9b784693bc7b77b3c7533_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a system to classify human emotions (happiness, sadness, relaxation) from EEG signals using a Random Forest model. The model was trained on data from 50 participants and achieved high accuracy, especially for happiness. The work was extended to create a real-time prediction algorithm that outputs the result with representative images.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [LoRA, catastrophic forgetting, KL divergence, instruction-tuning, parameter-efficient fine-tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Matthew Riemer, Erik Miehling, Miao Liu, Djallel Bouneffouf, Murray Campbell</p>
</li>
<li class="">
<p><strong>institution:</strong> IBM Research, Mila, Université de Montréal</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22337" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22337</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that catastrophic forgetting is a severe problem even during parameter-efficient fine-tuning (LoRA) of LLMs on small datasets. 2. Proposes a simple, low-overhead regularized approximate replay method that penalizes KL divergence from the initial model and interleaves next-token prediction data. 3. Shows that this method effectively preserves the model&#x27;s general knowledge while maintaining plasticity for new tasks, applied to Qwen models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2e24fadff03a1d6696f3147893642a90d9f500d5c68233669842e5792db7332_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2e24fadff03a1d6696f3147893642a90d9f500d5c68233669842e5792db7332_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies that catastrophic forgetting is a major issue during LoRA-based supervised fine-tuning of large language models, even with small datasets. To solve this, the authors propose a regularized approximate replay method that uses KL divergence regularization and interleaves general pre-training-like data. Their approach successfully preserves the model&#x27;s original capabilities while allowing adaptation to new instructions, with minimal computational overhead.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [pseudo-colouring, few-shot learning, prototypical networks, ResNet-18, explainability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alaa Alahmadi, Mohamed Hasan</p>
</li>
<li class="">
<p><strong>institution:</strong> Newcastle University, University of Leeds</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22349" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22349</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a perception-informed pseudo-colouring technique to encode clinically salient temporal ECG features (like QT-interval) into structured colour representations. 2. Demonstrates that this technique enables effective few-shot and one-shot learning for a complex physiological data task (drug-induced LQTS) using prototypical networks and ResNet-18. 3. Shows that the method improves model explainability by guiding attention to clinically meaningful features and that aggregating multiple cardiac cycles (mirroring human perceptual averaging) further boosts performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problems of data inefficiency and poor interpretability in deep learning models for physiological signal analysis. It proposes a human-inspired pseudo-colouring technique to encode ECG features, enabling effective few-shot learning and improving model explainability by focusing on clinically relevant signal components. The results demonstrate that incorporating human-like perceptual encoding can bridge data efficiency and interpretability in medical AI.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PHANTOM: Physics-Aware Adversarial Attacks against Federated Learning-Coordinated EV Charging Management System</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [Cyber-Physical Systems Security], [False Data Injection (FDI), Physics-Informed Neural Network (PINN), Multi-Agent Reinforcement Learning (MARL)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mohammad Zakaria Haider, Amit Kumar Podder, Prabin Mali, Aranya Chakrabortty, Sumit Paudyal, Mohammad Ashiqur Rahman</p>
</li>
<li class="">
<p><strong>institution:</strong> Florida International University, North Carolina State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22381" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22381</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes PHANTOM, a physics-aware adversarial attack framework that integrates a federated learning-enabled PINN as a digital twin for accurate modeling of EV charging systems. 2. Develops a multi-agent RL environment using DQN and SAC to generate stealthy FDI attack strategies that bypass conventional detection. 3. Constructs a T&amp;D co-simulation platform to demonstrate the cascading, cross-boundary grid impacts (e.g., load imbalance, voltage instability) of the learned attacks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc4a49b77c0e517eadb20d321d77564888677d1f33b27adf452e13f7c0ffcb8c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc4a49b77c0e517eadb20d321d77564888677d1f33b27adf452e13f7c0ffcb8c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes PHANTOM, a physics-aware adversarial attack framework against federated learning-coordinated EV charging management. It uses a PINN-based digital twin and multi-agent RL to generate stealthy false data injection attacks, which are shown through co-simulation to cause significant grid instability, highlighting the need for physics-aware cybersecurity in vehicle-grid integration.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Self-Evaluation Unlocks Any-Step Text-to-Image Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [text-to-image generation, flow matching, self-evaluation, any-step inference, from-scratch training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xin Yu, Xiaojuan Qi, Zhengqi Li, Kai Zhang, Richard Zhang, Zhe Lin, Eli Shechtman, Tianyu Wang, Yotam Nitzan</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Hong Kong, Adobe Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22374" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22374</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Self-Evaluating Model (Self-E), a novel from-scratch training framework that combines local flow matching with a self-evaluation mechanism, eliminating the need for a pretrained teacher model. 2. Enables &quot;any-step&quot; inference, allowing the same model to perform both high-quality few-step and many-step generation, bridging the gap between local supervision and global matching paradigms. 3. Demonstrates competitive performance with state-of-the-art flow matching models at high step counts while excelling at very low step counts, offering a unified and scalable solution.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that traditional diffusion/flow models require many inference steps, and distillation methods need a pretrained teacher. It proposes Self-E, a model trained from scratch that uses self-evaluation as a dynamic teacher to enable high-quality generation at any number of steps. The results show Self-E excels at few-step generation and is competitive at many steps, providing a unified framework for efficient and scalable text-to-image synthesis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [hyperparameter transfer, Complete(d)P parameterisation, per-module hyperparameter optimisation, scaling laws, evolutionary strategy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bruno Mlodozeniec, Pierre Ablin, Louis Béthune, Dan Busbridge, Michal Klein, Jason Ramapuram, Marco Cuturi</p>
</li>
<li class="">
<p><strong>institution:</strong> Apple, University of Cambridge</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22382" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22382</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the Complete(d)P parameterisation, a unified framework for scaling hyperparameters across model width, depth, batch size, and training duration. 2. Investigates and enables the transfer of per-module hyperparameters (e.g., learning rates, weight decay) across model scales, moving beyond global hyperparameter transfer. 3. Provides practical guidelines for navigating the high-dimensional per-module hyperparameter optimization landscape and demonstrates significant training speed improvements in Large Language Models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06e0593eb453191fce60eeebdd4eb6147ab462084db9eb8d81ea8e2486d468be_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06e0593eb453191fce60eeebdd4eb6147ab462084db9eb8d81ea8e2486d468be_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of hyperparameter transfer across different model scales and configurations. It introduces the Complete(d)P parameterisation to unify scaling across width, depth, batch size, and duration, and demonstrates that with this method, even granular per-module hyperparameters can be optimized on a small model and successfully transferred to much larger models, leading to faster training and improved performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] BLISS: Bandit Layer Importance Sampling Strategy for Efficient Training of Graph Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [Graph Neural Networks, Multi-armed Bandits, Layer-wise Sampling, Node Importance, Efficient Training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Omar Alsaqa, Linh Thi Hoang, Muhammed Fatih Balin</p>
</li>
<li class="">
<p><strong>institution:</strong> Wilfrid Laurier University, Singapore Management University, Georgia Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22388" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22388</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces BLISS, a novel adaptive sampling strategy using multi-armed bandits to dynamically select informative nodes at each GNN layer. 2. Balances exploration and exploitation to ensure comprehensive graph coverage and adapts to evolving node importance, unlike static methods. 3. Demonstrates versatility by integrating with different GNN architectures (GCNs and GATs) and maintaining or exceeding full-batch training accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1be93ab34a4af58594bc48c8d52cc9f7177c298fc314982c8ac7ae27b2086b70_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1be93ab34a4af58594bc48c8d52cc9f7177c298fc314982c8ac7ae27b2086b70_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the computational bottleneck in training Graph Neural Networks (GNNs) on large graphs by proposing BLISS, a Bandit Layer Importance Sampling Strategy. BLISS uses multi-armed bandits to dynamically and adaptively sample the most informative nodes at each layer. Experiments show that this method maintains or even surpasses the accuracy of full-batch training while being more efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Differentiable Inverse Modeling with Physics-Constrained Latent Diffusion for Heterogeneous Subsurface Parameter Fields</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [latent diffusion model, differentiable physics, inverse problems, parameter estimation, flow in porous media]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zihan Lin, QiZhi He</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Minnesota</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22421" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22421</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LD-DIM, a novel framework that integrates a pretrained latent diffusion prior with a differentiable PDE solver for high-dimensional inverse problems. 2. Enables stable gradient-based optimization in a low-dimensional latent space, improving numerical conditioning and preserving sharp discontinuities. 3. Demonstrates superior numerical stability and reconstruction accuracy compared to PINNs and physics-embedded VAE baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb75073de82d3659f4e40522fca5a1fe8743332c5df8e21ca285525b2a200bca_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb75073de82d3659f4e40522fca5a1fe8743332c5df8e21ca285525b2a200bca_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces LD-DIM, a method that combines a latent diffusion model with a differentiable numerical solver to reconstruct heterogeneous parameter fields from sparse observations in PDE-constrained inverse problems. It shows improved stability and accuracy over existing baselines while maintaining sharp material interfaces.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [hyperspherical learning, native sparse attention, anisotropic patch embed]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amil Khan, Matheus Palhares Viana, Suraj Mishra, B.S. Manjunath</p>
</li>
<li class="">
<p><strong>institution:</strong> UC Santa Barbara, Allen Institute for Cell Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22423" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22423</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A 4B-parameter foundation model (Bright-4B) that learns on the unit hypersphere for segmenting subcellular structures directly from 3D brightfield volumes. 2. Novel architectural components: a hardware-aligned Native Sparse Attention mechanism, depth-width residual HyperConnections, and a soft Mixture-of-Experts for adaptive capacity. 3. A plug-and-play anisotropic patch embed that respects confocal point-spread and axial thinning for geometry-faithful 3D tokenization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces Bright-4B, a 4-billion parameter foundation model designed for volumetric segmentation of subcellular structures directly from label-free 3D brightfield microscopy images. It employs novel architectural components like hyperspherical learning, native sparse attention, and an anisotropic patch embed to handle 3D context and anisotropic sampling. The model outperforms contemporary baselines in preserving fine structural detail across depth and cell types without requiring fluorescence or heavy post-processing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Causality-Inspired Safe Residual Correction for Multivariate Time Series</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [residual correction, causality-inspired encoder, non-degradation guarantee, safety mechanism, multivariate time series]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jianxiang Xie, Yuncheng Hua</p>
</li>
<li class="">
<p><strong>institution:</strong> University of New South Wales, University of Science and Technology of China, The Hong Kong University of Science and Technology (Guangzhou)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22428" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22428</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CRC, a plug-and-play residual correction framework explicitly designed to guarantee non-degradation of model performance. 2. Introduces a causality-inspired encoder that decouples self- and cross-variable dynamics to expose direction-aware structure for safer correction. 3. Designs a strict four-fold safety mechanism to govern the correction process and prevent harmful updates, ensuring high non-degradation rates.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7659f53cdce63801de4caead558593caeed0c4662c0c76c85dae564f12a6d78_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7659f53cdce63801de4caead558593caeed0c4662c0c76c85dae564f12a6d78_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that existing post-hoc residual correction methods for multivariate time series forecasting are greedy and can degrade performance. To solve this, it proposes CRC, a causality-inspired safe residual correction framework with a four-fold safety mechanism. Experiments show CRC consistently improves accuracy while ensuring exceptionally high non-degradation rates.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [analog circuit design, multi-agent framework, stratified memory, simulation-grounded feedback, self-evolving]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zining Wang, Jian Gao, Weimin Fu, Xiaolong Guo, Xuan Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Northeastern University, Kansas State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22435" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22435</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes AnalogSAGE, an open-source self-evolving multi-agent framework for analog circuit design that coordinates three-stage agent explorations through four stratified memory layers, enabling iterative refinement with simulation-grounded feedback. 2. Introduces a stratified context mechanism to selectively preserve stage-relevant information, enhancing long-horizon reasoning and reliability under stringent specifications. 3. Demonstrates significant improvements in pass rates and search space reduction through a benchmark of ten operational amplifier design problems using the open-source SKY130 PDK and ngspice.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf0a7cad048acb4f93f29281281d9e76543f81e5aa1dd6e183e005cda30a7c56_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf0a7cad048acb4f93f29281281d9e76543f81e5aa1dd6e183e005cda30a7c56_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of automating analog circuit design, which traditionally relies heavily on human intuition, by introducing AnalogSAGE, a self-evolving multi-agent framework with stratified memory and simulation-grounded feedback. This approach enables iterative refinement across topology selection, refinement, and parameter optimization stages. Evaluations show it achieves a 10x overall pass rate and 4x reduction in parameter search space compared to existing methods, enhancing reliability and autonomy in analog design automation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [hierarchical filtering, two-pass generation, citation verification, query formulation, model cascade]</p>
</li>
<li class="">
<p><strong>authors:</strong> Cattalyya Nuengsigkapian</p>
</li>
<li class="">
<p><strong>institution:</strong> Google</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22442" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22442</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hierarchical content filtering pipeline to replace standard vector similarity search, improving context precision. 2. Introduces a model cascade strategy using a cost-efficient model (Gemini 2.5 Flash) for filtering and a powerful model (Gemini 2.5 Pro) for final generation. 3. Demonstrates significant performance gains on the MMU-RAGent benchmark and a custom dataset for post-cutoff knowledge.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9840d615edee0e72c18b93838479ebb342195ea1805803858fb9effaf9ba2e95_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9840d615edee0e72c18b93838479ebb342195ea1805803858fb9effaf9ba2e95_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents HiFi-RAG, a system designed to improve open-domain RAG by addressing irrelevant retrieved information. The method uses a multi-stage pipeline with hierarchical filtering and a two-pass generation strategy employing different LLMs for efficiency and quality. The system won a NeurIPS 2025 competition and showed substantial improvements over baselines in evaluation metrics.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [LoRA, Parameter-Efficient Fine-Tuning, Activation Function Annealing, Non-linear Adaptation, Model Merging]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiacheng Li, Jianchao Tan, Zhidong Yang, Feiye Huo, Yerui Sun, Yuchen Xie, Xunliang Cai</p>
</li>
<li class="">
<p><strong>institution:</strong> Meituan, Hong Kong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22455" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22455</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes AFA-LoRA, a novel training strategy that introduces non-linear expressivity into LoRA while preserving its seamless mergeability., 2. Introduces an annealed activation function that transitions from non-linear to linear during training, enabling strong initial learning and final linear integration., 3. Demonstrates the method&#x27;s effectiveness across multiple tasks, including supervised fine-tuning, reinforcement learning, and speculative decoding, reducing the performance gap with full-parameter training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a5573f75296283a39c5bdbbb0c94652e0aeb935ae378c12528544ca5e188deb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a5573f75296283a39c5bdbbb0c94652e0aeb935ae378c12528544ca5e188deb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the limited expressive power of linear Low-Rank Adaptation (LoRA) by proposing AFA-LoRA, a method that uses an annealed activation function to enable non-linear training while ensuring the final adapter remains mergeable. This approach narrows the performance gap between LoRA and full-parameter fine-tuning across various tasks, offering a more powerful and practical parameter-efficient adaptation paradigm.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AMBIT: Augmenting Mobility Baselines with Interpretable Trees</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [urban computing, spatial data science], [origin-destination flow prediction, spatial interaction models, gradient-boosted trees, SHAP analysis, gray-box model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qizhi Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> PingCAP, Data &amp; AI-Innovation Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22466" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22466</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducts a comprehensive audit of classical spatial interaction models on high-resolution mobility data, identifying PPML gravity as the strongest physical baseline. 2. Proposes AMBIT, a gray-box framework that augments interpretable physical baselines with gradient-boosted trees to learn residuals, balancing accuracy and interpretability. 3. Demonstrates that physics-grounded and POI-anchored residual learners achieve competitive accuracy and robust spatial generalization, providing a reproducible pipeline with diagnostics for urban decision-making.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d0d9ab13b8b2f60e318c90f38821b29e87e0bdd9bf0d9bc963b48c5ac7c2759_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d0d9ab13b8b2f60e318c90f38821b29e87e0bdd9bf0d9bc963b48c5ac7c2759_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes AMBIT, a gray-box framework that combines interpretable physical mobility baselines with gradient-boosted trees to predict origin-destination flows. It shows that learning residuals on top of physics-based models can achieve accuracy close to strong black-box predictors while maintaining interpretable structure, with POI-anchored residuals being particularly robust for spatial generalization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GLUE: Gradient-free Learning to Unify Experts</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [expert mixing, model initialization, gradient-free optimization, SPSA, transfer learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jong-Ik Park, Shreyas Chaudhari, Srinivasa Pranav, Carlee Joe-Wong, José M. F. Moura</p>
</li>
<li class="">
<p><strong>institution:</strong> Carnegie Mellon University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22467" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22467</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GLUE, a gradient-free method to learn mixture coefficients for blending expert models into a single initialization prior for a target domain. 2. Introduces a two-point (SPSA) update rule that requires only two forward passes per step, avoiding expensive backpropagation through the full network. 3. Demonstrates that GLUE outperforms heuristic blending baselines and matches or outperforms gradient-based learning of mixture coefficients across multiple datasets and architectures.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58e1db10a62e7b3683bf994e8914b7bcb88a1d8d308b99746a86c5ebb6d9b5c6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58e1db10a62e7b3683bf994e8914b7bcb88a1d8d308b99746a86c5ebb6d9b5c6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of initializing a model for a new target domain by blending multiple pretrained expert models. It proposes GLUE, a gradient-free method that learns the blending coefficients efficiently using only forward passes. Experiments show GLUE creates a better initialization prior, leading to higher fine-tuned accuracy than heuristic methods and comparable or better performance than gradient-based approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Bayesian Geometry of Transformer Attention</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [interpretability], [Bayesian inference, transformer attention, mechanistic interpretability, Bayesian wind tunnels, geometric analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra</p>
</li>
<li class="">
<p><strong>institution:</strong> Columbia University, Dream Sports, Google DeepMind</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22471" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22471</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces &quot;Bayesian wind tunnels&quot; as controlled environments to rigorously test if transformers perform Bayesian inference, where true posteriors are known and memorization is impossible. 2. Demonstrates that small transformers implement Bayesian inference via a consistent geometric mechanism (residual streams as belief substrate, FFNs for updates, attention for routing), while MLPs fail. 3. Identifies specific geometric diagnostics (orthogonal key bases, query-key alignment, low-dimensional value manifold) and a frame-precision dissociation during training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5333e070f21cfd2abea4d13cddb84bf6d9f3c3124c93bf9142879f24f1e739b1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5333e070f21cfd2abea4d13cddb84bf6d9f3c3124c93bf9142879f24f1e739b1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether transformers perform genuine Bayesian inference. To test this, the authors create controlled &quot;Bayesian wind tunnel&quot; tasks with known posteriors and show that small transformers accurately reproduce Bayesian posteriors via a specific geometric mechanism, while MLPs fail, establishing attention as crucial for this capability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Collaborative Optimization of Multiclass Imbalanced Learning: Density-Aware and Region-Guided Boosting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [imbalanced learning], [collaborative optimization, density-aware, region-guided boosting, sample weight update, dynamic sampling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chuantao Li, Zhi Li, Jiahao Xu, Jie Li, Sheng Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Guangdong Ocean University, University of Electronic Science and Technology of China</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22478" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22478</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/ChuantaoLi/DARG" target="_blank" rel="noopener noreferrer" class="">https://github.com/ChuantaoLi/DARG</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a collaborative optimization Boosting model for multiclass imbalanced learning that integrates imbalanced learning and model training. 2. Designs a noise-resistant weight update mechanism and a dynamic sampling strategy using density and confidence factors. 3. Achieves tight integration of modules for weight updates, sample region partitioning, and region-guided sampling to enhance performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d829fa49f320d741a98494ca09a1f6019a6cccef04a93af08effdf4810adc20_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d829fa49f320d741a98494ca09a1f6019a6cccef04a93af08effdf4810adc20_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of classification bias in multiclass imbalanced data by proposing a collaborative optimization Boosting model. The method integrates density-aware and region-guided techniques to update sample weights and perform dynamic sampling, achieving improved performance over existing baselines on 20 public datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding for Fine-Grained sEMG-Based Movement Decoding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [biomedical signal processing], [self-supervised learning, surface electromyography, rotary position encoding, spectral pre-training, movement decoding]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zihan Weng, Chanlin Yi, Pouya Bashivan, Jing Lu, Fali Li, Dezhong Yao, Jingming Hou, Yangsong Zhang, Peng Xu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22481" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22481</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel self-supervised pre-training task that uses masked prediction of clustered STFT pseudo-labels to learn robust, physiologically relevant frequency patterns from sEMG signals. 2. A novel Cylindrical Rotary Position Embedding (CyRoPE) that factorizes embeddings along temporal and annular spatial dimensions to explicitly model the cylindrical topology of forearm electrode arrays. 3. The SPECTRE framework, which integrates these contributions to establish a new state-of-the-art for fine-grained movement decoding, validated on multiple datasets including data from individuals with amputation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5801cbdcbac93bf7df15e18531c5ff2653259e25003568da5b53b240d06d32c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5801cbdcbac93bf7df15e18531c5ff2653259e25003568da5b53b240d06d32c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces SPECTRE, a domain-specific self-supervised learning framework for decoding fine-grained movements from surface electromyography (sEMG) signals. It proposes a spectral pre-training task using masked pseudo-label prediction and a novel cylindrical rotary position encoding to model sensor topology. Evaluations show SPECTRE significantly outperforms existing supervised and generic self-supervised baselines, providing a robust foundation for practical myoelectric interfaces.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection via Latent Space Representation Learning and Alignment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [network intrusion detection], [concept drift, latent space alignment, graph neural network (GNN), IoT botnet detection, variational autoencoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hassan Wasswa, Timothy Lynar</p>
</li>
<li class="">
<p><strong>institution:</strong> University of New South Wales</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22488" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22488</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a scalable framework for adaptive IoT threat detection that eliminates the need for continuous classifier retraining, reducing computational overhead and catastrophic forgetting. 2. Introduces a method using latent space representation learning and an alignment model to map new traffic to a historical latent space, preserving knowledge of past attacks. 3. Transforms latent representations into a graph structure and uses a Graph Attention Network (GAT) to capture inter-instance relationships among attack samples for improved detection.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4c1a80d42c79bd3adbbc2c072359068d65253efabadc2bd6d3617f632439f72_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4c1a80d42c79bd3adbbc2c072359068d65253efabadc2bd6d3617f632439f72_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of concept drift in IoT botnet detection by proposing a framework that trains a classifier once on historical traffic representations. It uses an alignment model to map new traffic to this learned latent space and a graph neural network to classify the data, maintaining robust performance without retraining. Experimental results on real-world datasets show the framework&#x27;s effectiveness in dynamic IoT environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Quest for Winning Tickets in Low-Rank Adapters</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [parameter-efficient fine-tuning], [Lottery Ticket Hypothesis, Low-Rank Adaptation, Parameter-Efficient Fine-Tuning, Sparse Subnetworks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hamed Damirchi, Cristian Rodriguez-Opazo, Ehsan Abbasnejad, Zhen Zhang, Javen Shi</p>
</li>
<li class="">
<p><strong>institution:</strong> Australian Institute for Machine Learning (Adelaide University), Monash University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22495" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22495</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Empirically validates that the Lottery Ticket Hypothesis (LTH) holds within Low-Rank Adaptation (LoRA) methods, revealing the existence of sparse, high-performing subnetworks (&quot;winning tickets&quot;) in adapters. 2. Discovers that the effectiveness of these sparse subnetworks depends more on the sparsity distribution across layers than on the specific weights selected. 3. Proposes Partial-LoRA, a novel method to systematically identify and train these sparse low-rank adapters, achieving significant parameter reduction (up to 87%) while maintaining or improving performance across vision and language tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19921047ca06f45b5a51cefcf5b1cd9502b9ad5650b66c746d8b15f622036ef0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19921047ca06f45b5a51cefcf5b1cd9502b9ad5650b66c746d8b15f622036ef0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether the Lottery Ticket Hypothesis extends to parameter-efficient fine-tuning, specifically Low-Rank Adaptation (LoRA). The authors propose Partial-LoRA, a method to identify and train sparse, task-aligned subnetworks within LoRA adapters. Experiments show Partial-LoRA can reduce trainable parameters by up to 87% while matching or surpassing the performance of dense adapters.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Role-Based Fault Tolerance System for LLM RL Post-Training</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [fault-tolerance], [role-based fault tolerance, RL post-training, UCX communication, warm standby, Effective Training Time Ratio (ETTR)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhenqian Chen, Baoquan Zhong, Xiang Li, Qing Dai, Xinkui Zhao, Miao Ye, Ren Cheng, Lufei Zhang, Jianwei Yin</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, State Key Laboratory of Mathematical Engineering and Advanced Computing</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22492" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22492</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a role-based fault isolation and recovery system (RobustRL) for RL post-training, enabling recovery of only the failed component (trainer, rollout) instead of restarting the entire task. 2. Introduces a role-aware monitoring mechanism to accurately detect failures and avoid false positives/delays specific to different RL roles. 3. Implements dynamic, UCX-based point-to-point communication to reconnect recovered roles and synchronize weights immediately, replacing static collective communication.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceff77cfb9d0c7d7e763916b77716ee6534c3aa3d54d41253fef6ea4d9a86ec0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceff77cfb9d0c7d7e763916b77716ee6534c3aa3d54d41253fef6ea4d9a86ec0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the lack of fault tolerance for RL post-training of LLMs, which interleaves training and inference workloads. It proposes RobustRL, a system that isolates and recovers failed roles (e.g., trainer, rollout) individually using a Detect-Restart-Reconnect paradigm, instead of restarting the entire job. This approach significantly improves the Effective Training Time Ratio and reduces end-to-end training time compared to baseline methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Decomposing Task Vectors for Refined Model Editing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [model editing], [task vector, parameter decomposition, invariant subspace, concept interference, LoRA]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hamed Damirchi, Ehsan Abbasnejad, Zhen Zhang, Javen Shi</p>
</li>
<li class="">
<p><strong>institution:</strong> Australian Institute for Machine Learning (University of Adelaide), Monash University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22511" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22511</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A principled decomposition method to separate a task vector into shared and unique components., 2. Application of the decomposition to improve multi-task merging in image classification, enable clean style mixing in diffusion models, and reduce toxicity in language models., 3. A new framework for understanding and controlling task vector arithmetic to address interference during concept composition.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e938f563a374559600295d0e58643d1a4f7e55e5b56e3e6aca0b3daec488d0b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e938f563a374559600295d0e58643d1a4f7e55e5b56e3e6aca0b3daec488d0b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of unpredictable outcomes when performing arithmetic operations on task vectors due to overlapping concepts. It proposes a method to decompose each task vector into shared and unique components using invariant subspaces. This enables more precise model editing, demonstrated by improved multi-task merging, clean style mixing, and significant toxicity reduction while preserving general knowledge.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [hallucination detection], [correctness prediction, metadata signals, prompting strategies, log probability, response consistency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lucky Susanto, Anasta Pranawijayana, Cortino Sukotjo, Soni Prasad, Derry Wijaya</p>
</li>
<li class="">
<p><strong>institution:</strong> Monash University Indonesia, University of Pittsburgh, University of North Carolina Adams School of Dentistry, Boston University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22508" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22508</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel method to predict LLM correctness (not just hallucination) using metadata and hallucination signals in a high-stakes medical domain (prosthodontics). 2. Demonstrates that metadata-based predictors can improve accuracy over a naive baseline and achieve high precision, but are not reliable for directly predicting hallucination. 3. Shows that prompting strategies significantly alter model internal behavior and the predictive utility of metadata, despite not changing overall task accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbe159260b97cf5e7a59edbee0a23b697a76a89a0fdbf6e0c9797739cc322b42_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbe159260b97cf5e7a59edbee0a23b697a76a89a0fdbf6e0c9797739cc322b42_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study investigates predicting the correctness of LLM answers on a prosthodontics exam using metadata (like log probability and consistency) and hallucination signals. The method, applied to GPT-4o and OSS-120B across different prompts, shows improved accuracy over a baseline but is not yet robust for high-stakes deployment. The research highlights that prompting strategies change model behavior and metadata utility, offering a direction for reliability signals.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [adversarial robustness], [Spiking Neural Networks, surrogate gradient, adversarial attack, gradient vanishing, adaptive optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jihang Wang, Dongcheng Zhao, Ruolin Chen, Qian Zhang, Yi Zeng</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Beijing Institute of AI Safety and Governance</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22522" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22522</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Theoretical analysis of gradient vanishing in surrogate gradient methods for SNNs. 2. Proposal of Adaptive Sharpness Surrogate Gradient (ASSG) to adaptively adjust the surrogate function for more accurate gradients. 3. Design of Stable Adaptive Projected Gradient Descent (SA-PGD), an adversarial attack with adaptive step size for stable convergence under imprecise gradients.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7b10ed2a96f6e46c5c2afb21a8e1184dd9c5e1f342efdb93f3cd317a08c20ea_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7b10ed2a96f6e46c5c2afb21a8e1184dd9c5e1f342efdb93f3cd317a08c20ea_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the unreliable evaluation of adversarial robustness in Spiking Neural Networks (SNNs) caused by gradient vanishing from surrogate gradients. The authors propose a framework combining an adaptive surrogate gradient method (ASSG) and an adaptive-step attack (SA-PGD) to generate stronger attacks. Experiments show this framework significantly increases attack success rates, revealing that current SNN robustness is overestimated and highlighting the need for more reliable adversarial training.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time-series forecasting], [encoder-decoder, latent bottleneck representations, learnable queries, generalized forecasting]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jaebin Lee, Hankook Lee</p>
</li>
<li class="">
<p><strong>institution:</strong> Sungkyunkwan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22550" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22550</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/efficient-learning-lab/TimePerceiver" target="_blank" rel="noopener noreferrer" class="">https://github.com/efficient-learning-lab/TimePerceiver</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Generalizes the time-series forecasting task to include diverse objectives like extrapolation, interpolation, and imputation. 2. Proposes a novel encoder-decoder architecture with latent bottleneck representations to capture temporal and cross-channel dependencies. 3. Introduces learnable queries for decoding to effectively retrieve information for arbitrarily positioned target timestamps.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/091fdcd1058e0acfad18820d025c1fe50ff4315cbd5ec8a06f5d86eb9767513c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/091fdcd1058e0acfad18820d025c1fe50ff4315cbd5ec8a06f5d86eb9767513c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes TimePerceiver, a unified encoder-decoder framework for generalized time-series forecasting. It handles diverse prediction objectives by using latent bottleneck encodings and learnable query-based decoding. Extensive experiments show it consistently outperforms prior state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Computing Pure-Strategy Nash Equilibria in a Two-Party Policy Competition: Existence and Algorithmic Approaches</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [game theory], [pure-strategy Nash equilibrium, continuous games, policy competition, gradient-based algorithm, grid-based search]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chuang-Chieh Lin, Chi-Jen Lu, Po-An Chen, Chih-Chieh Hung</p>
</li>
<li class="">
<p><strong>institution:</strong> National Taiwan Ocean University, Academia Sinica, National Yang Ming Chiao Tung University, National Chung Hsing University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22552" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22552</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulated a two-party policy competition game and validated the isotonicity hypothesis of winning probability through simulations. 2. Proved the existence of a pure-strategy Nash equilibrium (PSNE) in both one- and multi-dimensional policy spaces. 3. Proposed and experimentally validated a decentralized gradient-based algorithm and a polynomial-time grid-based search algorithm for finding approximate PSNEs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ef0be95e6f7da168b174f0a6d600f4ee8f0f3d2bcaba49c996d43be6d1be4b7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ef0be95e6f7da168b174f0a6d600f4ee8f0f3d2bcaba49c996d43be6d1be4b7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper models two-party policy competition as a continuous non-cooperative game where parties choose policy vectors and a party&#x27;s payoff is the expected utility for its supporters. The authors prove the existence of a pure-strategy Nash equilibrium and propose two algorithms—a gradient-based method and a grid-based search—that efficiently find approximate equilibria.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [disaggregated infrastructure, hardware-affinity mapping, fine-grained asynchrony]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wei Gao, Yuheng Zhao, Tianyuan Wu, Shaopan Xiong, Weixun Wang, Dakai An, Lunxi Cao, Dilxat Muhtar, Zichen Liu, Haizhou Zhao, Ju Huang, Siran Yang, Yongbin Li, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> HKUST, Alibaba Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22560" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22560</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/alibaba/ROLL" target="_blank" rel="noopener noreferrer" class="">https://github.com/alibaba/ROLL</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A hardware-affinity workload mapping strategy that routes compute-bound and bandwidth-bound tasks to best-fit GPU devices. 2. A fine-grained asynchrony mechanism that manages execution at the trajectory level to mitigate resource bubbles and improve utilization. 3. A statefulness-aware computation design that offloads stateless components to serverless infrastructure for elastic scaling.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc8e408c613097b7b60bc32e41ec3137faa8dd94184658c8a802b65cbf57c593_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc8e408c613097b7b60bc32e41ec3137faa8dd94184658c8a802b65cbf57c593_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents RollArt, a distributed system designed to scale agentic reinforcement learning training on disaggregated infrastructure. It addresses the heterogeneity of agentic RL workloads by proposing three core techniques: hardware-affinity workload mapping, fine-grained asynchrony, and statefulness-aware computation. The system demonstrates significant improvements in training throughput, achieving 1.35-2.05x speedup over baselines, and scales to thousands of GPUs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On Admissible Rank-based Input Normalization Operators</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [machine learning theory], [rank-based normalization, differentiable sorting, monotone invariance, batch independence, Lipschitz continuity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Taeyun Kim</p>
</li>
<li class="">
<p><strong>institution:</strong> Seoul National University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22587" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22587</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identified and formalized three axioms (invariance and stability properties) that a valid rank-based input normalization operator must satisfy. 2. Proved a structural characterization theorem showing any admissible operator must factor into a feature-wise rank representation and a monotone-Lipschitz scalarization map. 3. Constructed a minimal operator meeting the proposed axioms, empirically demonstrating the non-triviality of the constraints and delineating the design space from existing differentiable sorting methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49ce297e3baab406a1befffbcf116551c1492e7a6451fb1af02514d5f1ddb86b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49ce297e3baab406a1befffbcf116551c1492e7a6451fb1af02514d5f1ddb86b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that widely used differentiable sorting/ranking operators are structurally unstable under monotone transformations and batch variations. To address this, it proposes formal axioms for stable rank-based normalization and proves that any admissible operator must have a specific factored structure. The authors construct a minimal operator satisfying these axioms, formally separating valid normalization from existing differentiable sorting approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Data-Driven Analysis of Crash Patterns in SAE Level 2 and Level 4 Automated Vehicles Using K-means Clustering and Association Rule Mining</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [data mining], [K-means clustering, Association Rule Mining, crash pattern analysis, automated vehicles, NHTSA data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jewel Rana Palit, Vijayalakshmi K Kumarasamy, Osama A. Osman</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Tennessee at Chattanooga, Collier County Government</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22589" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22589</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a two-stage data mining framework combining K-means clustering and Association Rule Mining to analyze AV crash data. 2. Applied the framework to a large-scale dataset of over 2,500 AV crash records from NHTSA, covering SAE Levels 2 and 4. 3. Uncovered interpretable multivariate relationships between crash patterns and environmental/operational factors, providing actionable insights for AV safety.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c3b11717362343c168a38a05a8cdd65b40a65cd3df1b048cfb816c2493f25b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c3b11717362343c168a38a05a8cdd65b40a65cd3df1b048cfb816c2493f25b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study analyzes crash patterns in SAE Level 2 and 4 Automated Vehicles using a large-scale NHTSA dataset. It proposes a two-stage data mining framework: first using K-means clustering to segment crashes into behavioral clusters, then applying Association Rule Mining to find relationships between crash factors within each cluster. The results provide actionable guidance for improving AV safety and deployment strategies.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Cryptocurrency Price Prediction Using Parallel Gated Recurrent Units</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [Gated Recurrent Units, parallel neural networks, cryptocurrency price prediction, mean absolute percentage error, recurrent neural networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Milad Asadpour, Alireza Rezaee, Farshid Hajati</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Tehran, University of New England</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22599" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22599</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Parallel Gated Recurrent Units (PGRU) model for cryptocurrency price forecasting. 2. Employs parallel and independent recurrent neural networks with distinct price-related feature inputs. 3. Demonstrates higher accuracy and efficiency with lower computational cost and less input data compared to existing methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1b0d946f543a3b63440ffb89cc57536eb655abef18c7f847881484fd9e06122_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1b0d946f543a3b63440ffb89cc57536eb655abef18c7f847881484fd9e06122_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a new deep learning model called Parallel Gated Recurrent Units (PGRU) for predicting cryptocurrency prices. The model uses parallel recurrent neural networks that process different price features independently, and their outputs are combined by another neural network for the final forecast. Experimental results show the model achieves low prediction errors (e.g., 2.641% MAPE) with higher efficiency and lower computational cost than previous methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Energy-Guided Flow Matching Enables Few-Step Conformer Generation and Ground-State Identification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models], [flow matching, conformer generation, energy guidance, ground-state identification, molecular geometry]</p>
</li>
<li class="">
<p><strong>authors:</strong> Guikun Xu, Xiaohan Yi, Peilin Zhao, Yatao Bian</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, National University of Singapore, Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22597" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22597</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Rich-XGK/EnFlow.git" target="_blank" rel="noopener noreferrer" class="">https://github.com/Rich-XGK/EnFlow.git</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes EnFlow, a unified framework that couples flow matching with an explicitly learned energy model for conformer generation. 2. Introduces an energy-guided sampling scheme along a non-Gaussian flow matching path to steer trajectories toward lower-energy regions, improving fidelity in few-step regimes. 3. Enables accurate ground-state identification by using the learned energy function to rank generated ensembles, reducing prediction errors.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8be1fad6f55102be97d0cd80a5b8da136a8f70c287b7f11b3a11156b5bc80a04_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8be1fad6f55102be97d0cd80a5b8da136a8f70c287b7f11b3a11156b5bc80a04_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents EnFlow, a method that integrates flow matching with an energy model to guide the generation of molecular conformers. By using energy-gradient guidance during sampling, it improves conformational accuracy with few steps and enables better identification of the ground-state structure. Experiments show it outperforms state-of-the-art methods on standard benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Gold Price Prediction Using Long Short-Term Memory and Multi-Layer Perceptron with Gray Wolf Optimizer</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [LSTM, Multilayer Perceptron (MLP), Gray Wolf Optimizer (GWO), gold price prediction, trading strategy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hesam Taghipour, Alireza Rezaee, Farshid Hajati</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Tehran, University of New England</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22606" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22606</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a hybrid LSTM-MLP model for multi-timeframe (daily and monthly) gold price forecasting. 2. Utilized the Gray Wolf Optimizer (GWO) to optimize the number of neurons in the neural networks for improved accuracy. 3. Developed and backtested a trading strategy based on the model&#x27;s predictions, reporting a high simulated return of 171% over three months.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46725f71da8c268509e63f86ad823980c405c360b229a1a903d358d4d2dd61d5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46725f71da8c268509e63f86ad823980c405c360b229a1a903d358d4d2dd61d5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents an AI-based model for predicting gold prices. It uses two LSTM networks for daily and monthly forecasts, integrates their outputs with an MLP, and optimizes the network structure using the Gray Wolf Optimizer. The model achieved low prediction errors and a high simulated trading return, demonstrating its potential for financial forecasting.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Communication Compression for Distributed Learning with Aggregate and Server-Guided Feedback</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [communication compression, error feedback, biased compression, control variates, distributed gradient descent]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tomas Ortega, Chun-Yin Huang, Xiaoxiao Li, Hamid Jafarkhani</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Irvine; University of British Columbia; Vector Institute</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22623" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22623</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Compressed Aggregate Feedback (CAFe), a novel framework for biased compression in distributed learning that uses the previous round&#x27;s aggregated update as a shared control variate, eliminating the need for client-side state. 2. Proposed Server-Guided Compressed Aggregate Feedback (CAFe-S), an extension that leverages a small server-side dataset to generate a more accurate predictor update, improving convergence when server data is representative. 3. Provided theoretical convergence guarantees for both CAFe and CAFe-S in non-convex settings, proving CAFe&#x27;s superiority over standard distributed compressed gradient descent and showing CAFe-S&#x27;s improved rate with more representative server data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/988420401d89faee52e66a7e209751875e33b7f833b8dc7f136f1f1d2650454d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/988420401d89faee52e66a7e209751875e33b7f833b8dc7f136f1f1d2650454d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the communication bottleneck and privacy issues in Federated Learning by proposing two novel compression frameworks, CAFe and CAFe-S, which enable biased compression without requiring client-side state. CAFe uses the previous aggregated update as a shared control variate, while CAFe-S leverages a small server dataset for a better predictor. Theoretical and experimental results demonstrate their superiority over existing compression schemes.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Tree Meets Transformer: A Hybrid Architecture for Scalable Power Allocation in Cell-Free Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [binary tree compression, Transformer, scalable inference, power allocation, cell-free]</p>
</li>
<li class="">
<p><strong>authors:</strong> Irched Chafaa, Giacomo Bacci, Luca Sanguinetti</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Pisa</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22639" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22639</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel hybrid Tree-Transformer architecture for scalable per-user power allocation in wireless networks. 2. Introduces a method that compresses user features via a binary tree to a global root representation, applies a Transformer encoder only to the root, and decodes powers with a shared decoder, achieving logarithmic depth and linear total complexity. 3. Demonstrates that the model achieves near-optimal performance for the max-min fairness problem in cell-free massive MIMO systems while significantly reducing inference time compared to full-attention baselines, without needing retraining for different network sizes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/967237dcd593c2fe2d6f3b16cb72307d4ae0dbaa94a8031ad460a273da33c12a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/967237dcd593c2fe2d6f3b16cb72307d4ae0dbaa94a8031ad460a273da33c12a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high computational cost of Transformer models for power allocation in large-scale wireless networks. It proposes a hybrid Tree-Transformer architecture that compresses user features into a root node for processing, achieving linear complexity and scalable inference. The model demonstrates near-optimal performance with significantly reduced inference time for cell-free massive MIMO systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Scaling Unverifiable Rewards: A Case Study on Visual Insights</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [Test-Time Scaling, multi-agent pipeline, process-based refinement, LLM-as-Judge, unverifiable rewards]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuyu Gan, James Mooney, Pan Hao, Renxiang Wang, Mingyi Hong, Qianwen Wang, Dongyeop Kang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Minnesota</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22650" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22650</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://minnesotanlp.github.io/insight-scaling-webpage" target="_blank" rel="noopener noreferrer" class="">https://minnesotanlp.github.io/insight-scaling-webpage</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Selective Test-Time Scaling, a process-based refinement framework that scales inference across stages in multi-agent pipelines instead of repeated refinement over time. 2. Designed a reliable LLM-based judge model aligned with human experts for evaluating visual insights. 3. Demonstrated improved insight quality under fixed compute budget in a data science pipeline application.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60e14b2c6ac8597444bea3610d692bad067ed798ec62c0920b0fe7c54b7fcd14_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60e14b2c6ac8597444bea3610d692bad067ed798ec62c0920b0fe7c54b7fcd14_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of scaling LLM agents for tasks with unverifiable rewards by introducing Selective Test-Time Scaling, which distributes compute across pipeline stages and prunes low-quality branches early using process-specific judges. Applied to generating visual insights from datasets, the method increases mean quality scores and reduces variance compared to traditional time-based refinement. The work provides a foundation for scaling complex, open-ended tasks like scientific discovery and story generation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Clinically Calibrated Machine Learning Benchmarks for Large-Scale Multi-Disorder EEG Classification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [medical signal processing], [electroencephalography, multi-disorder classification, sensitivity-oriented modeling, clinical calibration, feature importance analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Argha Kamal Samanta, Deepak Mewada, Monalisa Sarma, Debasis Samanta</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology, Kharagpur</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22656" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22656</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a clinically calibrated, sensitivity-prioritized machine learning framework for classifying eleven diverse neurological disorders from EEG data, addressing severe class imbalance. 2. Establishes realistic performance baselines for multi-disorder EEG classification, demonstrating recall exceeding 80% for most disorders with significant gains for low-prevalence conditions after threshold calibration. 3. Provides physiologically plausible feature importance analysis that aligns with established clinical EEG markers, validating the model&#x27;s clinical relevance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/636894ed25045336665fcbac593a58130411dff99c5d2a3e5a0cd50067ee44ec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/636894ed25045336665fcbac593a58130411dff99c5d2a3e5a0cd50067ee44ec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study addresses the challenge of automated, multi-disorder screening from clinical EEG data by developing disorder-aware machine learning models with decision thresholds explicitly calibrated to prioritize diagnostic sensitivity. The method uses a multi-domain feature set and is evaluated on a large, heterogeneous dataset, achieving high recall for most neurological disorder categories. The results establish performance baselines and demonstrate that sensitivity-prioritized automated analysis can support scalable EEG screening and triage in clinical practice.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival Melanocytic Intraepithelial Lesion Grading</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [multi-task learning, inter-task consistency, digital pathology, foundation models, combinatorial partial supervision]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mert Ikinci, Luna Toma, Karin U. Loeffler, Leticia Ussem, Daniela Süsskind, Julia M. Weller, Yousef Yeganeh, Martina C. Herwig-Carl, Shadi Albarqouni</p>
</li>
<li class="">
<p><strong>institution:</strong> University Hospital Bonn, Technical University of Munich</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22666" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22666</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces INTERACT-CMIL, a multi-head deep learning framework for jointly predicting five histopathological axes for CMIL grading. 2. Proposes a Shared Feature Learning with Combinatorial Partial Supervision and an Inter-Dependence Loss to enforce cross-task consistency. 3. Curates and evaluates on a new multi-center dataset, showing significant performance improvements over baselines and providing a reproducible benchmark.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ed4ee67160f114b6db4089d38a1fbeae9ea01e9231d6d4df14bea6d6144469c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ed4ee67160f114b6db4089d38a1fbeae9ea01e9231d6d4df14bea6d6144469c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the difficult problem of grading Conjunctival Melanocytic Intraepithelial Lesions (CMIL) by introducing INTERACT-CMIL, a multi-task deep learning framework that uses shared feature learning and an inter-dependence loss to jointly and consistently predict five diagnostic criteria. Evaluated on a new multi-center dataset, the method outperforms CNN and foundation model baselines, offering a coherent and interpretable computational tool for standardized diagnosis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [3D Inception, two-stream networks, CNN-RNN, EchoNet-Dynamic, video analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shravan Saranyan, Pramit Saha</p>
</li>
<li class="">
<p><strong>institution:</strong> Branham High School, University of Oxford</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22657" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22657</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A systematic investigation and comparison of multiple deep learning architectures (3D Inception, two-stream, CNN-RNN) for automated LVEF estimation from echocardiography videos. 2. Identification that modified 3D Inception architectures achieve the best performance (RMSE of 6.79%) on the EchoNet-Dynamic dataset. 3. Key insights on model design, including the tendency for smaller, simpler models to generalize better and the high sensitivity of performance to hyperparameters like kernel size and normalization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates deep learning models for automating the estimation of left ventricular ejection fraction (LVEF) from echocardiography videos to address the time-consuming and variable nature of manual assessment. The authors systematically evaluate and modify several architectures, including 3D Inception, two-stream, and CNN-RNN models, finding that a modified 3D Inception model performs best. The study concludes that while these models show promise, they are prone to overfitting and their performance is highly sensitive to specific hyperparameter choices.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Quantum Generative Models for Computational Fluid Dynamics: A First Exploration of Latent Space Learning in Lattice Boltzmann Simulations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [Quantum Generative Models, Computational Fluid Dynamics, Lattice Boltzmann Method, Vector Quantized Variational Autoencoder, Quantum Circuit Born Machine]</p>
</li>
<li class="">
<p><strong>authors:</strong> Achraf Hsain, Fouad Mohammed Abbou</p>
</li>
<li class="">
<p><strong>institution:</strong> Al Akhawayn University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22672" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22672</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A complete open-source pipeline bridging CFD simulation and quantum machine learning. 2. The first empirical study of quantum generative modeling on compressed latent representations of physics simulations. 3. A comparative analysis of quantum (QCBM, QGAN) and classical (LSTM) generative models for a physics-derived latent distribution.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6094a8215d7304400e5580e08cc0e81135106aaf9b51ef11b7f4c5d62734237e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6094a8215d7304400e5580e08cc0e81135106aaf9b51ef11b7f4c5d62734237e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper explores the application of quantum generative models to Computational Fluid Dynamics (CFD) data. The authors compress fluid simulation data into a discrete latent space using a VQ-VAE and then compare quantum (QCBM, QGAN) and classical (LSTM) models for generating samples from this distribution. Under their experimental conditions, the quantum models, particularly the QCBM, outperformed the classical baseline in generating samples closer to the true distribution.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [width pruning, expansion ratio, Maximum Absolute Weight (MAW), GLU-MLP, instruction-following]</p>
</li>
<li class="">
<p><strong>authors:</strong> Pere Martra</p>
</li>
<li class="">
<p><strong>institution:</strong> Universidad Internacional Menéndez Pelayo (UIMP)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22671" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22671</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Systematically characterizes a capability dichotomy where structured width pruning degrades parametric knowledge (e.g., MMLU) but significantly improves instruction-following (e.g., IFEval). 2. Discovers and quantifies a robust inverse correlation between factual knowledge and truthfulness, linking knowledge degradation under pruning to improved misconception discrimination. 3. Identifies the expansion ratio as a critical architectural parameter that selectively modulates cognitive capabilities, rather than just a compression metric, and quantifies its context-dependent efficiency trade-offs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4df6003ce0dd5672f67ef0c36b943a93c7cbb475cc96f2c7592e1f230af17d53_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4df6003ce0dd5672f67ef0c36b943a93c7cbb475cc96f2c7592e1f230af17d53_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates structured width pruning of GLU-MLP layers in Llama-3.2 models using the Maximum Absolute Weight (MAW) criterion. It finds that pruning creates a dichotomy: while parametric knowledge degrades, instruction-following improves and multi-step reasoning remains robust, challenging the assumption of uniform degradation. The main conclusion is that width pruning acts as a selective filter, reducing knowledge but preserving or enhancing behavioral alignment, with identified trade-offs in efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Beyond Centralization: Provable Communication Efficient Decentralized Multi-Task Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [decentralized learning, multi-task representation learning, low-rank structure, communication complexity, projected gradient descent]</p>
</li>
<li class="">
<p><strong>authors:</strong> Donghwa Kang, Shana Moothedath</p>
</li>
<li class="">
<p><strong>institution:</strong> Iowa State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22675</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new alternating projected gradient descent and minimization algorithm for decentralized multi-task representation learning with provable accuracy guarantees. 2. Provides comprehensive theoretical analysis of time, communication, and sample complexities, showing communication complexity is independent of target accuracy. 3. Identifies regimes (e.g., large number of nodes, low bandwidth) where the decentralized algorithm can outperform centralized federated learning approaches.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96a6cd15411863b0b8dc43a27acc5efd5e57130db256aae85b8195706d6a64ca_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96a6cd15411863b0b8dc43a27acc5efd5e57130db256aae85b8195706d6a64ca_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies decentralized multi-task representation learning where tasks share a low-rank structure. The authors propose a new alternating projected gradient descent algorithm with provable guarantees, showing its communication cost is independent of the target accuracy. Numerical simulations validate the theory and demonstrate scenarios where decentralized learning outperforms centralized federated methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image registration], [Neural ODEs, Structural Descriptors, Diffeomorphic Registration, Multimodal, Local Mutual Information]</p>
</li>
<li class="">
<p><strong>authors:</strong> Salvador Rodriguez-Sanz, Monica Hernandez</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Zaragoza, Aragon Institute for Engineering Research (I3A)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22689" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22689</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an instance-specific multimodal diffeomorphic registration framework using Neural ODEs, eliminating the need for large training datasets and avoiding performance drop on unseen modalities. 2. Introduces three method variants that integrate modality-agnostic structural descriptors (image-based or feature-based) with local mutual information for similarity measurement. 3. Demonstrates superior performance, robustness to varying regularization, suitability for different deformation scales, and computational efficiency compared to state-of-the-art baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/064c09ec95bfe97d740e2afb1b657324c426af97aabecb55dcfa4db080514f55_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/064c09ec95bfe97d740e2afb1b657324c426af97aabecb55dcfa4db080514f55_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of existing nonrigid registration methods, which often require intensity correlation and are limited to monomodal settings. It proposes a multimodal diffeomorphic registration method that combines Neural ODEs with structural descriptors and local mutual information. The method shows superior accuracy, robustness, and efficiency in aligning medical images from different modalities without requiring extensive training data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Learning with the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span></span></span></span>-adics</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [representation learning], [p-adic numbers, ultrametric space, hierarchical representation, non-archimedean geometry, semantic networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> André F. T. Martins</p>
</li>
<li class="">
<p><strong>institution:</strong> Instituto Superior Técnico, Universidade de Lisboa; Instituto de Telecomunicações</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22692" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22692</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes using the p-adic number field (Q_p) as an alternative to real numbers for machine learning, leveraging its ultrametric and hierarchical structure. 2. Develops foundational building blocks for classification, regression, and representation learning models and algorithms within the p-adic framework. 3. Demonstrates a novel application by representing simple Quillian semantic networks as compact p-adic linear networks, which is not achievable with real numbers.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec090da18463864436ff27e6cb4651eb7e01719b66dcdae5d6644770e6a7b488_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec090da18463864436ff27e6cb4651eb7e01719b66dcdae5d6644770e6a7b488_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper explores using p-adic numbers, an ultrametric and non-archimedean field, as an alternative to real numbers for machine learning. It introduces theoretical models and algorithms for classification, regression, and representation learning, showing that p-adics enable compact representations of hierarchical structures like semantic networks. The work opens new research directions by leveraging the unique geometric properties of p-adic spaces.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Predictive Modeling of Power Outages during Extreme Events: Integrating Weather and Socio-Economic Factors</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [predictive modeling], [power outage prediction, LSTM, socio-economic factors, machine learning, resilience]</p>
</li>
<li class="">
<p><strong>authors:</strong> Antar Kumar Biswas, Masoud H. Nazari</p>
</li>
<li class="">
<p><strong>institution:</strong> Wayne State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22699" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22699</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel learning-based framework specifically for predicting low-probability, high-consequence (LPHC) power outages during extreme events. 2. Integrates a comprehensive set of features from public data, including weather, socio-economic, infrastructure, and seasonal event data, to reveal community vulnerability patterns. 3. Empirically validates the framework on a large-scale Michigan dataset, demonstrating that the LSTM model achieves the lowest prediction error and identifying correlations between economic/infrastructure factors and outage occurrence.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3d66053b661eb55e66139efa4ce581315354dbf33b1490862dcf700576f9ef5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3d66053b661eb55e66139efa4ce581315354dbf33b1490862dcf700576f9ef5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a machine learning framework to predict power outages caused by extreme events by integrating weather, socio-economic, and infrastructure data. The authors evaluate four models (RF, SVM, AdaBoost, LSTM) on a dataset from Michigan and find that the LSTM performs best, with results showing that better economic conditions and infrastructure are linked to fewer outages.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] What Matters in Deep Learning for Time Series Forecasting?</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [channel-independence, locality, globality, forecasting model card]</p>
</li>
<li class="">
<p><strong>authors:</strong> Valentina Moretti, Andrea Cini, Ivan Marisca, Cesare Alippi</p>
</li>
<li class="">
<p><strong>institution:</strong> IDSIA (Università della Svizzera italiana), Politecnico di Milano</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22702" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22702</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Analyzes the design space of deep learning for time series forecasting, emphasizing principles like locality and globality over specific sequence modeling layers. 2. Highlights how overlooked implementation details (e.g., parameter sharing) fundamentally alter model classes and empirical results. 3. Proposes an auxiliary forecasting model card to systematically characterize architectures based on key design choices, advocating for improved benchmarking practices.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97a34d1ef2fb71ea41bf423f0bb9c22a61efb8b9f97da7ec25666712a5fa044b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97a34d1ef2fb71ea41bf423f0bb9c22a61efb8b9f97da7ec25666712a5fa044b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper critically examines deep learning architectures for time series forecasting, arguing that foundational design principles (e.g., locality vs. globality) are more crucial for accuracy than complex sequence modeling layers. It shows that simple, well-designed models can match state-of-the-art performance and reveals how implementation details significantly impact results. The authors propose a forecasting model card to standardize architecture characterization and call for a rethink of benchmarking practices.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [hope speech detection], [transformer models, multilingual classification, low-resource languages, XLM-RoBERTa, UrduBERT]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ahmed Abdullah, Sana Fatima, Haroon Mahmood</p>
</li>
<li class="">
<p><strong>institution:</strong> FAST-National University, Al Ain University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22705" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22705</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a multilingual framework for hope speech detection, specifically addressing the underrepresentation of low-resource languages like Urdu. 2. Applies and evaluates multiple pretrained transformer models (XLM-RoBERTa, mBERT, EuroBERT, UrduBERT) on the PolyHope-M 2025 benchmark for this task. 3. Demonstrates strong performance, achieving high F1-scores for Urdu classification, validating the use of existing multilingual models in low-resource settings.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c79c484e6d35762080aa8d6e1dbf075222d30335d656555a46ddf73380d7fe88_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c79c484e6d35762080aa8d6e1dbf075222d30335d656555a46ddf73380d7fe88_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of resources for hope speech detection in low-resource languages by proposing a multilingual framework using pretrained transformer models like XLM-RoBERTa and UrduBERT. The method involves simple preprocessing and training classifiers, which achieve high F1-scores on the PolyHope-M 2025 benchmark, particularly for Urdu. The results show that existing multilingual models can be effectively implemented to identify hope speech and foster positive digital discourse in low-resource environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Memento-II: Learning by Stateful Reflective Memory</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [stateful reflective decision process, episodic memory, policy iteration, continual learning, retrieval-augmented generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jun Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> University College London (UCL)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22716" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22716</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Stateful Reflective Decision Process (SRDP), a formal theoretical framework that models continual learning in LLM agents as a two-stage read-write interaction with episodic memory, linking it to policy evaluation and improvement. 2. Provides a theoretical analysis showing that the reflective learning process induces an equivalent Markov Decision Process, enabling the use of classical dynamic programming and RL tools, and establishes convergence guarantees when instantiated with entropy-regularised policy iteration. 3. Unifies heuristic approaches like case-based reasoning and retrieval-augmented generation with principled reinforcement learning, offering a rigorous mathematical foundation for building memory-augmented agents capable of online adaptation without parameter updates.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a theoretical framework for continual learning in LLM agents that uses episodic memory and reflection instead of back-propagation. The core method formalizes learning as a Stateful Reflective Decision Process, where writing to memory is policy evaluation and reading from it is policy improvement. The main conclusion is that this framework provides a principled, convergent foundation for agents to self-improve through interaction without fine-tuning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Data Augmentation for Classification of Negative Pregnancy Outcomes in Imbalanced Data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [text classification], [data augmentation, imbalanced dataset, social media analysis, natural language processing, pregnancy outcome]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Badsha Biswas</p>
</li>
<li class="">
<p><strong>institution:</strong> George Mason University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22732" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22732</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel approach to use public social media data (e.g., Twitter) as an adjunctive resource for studying negative pregnancy outcomes, addressing data scarcity in traditional epidemiological research. 2. Constructs an NLP pipeline to automatically identify and classify pregnancy experiences from unstructured, noisy social media text, distinguishing between positive and negative outcomes. 3. Investigates and evaluates various data augmentation techniques specifically to address the severe class imbalance inherent in social media data for this sensitive health domain.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb9c28a95fb880100ca20beffad94909ef9e73c38a75789c38961258454c014a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb9c28a95fb880100ca20beffad94909ef9e73c38a75789c38961258454c014a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of classifying negative pregnancy outcomes from imbalanced social media data. It proposes an NLP pipeline to extract and categorize pregnancy experiences from Twitter and investigates data augmentation techniques to balance the dataset. The research demonstrates the viability of social media data as a supplementary resource for epidemiological studies on pregnancy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [context folding, long-horizon RL, non-stationary observation, gradient dilution, selective segment training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiaqi Shao, Yufeng Miao, Wei Zhang, Bing Luo</p>
</li>
<li class="">
<p><strong>institution:</strong> Hong Kong University of Science and Technology, Duke Kunshan University, Microsoft AI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22733" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22733</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/SHAO-Jiaqi757/FoldAct" target="_blank" rel="noopener noreferrer" class="">https://github.com/SHAO-Jiaqi757/FoldAct</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Separated loss computation for independent gradient signals on summary and action tokens to address gradient dilution. 2. Full context consistency loss to reduce distribution shift caused by policy-dependent observation changes. 3. Selective segment training to reduce computational cost by processing unique contexts efficiently.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebdb4b7ca8ea3a44c0e368eed5fbbebfc656b663cf280d1891abfbf823c742fa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebdb4b7ca8ea3a44c0e368eed5fbbebfc656b663cf280d1891abfbf823c742fa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that treating context folding (history summarization) as a standard action in long-horizon RL for LLMs creates a non-stationary observation distribution, leading to training instability and inefficiency. It proposes FoldAct, a framework with three innovations—separated loss, consistency loss, and selective training—to stabilize training and improve efficiency. The method achieves stable training and a 5.19× speedup.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] When Does Multi-Task Learning Fail? Quantifying Data Imbalance and Task Independence in Metal Alloy Property Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-task learning], [negative transfer, data imbalance, task independence, metal alloys, property prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sungwoo Kang</p>
</li>
<li class="">
<p><strong>institution:</strong> Korea University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22740" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22740</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Empirical demonstration of a dichotomy where MTL degrades regression but improves classification for metal alloy property prediction. 2. Quantitative analysis linking MTL failure to severe data imbalance and near-zero inter-task dependencies. 3. Practical guidelines for materials informatics: use independent models for precise regression and MTL for classification tasks requiring high recall.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c5a4e5491f4e12b8112ec2efc4d5432ecc40f379be7394990c01d0cb507caab_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c5a4e5491f4e12b8112ec2efc4d5432ecc40f379be7394990c01d0cb507caab_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tests the assumption of Multi-Task Learning (MTL) in materials informatics by predicting three metal alloy properties. The results show MTL harms regression performance due to negative transfer from data imbalance but improves classification recall, as the properties are found to be independent. The work concludes with recommendations for when to use or avoid MTL in material discovery.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Bridging Global Intent with Local Details: A Hierarchical Representation Approach for Semantic Validation in Text-to-SQL</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [text-to-sql], [semantic validation, hierarchical representation, logical plan, abstract syntax tree, nested message passing neural network]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rihong Qiu, Zhibang Yang, Xinke Jiang, Weibin Liao, Xin Gao, Xu Chu, Junfeng Zhao, Yasha Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22744" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22744</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed HEROSQL, a hierarchical SQL representation approach that integrates global intent (via Logical Plans) and local details (via Abstract Syntax Trees) for semantic validation. 2. Employed a Nested Message Passing Neural Network (NMPNN) to capture relational information in SQL and aggregate schema-guided semantics across LPs and ASTs. 3. Introduced an AST-driven sub-SQL augmentation strategy to generate high-quality negative samples for robust optimization of fine-grained semantic inconsistencies.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2268a4534cffc0a143786b6b8d80dbddb03e7bf7ffd9234c8d468b2a06e151f0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2268a4534cffc0a143786b6b8d80dbddb03e7bf7ffd9234c8d468b2a06e151f0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of semantic validation in Text-to-SQL systems by proposing HEROSQL, a hierarchical representation method that combines global intent and local SQL details using Logical Plans and Abstract Syntax Trees, enhanced by a Nested Message Passing Neural Network. It also introduces an AST-driven augmentation strategy for generating negative samples. Experiments show that HEROSQL outperforms state-of-the-art methods in detecting semantic inconsistencies, improving AUPRC by 9.40% and AUROC by 12.35% on average.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] From Confounding to Learning: Dynamic Service Fee Pricing on Third-Party Platforms</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [demand learning, instrumental variables, deep neural networks, regret bound, confounding]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rui Ai, David Simchi-Levi, Feng Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> Massachusetts Institute of Technology (MIT)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22749" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22749</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed an algorithm for dynamic service fee pricing with optimal regret, showing a phase transition based on supply-side noise. 2. Demonstrated that non-i.i.d. actions can serve as instrumental variables to address confounding in demand learning. 3. Proposed a novel homeomorphic construction to establish estimation bounds for learning demand with deep neural networks without requiring star-shapedness assumptions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a53b1242a3048a4f1795be36eec0fe0875f0b532bc8001a000dcba26693c66da_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a53b1242a3048a4f1795be36eec0fe0875f0b532bc8001a000dcba26693c66da_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the problem of dynamic service fee pricing on third-party platforms, where only equilibrium price and quantity are observable, creating a confounding demand learning problem. The authors develop an algorithm with optimal regret, using non-i.i.d. actions as instrumental variables and a novel homeomorphic construction for deep neural network-based demand estimation. The results show that supply-side noise fundamentally impacts learnability and the method is validated with simulations and real-world data from Zomato and Lyft.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Micro-Macro Machine Learning Framework for Predicting Childhood Obesity Risk Using NHANES and Environmental Determinants</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [public health informatics], [multi-level modeling, XGBoost, environmental vulnerability index, NHANES, machine learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Eswarasanthosh Kumar Mamillapalli, Nishtha Sharma</p>
</li>
<li class="">
<p><strong>institution:</strong> n/a (Inferred from email: nishtha <a href="mailto:sharma@nh.gov" target="_blank" rel="noopener noreferrer" class="">sharma@nh.gov</a> suggests potential affiliation with a state health department, but no clear academic/research institution is specified. The other author&#x27;s email is a personal Gmail address.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22758" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22758</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a novel micro-macro machine learning framework that integrates individual-level health data with macro-level environmental data to predict childhood obesity risk. 2. Constructed a composite environmental vulnerability index (EnvScore) from USDA and EPA data to quantify state-level structural risk factors. 3. Demonstrated a scalable, data-driven modeling pipeline that reveals geographic alignment between high environmental burden and predicted individual obesity risk, enabling identification of environment-driven health disparities.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7578fbec69afd6c4e44bd1d6715e6f98a34a451a47c81c03388bed0ae331bd4c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7578fbec69afd6c4e44bd1d6715e6f98a34a451a47c81c03388bed0ae331bd4c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a machine learning framework that combines individual health data from NHANES with environmental data (food access, air quality) from USDA/EPA to predict childhood obesity. The best-performing model was XGBoost, and a state-level environmental risk score was created. The study found a strong geographic correlation between areas of high environmental burden and high predicted obesity risk, showing the value of multi-scale data integration for public health.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Active Constraint Learning in High Dimensions from Demonstrations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robot learning], [active learning, constraint inference, Gaussian processes, learning from demonstration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zheng Qiu, Chih-Yuan Chiu, Glen Chou</p>
</li>
<li class="">
<p><strong>institution:</strong> Georgia Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22757" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22757</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an iterative active constraint learning (ACL) algorithm that intelligently queries for new demonstrations to reduce constraint uncertainty. 2. Integrates a Gaussian process (GP) model within the learning from demonstrations (LfD) paradigm to represent and infer unknown constraints. 3. Demonstrates superior performance over a random-sampling baseline in recovering nonlinear constraints from sparse, informative demonstrations in high-dimensional settings with nonlinear dynamics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5acbf135d5aa431114b6b72db2fb101427cb6bd1e55fb1a8afd3028c0874cb4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5acbf135d5aa431114b6b72db2fb101427cb6bd1e55fb1a8afd3028c0874cb4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the data inefficiency of learning unknown constraints from demonstrations by proposing an active learning algorithm. The method iteratively trains a Gaussian process on demonstration data to model constraints and uses the model&#x27;s uncertainty to query for new, informative start/goal states to generate more demonstrations. Experiments show the approach outperforms a random-sampling baseline in accurately inferring constraints from fewer demonstrations in high-dimensional, nonlinear environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Understanding the Mechanisms of Fast Hyperparameter Transfer</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [hyperparameter optimization], [hyperparameter transfer, scale-aware hyperparameters, Maximal Update Parameterization (μP), compute-optimal grid search]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nikhil Ghosh, Denny Wu, Alberto Bietti</p>
</li>
<li class="">
<p><strong>institution:</strong> Flatiron Institute, New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22768" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22768</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Develops a formal conceptual framework defining &quot;fast&quot; hyperparameter transfer and proves its equivalence to &quot;useful&quot; transfer for compute-optimal grid search. 2. Demonstrates that the fast transfer property is not universal and depends critically on problem structure, showing synthetic cases where it succeeds or fails. 3. Proposes and provides empirical evidence for a mechanistic hypothesis explaining fast transfer, decomposing the loss reduction into width-stable and width-sensitive components.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b19c9825d64e8e70117e18cd478466aaf2627a7a5167d401bcac21353870d508_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b19c9825d64e8e70117e18cd478466aaf2627a7a5167d401bcac21353870d508_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the mechanisms behind fast hyperparameter transfer, a strategy to reduce tuning costs by transferring optimal hyperparameters from small to large models. It formally defines fast transfer and shows it is computationally advantageous, then explains the phenomenon by hypothesizing a decomposition of the optimization trajectory into stable and sensitive components, supported by empirical evidence.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [causal inference], [causal transportability, domain adaptation, few-shot learning, circuit composition, distribution shift]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kasra Jalaldoust, Elias Bareinboim</p>
</li>
<li class="">
<p><strong>institution:</strong> Columbia University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22777" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22777</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Circuit-TR, an algorithm for zero-shot compositional generalization based on causal transportability theory, using modules learned from source data. 2. Introduced a supervised domain adaptation scheme that leverages circuit transportability without requiring an explicit causal graph, using only limited target data. 3. Provided theoretical characterization of few-shot learnable tasks using graphical circuit transportability criteria, linking generalizability to circuit size complexity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b025a886304d6097d2893ec4af3bb34a59528db65e22ddf5b4dfff4439a096_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b025a886304d6097d2893ec4af3bb34a59528db65e22ddf5b4dfff4439a096_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of generalization under distribution shift by proposing a method based on causal transportability theory. The method, Circuit-TR, learns local predictors (modules) from source data and composes them into a circuit for prediction in a target domain, enabling both zero-shot and few-shot adaptation. The theoretical results connect few-shot learnability to circuit transportability criteria and complexity, which are supported by simulations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [temporal graph neural networks, explainable ai, graph explanation, recurrent neural networks, breadth-first search]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xuyan Li, Jie Wang, Zheng Yan</p>
</li>
<li class="">
<p><strong>institution:</strong> Xidian University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22772" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22772</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GRExplainer, a universal explanation method applicable to both snapshot-based and event-based Temporal Graph Neural Networks (TGNNs). 2. Introduces an efficient approach using breadth-first search and temporal information to construct node sequences, reducing computational cost. 3. Designs a user-friendly generative model based on Recurrent Neural Networks (RNNs) for automated and continuous explanation generation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0722c64cfeebe092d7b253f417faaa51990e69198068745c39fe241716082408_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0722c64cfeebe092d7b253f417faaa51990e69198068745c39fe241716082408_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of explainability in Temporal Graph Neural Networks (TGNNs) by proposing GRExplainer, a universal and efficient method that uses node sequences and an RNN-based generative model to provide explanations. Experiments on six datasets with three TGNNs demonstrate that GRExplainer outperforms existing methods in generality, efficiency, and user-friendliness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Schrodinger AI: A Unified Spectral-Dynamical Framework for Classification, Reasoning, and Operator-Based Generalization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [quantum-inspired machine learning], [spectral decomposition, Hamiltonian learning, semantic wavefunctions, operator calculus, emergent manifolds]</p>
</li>
<li class="">
<p><strong>authors:</strong> Truong Son Nguyen</p>
</li>
<li class="">
<p><strong>institution:</strong> Arizona State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22774" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22774</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A unified spectral-dynamical framework for machine learning inspired by quantum mechanics, comprising a time-independent wave-energy solver, a time-dependent dynamical solver, and a low-rank operator calculus. 2. Demonstration of emergent semantic manifolds that reflect class relations without explicit supervision, dynamic reasoning that adapts to changing environments, and exact operator generalization on symbolic tasks. 3. Proposing a new foundational direction for ML where learning is cast as discovering and navigating an underlying semantic energy landscape, offering an alternative to cross-entropy training and transformer attention.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58825c1201e17641b4e19a581a742615913539c66fbf08e5c113620bf7eec6de_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58825c1201e17641b4e19a581a742615913539c66fbf08e5c113620bf7eec6de_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Schrödinger AI, a novel machine learning framework inspired by quantum mechanics that treats perception as spectral decomposition and reasoning as wavefunction dynamics. The method demonstrates robust generalization, interpretable semantics, and emergent topology on tasks like classification, maze navigation, and modular arithmetic. The results suggest a promising new paradigm for AI that learns by discovering an underlying semantic energy landscape.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Discovering Transmission Dynamics of COVID-19 in China</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [information extraction], [natural language processing, transmission chain, epidemiological analysis, data mining, statistical analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhou Yang, Edward Dougherty, Chen Zhang, Zhenhe Pan, Fang Jin</p>
</li>
<li class="">
<p><strong>institution:</strong> George Washington University, Roger Williams University, Texas Tech University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22787" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22787</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Constructed transmission/tracking chains for COVID-19 in China by applying NLP and manual curation to data mined from diverse public sources. 2. Conducted a comprehensive spatiotemporal analysis by integrating case tracking data with population mobility data from Wuhan. 3. Quantified key transmission dynamics, revealing regional differences, hospitalization timelines, and the evolution of infection sources over the course of the pandemic.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eca2aa3a96ff37b70cb26e6ad2cfa0f84cc9a5cf8c76b6e1404ce5fa2474828e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eca2aa3a96ff37b70cb26e6ad2cfa0f84cc9a5cf8c76b6e1404ce5fa2474828e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the transmission dynamics of COVID-19 in China by mining and processing public case reports using NLP to construct transmission chains. The analysis integrates these chains with mobility data to quantify spatiotemporal spread. Key findings include significant regional differences in infection rates, rapid hospitalization of symptomatic cases, and a shift in infection sources from travel to social activities over time.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] CNSight: Evaluation of Clinical Note Segmentation Tools</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [text segmentation], [clinical note segmentation, transformer models, large language models, MIMIC-IV, rule-based baselines]</p>
</li>
<li class="">
<p><strong>authors:</strong> Risha Surana, Adrian Law, Sunwoo Kim, Rishab Sridhar, Angxiao Han, Peiyu Hong</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Southern California</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22795" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22795</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A comprehensive evaluation of diverse methods (rule-based, domain-specific transformers, and large language models) for the task of clinical note segmentation. 2. The curation and use of a dataset of 1,000 notes from MIMIC-IV for benchmarking segmentation performance. 3. Empirical findings that large API-based models (e.g., GPT-5-mini) achieve the best overall performance, while lightweight baselines remain competitive only on structured tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be9738f44f0f558dc344bd32b267879341b566035c5dbe67f97ac2e4529b479_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be9738f44f0f558dc344bd32b267879341b566035c5dbe67f97ac2e4529b479_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper evaluates various methods for segmenting unstructured clinical notes into distinct sections. It compares rule-based baselines, domain-specific transformers, and large language models on a curated dataset from MIMIC-IV. The main conclusion is that large API-based models like GPT-5-mini achieve the best overall segmentation performance, providing guidance for method selection in downstream clinical applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition via Spherical Normalization and Mahalanobis Distance</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [open-set recognition], [spherical normalization, Mahalanobis distance, electronic nose, open-set recognition, feature drift]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuai Chen, Chen Wang, Ziran Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> School of Mechanical Engineering, Shandong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22792" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22792</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A geometric decoupling mechanism using cascaded batch normalization and L2 normalization to project features onto a unit hypersphere, eliminating signal intensity fluctuations. 2. The introduction of Mahalanobis distance as a scoring mechanism to construct adaptive ellipsoidal decision boundaries that account for anisotropic feature distributions. 3. A universal, architecture-agnostic framework (SNM-Net) that can be seamlessly integrated with various backbone networks (CNN, RNN, Transformer) for robust open-set gas recognition.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d69d593ebbfea881a49530f570b5c2a934cb8b5cd782c1d7e7c9fba99a92906_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d69d593ebbfea881a49530f570b5c2a934cb8b5cd782c1d7e7c9fba99a92906_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes SNM-Net, a universal framework for robust open-set gas recognition in electronic nose systems. It addresses signal drift and unknown interference by projecting features onto a hypersphere for intensity normalization and using Mahalanobis distance for scoring. The method achieves state-of-the-art performance with high accuracy and exceptional robustness across different sensor conditions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ReDiF: Reinforced Distillation for Few Step Diffusion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [reinforcement learning, knowledge distillation, policy optimization, denoising paths, model agnostic]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amirhossein Tighkhorshid, Zahra Dehghanian, Gholamali Aminian, Chengchun Shi, Hamid R. Rabiee</p>
</li>
<li class="">
<p><strong>institution:</strong> Sharif University of Technology, Alan Turing Institute, London School of Economics</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22802" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22802</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel reinforcement learning framework for distilling diffusion models, treating distillation as a policy optimization problem. 2. Introduces a reward signal based on alignment with teacher outputs, allowing the student model to explore multiple denoising paths and take longer, optimized steps. 3. Demonstrates a model-agnostic framework that achieves superior performance with fewer inference steps and computational resources compared to existing distillation techniques.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73d47eb5443f9f8848454e65825da3569c70d954239d9794e6cff086fa5bc17a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73d47eb5443f9f8848454e65825da3569c70d954239d9794e6cff086fa5bc17a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the slow sampling problem in diffusion models by proposing ReDiF, a reinforcement learning-based distillation framework. Instead of using fixed losses, it treats distillation as policy optimization, using a reward signal to guide the student to take longer, optimized steps. The method achieves better performance with fewer steps and is applicable to various diffusion models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MoR: Mixture Of Representations For Mixed-Precision Training</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [mixed-precision training, FP8, dynamic quantization, tensor representation, low-precision training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bor-Yiing Su, Peter Dykas, Mike Chrzanowski, Jatin Chhugani</p>
</li>
<li class="">
<p><strong>institution:</strong> Nvidia, Meta</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22804" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22804</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Mixture-of-Representations (MoR), a novel per-tensor and sub-tensor level quantization framework that dynamically selects numerical representations based on tensor properties. 2. Introduces and experiments with concrete algorithms that dynamically choose between FP8 and BF16 representations at different granularities. 3. Demonstrates a universal approach that preserves model quality across datasets and achieves state-of-the-art results with 98.38% of tensors quantized to FP8, showing potential for even lower precision formats like NVFP4.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14c70a65c4f996e1c88d9996c77e4d780e13466bf11a0601ad82d27376753968_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14c70a65c4f996e1c88d9996c77e4d780e13466bf11a0601ad82d27376753968_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces MoR, a dynamic quantization framework for mixed-precision training that analyzes tensor properties to select between representations like FP8 and BF16. It achieves high FP8 quantization rates (98.38%) while maintaining model quality, offering a robust approach for low-precision training that can be combined with other methods for even lower precision formats.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Long-Range Distillation: Distilling 10,000 Years of Simulated Climate into Long Timestep AI Weather Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [climate informatics], [long-range distillation, synthetic training data, subseasonal-to-seasonal forecasting, probabilistic forecasting, autoregressive models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Scott A. Martin, Noah Brenowitz, Dale Durran, Michael Pritchard</p>
</li>
<li class="">
<p><strong>institution:</strong> NVIDIA Research, University of Washington</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22814" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22814</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes long-range distillation, a method to train a long-timestep probabilistic model using massive synthetic data from an autoregressive teacher model. 2. Demonstrates the generation and use of over 10,000 years of simulated climate data from the DLESyM model for training. 3. Shows that distilled models achieve S2S forecast skill comparable to ECMWF ensembles after fine-tuning, with skill scaling with synthetic data volume.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4b3dd444e162236a7d1cc0aaa69d999407d7ebf78184fada39f979dccbd0692f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4b3dd444e162236a7d1cc0aaa69d999407d7ebf78184fada39f979dccbd0692f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of long-range weather forecasting by introducing long-range distillation, a method that trains a single-step probabilistic model using a massive synthetic dataset generated by an autoregressive AI model. The distilled model, trained on over 10,000 years of simulated climate, achieves subseasonal-to-seasonal forecast skill comparable to state-of-the-art ensemble methods, demonstrating that AI-generated synthetic data can effectively scale long-range forecast skill.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [curriculum learning, goal-conditioned reinforcement learning, temporal variance, student-teacher paradigm, Q-function]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gaurav Chaudhary, Laxmidhar Behera</p>
</li>
<li class="">
<p><strong>institution:</strong> IIT Kanpur</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22824" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22824</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Student-Teacher learning paradigm with a Temporal Variance-Driven Curriculum for accelerating Goal-Conditioned RL. 2. Establishes a theoretical connection between the temporal variance of Q-values and policy evolution. 3. Demonstrates the algorithm-agnostic nature of the approach, showing consistent improvements across 11 robotic manipulation and maze navigation tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c410b802a9fdb44624e8bf6d607803fec918def24d7a3c81bf3fb12d7fb2e63_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c410b802a9fdb44624e8bf6d607803fec918def24d7a3c81bf3fb12d7fb2e63_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the sample inefficiency of uniform goal selection in multi-goal reinforcement learning. It proposes a TEACH framework where a teacher module dynamically selects goals with the highest temporal variance in Q-values to create an adaptive curriculum. The method is shown to improve learning efficiency over state-of-the-art curriculum learning methods across diverse robotic tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [Human-Object Interaction, Diffusion Transformer, Relative Coordinate Maps, Progressive Curriculum Learning, Geometry Consistency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bangya Liu, Xinyu Gong, Zelin Zhao, Ziyang Song, Yulei Lu, Suhui Wu, Jun Zhang, Suman Banerjee, Hao Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Wisconsin-Madison, ByteDance, Georgia Institute of Technology, The Hong Kong Polytechnic University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22854" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22854</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://neutrinoliu.github.io/byteloom/" target="_blank" rel="noopener noreferrer" class="">https://neutrinoliu.github.io/byteloom/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ByteLoom, a Diffusion Transformer-based framework for generating realistic HOI videos with geometrically consistent objects. 2. Introduces the RCM-cache mechanism using Relative Coordinate Maps to maintain object geometry consistency and control 6-DoF transformations. 3. Designs a progressive training curriculum to compensate for HOI dataset scarcity and relax the need for fine-grained hand mesh annotations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9f5ff16308904b889be6695aafd24bfc28b798f080cc6c723a25066bcdc2bdf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9f5ff16308904b889be6695aafd24bfc28b798f080cc6c723a25066bcdc2bdf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of poor cross-view consistency and reliance on hand mesh annotations in Human-Object Interaction (HOI) video generation. It proposes ByteLoom, a framework that uses a novel RCM-cache mechanism for geometry consistency and a progressive curriculum learning strategy for training. The method effectively preserves human identity and object geometry while generating smooth motion.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [blockchain security, IoT security, adversarial machine learning], [Fully Homomorphic Encryption, Attribute-Based Access Control, Multi-Agent Reinforcement Learning, Byzantine Fault Tolerance, Trust-Based Consensus]</p>
</li>
<li class="">
<p><strong>authors:</strong> Soham Padia, Dhananjay Vaidya, Ramchandra Mangrulkar</p>
</li>
<li class="">
<p><strong>institution:</strong> Northeastern University, Dwarkadas J. Sanghvi College of Engineering</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22860" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22860</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel trust-based delegated consensus framework for blockchain IoT that integrates Fully Homomorphic Encryption (FHE) with Attribute-Based Access Control (ABAC) for privacy-preserving policy evaluation. 2. Systematically compares the performance of three reinforcement learning approaches (RL, DRL, MARL) against five distinct and sophisticated adversarial attack families. 3. Empirically demonstrates that Multi-Agent RL (MARL) provides superior defense against collusive attacks and identifies the catastrophic vulnerability of all learning agents to Time-Delayed Poisoning (sleeper) attacks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373f990d6c218afb4f9e660bb84fd86dc8e9d7006d2b7bdef3d956a991960bff_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373f990d6c218afb4f9e660bb84fd86dc8e9d7006d2b7bdef3d956a991960bff_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses securing blockchain-enabled IoT networks by proposing a trust-based consensus framework that combines privacy-preserving techniques (FHE and ABAC) with learning-based defenses. It compares RL, DRL, and MARL against five attack types, finding MARL most effective against collusive attacks but revealing that all methods are highly vulnerable to time-delayed poisoning attacks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent reinforcement learning], [Reinforcement Networks, directed acyclic graph (DAG), credit assignment, LevelEnv, hierarchical RL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Maksim Kryzhanovskiy, Svetlana Glazyrina, Roman Ischenko, Konstantin Vorontsov</p>
</li>
<li class="">
<p><strong>institution:</strong> Lomonosov Moscow State University, Institute for Artificial Intelligence, Lomonosov Moscow State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22876" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22876</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Reinforcement Networks framework, a general approach for collaborative MARL that organizes agents as vertices in a directed acyclic graph (DAG)., 2. Formalizes training and inference methods for the framework and connects it to the LevelEnv concept for reproducible construction and evaluation., 3. Demonstrates improved performance over standard MARL baselines and unifies hierarchical, modular, and graph-structured views of MARL.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b613175c92e9bdddfbd57ed84d044a5846b7b8148cca8ab0405e69847f66c33a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b613175c92e9bdddfbd57ed84d044a5846b7b8148cca8ab0405e69847f66c33a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of end-to-end training for AI systems with multiple learnable components. It proposes Reinforcement Networks, a framework that organizes agents in a directed acyclic graph for flexible credit assignment and coordination in multi-agent reinforcement learning. The method shows improved performance over baselines and provides a principled foundation for designing complex multi-agent systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Fundamental Novel Consistency Theory: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span></span></span></span>-Consistency Bounds</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [statistical learning theory], [H-consistency bounds, surrogate loss, minimizability gaps, adversarial robustness, comp-sum losses]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yutao Zhong</p>
</li>
<li class="">
<p><strong>institution:</strong> New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22880" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22880</a></p>
</li>
<li class="">
<p><strong>contributions:</strong>  1. Introduces a novel theoretical framework for deriving H-consistency bounds, which provide stronger and more informative guarantees than Bayes-consistency or H-calibration by accounting for the hypothesis set. 2. Establishes the first H-consistency bounds for a wide range of losses in binary and multi-class classification, including convex surrogates, max/sum/constrained losses, and comp-sum losses (e.g., cross-entropy), and extends the analysis to adversarial scenarios. 3. Analyzes the growth rates of H-consistency bounds, proving a universal square-root growth rate for smooth surrogates, and introduces the analysis of minimizability gaps to guide the selection of surrogate loss functions for learning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16541a1efc1f9f95010f468aecfaeb95d92c6aea20df9566f6ef60456e371fde_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16541a1efc1f9f95010f468aecfaeb95d92c6aea20df9566f6ef60456e371fde_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a new theoretical framework for analyzing the estimation error of target losses using surrogate losses in machine learning. It introduces H-consistency bounds, which offer stronger guarantees by incorporating the hypothesis set, and derives these bounds for various loss functions in both standard and adversarial settings. The main conclusion is that these bounds provide a more precise tool for understanding surrogate loss performance and can guide the design of robust learning algorithms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Theory and Algorithms for Learning with Multi-Class Abstention and Multi-Expert Deferral</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [machine learning theory], [multi-expert deferral, abstention, H-consistency, surrogate losses, two-stage learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Anqi Mao</p>
</li>
<li class="">
<p><strong>institution:</strong> New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22886" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22886</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced new surrogate loss families and proved strong consistency guarantees for multi-class learning with abstention, resolving open questions. 2. Designed new surrogate losses with H-consistency bounds for general multi-expert deferral in classification, leading to effective algorithms. 3. Proposed a novel framework and surrogate losses for regression with deferral, accommodating multiple experts and various cost structures.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed6952a3ce4104bfb4e2da48692fa87841baf950369a1d08640407f2002ece4c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed6952a3ce4104bfb4e2da48692fa87841baf950369a1d08640407f2002ece4c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This thesis addresses the problems of learning with abstention and multi-expert deferral to improve the reliability and efficiency of models like LLMs. It proposes new surrogate loss formulations for classification and regression, proves strong theoretical consistency guarantees, and demonstrates the empirical effectiveness of the resulting algorithms on datasets like CIFAR-10 and CIFAR-100.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Debugging Tabular Log as Dynamic Graphs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [dynamic graph, graph neural network, tabular log, log debugging, heterogeneous nodes]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chumeng Liang, Zhanyang Jin, Zahaib Akhtar, Mona Pereira, Haofei Yu, Jiaxuan You</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Illinois Urbana-Champaign, Amazon</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22903" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22903</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GraphLogDebugger, a novel framework that models tabular log data as dynamic graphs with heterogeneous nodes for objects and events, 2. Demonstrates that a simple dynamic GNN can outperform large language models (LLMs) in debugging tasks using this graph representation, 3. Validates the approach on real-world datasets from computer systems and academic papers, showing improved flexibility and scalability over LLM-based methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfc0dd96717274f366abd002f1a9147f7afbdaba795d251628919a0d25289925_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfc0dd96717274f366abd002f1a9147f7afbdaba795d251628919a0d25289925_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces GraphLogDebugger, a framework that converts tabular log data into dynamic graphs to detect inconsistencies in real-world systems. By representing logs as evolving graphs with object and event nodes, a lightweight dynamic Graph Neural Network effectively debugs logs, outperforming larger LLM-based models in experiments on system and academic log datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Neural Network-Based Real-time Casing Collar Recognition System for Downhole Instruments</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [Casing Collar Locator (CCL), ARM Cortex-M7, Depthwise Separable Convolutions, MACs, Inference Latency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Si-Yu Xiao, Xin-Di Zhao, Xiang-Zhan Wang, Tian-Hao Mao, Ying-Kai Liao, Xing-Yu Liao, Yu-Qiao Chen, Jun-Jie Wang, Shuang Liu, Tu-Pei Chen, Yang Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China, China National Petroleum Corporation Logging Co., Ltd., Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22901" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22901</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an in-situ, real-time collar recognition system using embedded neural networks to overcome signal degradation in traditional surface-based monitoring. 2. Introduces lightweight &quot;Collar Recognition Nets&quot; (CRNs) optimized for resource-constrained ARM Cortex-M7 microprocessors, using temporal and depthwise separable convolutions. 3. Demonstrates a highly efficient model achieving 8,208 MACs, an F1 score of 0.972, and an average inference latency of 343.2 µs, proving feasibility for downhole power/space constraints.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/235a8bd15d5c0da93f1ea31dd4b6da44b238cc7be57fd42a7bef3e629f9c6495_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/235a8bd15d5c0da93f1ea31dd4b6da44b238cc7be57fd42a7bef3e629f9c6495_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of accurate downhole positioning in oil/gas operations by developing a real-time, embedded neural network system for casing collar recognition. The method introduces lightweight &quot;Collar Recognition Nets&quot; optimized for ARM Cortex-M7 processors, achieving high accuracy with minimal computational cost. The results demonstrate that robust, autonomous signal processing is feasible within the severe power and space limitations of downhole instrumentation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Federated Multi-Task Clustering</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [federated clustering, spectral clustering, multi-task learning, tensor methods, ADMM]</p>
</li>
<li class="">
<p><strong>authors:</strong> S. Dai, G. Sun, F. Li, X. Tang, Q. Wang, Y. Cong</p>
</li>
<li class="">
<p><strong>institution:</strong> South China University of Technology, Dalian University of Technology, Xidian University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22897" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22897</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Federated Multi-Task Clustering (FMTC) framework that learns personalized models for heterogeneous clients while capturing shared knowledge in a privacy-preserving manner. 2. Introduces a client-side parameterized mapping model for robust out-of-sample inference, eliminating the need for unreliable pseudo-labels. 3. Develops a server-side tensorial correlation module using low-rank regularization on a unified model tensor to explicitly discover common subspace across clients, solved via a privacy-preserving ADMM-based distributed algorithm.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e64c06637c228c53ac30a2069610927ac906eea1fc53a050d87f6f985e001ab_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e64c06637c228c53ac30a2069610927ac906eea1fc53a050d87f6f985e001ab_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes FMTC, a federated multi-task clustering framework that addresses the limitations of centralized spectral clustering and unreliable pseudo-labels in federated settings. It combines client-side personalized clustering models with a server-side tensorial module to capture shared knowledge, using an ADMM-based algorithm for efficient, privacy-preserving optimization. Experiments show FMTC outperforms existing federated clustering methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MetaCD: A Meta Learning Framework for Cognitive Diagnosis based on Continual Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [educational data mining], [cognitive diagnosis, meta-learning, continual learning, long-tailed distribution, parameter protection mechanism]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jin Wu, Chanjin Zheng</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Institute of Artificial Intelligence for Education, East China Normal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22904" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22904</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes MetaCD, a novel framework that integrates meta-learning and continual learning for cognitive diagnosis. 2. Uses meta-learning to learn an optimal initialization to alleviate the long-tailed data problem, enabling good performance with few samples. 3. Incorporates a continual learning parameter protection mechanism to adapt to dynamic data changes and new tasks while preventing catastrophic forgetting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d62de4bde455a8e131e7247a8b8ce8fa94feb82289f4b3dc1660955e7645dfa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d62de4bde455a8e131e7247a8b8ce8fa94feb82289f4b3dc1660955e7645dfa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes MetaCD, a meta-learning framework based on continual learning, to address the challenges of long-tailed data distribution and dynamic changes in cognitive diagnosis for intelligent education. It uses meta-learning for optimal initialization and a parameter protection mechanism for continual adaptation, achieving superior accuracy and generalization on five real-world datasets compared to baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Q-learning, ensemble learning, satisficing, distillation, bounded rationality]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ünver Çiftçi</p>
</li>
<li class="">
<p><strong>institution:</strong> Tekirdağ Namık Kemal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22910" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22910</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a two-phase framework (Sat-EnQ) that first trains an ensemble of lightweight Q-networks using a satisficing objective to limit early value growth and reduce variance. 2. Provides theoretical proof that the satisficing objective induces bounded updates and cannot increase target variance, with a corollary for substantial reduction. 3. Demonstrates empirical results including significant variance reduction, elimination of catastrophic failures, robustness to noise, and improved compute efficiency compared to baseline methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d12c2382ed5ee9e4da47d1775097d950b626f676e5d3552cd0ff19b6c385b2a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d12c2382ed5ee9e4da47d1775097d950b626f676e5d3552cd0ff19b6c385b2a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the instability of deep Q-learning, especially early in training, by introducing Sat-EnQ. This framework first trains a satisficing ensemble of weak Q-learners to produce stable, low-variance estimates, then distills and fine-tunes the ensemble. The method significantly improves training reliability, robustness, and computational efficiency compared to standard approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Geometric Structural Knowledge Graph Foundation Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [knowledge graph reasoning], [structural foundation model, geometric attention, inductive link prediction, multi-head transformation, relational fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ling Xin, Mojtaba Nayyeri, Zahra Makki Nayeri, Steffen Staab</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Stuttgart, University of Southampton, Shahrood University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22931" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22931</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Gamma, a novel structural KG foundation model that replaces the single relational transformation with multiple parallel geometric transformations (real, complex, split-complex, dual). 2. Introduces a relational conditioned attention fusion mechanism with entropy regularization to adaptively fuse these geometric representations at the link level. 3. Provides a full formalization of the algebraic message functions and demonstrates through extensive experiments on 56 KGs that Gamma consistently outperforms the prior state-of-the-art (Ultra) in zero-shot inductive link prediction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1926af9f65861ace968c86c6c18e3eaa892e2114d0d505e3fce8a7ae39975f1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1926af9f65861ace968c86c6c18e3eaa892e2114d0d505e3fce8a7ae39975f1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies a key limitation in existing structural knowledge graph foundation models: their reliance on a single relational transformation limits their ability to capture diverse relational patterns. To address this, the authors propose Gamma, a new model that employs multi-head geometric attention, using parallel transformations from different algebraic spaces and a fusion mechanism to adaptively combine them. Comprehensive experiments show that Gamma outperforms the previous best model, Ultra, in zero-shot inductive link prediction across diverse benchmarks, demonstrating the benefit of complementary geometric representations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multiple Token Divergence: Measuring and Steering In-Context Computation Density</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [language model interpretability], [in-context computation, KL divergence, decoding method, computational effort, prediction head]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vincent Herrmann, Eric Alcaide, Michael Wand, Jürgen Schmidhuber</p>
</li>
<li class="">
<p><strong>institution:</strong> The Swiss AI Lab IDSIA/USI/SUPSI, King Abdullah University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22944" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22944</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Multiple Token Divergence (MTD), a simple and non-invasive metric to measure a language model&#x27;s in-context computational effort by comparing the output distributions of the full model and a shallow auxiliary head. 2. Introduces Divergence Steering, a novel decoding method that uses MTD to control the computational character of generated text. 3. Empirically demonstrates that MTD effectively distinguishes task complexity, correlates with problem difficulty, and that lower MTD is associated with more accurate reasoning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef98fdc1c198bde00876e87ff3cca172c3925998a10f9847042108bebc8c5e99_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef98fdc1c198bde00876e87ff3cca172c3925998a10f9847042108bebc8c5e99_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of measuring the in-context computational effort of language models. It proposes Multiple Token Divergence (MTD), a lightweight metric based on KL divergence between output distributions, and a corresponding decoding method called Divergence Steering. The authors show that MTD effectively correlates with task difficulty and can be used to analyze and steer model computation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] APO: Alpha-Divergence Preference Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning from human feedback (rlhf)], [alpha-divergence, preference optimization, mode collapse, anchored coordinates, gradient variance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wang Zixian</p>
</li>
<li class="">
<p><strong>institution:</strong> China Mobile Communications Group Shandong Co., Ltd. Tai’an Branch</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22953" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22953</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces APO, an anchored framework using Csiszár alpha-divergence to continuously interpolate between forward and reverse KL behavior for RLHF. 2. Derives unified gradient dynamics parameterized by alpha and analyzes gradient variance properties. 3. Proposes a practical reward-and-confidence-guarded alpha schedule to transition from mode-covering to mode-seeking behavior safely.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a407212dc95985ef8918d58e7c65f70fd3f6adf8764c95f85871cd1924b3528_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a407212dc95985ef8918d58e7c65f70fd3f6adf8764c95f85871cd1924b3528_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the trade-off between stable but under-exploitative mode-covering updates and high-reward but unstable mode-seeking updates in LLM alignment. It proposes APO, an anchored preference optimization framework that uses alpha-divergence to smoothly interpolate between these regimes via a guarded schedule. Experiments show APO achieves competitive performance while maintaining training stability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FLOW: A Feedback-Driven Synthetic Longitudinal Dataset of Work and Wellbeing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [synthetic data generation], [synthetic dataset, longitudinal data, feedback-driven simulation, behavioral modeling, benchmarking]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wafaa El Husseini</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated. Affiliation inferred from email domain (gmail) is insufficient. Likely independent or institutional affiliation not provided in the excerpt.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22956" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22956</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces FLOW, a novel synthetic longitudinal dataset modeling daily interactions between workload, lifestyle, and wellbeing. 2. Provides a configurable data generation tool for reproducible experimentation under adjustable assumptions. 3. Creates a publicly available, controlled experimental environment for methodological development and benchmarking where real-world data is inaccessible.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abeb0ef64c6a9a1178335cb2abf4717155e2b0e41b97fbf7173bb6448bdb8de9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abeb0ef64c6a9a1178335cb2abf4717155e2b0e41b97fbf7173bb6448bdb8de9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces FLOW, a synthetic longitudinal dataset generated via a rule-based, feedback-driven simulation to model daily interactions between work and wellbeing variables. It addresses the lack of accessible real-world data due to privacy and logistical constraints. The dataset and its configurable generation tool are released as a public resource to support reproducible research, methodological benchmarking, and education.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Context-Aware Temporal Modeling through Unified Multi-Scale Temporal Encoding and Hierarchical Sequence Learning for Single-Channel EEG Sleep Staging</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [biomedical signal processing], [multi-scale feature extraction, hierarchical BiLSTM, class-weighted loss, temporal modeling, sleep staging]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amirali Vakili, Salar Jahanshiri, Armin Salimi-Badr</p>
</li>
<li class="">
<p><strong>institution:</strong> Shahid Beheshti University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22976" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22976</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a context-aware and interpretable framework combining compact multi-scale feature extraction with hierarchical temporal modeling (BiLSTM) for single-channel EEG sleep staging. 2. Addresses class imbalance, especially for the N1 stage, using class-weighted loss functions and data augmentation techniques. 3. Introduces a sub-epoch chunking and probability averaging strategy to enhance contextual representation and robustness in predictions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c34e7add689f932eed2a6d2c02f530d1c09fc8e52104033716567656d05392d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c34e7add689f932eed2a6d2c02f530d1c09fc8e52104033716567656d05392d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a novel deep learning framework for automatic sleep staging using single-channel EEG. The method integrates multi-scale feature extraction with hierarchical sequence learning (BiLSTM) and employs strategies like chunk-based probability averaging to handle class imbalance and improve context modeling. The approach achieves state-of-the-art performance on the SleepEDF datasets, with a significant improvement in detecting the challenging N1 sleep stage, while maintaining model interpretability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Fusion or Confusion? Multimodal Complexity Is Not All You Need</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [multimodal learning, late-fusion, hyperparameter tuning, empirical study, reliability checklist]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tillmann Rheude, Roland Eils, Benjamin Wild</p>
</li>
<li class="">
<p><strong>institution:</strong> Berlin Institute of Health at Charité - Universitätsmedizin Berlin, Intelligent Medicine Institute at Fudan University, Freie Universität Berlin</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22991" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22991</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A large-scale benchmark of 19 multimodal architectures under a unified experimental protocol across nine diverse datasets. 2. The proposal of SimBaMM, a simple late-fusion Transformer baseline, which performs comparably to more complex methods under standardized conditions. 3. The provision of a pragmatic reliability checklist to promote robust and trustworthy future evaluations in multimodal learning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d808ed59e8dc58198573816741433c5c0873c34c1d5fe67d63a8ffb9d40342a6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d808ed59e8dc58198573816741433c5c0873c34c1d5fe67d63a8ffb9d40342a6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper challenges the assumption that architectural complexity is necessary for performance in multimodal learning. Through a large-scale empirical study, it shows that a simple late-fusion Transformer baseline (SimBaMM) performs comparably to 19 more complex methods when all are rigorously tuned and evaluated under standardized conditions. The authors argue for a shift in research focus from architectural novelty to methodological rigor.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Merge before Forget: A Single LoRA Continual Learning via Continual Merging</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [continual learning, LoRA, catastrophic forgetting, orthogonal basis, parameter-efficient]</p>
</li>
<li class="">
<p><strong>authors:</strong> Fuli Qiao, Mehrdad Mahdavi</p>
</li>
<li class="">
<p><strong>institution:</strong> The Pennsylvania State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23017" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23017</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel continual learning method that sequentially merges LoRA updates into a single unified LoRA, maintaining constant memory complexity with respect to the number of tasks. 2. Introduces orthogonal basis extraction from previous LoRA to initialize new task learning, minimizing task interference. 3. Employs a time-aware scaling mechanism to balance new and old knowledge during merging, improving performance over asymmetric LoRA merging.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68eb3af6647a5e47f9615997c14929688be663c5428838d0271ab503cc1b8c78_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68eb3af6647a5e47f9615997c14929688be663c5428838d0271ab503cc1b8c78_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the issues of memory growth and task interference in LoRA-based continual learning for LLMs. It proposes a method that orthogonally initializes and sequentially merges LoRAs into a single LoRA, using a time-aware scaling mechanism. The approach demonstrates effectiveness and efficiency in mitigating catastrophic forgetting while maintaining constant memory usage.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [interpretability], [chain-of-thought, faithfulness, causal mediation analysis, biasing features, explainability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kerem Zaman, Shashank Srivastava</p>
</li>
<li class="">
<p><strong>institution:</strong> UNC Chapel Hill</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23032" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23032</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Argues that the Biasing Features metric conflates unfaithfulness with incompleteness in Chain-of-Thought explanations. 2. Introduces a new faithful@k metric showing increased token budgets improve hint verbalization. 3. Uses Causal Mediation Analysis to show non-verbalized hints can still causally mediate predictions through the CoT.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/527440e442abe55ce371c5ad3ce8f49609f0398a6001b33523b6a3aa4bbc6e44_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/527440e442abe55ce371c5ad3ce8f49609f0398a6001b33523b6a3aa4bbc6e44_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper challenges the use of hint-verbalization metrics like Biasing Features for evaluating the faithfulness of Chain-of-Thought reasoning. It proposes that apparent unfaithfulness is often due to incompleteness from lossy compression and tight token limits, not a lack of alignment, and demonstrates this using new metrics and causal mediation analysis. The conclusion advocates for a broader interpretability toolkit beyond hint-based evaluations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Mechanistic Analysis of Circuit Preservation in Federated Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [Mechanistic Interpretability, circuit collapse, weight sparsity, Intersection-over-Union, Non-IID data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Muhammad Haseeb, Salaar Masood, Muhammad Abdullah Sohail</p>
</li>
<li class="">
<p><strong>institution:</strong> Lahore University of Management Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23043" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23043</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/ha405/FedMI" target="_blank" rel="noopener noreferrer" class="">https://github.com/ha405/FedMI</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel mechanistic interpretability (MI) framework to analyze the internal failure mode of FedAvg under Non-IID data, introducing the concept of &quot;circuit collapse&quot;. 2. Demonstrates the use of inherently interpretable, weight-sparse neural networks to identify and track functional circuits across clients and communication rounds in FL. 3. Provides the first mechanistic evidence, quantified via Intersection-over-Union (IoU), that Non-IID data causes structural circuit divergence and degradation, reframing statistical drift as a failure of mechanistic preservation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34be0814c16f6dee307b048c588d94abf81104abdb1f1491d1c23901df46fc7d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34be0814c16f6dee307b048c588d94abf81104abdb1f1491d1c23901df46fc7d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates why Federated Learning (FedAvg) performance degrades under Non-IID data by applying Mechanistic Interpretability. The method uses weight-sparse networks to identify and track functional &quot;circuits&quot; across clients, measuring their preservation with IoU. The main conclusion is that Non-IID data causes &quot;circuit collapse&quot; due to conflicting updates, providing a mechanistic explanation for the accuracy drop.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PI-MFM: Physics-informed multimodal foundation model for solving partial differential equations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scientific machine learning], [physics-informed neural networks, multimodal foundation model, partial differential equations, multi-operator learning, zero-shot fine-tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Min Zhu, Jingmin Sun, Zecheng Zhang, Hayden Schaeffer, Lu Lu</p>
</li>
<li class="">
<p><strong>institution:</strong> Yale University, Johns Hopkins University, University of Notre Dame, University of California Los Angeles</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23056" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23056</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a physics-informed multimodal foundation model (PI-MFM) framework that directly enforces governing PDEs during both pretraining and adaptation, moving beyond purely data-driven approaches. 2. Introduces a method that takes symbolic PDE representations as input and automatically assembles PDE residual losses via vectorized derivative computation, enabling unified physics-informed training across diverse equation families. 3. Demonstrates effective zero-shot physics-informed fine-tuning to unseen PDE families, achieving low error using only PDE residuals and initial/boundary conditions without labeled solution data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db03e9853bb30e32977fcd1b0e24e9037354712e7841661bdbb52b955e3b2849_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db03e9853bb30e32977fcd1b0e24e9037354712e7841661bdbb52b955e3b2849_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes PI-MFM, a physics-informed multimodal foundation model framework for solving partial differential equations. It integrates governing equations directly into the training and adaptation process, enabling data-efficient and transferable learning of PDE solution operators. The method outperforms data-driven models, especially with sparse data, and shows strong zero-shot adaptation capabilities to new PDE families.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Reward Model Selection Crisis in Personalized Alignment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [alignment &amp; personalization], [reward-guided decoding, policy accuracy, Pref-LaMP benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Fady Rezk, Yuangang Pan, Chuan-Sheng Foo, Xun Xu, Nancy Chen, Henry Gouk, Timothy Hospedales</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Edinburgh, Agency for Science, Technology and Research (A*STAR)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23067" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23067</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and demonstrates the failure of standard reward model (RM) accuracy as a selection criterion for deployment-ready personalized alignment. 2. Introduces a new metric, policy accuracy, to evaluate the token-level discrimination ability of reward models under inference-time adaptation (reward-guided decoding). 3. Introduces Pref-LaMP, the first personalized alignment benchmark with ground-truth user completions, enabling direct behavioral evaluation and revealing a decoupling between reward discrimination and actual generation quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a0a1bd7d940db8c394f189ea84b7dbcfae7cd34b8e3662ea7c7a8babfdfefe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a0a1bd7d940db8c394f189ea84b7dbcfae7cd34b8e3662ea7c7a8babfdfefe_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies a crisis in personalized alignment, showing that optimizing reward models for preference ranking accuracy does not translate to effective behavioral adaptation under realistic deployment constraints like reward-guided decoding. The authors propose a new metric (policy accuracy) and a new benchmark (Pref-LaMP) to evaluate this gap, finding that reward model accuracy poorly predicts generation quality and that simple in-context learning often outperforms reward-guided methods for larger models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Breaking the Memory Wall: Exact Analytical Differentiation via Tiled Operator-Space Evolution</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [memory &amp; caching], [Selective State Space Models, analytical differentiation, memory complexity, Tiled Operator-Space Evolution, Phase Gradient Flow]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuhuan Wang, Yuzhen Xie, Jiayi Li, Yinliang Diao</p>
</li>
<li class="">
<p><strong>institution:</strong> South China Agricultural University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23068" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23068</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Phase Gradient Flow (PGF), a framework for computing exact analytical derivatives for SSMs without materializing the intermediate computational graph. 2. Proposes Tiled Operator-Space Evolution (TOSE) to reframe SSM dynamics, achieving O(1) memory complexity relative to sequence length. 3. Demonstrates significant practical improvements, including a 94% reduction in peak VRAM and a 23x throughput increase, enabling chromosome-scale sensitivity analysis on a single GPU.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/821c684e39ce34e6f8c4157ec1cc31189105864e7ee30f0d13f0b32203a73fc5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/821c684e39ce34e6f8c4157ec1cc31189105864e7ee30f0d13f0b32203a73fc5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the O(L) memory bottleneck in gradient-based sensitivity analysis for Selective State Space Models (SSMs). It proposes Phase Gradient Flow (PGF), which uses Tiled Operator-Space Evolution (TOSE) to compute exact analytical derivatives with O(1) memory complexity. This enables the handling of extreme-length sequences (e.g., 128,000 steps) on consumer hardware, significantly reducing memory usage and increasing throughput compared to standard backpropagation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [vision-language models], [Mask Fine-Tuning (MFT), Parameter Efficient Fine-Tuning (PEFT), Low-Rank Adaptation (LoRA), structural reparameterization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mingyuan Zhang, Yue Bai, Yifan Wang, Yiyang Huang, Yun Fu</p>
</li>
<li class="">
<p><strong>institution:</strong> Northeastern University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23073" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23073</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Ming-K9/MFT-VLM" target="_blank" rel="noopener noreferrer" class="">https://github.com/Ming-K9/MFT-VLM</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes applying Mask Fine-Tuning (MFT), a structural reparameterization method, to Vision-Language Models (VLMs) for adaptation., 2. Demonstrates that MFT, which learns gating scores for weights instead of updating them, consistently outperforms LoRA variants and full fine-tuning., 3. Reveals that effective adaptation can emerge from reestablishing connections within a model&#x27;s existing knowledge, not just from updating weights.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92b8916bd08492d9f9fbfd3b3a163d12c7de7a6a1fe0822bc6d711ca118dfb28_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92b8916bd08492d9f9fbfd3b3a163d12c7de7a6a1fe0822bc6d711ca118dfb28_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper rethinks fine-tuning for Vision-Language Models (VLMs) by applying Mask Fine-Tuning (MFT), a method that learns to gate existing weights instead of updating them. Experiments show MFT surpasses both Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA and full fine-tuning, demonstrating that reorganizing internal subnetworks is a powerful alternative to weight updates.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FLEX-MoE: Federated Mixture-of-Experts with Load-balanced Expert Assignment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [Mixture-of-Experts, Federated Learning, Load Balancing, Expert Assignment, Non-IID Data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Boyang Zhang, Xiaobing Chen, Songyang Zhang, Shuai Zhang, Xiangwei Zhou, Mingxuan Sun</p>
</li>
<li class="">
<p><strong>institution:</strong> The affiliations are not explicitly listed in the provided content. Based on the author names and common patterns, it is likely from a Chinese university or research institute (e.g., Tsinghua University, Peking University, Chinese Academy of Sciences). A specific institution cannot be reliably inferred.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23070" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23070</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FLEX-MoE, a novel federated MoE framework that jointly optimizes expert assignment and load balancing under limited client capacity. 2. Introduces client-expert fitness scores to quantify expert suitability for local datasets using training feedback. 3. Employs an optimization-based algorithm to maximize client-expert specialization while enforcing balanced expert utilization system-wide.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4afb2d6d1d993854d25c34b1c5c48d2b0479953fa05feac83a247dd11404202_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4afb2d6d1d993854d25c34b1c5c48d2b0479953fa05feac83a247dd11404202_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenges of deploying Mixture-of-Experts models in Federated Learning, specifically resource constraints on edge devices and expert load imbalance caused by non-IID data. It proposes FLEX-MoE, a framework that uses client-expert fitness scores and an optimization algorithm to assign experts to clients for specialization while balancing system-wide expert utilization. Experiments on three datasets show that FLEX-MoE achieves superior performance and maintains balanced expert utilization in resource-constrained scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Trust Region Masking for Long-Horizon LLM Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [trust region, policy gradient, off-policy mismatch, KL divergence, sequence-level masking]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yingru Li, Jiacai Liu, Jiawei Xu, Yuxuan Tong, Ziniu Li, Baoxiang Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> (Institutions not explicitly listed in provided content; inferred from author names and common affiliations in the field, but not specified. Therefore, output is left blank.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23075" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23075</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Deriving two novel, tighter theoretical bounds (Pinsker-Marginal and Mixed) on the approximation error in off-policy LLM-RL, scaling better with sequence length than classical O(T^2) bounds. 2. Identifying that these bounds depend on a sequence-level quantity (maximum token-level KL divergence) that cannot be controlled by token-independent methods like PPO clipping. 3. Proposing the Trust Region Masking (TRM) algorithm, which masks entire sequences from gradient updates to enforce the trust region, providing non-vacuous monotonic improvement guarantees for long-horizon tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74d8872516a568f2933a83e36b0cf25d414f3a25ffa113cd0cee809d2b39c6ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74d8872516a568f2933a83e36b0cf25d414f3a25ffa113cd0cee809d2b39c6ac_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that classical trust region bounds become vacuous for long-horizon LLM reinforcement learning due to unavoidable off-policy mismatch. It proposes Trust Region Masking (TRM), a method that excludes entire sequences from gradient computation if any token violates a trust region constraint. This approach, supported by new tighter theoretical bounds, provides the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multimodal Functional Maximum Correlation for Emotion Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal learning], [self-supervised learning, dual total correlation, functional maximum correlation analysis, affective computing, physiological signals]</p>
</li>
<li class="">
<p><strong>authors:</strong> Deyang Zheng, Tianyi Zhang, Wenming Zheng, Shujian Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> Southeast University, Westlake University, Vrije Universiteit Amsterdam</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23076" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23076</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/DY9910/MFMC" target="_blank" rel="noopener noreferrer" class="">https://github.com/DY9910/MFMC</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel self-supervised learning framework (MFMC) that maximizes higher-order multimodal dependence using a Dual Total Correlation objective. 2. Derives a tight sandwich bound and optimizes it using a functional maximum correlation analysis-based trace surrogate to capture joint interactions. 3. Demonstrates state-of-the-art or competitive performance on affective computing benchmarks, showing robustness to inter-subject variability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369b23e1c7a17085940402e88d2347afb3386819a237c48ac347be73127aea2a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369b23e1c7a17085940402e88d2347afb3386819a237c48ac347be73127aea2a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of learning joint dynamics from scarce and subjective emotion labels by proposing a self-supervised learning framework called MFMC. It captures higher-order multimodal dependencies beyond pairwise alignment, leading to improved emotion recognition performance on physiological signal benchmarks. The results show significant accuracy gains, particularly in subject-independent settings, highlighting the method&#x27;s effectiveness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [reinforcement learning, training-inference mismatch, vocabulary pruning, gradient estimation, numerical stability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yingru Li, Jiawei Xu, Jiacai Liu, Yuxuan Tong, Ziniu Li, Tianle Cai, Ge Zhang, Qian Liu, Baoxiang Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> (Institutions not explicitly listed in provided content. Affiliation inference requires author list with affiliations or email domains, which are not present in the given text. Therefore, cannot be determined from the provided snippet.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23087" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23087</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proves that the training-inference mismatch in LLM RL has an asymmetric effect, where the bound on log-probability mismatch scales with (1-p), making low-probability &quot;tail&quot; tokens the primary source of instability. 2. Proposes a novel method to stabilize RL training by dynamically pruning the vocabulary to exclude the extreme tail tokens, trading large, biased mismatches for a small, bounded optimization bias. 3. Provides both empirical demonstration of stable training and a theoretical bound on the optimization bias introduced by the proposed vocabulary pruning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83b750895381feb238b14991a4015088fa8d05eb24ab0374082f6c25fb3ddd7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83b750895381feb238b14991a4015088fa8d05eb24ab0374082f6c25fb3ddd7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a fundamental training-inference mismatch in LLM reinforcement learning caused by differing numerical precision between high-throughput inference and stable training systems. To address this, the authors propose dynamically pruning low-probability &quot;tail&quot; tokens from the vocabulary during RL optimization, which stabilizes training by replacing large, biased errors with a small, bounded bias. Both theoretical analysis and empirical results support the effectiveness of this method.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [Imitation Learning, Reinforcement Learning, KL divergence, Dense Gradient, Sparse Gradient]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yingru Li, Ziniu Li, Jiacai Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in provided content.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23097" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23097</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Derives the exact gradient decomposition of a unified KL+reward objective into analytic Dense and sampled Sparse terms. 2. Provides an efficient logit-level gradient formula for GPU implementation. 3. Establishes mathematical equivalence to KL-regularized RLHF and discusses training curriculum implications.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c49fbea41eedc7a9f58604cc114a9246db61882fc20a851c8ec68a24ff6b343_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c49fbea41eedc7a9f58604cc114a9246db61882fc20a851c8ec68a24ff6b343_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a unified framework for fine-tuning LLMs that integrates Imitation Learning and Reinforcement Learning. It analyzes the gradient of a combined objective to decompose it into a token-level Dense Gradient and a long-horizon Sparse Gradient, enabling efficient implementation. The work clarifies its relationship to existing methods like RLHF and discusses practical training considerations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [reinforcement learning, vision-language model, supervised fine-tuning, generalization paradox, cross-dataset transferability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Armin Berger, Manuela Bergau, Helen Schneider, Saad Ahmad, Tom Anglim Lagones, Gianluca Brugnara, Martha Foltyn-Dumitru, Kai Schlamp, Philipp Vollmuth, Rafet Sifa</p>
</li>
<li class="">
<p><strong>institution:</strong> Fraunhofer IAIS, University of Bonn, Lamarr Institute, Department of Health Queensland, Griffith University, University Hospital Bonn</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23090" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23090</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced ChexReason, a resource-efficient vision-language model for medical imaging trained with an R1-style (SFT+GRPO) method using minimal data and compute. 2. Identified a fundamental tension where RL optimization (GRPO) improves in-distribution benchmark performance but significantly degrades cross-dataset generalization, a pattern also observed in high-resource models. 3. Discovered a generalization paradox where the SFT checkpoint uniquely improves cross-dataset performance, suggesting teacher-guided reasoning captures more institution-agnostic features than RL optimization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c87b0b1a571aa28f5dd9685e96b13ff3420ed36b0e1d569fff8b1394d564751f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c87b0b1a571aa28f5dd9685e96b13ff3420ed36b0e1d569fff8b1394d564751f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper investigates applying reinforcement learning (RL) to vision-language models for medical imaging, finding that while RL improves performance on the training benchmark, it harms the model&#x27;s ability to generalize to new datasets. The authors conclude that for clinical robustness, curated supervised fine-tuning may be more effective than aggressive RL optimization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Osmotic Learning: A Self-Supervised Paradigm for Decentralized Contextual Data Representation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [self-supervised learning, representation learning, distributed learning, decentralized clustering, contextual data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mario Colosi, Reza Farahani, Maria Fazio, Radu Prodan, Massimo Villari</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Messina, University of Klagenfurt, University of Innsbruck</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23096" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23096</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Osmotic Learning (OSM-L), a novel self-supervised paradigm for learning from distributed data without raw data exchange. 2. Proposes an &quot;osmosis&quot; process that aligns local representations to converge to a dynamic equilibrium, capturing contextual patterns. 3. Demonstrates that OSM-L functions as a decentralized clustering mechanism, identifying correlated data groups during training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2b452fb94443ab9846af33524787f0bc6c709b6e90ae3be4e653738c6fe592b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2b452fb94443ab9846af33524787f0bc6c709b6e90ae3be4e653738c6fe592b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Osmotic Learning (OSM-L), a self-supervised distributed learning paradigm that extracts higher-level latent knowledge from decentralized data sources without sharing raw data. It achieves this through an iterative &quot;osmosis&quot; process that aligns local representations to converge to a contextual equilibrium, also enabling decentralized clustering. Experimental results show OSM-L achieves high accuracy in local information alignment while preserving contextual integrity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] How Much Data Is Enough? Uniform Convergence Bounds for Generative &amp; Vision-Language Models under Low-Dimensional Structure</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [statistical learning theory], [uniform convergence, calibration, low-dimensional structure, vision-language models, sample complexity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Paul M. Thompson</p>
</li>
<li class="">
<p><strong>institution:</strong> Stevens Institute of Neuroimaging and Informatics, University of Southern California</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23109" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23109</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides finite-sample uniform convergence bounds for accuracy and calibration of VLM-induced classifiers under Lipschitz stability assumptions. 2. Derives sample complexity bounds that depend on the intrinsic/effective dimension of the embedding space, not the ambient dimension. 3. Offers spectrum-dependent bounds that explicitly link eigenvalue decay in embedding covariance to data requirements, explaining reliable generalization with fewer samples.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dca56fb1537f7d92cc10d66ba9adb9e3272fcc872fe5fdbf475ccf92cc17e24_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dca56fb1537f7d92cc10d66ba9adb9e3272fcc872fe5fdbf475ccf92cc17e24_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies when generative and vision-language models can achieve uniformly accurate and calibrated predictions with practical sample sizes. By assuming model outputs depend smoothly on a low-dimensional semantic representation, it derives finite-sample uniform convergence bounds for VLM-induced classifiers. The main conclusion is that sample complexity depends on intrinsic dimension and eigenvalue decay, providing a framework to assess data sufficiency for reliable biomedical predictions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [regression], [squeeze and excitation, channel attention, residual connections, multi-layer perceptron, penetration acceleration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yankang Li, Changsheng Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanjing University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23131" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23131</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed SE-MLP, a novel MLP architecture integrating a channel attention mechanism for feature prediction. 2. Incorporated residual connections into the MLP framework to enhance model stability and performance. 3. Demonstrated the model&#x27;s superior accuracy, generalization, and engineering applicability for rapidly predicting penetration acceleration features.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0dab35d2c00d22564e8f3e36c067728bb8b4d1bb60f2ed675bfe34288c07503a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0dab35d2c00d22564e8f3e36c067728bb8b4d1bb60f2ed675bfe34288c07503a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes SE-MLP, a multi-layer perceptron model enhanced with squeeze-and-excitation channel attention and residual connections, to rapidly predict prior acceleration features for penetration signals. The model establishes a nonlinear mapping from physical parameters to acceleration features, outperforming baseline models like MLP, XGBoost, and Transformer in accuracy and stability. The results validate its feasibility and provide a practical basis for engineering applications in penetration fuse design.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning from human feedback (rlhf)], [preference optimization, direct preference optimization, self-reflection, invariance, bradley-terry model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yu Li, Tian Lan, Zhengling Qi</p>
</li>
<li class="">
<p><strong>institution:</strong> George Washington University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23126" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23126</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies two fundamental limitations of DPO: lack of invariance to modeling choices and theoretical suboptimality due to ignoring comparative information in pairwise data. 2. Proposes Intrinsic Self-reflective Preference Optimization (InSPO), a novel family of methods that derives a globally optimal policy conditioned on both context and alternative responses, formalizing self-reflection. 3. Theoretically demonstrates InSPO&#x27;s superiority over DPO/RLHF and its invariance properties, and practically shows it as a plug-and-play enhancement that improves win rates and length-controlled metrics without inference overhead.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4933654befdce9d244a4f36811432e84021ec775f760f24a3cb71dec1951db76_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4933654befdce9d244a4f36811432e84021ec775f760f24a3cb71dec1951db76_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies limitations in Direct Preference Optimization (DPO), such as its sensitivity to modeling choices and failure to use comparative data fully. It proposes InSPO, a method that conditions the policy on both the context and the alternative response to enable intrinsic self-reflection. Experiments show InSPO consistently improves model alignment and robustness as a plug-and-play enhancement to DPO-family algorithms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [machine learning security], [TTPs, threat graph, multi-agent RAG, model stealing, jailbreaking]</p>
</li>
<li class="">
<p><strong>authors:</strong> Armstrong Foundjem, Lionel Nganyewou Tidjon, Leuson Da Silva, Foutse Khomh</p>
</li>
<li class="">
<p><strong>institution:</strong> Polytechnique Montréal (based on author affiliations and sMIEEE notation)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23132" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23132</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a large-scale empirical analysis of ML security, extracting 93 distinct threats from multiple sources including real-world incidents and code repositories. 2. Developed a multi-agent RAG system to automatically build an ontology-driven threat graph linking TTPs, vulnerabilities, and lifecycle stages from over 300 articles. 3. Identified unreported threats and dominant attack patterns (e.g., commercial LLM API model stealing, preference-guided jailbreaks) and highlighted vulnerability clusters in ML libraries with poor patch propagation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3da226bc9e7639392b95f6f00808f162580d1a90050c1261b3a0703259e42cf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3da226bc9e7639392b95f6f00808f162580d1a90050c1261b3a0703259e42cf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper characterizes modern security risks in AI systems by analyzing threats from multiple sources and using a multi-agent RAG system to construct a threat graph. The analysis uncovers unreported attack vectors and dominant TTPs, concluding that adaptive, ML-specific security frameworks are urgently needed to mitigate supply-chain and inference-time risks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Principled Algorithms for Optimizing Generalized Metrics in Binary Classification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [binary classification], [metric optimization, H-consistency, surrogate loss, cost-sensitive learning, METRO]</p>
</li>
<li class="">
<p><strong>authors:</strong> Anqi Mao, Mehryar Mohri, Yutao Zhong</p>
</li>
<li class="">
<p><strong>institution:</strong> Courant Institute of Mathematical Sciences (NYU), Google Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23133" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23133</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Reformulated the optimization of generalized classification metrics (e.g., Fβ, Jaccard) as a generalized cost-sensitive learning problem. 2. Designed novel surrogate loss functions with provable H-consistency guarantees for this problem. 3. Developed the METRO algorithm with strong finite-sample generalization bounds, offering a principled alternative to existing threshold-based methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57d73e082d311622a2b2e8ab3bd4da18cd359831ac4e0139fd4555ea2ba93861_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57d73e082d311622a2b2e8ab3bd4da18cd359831ac4e0139fd4555ea2ba93861_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of directly optimizing non-decomposable binary classification metrics like the Fβ-measure. The authors propose a principled framework that reformulates metric optimization as a cost-sensitive learning problem, leading to new surrogate losses and the METRO algorithm. Experiments show the proposed method is effective and outperforms prior baseline approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Graph Neural Networks with Transformer Fusion of Brain Connectivity Dynamics and Tabular Data for Forecasting Future Tobacco Use</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [graph neural networks, transformer, dynamic functional connectivity, longitudinal fMRI, multimodal fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Runzhi Zhou, Xi Luo</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Texas Health Science Center at Houston</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23137" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23137</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel time-aware Graph Neural Network model with Transformer Fusion (GNN-TF) for integrating non-Euclidean brain connectivity and Euclidean tabular data. 2. Introduces an end-to-end framework that leverages the temporal order of longitudinal data for forecasting future clinical outcomes. 3. Demonstrates superior predictive performance for forecasting future tobacco use compared to established machine learning and deep learning models on a longitudinal fMRI dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6106dd6744068e14ed98be012be1d450afa8c685980d7b00ae696592ebd7665_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6106dd6744068e14ed98be012be1d450afa8c685980d7b00ae696592ebd7665_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces GNN-TF, a time-aware model that integrates dynamic brain connectivity graphs and tabular data using a transformer for fusion, to forecast future tobacco use. It is evaluated on longitudinal fMRI data from the NCANDA study and is shown to outperform other state-of-the-art methods in predictive accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Weak Signal Learning Dataset and Its Baseline Method</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [weak signal learning], [weak signal learning, dual-view representation, class imbalance, low SNR, multi-source complementarity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xianqi Liu, Xiangru Li, Lefeng He, Ziyu Fang</p>
</li>
<li class="">
<p><strong>institution:</strong> South China Normal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23160" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23160</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Constructed the first specialized dataset for weak signal feature learning, featuring low SNR dominance and extreme class imbalance., 2. Proposed a dual-view representation (vector + time-frequency map) and the PDVFN model tailored for low SNR, distribution skew, and dual imbalance., 3. Established a foundational benchmark for future weak signal learning (WSL) research, demonstrating improved accuracy and robustness in challenging scenarios.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/426f082649cd556014218d670accdb2545dfd625d214b420b39fcce46739f01d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/426f082649cd556014218d670accdb2545dfd625d214b420b39fcce46739f01d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of weak signal learning (WSL) by constructing the first dedicated dataset with low SNR and class imbalance, and proposes a dual-view PDVFN model to extract complementary features. The method shows higher accuracy and robustness in handling weak signals, noise, and imbalance. This work provides a dataset, baseline model, and foundation for future WSL research.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Diffusion-based Decentralized Federated Multi-Task Representation Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [decentralized learning, multi-task representation learning, projected gradient descent, diffusion-based consensus, sample complexity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Donghwa Kang, Shana Moothedath</p>
</li>
<li class="">
<p><strong>institution:</strong> Iowa State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23161" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23161</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel decentralized projected gradient descent-based algorithm for multi-task representation learning using a diffusion-based communication strategy. 2. Provides theoretical guarantees, including a lower bound on sample complexity and an upper bound on iteration complexity for the proposed algorithm. 3. Demonstrates through analysis and simulations that the algorithm is fast, communication-efficient, and outperforms benchmark methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb19ab164bf383bc246da4c65f212d69031d00619ab4c384a67af532a30b00e2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb19ab164bf383bc246da4c65f212d69031d00619ab4c384a67af532a30b00e2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of decentralized multi-task representation learning, where multiple linear regression models share a common low-dimensional representation across a network of nodes. The authors propose a diffusion-based decentralized algorithm using alternating projected gradient descent and minimization to recover the shared feature matrix. Theoretical analysis proves the algorithm&#x27;s efficiency in terms of sample and iteration complexity, and numerical simulations validate its superior performance compared to benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Evaluating Parameter Efficient Methods for RLVR</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Parameter-Efficient Fine-Tuning, Reinforcement Learning with Verifiable Rewards, LoRA, Spectral Collapse, Mathematical Reasoning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qingyu Yin, Yulun Wu, Zhennan Shen, Sunbowen Li, Zhilin Wang, Yanshu Li, Chak Tou Leong, Jiale Kang, Jinjin Gu</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, HKUST, WUST, USTC, Brown University, Hong Kong Polytechnic University, INSAIT</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23165" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23165</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted the first comprehensive evaluation of over 12 PEFT methods for RLVR, challenging the default use of standard LoRA. 2. Identified that structural PEFT variants (DoRA, AdaLoRA, MiSS) consistently outperform LoRA in this setting. 3. Discovered and explained the failure of SVD-informed initialization methods (e.g., PiSSA) due to a &quot;spectral collapse&quot; phenomenon and misalignment with RL optimization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87ad6f372a6a1a3b13e37f8468a6816e52a585402f7e4505a01391ffaed0621c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87ad6f372a6a1a3b13e37f8468a6816e52a585402f7e4505a01391ffaed0621c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper systematically evaluates Parameter-Efficient Fine-Tuning (PEFT) methods for Reinforcement Learning with Verifiable Rewards (RLVR) on mathematical reasoning tasks. It finds that structural variants like DoRA outperform standard LoRA, while SVD-based methods fail due to spectral collapse, and extreme parameter reduction bottlenecks performance. The work provides a guide for selecting PEFT methods in RLVR.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [LLM planning, Monte Carlo Tree Search (MCTS), multi-agent architecture, symbolic reasoning, self-correction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yifan Zhang, Giridhar Ganapavarapu, Srideepika Jayaraman, Bhavna Agrawal, Dhaval Patel, Achille Fokoue</p>
</li>
<li class="">
<p><strong>institution:</strong> IBM T.J. Watson Research Center, Vanderbilt University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23167" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23167</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/IBM/SPIRAL" target="_blank" rel="noopener noreferrer" class="">https://github.com/IBM/SPIRAL</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces SPIRAL, a novel framework that embeds a cognitive architecture of three specialized LLM agents (Planner, Simulator, Critic) into an MCTS loop for planning. 2. Transforms MCTS from a brute-force search into a guided, self-correcting reasoning process by leveraging dense, semantic-aware feedback from the agents. 3. Demonstrates superior performance and token efficiency on benchmark datasets (e.g., DailyLifeAPIs) compared to Chain-of-Thought and other state-of-the-art planning agents.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c24b184565ce8e4c4a35d80f46a857779e111625dc4ea57e56333bef27bea7e6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c24b184565ce8e4c4a35d80f46a857779e111625dc4ea57e56333bef27bea7e6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of LLMs struggling with complex planning tasks due to linear reasoning and lack of self-correction. It proposes SPIRAL, a framework that integrates three specialized LLM agents into a Monte Carlo Tree Search loop to create a guided, reflective, and grounded planning process. The method significantly outperforms existing planning approaches in accuracy and efficiency on benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [vertical federated learning, machine unlearning, primal-dual optimization, sample unlearning, label unlearning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yu Jiang, Xindi Tong, Ziyao Liu, Xiaoxi Zhang, Kwok-Yan Lam, Chee Wei Tan</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanyang Technological University, Sun Yat-sen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23171" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23171</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FedORA, a primal-dual optimization framework for sample and label unlearning in Vertical Federated Learning (VFL). 2. Introduces a new unlearning loss function that promotes classification uncertainty instead of misclassification. 3. Employs an adaptive step size and an asymmetric batch design to enhance stability and reduce computational costs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05dcd254c3941fc5cbf6bc3c063f2a726b8607659a3f7b4a526ad900e9e2b5de_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05dcd254c3941fc5cbf6bc3c063f2a726b8607659a3f7b4a526ad900e9e2b5de_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of data removal (unlearning) in Vertical Federated Learning (VFL), where different parties hold different features of the same data samples. The authors propose FedORA, a method that formulates unlearning as a constrained optimization problem solved via a primal-dual algorithm. Experiments show FedORA achieves unlearning effectiveness and model utility comparable to retraining from scratch, but with lower computational and communication costs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [molecular language modeling], [HELM notation, DeBERTa, cyclic peptide, membrane permeability, peptide-protein interaction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Seungeon Lee, Takuto Koyama, Itsuki Maeda, Shigeyuki Matsumoto, Yasushi Okuno</p>
</li>
<li class="">
<p><strong>institution:</strong> Kyoto University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23175" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23175</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes HELM-BERT, the first encoder-based peptide language model trained on HELM notation, designed to capture hierarchical dependencies. 2. Pre-trains the model on a curated corpus of 39,079 chemically diverse linear and cyclic peptides. 3. Demonstrates superior performance over SMILES-based models in downstream tasks like cyclic peptide membrane permeability and peptide-protein interaction prediction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83ec939e21fe471473f433077555782bca673e92ad1bfd3578b0fe729e20446_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83ec939e21fe471473f433077555782bca673e92ad1bfd3578b0fe729e20446_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces HELM-BERT, a transformer model based on DeBERTa and trained on HELM notation to better represent therapeutic peptides. It shows that this approach significantly outperforms existing SMILES-based models in predicting key peptide properties, demonstrating the data-efficiency advantages of topology-aware representations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Machine Learning-Assisted Vocal Cord Ultrasound Examination: Project VIPR</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [vocal cord ultrasound, image segmentation, VIPRnet, vocal cord paralysis, classification model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Will Sebelik-Lassiter, Evan Schubert, Muhammad Alliyu, Quentin Robbins, Excel Olatunji, Mustafa Barry</p>
</li>
<li class="">
<p><strong>institution:</strong> Milwaukee School of Engineering, Emory University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23177" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23177</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a machine learning pipeline for automated analysis of vocal cord ultrasound (VCUS) videos. 2. Created a segmentation model to automatically identify vocal cords in ultrasound images with 96% validation accuracy. 3. Proposed VIPRnet, a classification model to distinguish normal vocal cords from vocal cord paralysis (VCP) with 99% validation accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5abcb457a7fbefffa7d7836af553a8db8fa248fdd34a0459a027299b3ce2ce44_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5abcb457a7fbefffa7d7836af553a8db8fa248fdd34a0459a027299b3ce2ce44_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a machine learning-assisted system to automate the analysis of vocal cord ultrasound (VCUS) to reduce operator dependency. The method involves segmenting the vocal cords and classifying them as normal or paralyzed using models trained on frames from volunteer videos. The results show high validation accuracy (96% for segmentation, 99% for classification), indicating promise for improving diagnostic accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PGOT: A Physics-Geometry Operator Transformer for Complex PDEs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [neural operator learning], [Physics-Geometry Operator Transformer, Spectrum-Preserving Geometric Attention, geometric aliasing, linear complexity, spatially adaptive routing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhuo Zhang, Xi Yang, Yuan Zhao, Canqun Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Defense Technology, National SuperComputer Center in Tianjin</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23192" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23192</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes PGOT, a novel transformer architecture designed to reconstruct physical feature learning through explicit geometry awareness for solving PDEs on complex geometries. 2. Introduces Spectrum-Preserving Geometric Attention (SpecGeo-Attention), which uses a physics slicing-geometry injection mechanism to incorporate multi-scale geometric encodings, preserving critical boundary information while maintaining linear computational complexity. 3. Implements a dynamic routing mechanism that adaptively selects low-order linear paths for smooth regions and high-order non-linear paths for discontinuities, enabling high-precision, spatially adaptive modeling.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1247f066008cc6ba78670544cbf446b4e7c3e18fb9b55a53416b7a831c93e80f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1247f066008cc6ba78670544cbf446b4e7c3e18fb9b55a53416b7a831c93e80f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of modeling PDEs on large-scale unstructured meshes with complex geometries using transformers, where efficient architectures often lose critical boundary information due to geometric aliasing. It proposes the Physics-Geometry Operator Transformer (PGOT), which introduces a geometry-aware attention mechanism and adaptive computational routing to preserve multi-scale features and model shocks precisely. PGOT achieves state-of-the-art performance on standard benchmarks and excels in large-scale industrial design tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Simple, Optimal and Efficient Algorithm for Online Exp-Concave Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [online learning], [Online Newton Step, Mahalanobis projection, regret minimization, exp-concave optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yi-Han Wang, Peng Zhao, Zhi-Hua Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> National Key Laboratory for Novel Software Technology, Nanjing University; School of Artificial Intelligence, Nanjing University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23190" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23190</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LightONS, a simple variant of ONS that reduces computational cost by delaying expensive Mahalanobis projections via a hysteresis mechanism. 2. Achieves optimal O(d log T) regret with a total runtime of O(d²T + d^ω √(T log T)), improving over ONS&#x27;s O(d^ω T) runtime. 3. Provides an SXO algorithm with runtime ~O(d³/ε), solving the COLT&#x27;13 open problem posed by Koren.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2632ee5796bfd9a4795aa8e1212d8f65e7f8732a96264f4ae4629a2216e3e5ba_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2632ee5796bfd9a4795aa8e1212d8f65e7f8732a96264f4ae4629a2216e3e5ba_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the computational bottleneck of the Online Newton Step (ONS) algorithm for online exp-concave optimization, where the Mahalanobis projection step is costly. The authors propose LightONS, a simple variant that introduces a hysteresis mechanism to delay expensive projections, preserving optimal regret while significantly reducing runtime. This leads to an efficient stochastic optimization method that resolves a long-standing open problem.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Energy and Memory-Efficient Federated Learning With Ordered Layer Freezing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [Ordered Layer Freezing, Tensor Operation Approximation, Non-IID Data, Edge Computing, Model Compression]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ziru Niu, Hai Dong, A.K. Qin, Tao Gu, Pengcheng Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> RMIT University, Swinburne University of Technology, Macquarie University, Hohai University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23200" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23200</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Federated Learning with Ordered Layer Freezing (FedOLF), a method that freezes model layers in a predefined order before training to reduce computation and memory requirements. 2. Introduced Tensor Operation Approximation (TOA) as a lightweight alternative to conventional quantization to further reduce communication and energy costs while preserving model accuracy. 3. Demonstrated superior performance of FedOLF over non-IID data across multiple datasets and model architectures, achieving higher accuracy, energy efficiency, and lower memory footprint compared to existing works.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cf67406a118e6d8eff8ded5d05e3b12c6eeded41300a0b8517fec19cbc8d930_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cf67406a118e6d8eff8ded5d05e3b12c6eeded41300a0b8517fec19cbc8d930_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces FedOLF, a federated learning method that freezes model layers in a predefined order to reduce resource demands on edge devices, combined with a Tensor Operation Approximation technique for efficient communication. The proposed approach is shown to achieve higher accuracy and better energy/memory efficiency than existing methods when training on non-IID data across several benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Anka: A Domain-Specific Language for Reliable LLM Code Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [Domain-Specific Language, Constrained Syntax, Code Generation, Data Transformation Pipeline, In-Context Learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Saif Khalfan Saif Al Mazrouei</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Wisconsin-Madison</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23214" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23214</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced Anka, a domain-specific language (DSL) with explicit, constrained syntax designed to reduce ambiguity in LLM code generation. 2. Demonstrated that LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy without prior training. 3. Showed that purposefully designed DSLs can outperform general-purpose languages (e.g., Python) on complex multi-step tasks, significantly reducing errors in operation sequencing and state management.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6e450f3b6354a05c4c0dfa0c22c4f8b8dfc33c08282380080deb2d2f3a335d4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6e450f3b6354a05c4c0dfa0c22c4f8b8dfc33c08282380080deb2d2f3a335d4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper hypothesizes that the flexibility of general-purpose languages leads to systematic errors in LLM code generation for complex tasks. To test this, it introduces Anka, a constrained DSL for data transformation pipelines. The results show that LLMs can learn Anka from prompts and achieve significantly higher accuracy on multi-step tasks compared to Python, demonstrating the advantage of constrained syntax for reliable code generation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [graph federated learning, fairness, overlapping subgraphs, privacy-preserving, weighted aggregation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zihao Zhou, Shusen Yang, Fangyuan Zhao, Xuebin Ren</p>
</li>
<li class="">
<p><strong>institution:</strong> Xi&#x27;an Jiaotong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23235" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23235</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Uncover and theoretically analyze the unfairness issue in graph federated learning caused by imbalanced overlapping subgraphs across clients. 2. Propose FairGFL, a novel algorithm that uses a privacy-preserving estimation of overlapping ratios and an interpretable weighted aggregation approach to enhance cross-client fairness. 3. Improve the tradeoff between model utility and fairness by integrating a carefully crafted regularizer into the federated composite loss function.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c67614889dfdf1e0e6de4fd0bd950e8649eb4d988fb1661c29ef6c14b73bba25_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c67614889dfdf1e0e6de4fd0bd950e8649eb4d988fb1661c29ef6c14b73bba25_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a fairness problem in graph federated learning when client subgraphs overlap in an imbalanced way. To solve this, it proposes FairGFL, a method that uses privacy-preserving overlap estimation and a fairness-aware regularizer to balance utility and fairness. Experiments show FairGFL outperforms baselines in both utility and fairness on benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [gpu kernels], [agentic kernel coding, heterogeneous accelerators, retrieval-augmented prompt synthesis, graph-based search, Triton/CuTe DSL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gang Liao, Hongsen Qin, Ying Wang, Alicia Golden, Michael Kuchnik, Yavuz Yetim, Jia Jiunn Ang, Chunli Fu, Yihan He, Samuel Hsia, Zewei Jiang, Dianshi Li, Uladzimir Pashkevich, Varna Puvvada, Feng Shi, Matt Steiner, Ruichao Xiao, Nathan Yan, Xiayu Yu, Zhou Fang, Abdul Zainul-Abedin, Ketan Singh, Hongtao Yu, Wenyuan Chi, Barney Huang, Sean Zhang, Noah Weller, Zach Marine, Wyatt Cook, Carole-Jean Wu, Gaoxiang Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Meta Platforms</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23236" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23236</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes KernelEvolve, an agentic framework that automates kernel generation and optimization for DLRMs across heterogeneous hardware (NVIDIA/AMD GPUs, Meta accelerators) by operating at multiple programming abstractions. 2. Introduces a kernel optimization process modeled as a graph-based search with dynamic adaptation to runtime context via retrieval-augmented prompt synthesis and a persistent hardware knowledge base. 3. Demonstrates the system&#x27;s effectiveness by achieving 100% correctness on benchmark suites and substantial performance speedups (up to 17x) in production, reducing development time from weeks to hours and lowering the programmability barrier for new AI hardware.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/641ba327ba0d01461cd8fabad9a237e7b6667ce170be08aa3e89e6624ada0d38_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/641ba327ba0d01461cd8fabad9a237e7b6667ce170be08aa3e89e6624ada0d38_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents KernelEvolve, an agentic framework that automates the generation and optimization of compute kernels for deep learning recommendation models to address challenges posed by model, kernel, and hardware heterogeneity. The method uses a graph-based search process enhanced with retrieval-augmented prompts and operates across multiple programming abstractions. The system was validated on production models and benchmarks, showing significant performance improvements and reduced development time, effectively mitigating the programmability barrier for new AI accelerators.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On the Inverse Flow Matching Problem in the One-Dimensional and Gaussian Cases</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models], [flow matching, inverse problem, uniqueness, generative AI, continuity equation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alexander Korotin, Gudmund Pammer</p>
</li>
<li class="">
<p><strong>institution:</strong> Applied AI Institute, Graz University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23265" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23265</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formally defines the inverse problem of flow matching (FM) for distributions with finite exponential moment, 2. Establishes the uniqueness of the solution to the inverse FM problem in the one-dimensional (D=1) setting, 3. Establishes the uniqueness of the solution to the inverse FM problem in the Gaussian case.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3894b8881e76b35830e6504158fd61b8ec4e18f8adfb99a0b03911f3928bd297_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3894b8881e76b35830e6504158fd61b8ec4e18f8adfb99a0b03911f3928bd297_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the inverse problem of flow matching, aiming to recover the original transport plan given the initial distribution and the learned velocity field. It proves that the solution to this inverse problem is unique in two specific cases: one-dimensional distributions and Gaussian distributions. The general multidimensional case remains an open problem for future research.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PFed-Signal: An ADR Prediction Model based on Federated Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [federated learning, transformer, euclidean distance, adverse drug reaction, data cleaning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tao Li, Peilin Li, Kui Lu, Yilei Wang, Junliang Shang, Guangshun Li, Huiyu Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> Qufu Normal University, University of Leicester</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23262" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23262</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed PFed-Split, a method to split the original FAERS dataset based on Adverse Drug Reactions (ADRs). 2. Introduced a federated learning-based biased data identification method that uses Euclidean distance to filter out noisy records and generate a clean dataset. 3. Developed an ADR prediction model based on the Transformer architecture, trained on the cleaned dataset, which achieves superior performance in accuracy and signal detection metrics (ROR, PRR) compared to traditional statistical methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c6bff5c67d3939353727926690ff57b0800331885e90983cf3850de46090975_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c6bff5c67d3939353727926690ff57b0800331885e90983cf3850de46090975_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes PFed-Signal, a federated learning-based model for predicting Adverse Drug Reactions (ADRs). The method first cleans biased data from the FAERS database using Euclidean distance within a federated framework and then trains a Transformer model on the cleaned data for prediction. The results show that this approach outperforms traditional statistical methods in key metrics like accuracy, F1 score, and AUC.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [Sparse Autoencoders (SAEs), Low-Rank Adaptation (LoRA), Safety Alignment, Interpretability, Parameter-efficient Fine-tuning (PEFT)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dianyun Wang, Qingsen Ma, Yuhu Shang, Zhifeng Lu, Lechen Ning, Zhenbo Xu, Huijia Wu, Zhaofeng He</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing University of Posts and Telecommunications</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23260" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23260</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel method that uses pre-trained Sparse Autoencoders (SAEs) to construct an explicit, interpretable low-rank subspace for adapter initialization, addressing the black-box nature of traditional LoRA. 2. Provides theoretical analysis proving that SAE-based subspace identification achieves arbitrarily small recovery error under monosemanticity, while direct identification suffers an irreducible error floor due to polysemanticity. 3. Demonstrates state-of-the-art performance on safety alignment, achieving up to 99.6% safety rate while updating only 0.19-0.24% of parameters, and provides interpretable insights into the learned alignment subspace.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e615e6d561cef5b79dc991ed964fd9b6fb069427af26a4b7b42cd33cea4315a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e615e6d561cef5b79dc991ed964fd9b6fb069427af26a4b7b42cd33cea4315a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of interpretability in standard Low-Rank Adaptation (LoRA) methods for fine-tuning large language models. The proposed method leverages Sparse Autoencoders (SAEs) to identify task-relevant features in a disentangled space and uses them to construct an explicit, interpretable low-rank subspace for adapter initialization. The approach achieves superior safety alignment performance and provides transparency into the learned adaptation process.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Revealing design archetypes and flexibility in e-molecule import pathways using Modeling to Generate Alternatives and interpretable machine learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [Energy Systems Optimization], [Modeling to Generate Alternatives, Interpretable Machine Learning, Decision Trees, Energy System Optimization Model, E-molecules]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mahdi Kchaou, Francesco Contino, Diederik Coppitters</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Mechanics, Materials and Civil Engineering (iMMC), Université catholique de Louvain (UCLouvain)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23284" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23284</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Applied Modeling to Generate Alternatives (MGA) to produce a diverse set of near-cost-optimal e-molecule import pathway designs, moving beyond a single optimal solution. 2. Used interpretable machine learning (specifically decision trees) to extract high-level insights and design archetypes from the complex, multi-dimensional solution space generated by MGA. 3. Demonstrated the flexibility of hydrogen import pathways, showing that specific technologies (solar, wind, storage) are not strictly required to stay within a 10% cost margin, and revealed how constraints shift the preferred design archetypes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b329c74249d56df55cd55db503dedb7b277979173c0419ff415764d3a34be11_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b329c74249d56df55cd55db503dedb7b277979173c0419ff415764d3a34be11_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of single cost-optimal designs for green e-molecule import pathways by using Modeling to Generate Alternatives to create diverse near-optimal solutions and then applying interpretable machine learning to analyze them. The method is applied to hydrogen import pathways considering different carriers. The main finding is a broad near-optimal space with significant flexibility, where specific renewable sources are not strictly necessary, and constraints shift preferences toward different carrier and technology combinations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Spectral Analysis of Hard-Constraint PINNs: The Spatial Modulation Mechanism of Boundary Functions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scientific machine learning], [Physics-Informed Neural Networks, Neural Tangent Kernel, spectral analysis, hard constraints, boundary functions]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuchen Xie, Honghang Chi, Haopeng Quan, Yahui Wang, Wei Wang, Yu Ma</p>
</li>
<li class="">
<p><strong>institution:</strong> Sun Yat-sen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23295" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23295</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Established a rigorous Neural Tangent Kernel (NTK) framework for Hard-Constraint PINNs (HC-PINNs), deriving the explicit kernel composition law. 2. Revealed that the boundary function acts as a multiplicative spatial modulator and spectral filter, fundamentally altering the learning landscape and potentially causing spectral collapse. 3. Identified the effective rank of the residual kernel as a superior, deterministic predictor of training convergence compared to classical condition numbers.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb45ca3a4beb6281022392033c68aa10cb18c53e62610b8818daca2495a7eee_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb45ca3a4beb6281022392033c68aa10cb18c53e62610b8818daca2495a7eee_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the training dynamics of Physics-Informed Neural Networks with hard constraints (HC-PINNs). It establishes an NTK framework to show that the boundary function acts as a spectral filter, and identifies the kernel&#x27;s effective rank as a key predictor of convergence. The work provides a theoretical foundation for designing boundary functions to avoid optimization stagnation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [domain-specific foundation model, agentic physical ai, variance collapse, physics-based validation, policy distillation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yoonpyo Lee, Kazuma Kobayashi, Sai Puppala, Sajedul Talukder, Seid Koric, Souvik Chakraborty, Syed Bahauddin Alam</p>
</li>
<li class="">
<p><strong>institution:</strong> Hanyang University, University of Illinois Urbana-Champaign, Southern Illinois University, University of Texas at El Paso, National Center for Supercomputing Applications, Indian Institute of Technology Delhi</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23292" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23292</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new paradigm of Agentic Physical AI, where policy optimization is driven by physics-based outcome validation instead of perceptual inference, addressing the structural limitation of general-purpose models in control tasks. 2. Demonstrates that scaling data for a compact (360M parameter) model induces a sharp phase transition and variance collapse (&gt;500x reduction), leading to stable, execution-level behavior for safety-critical control. 3. Shows the model autonomously distills a robust policy (concentrating on a single strategy) and its learned representations transfer across different physics and input modalities without architectural changes, exhibiting early foundation-model properties.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb921102dfde629395ab8293510cb369a00c1199cadd4c88269dc076f8774a1a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb921102dfde629395ab8293510cb369a00c1199cadd4c88269dc076f8774a1a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a fundamental limitation of general-purpose AI models in safety-critical physical control tasks, where they prioritize semantic plausibility over physical correctness. To address this, it introduces Agentic Physical AI, a paradigm using compact language models trained with physics-based validation on synthetic nuclear reactor control data. The key finding is that sufficient data scaling induces a sharp variance collapse, stabilizing the model&#x27;s behavior and enabling it to autonomously distill a reliable control policy that generalizes across tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [Lyapunov Optimization, Deep Reinforcement Learning, Edge-Cloud Partitioning, Transformer Decomposition, Queue Stability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abolfazl Younesi, Abbas Shabrang Maryan, Elyas Oustad, Zahra Najafabadi Samani, Mohsen Ansari, Thomas Fahringer</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Innsbruck, Sharif University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23310" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23310</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a fine-grained, adaptive partitioning framework (Splitwise) that decomposes transformer layers into attention heads and feed-forward sub-blocks, enabling exponentially more partition choices than layer-wise schemes. 2. Introduces a hierarchical DRL policy guided by Lyapunov optimization to jointly optimize latency, energy, and accuracy while guaranteeing queue stability under stochastic workloads and variable bandwidth. 3. Ensures robustness through partition checkpoints with exponential backoff recovery for communication failures, validated on real edge devices with large models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/121b71aa214f4a7c1671c9df76bf67a9cd64f3cb9e74186606a380ce86f7634f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/121b71aa214f4a7c1671c9df76bf67a9cd64f3cb9e74186606a380ce86f7634f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes Splitwise, a Lyapunov-assisted DRL framework for dynamically partitioning LLM inference between edge and cloud at a fine-grained sub-layer level. It aims to minimize latency and energy while maintaining accuracy under fluctuating network conditions. Experiments show Splitwise significantly reduces latency and energy consumption compared to existing methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Deep learning for pedestrians: backpropagation in Transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [backpropagation], [backpropagation, transformers, gradient derivation, LoRA, PyTorch implementation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Laurent Boué</p>
</li>
<li class="">
<p><strong>institution:</strong> Oracle, Microsoft</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23329" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23329</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a vectorized, index-free derivation of backpropagation for transformer architectures, extending previous work on CNNs. 2. Derives gradient expressions for key transformer components like embedding, multi-headed self-attention, layer normalization, and LoRA layers. 3. Includes a complete PyTorch implementation of a minimal GPT-like network alongside analytical gradient expressions for pedagogical clarity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d5ded3610ac31d4f19d95237254231c2e03d3870df7749cabfc471eeb5008ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d5ded3610ac31d4f19d95237254231c2e03d3870df7749cabfc471eeb5008ac_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper manually derives the backpropagation algorithm for transformer-based next-token-prediction models using a vectorized, index-free methodology. It provides gradient expressions for core layers (embedding, self-attention, layer norm) and LoRA, aiming to build deeper intuition for how operations influence the final output. A complete PyTorch implementation is also provided to illustrate the theoretical derivations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Visual Language Hypothesis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [representation learning], [visual language hypothesis, fiber bundle, semantic quotient, expand-and-snap, topology change]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiu Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Bytedance</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23335" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23335</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the &quot;Visual Language Hypothesis,&quot; framing visual understanding as requiring a discrete semantic language, leading to a fiber-bundle-like structure for the observation space. 2. Derives a theoretical requirement for semantic invariance, arguing it necessitates a non-homeomorphic, discriminative target (e.g., supervised labels, multimodal alignment) rather than smooth deformation alone. 3. Identifies an architectural requirement for models, proposing an &quot;expand-and-snap&quot; process to achieve the necessary topology change for semantic abstraction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d70511c4d2f57ecb4e49170ed73c0a81de0fcfcdbdb84cc7bcbdca254140c3f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d70511c4d2f57ecb4e49170ed73c0a81de0fcfcdbdb84cc7bcbdca254140c3f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes the &quot;Visual Language Hypothesis,&quot; which posits that visual understanding requires a discrete semantic structure, implying the visual observation space is organized like a fiber bundle. From this, the authors theoretically argue that achieving semantic invariance demands discriminative supervision and that model architectures must support a specific &quot;expand-and-snap&quot; process to change topology. The framework provides a topological interpretation for empirical patterns in large-scale discriminative and multimodal models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scaling laws], [scaling laws, model ensembling, multi-model collaboration, cross-entropy loss, parameter budget]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dakuan Lu, Jiaqi Zhang, Cheng Yuan, Jiawei Shao, Chi Zhang, Xuelong Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Artificial Intelligence (TeleAI), China Telecom</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23340" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23340</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the &quot;Law of Multi-model Collaboration,&quot; a novel scaling law for predicting the performance limits of LLM ensembles based on aggregated parameters. 2. Establishes a method-agnostic theoretical framework using an idealized integration oracle to quantify the intrinsic upper bound of multi-model collaboration. 3. Empirically demonstrates that multi-model systems follow a power-law scaling with better trends and lower loss floors than single models, and that heterogeneous ensembles outperform homogeneous ones.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68212ad5f9cd50ef959cdd80f4b7274178d9a6b124904010fc5b0cf0834b21a1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68212ad5f9cd50ef959cdd80f4b7274178d9a6b124904010fc5b0cf0834b21a1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of a theoretical framework for scaling in multi-model LLM systems. It proposes the &quot;Law of Multi-model Collaboration,&quot; a scaling law based on aggregated parameters, and finds that ensembles scale better and achieve lower loss than single models, with diversity being a key driver of gains.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ECG-RAMBA: Zero-Shot ECG Generalization by Morphology-Rhythm Disentanglement and Long-Range Modeling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [medical signal processing], [ECG classification, morphology-rhythm disentanglement, Mamba, zero-shot generalization, Power Mean pooling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hai Duong Nguyen, Xuan-The Tran</p>
</li>
<li class="">
<p><strong>institution:</strong> HAI-Smartlink Research Lab (Anchi STE Company), Vietnam Maritime University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23347" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23347</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ECG-RAMBA, a framework that explicitly disentangles ECG morphology (via MiniRocket) and rhythm (via HRV descriptors) before fusing them for robust classification. 2. Introduces a numerically stable Power Mean pooling operator (Q=3) for windowed inference to emphasize high-evidence segments. 3. Demonstrates strong zero-shot cross-dataset generalization for ECG classification using a bi-directional Mamba backbone for long-range contextual modeling.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f9b49242e586cadf1889fa40730ea1652c1b4ef5b15cf15697b58832c4dbf6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f9b49242e586cadf1889fa40730ea1652c1b4ef5b15cf15697b58832c4dbf6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of poor generalization of deep learning models for ECG classification across different datasets. It proposes ECG-RAMBA, a method that separates and then fuses morphological and rhythm features, using a Mamba backbone and a novel pooling operator. The results show that this approach achieves robust zero-shot performance on external datasets, outperforming a baseline model.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ISOPO: Proximal policy gradients without pi-old</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [natural policy gradient, proximal policy optimization, reinforcement learning fine-tuning, Fisher metric, neural tangent kernel]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nilin Abrahamsen</p>
</li>
<li class="">
<p><strong>institution:</strong> None (No affiliation or email domain provided in the given content)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23353" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23353</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces ISOPO, a method to approximate the natural policy gradient in a single gradient step, contrasting with existing methods like GRPO/PPO that require multiple steps. 2. Proposes a simple form of ISOPO that normalizes log-probability gradients in the Fisher metric before contracting with advantages. 3. Presents a variant that transforms microbatch advantages based on the neural tangent kernel layer-wise, enabling efficient implementation with negligible overhead compared to REINFORCE.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa80f3dc48bfe2cf2fc158ecc6c0bb7aefffb89047088c0b23d1d764889fea16_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa80f3dc48bfe2cf2fc158ecc6c0bb7aefffb89047088c0b23d1d764889fea16_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Isometric Policy Optimization (ISOPO), a new method for approximating the natural policy gradient in reinforcement learning fine-tuning. Unlike existing proximal policy methods like GRPO/PPO which require multiple gradient steps with a reference policy, ISOPO performs the approximation in a single step by normalizing gradients or transforming advantages, achieving this with minimal computational overhead.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [post-training quantization, W8A8, W4A8, Ascend NPU, Chain-of-Thought (CoT)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yilun Luo, HuaQing Zheng, Haoqian Meng, Wenyuan Liu, Peng Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tianjin University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23367" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23367</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a unified low-bit inference framework for openPangu-Embedded models, supporting INT8 (W8A8) and W4A8 quantization optimized for the Atlas A2 Ascend NPU. 2. Provides a comprehensive evaluation of quantization across three distinct CoT reasoning modes (slow_think, auto_think, no_think) on code generation benchmarks (HumanEval, MBPP). 3. Demonstrates that INT8 quantization preserves over 90% of FP16 accuracy with a 1.5x prefill speedup, while W4A8 significantly reduces memory consumption, enabling efficient CoT reasoning on edge NPUs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07e9cf3e7e8252aa7eae3fdf2e7647007d4786dd6ade9f5c4e940d1c74c4e2cd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07e9cf3e7e8252aa7eae3fdf2e7647007d4786dd6ade9f5c4e940d1c74c4e2cd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high memory and latency overhead of deploying Chain-of-Thought (CoT) enabled openPangu models on Ascend NPUs by applying post-training quantization (INT8 and W4A8). The proposed framework, optimized for the Atlas A2 hardware, maintains high accuracy for INT8 and reduces memory for W4A8, enabling efficient on-device CoT reasoning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Diffusion priors enhanced velocity model building from time-lag images using a neural operator</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [neural operator, velocity model building, reverse time migration, diffusion model, automatic differentiation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiao Ma, Mohammad Hasyim Taufik, Tariq Alkhalifah</p>
</li>
<li class="">
<p><strong>institution:</strong> King Abdullah University of Science and Technology (KAUST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23375" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23375</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel framework that combines generative models (diffusion priors) with neural operators for velocity model building. 2. Uses a neural operator as a fast surrogate for the forward modeling and migration process to generate time-lag images from velocity models. 3. Employs the trained neural operator and automatic differentiation to update the migration velocity, enhanced by a generative model as a regularizer to produce high-resolution, cleaner predictions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0c5cd7e3ef87aa48d65e94484e956e08ebef522c14cc7eee60600b85109e02a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0c5cd7e3ef87aa48d65e94484e956e08ebef522c14cc7eee60600b85109e02a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a new deep learning framework for efficient, high-resolution velocity model building in seismic imaging. The method combines a neural operator, which acts as a fast surrogate for seismic modeling and migration, with a diffusion generative model that serves as a prior to regularize the solution. Experiments on synthetic and field data show the approach effectively builds cleaner, higher-resolution velocity models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [log anomaly detection], [collaborative transformers, multi-head impressed attention, modality adaptation layer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mohammad Nasirzadeh, Jafar Tahmoresnezhad, Parviz Rashidi-Khazaee</p>
</li>
<li class="">
<p><strong>institution:</strong> Urmia University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23380" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23380</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/your-repo/CoLog" target="_blank" rel="noopener noreferrer" class="">https://github.com/your-repo/CoLog</a> (Note: The provided text states &quot;We also provide the implementation of CoLog atthis https URL.&quot; but the specific URL is cut off in the input. Based on the placeholder, the typical format is used. If the exact URL is required, it would be the one following &quot;atthis&quot; in the original text.)</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CoLog, a unified framework for detecting both point and collective anomalies in OS logs by applying multimodal sentiment analysis concepts. 2. Introduces collaborative transformers and multi-head impressed attention to learn interactions between different log data modalities. 3. Incorporates a modality adaptation layer to handle heterogeneity and adapt representations from different log modalities.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of log anomaly detection, where existing methods struggle with the multimodal nature of log data and the interactions between these modalities. It proposes CoLog, a framework that uses collaborative transformers and a modality adaptation layer to learn nuanced patterns across log modalities for comprehensive anomaly detection. Extensive experiments show CoLog achieves state-of-the-art performance, with mean precision, recall, and F1 scores over 99.5% across seven benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Beyond-Diagonal Reconfigurable Intelligent Surfaces for 6G Networks: Principles, Challenges, and Quantum Horizons</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [communication &amp; networking], [beyond-diagonal RIS, passive beamforming, hybrid quantum-classical ML, 6G networks, reconfigurable intelligent surfaces]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abd Ullah Khan, Uman Khalid, Muhammad Tanveer, Trung Q. Duong, Hyundong Shin</p>
</li>
<li class="">
<p><strong>institution:</strong> Kyung Hee University, University of Management and Technology, Memorial University of Newfoundland, Queen&#x27;s University Belfast</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23400" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23400</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a systematic introduction to Beyond-Diagonal Reconfigurable Intelligent Surfaces (BD-RIS), detailing its principles, architecture, advantages, and classification. 2. Presents a case study comparing four beamforming algorithms for BD-RIS, analyzing their performance in terms of sum rate and computation cost. 3. Proposes and analyzes hybrid quantum-classical machine learning models to enhance beam prediction for 6G BD-RIS, validated using the real-world DeepSense 6G dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4756534e0e202824dc7facc5a963e3a3205fc85e2596f60033bcdb6ae4cab497_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4756534e0e202824dc7facc5a963e3a3205fc85e2596f60033bcdb6ae4cab497_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Beyond-Diagonal Reconfigurable Intelligent Surfaces (BD-RIS) as a key technology for 6G networks to overcome high-frequency propagation challenges. It systematically reviews BD-RIS principles, analyzes beamforming algorithms, and explores quantum-enhanced machine learning for beam prediction. The work concludes with insights into the practical implications and future potential of BD-RIS for advanced wireless communication.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Task-driven Heterophilic Graph Structure Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [graph structure learning, heterophilic graphs, spectral filtering, topology inference, graph rewiring]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ayushman Raghuvanshi, Gonzalo Mateos, Sundeep Prabhakar Chepuri</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Science, University of Rochester</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23406" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23406</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FgGSL, an end-to-end framework that jointly learns complementary homophilic and heterophilic graph structures using a learnable masking function and processes them with low- and high-pass graph filter banks. 2. Introduces a label-based structural loss to explicitly promote the recovery of homophilic and heterophilic edges, enabling task-driven graph structure learning, and provides theoretical stability bounds for this loss and robustness guarantees for the filters. 3. Demonstrates through experiments on six heterophilic benchmarks that FgGSL consistently outperforms state-of-the-art GNNs and graph rewiring methods, validating the benefit of combining frequency information with supervised topology inference.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3fec9358f601e6c323ecab2e7bcfe1880fd9b9d4351503536e33c58d8cedb6e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3fec9358f601e6c323ecab2e7bcfe1880fd9b9d4351503536e33c58d8cedb6e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of learning discriminative node representations on heterophilic graphs, where connected nodes often have dissimilar labels. The authors propose FgGSL, a framework that jointly learns homophilic and heterophilic graph structures using spectral filters and a task-driven structural loss. Experiments show FgGSL outperforms existing methods, highlighting the advantage of combining frequency guidance with supervised graph inference.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On the Sample Complexity of Learning for Blind Inverse Problems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [inverse problems], [blind inverse problems, Linear Minimum Mean Square Estimators (LMMSEs), Tikhonov regularization, random forward operators, error bounds]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nathan Buskulic, Luca Calatroni, Lorenzo Rosasco, Silvia Villa</p>
</li>
<li class="">
<p><strong>institution:</strong> Università degli studi di Genova, Italian Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23405" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23405</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Deriving closed-form expressions for optimal Linear Minimum Mean Square Estimators (LMMSEs) for blind inverse problems and establishing their equivalence with distribution-dependent Tikhonov regularization. 2. Proving convergence results for these estimators under source condition assumptions. 3. Deriving rigorous finite-sample error bounds that quantify the impact of operator randomness, noise level, and sample count on estimator performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/daa1ffc1715fdd9a990c7a6af28c2a415193fdef2bd99ec47e6ece431b0f3406_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/daa1ffc1715fdd9a990c7a6af28c2a415193fdef2bd99ec47e6ece431b0f3406_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper provides a theoretical analysis of learning for blind inverse problems, where the forward operator is unknown. It focuses on Linear Minimum Mean Square Estimators (LMMSEs), deriving their optimal forms and connecting them to Tikhonov regularization. The main conclusion is the establishment of rigorous error bounds and convergence rates that characterize how estimator performance depends on noise, problem conditioning, and the randomness of the forward operator.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Theoretical Foundations of Scaling Law in Familial Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [familial models, scaling law, early exiting, IsoFLOP design, compute-optimal training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Huan Song, Qingfei Zhao, Ting Long, Shuyu Tian, Hongjun An, Jiawei Shao, Chi Zhang, Xuelong Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Artificial Intelligence (TeleAI), China Telecom</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23407" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23407</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Theoretically and empirically extends the neural scaling law to the &quot;familial models&quot; paradigm by introducing granularity (G) as a new fundamental scaling variable alongside model size (N) and tokens (D). 2. Proposes a rigorous IsoFLOP experimental design to decouple architectural impact from computational scale, enabling high-fidelity parameterization of the unified scaling law L(N, D, G). 3. Quantifies that the granularity penalty follows a multiplicative power law with an extremely small exponent (γ≈0.041), validating the &quot;train once, deploy many&quot; paradigm without compromising compute-optimality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc66a2a88c82327d2e67ccabca47fcc7a15e81e139a0ed0135b0f3ea93534985_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc66a2a88c82327d2e67ccabca47fcc7a15e81e139a0ed0135b0f3ea93534985_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of traditional neural scaling laws, which assume a single model, by extending them to familial models that generate multiple sub-models from one backbone. The authors propose a unified scaling law incorporating granularity (G) and validate it using a rigorous IsoFLOP experimental design. The key finding is that the performance penalty for increased granularity is very small, proving that deployment flexibility can be achieved efficiently.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Directly Constructing Low-Dimensional Solution Subspaces in Deep Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [intrinsic dimension, low-rank approximation, subspace-native distillation, weight matrices, empirical spectral density]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yusuf Kalyoncuoglu</p>
</li>
<li class="">
<p><strong>institution:</strong> RWTH Aachen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23410" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23410</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a constructive method to decouple solution geometry from the ambient search space, bypassing the non-convex optimization bottleneck. 2. Empirically demonstrates significant compression (e.g., factor of 16) of classification heads in models like ResNet-50, ViT, and BERT with minimal performance loss. 3. Introduces &quot;Subspace-Native Distillation&quot; as a novel paradigm to provide a stable geometric coordinate system for student models, enabling &quot;Train Big, Deploy Small&quot;.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d34f960cfbb8a9db281aca58a1b30934c42fc68964647e8066a3754aeb92d38_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d34f960cfbb8a9db281aca58a1b30934c42fc68964647e8066a3754aeb92d38_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the redundancy in large neural networks by proposing a method to directly construct low-dimensional solution subspaces, decoupling the solution geometry from the high-dimensional optimization search space. It shows that classification heads can be heavily compressed without significant performance drops. This leads to a new distillation paradigm that allows student models to learn in a stable, low-dimensional subspace, potentially realizing efficient deployment of compact models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Towards Integrating Uncertainty for Domain-Agnostic Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [semantic segmentation], [uncertainty quantification, domain-agnostic, Segment Anything Model (SAM), Laplace approximation, benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jesse Brouwers, Xiaoyan Xing, Alexander Timans</p>
</li>
<li class="">
<p><strong>institution:</strong> UvA-Bosch Delta Lab, University of Amsterdam</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23427" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23427</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/JesseBrouw/UncertSAM" target="_blank" rel="noopener noreferrer" class="">https://github.com/JesseBrouw/UncertSAM</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Curated UncertSAM, a benchmark of eight datasets to stress-test segmentation models under challenging conditions like shadows and camouflage. 2. Evaluated a suite of lightweight, post-hoc uncertainty estimation methods for segmentation foundation models. 3. Assessed a preliminary uncertainty-guided prediction refinement step, finding that last-layer Laplace approximation yields uncertainty estimates well-correlated with segmentation errors.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15865ed74ed61443aa69769e51585f4a7341748fc10c0250eef9243be675f215_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15865ed74ed61443aa69769e51585f4a7341748fc10c0250eef9243be675f215_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether uncertainty quantification can improve the robustness of foundation segmentation models like SAM in domain-agnostic settings. The authors propose a benchmark (UncertSAM) and evaluate several post-hoc uncertainty estimation methods, finding that a last-layer Laplace approximation provides meaningful uncertainty signals. The results indicate the potential of integrating uncertainty to enhance model generalizability, though refinement benefits are preliminary.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [gpu kernels], [kernel generation, multi-agent system, domain-specific languages (DSLs), performance tuning, Triton]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jinye Du, Quan Yuan, Zuyao Zhang, Yanzhi Yi, Jiahui Hu, Wangyi Chen, Yiyang Zhu, Qishui Zheng, Wenxiang Zou, Xiangyu Chang, Zuohe Zheng, Zichun Ye, Chao Liu, Shanni Li, Renwei Zhang, Yiping Deng, Xinwei Hu, Xuefeng Jin, Jie Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Huawei Technologies Co., Ltd., Hunan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23424" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23424</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed AKG kernel agent, a multi-agent framework that automates the generation, migration, and performance tuning of computational kernels for diverse hardware platforms. 2. Designed the system to support multiple Domain-Specific Languages (DSLs) like Triton, TileLang, CPP, and CUDA-C, enabling cross-platform portability and correctness. 3. Demonstrated the system&#x27;s effectiveness through evaluation on KernelBench, achieving an average 1.46x speedup over PyTorch Eager baselines on GPU and NPU backends.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40d5942348375e7b86203ab7a7420bba7105494296f349fdd48174b020a1527e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40d5942348375e7b86203ab7a7420bba7105494296f349fdd48174b020a1527e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes AKG kernel agent, a multi-agent framework that automates the development and optimization of high-performance computational kernels for modern AI workloads across diverse hardware. The system supports multiple DSLs for portability and uses LLMs for code generation and tuning. Evaluation shows it achieves a 1.46x average speedup over baseline implementations, effectively accelerating kernel development.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Stochastic Siamese MAE Pretraining for Longitudinal Medical Images</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [masked autoencoder, siamese network, stochastic process, longitudinal data, variational inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Taha Emre, Arunava Chakravarty, Thomas Pinetz, Dmitrii Lachinov, Martin J. Menten, Hendrik Scholl, Sobha Sivaprasad, Daniel Rueckert, Andrew Lotery, Stefan Sacu, Ursula Schmidt-Erfurth, Hrvoje Bogunović</p>
</li>
<li class="">
<p><strong>institution:</strong> Medical University of Vienna, Imperial College London, Technical University of Munich, University College London, University of Southampton</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23441" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23441</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed STAMP, a novel Siamese MAE framework that incorporates temporal information by conditioning on the time difference between input volumes. 2. Introduced a stochastic learning approach by reframing the MAE reconstruction loss as a conditional variational inference objective to model the uncertainty in disease progression. 3. Demonstrated superior performance of STAMP-pretrained models over existing temporal MAE methods and foundation models on predicting progression of Age-Related Macular Degeneration and Alzheimer&#x27;s Disease.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c93396b0ded8fc65364d5714bd12e652a23ffc22eb276cbbca01426ecd0db791_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c93396b0ded8fc65364d5714bd12e652a23ffc22eb276cbbca01426ecd0db791_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the lack of temporal awareness in self-supervised learning methods like MAE for longitudinal medical images. It proposes STAMP, a stochastic Siamese MAE framework that learns temporal dynamics by conditioning on time differences and using a variational inference objective. The method outperformed existing approaches on disease progression prediction tasks for OCT and MRI datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Assessing behaviour coverage in a multi-agent system simulation for autonomous vehicle testing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent systems], [Model Predictive Control, Coverage-Based Testing, Edge-Case Exploration, Multi-Agent Simulation, Behaviour Coverage]</p>
</li>
<li class="">
<p><strong>authors:</strong> Manuel Franco-Vivo</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Bristol</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23445" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23445</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A systematic approach to measure and assess behaviour coverage within a multi-agent simulation for autonomous vehicle testing. 2. The proposal of a Model Predictive Control (MPC) pedestrian agent designed to generate interesting tests and realistic behaviour. 3. Insights and analysis for improving and optimizing simulation frameworks through behaviour coverage metrics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385ed91e6ca24b7cc0e8d369cc587a3dd677cf4643f89f27d85aaadf4cd4ea70_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385ed91e6ca24b7cc0e8d369cc587a3dd677cf4643f89f27d85aaadf4cd4ea70_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the need for comprehensive testing of autonomous vehicles by analyzing behaviour coverage in multi-agent simulations. It proposes a systematic method to measure coverage and introduces an MPC-based pedestrian agent to generate more realistic and challenging test scenarios. The research concludes that assessing behaviour coverage is crucial for validating the robustness of autonomous systems and improving simulation frameworks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [Mixture-of-Experts, Router-Expert Coupling, Auxiliary Loss, Expert Specialization, Efficient Training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ang Lv, Jin Ma, Yiyuan Ma, Siyuan Qiao</p>
</li>
<li class="">
<p><strong>institution:</strong> ByteDance, Renmin University of China</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23447" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23447</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel lightweight auxiliary loss (ERC loss) to explicitly couple router decisions with expert capabilities in MoE models. 2. Introduces a computationally efficient method that scales with the square of the number of experts (n^2), independent of batch size, unlike prior token-dependent methods. 3. Enables flexible control and quantitative tracking of expert specialization levels during training, providing new insights into MoE model dynamics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6534d4f2f88c89a450614cf67b57f76f33fc90a18f24833876fd8e55d3e326b9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6534d4f2f88c89a450614cf67b57f76f33fc90a18f24833876fd8e55d3e326b9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the misalignment between router decisions and expert capabilities in Mixture-of-Experts (MoE) models. It proposes an Expert-Router Coupling (ERC) loss, a lightweight auxiliary loss that enforces constraints via perturbed router embeddings to ensure each expert specializes in its routed tokens and each router embedding faithfully represents its expert. The method is shown to be effective and computationally efficient, enabling better control and analysis of expert specialization during large-scale pre-training.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Dynamic Subspace Composition: Efficient Adaptation via Contractive Basis Expansion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [Mixture of Experts, Parameter-Efficient Fine-Tuning, Low-Rank Adaptation, Memory-Bandwidth Bottleneck, Dynamic Sparse Dictionary Learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vladimer Khasia</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23448" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23448</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/VladimerKhasia/DSC" target="_blank" rel="noopener noreferrer" class="">https://github.com/VladimerKhasia/DSC</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Dynamic Subspace Composition (DSC), a framework that models weight updates as a residual trajectory within a Star-Shaped Domain using Magnitude-Gated Simplex Interpolation for continuity. 2. Decouples storage and adaptation rank, constructing compositional approximations from a shared basis bank to reduce parameter complexity from O(Mrd) to O(Md) and memory traffic to O(Kd). 3. Introduces Frame-Theoretic regularization and spectral constraints to provide rigorous worst-case bounds on the dynamic update, addressing representation collapse and gradient instability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/713ad8af3e30c492117d0d3f26babbf5fe909df2e68e0ab479fc866276a3d80c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/713ad8af3e30c492117d0d3f26babbf5fe909df2e68e0ab479fc866276a3d80c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the memory-bandwidth bottleneck and optimization instability in Mixture of Experts (MoE) models. It proposes Dynamic Subspace Composition (DSC), a method that approximates context-dependent weights via a sparse expansion of a shared basis, reducing parameter complexity and memory traffic while ensuring stable updates. The main conclusion is that DSC offers a more efficient and theoretically grounded alternative to standard approaches like Mixture-of-LoRAs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [instruction following, hindsight replay, sample-efficient RL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kongcheng Zhang, Qi Yao, Shunyu Liu, Wenjian Zhang, Min Cen, Yang Zhou, Wenkai Fang, Yiru Zhao, Baisheng Lai, Mingli Song</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Cainiao Network, Nanyang Technological University, Dalian University of Technology, University of Science and Technology of China, Alibaba Cloud Computing, Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23457" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23457</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/zhangkc97/HiR" target="_blank" rel="noopener noreferrer" class="">https://github.com/zhangkc97/HiR</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Hindsight instruction Replay (HiR), a novel RL framework that replays failed attempts as successes using a select-then-rewrite strategy to address sparse rewards. 2. Theoretically frames the RL objective as dual-preference learning at both instruction- and response-level, enabling efficient optimization with only binary rewards. 3. Demonstrates sample efficiency and promising results across various instruction following tasks with reduced computational budget.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72e272e5c14b0f640f80b3e8859a1fd3a0a8b8e8608dfacee9528d242698f15_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72e272e5c14b0f640f80b3e8859a1fd3a0a8b8e8608dfacee9528d242698f15_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of sparse rewards in RL for aligning LLMs to follow complex instructions. It proposes HiR, a sample-efficient framework that replays failed responses as successful ones based on partially satisfied constraints, framed as dual-preference learning. Experiments show HiR achieves strong performance on instruction-following tasks while being more computationally efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning from human feedback (RLHF)], [reward model, inductive bias, information bottleneck, mutual information, reward hacking]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhuo Li, Pengyu Cheng, Zhechao Yu, Feifei Tong, Anningzhe Gao, Tsung-Hui Chang, Xiang Wan, Erchao Zhao, Xiaoxi Jiang, Guanjun Jiang</p>
</li>
<li class="">
<p><strong>institution:</strong> Alibaba, The Chinese University of Hong Kong, Shenzhen Research Institute of Big Data</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23461" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23461</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Qwen-Applications/DIR" target="_blank" rel="noopener noreferrer" class="">https://github.com/Qwen-Applications/DIR</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DIR, a novel information-theoretic debiasing method for reward models that maximizes mutual information with human preference while minimizing it with biased attributes. 2. Theoretically justifies the method&#x27;s ability to handle complex, non-linear inductive biases, extending beyond simple linear correlation models. 3. Empirically demonstrates DIR&#x27;s effectiveness in mitigating three types of biases (length, sycophancy, format) and shows it enhances RLHF performance and generalization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/522f5bbb5a5776cd8df024fb1b24faf19bb1a1ea6e0408c7951f37bfd1657846_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/522f5bbb5a5776cd8df024fb1b24faf19bb1a1ea6e0408c7951f37bfd1657846_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of inductive biases in reward models (RMs) for RLHF, which can lead to overfitting and reward hacking. It proposes DIR, an information-theoretic debiasing method inspired by the information bottleneck that optimizes mutual information to reduce bias. Experiments show DIR effectively mitigates multiple biases and improves RLHF performance and generalization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [Parameter-efficient fine-tuning, LoRA, full-rank adaptation, rotational degrees of freedom, hierarchical joint decomposition]</p>
</li>
<li class="">
<p><strong>authors:</strong> Guoan Wan, Tianyu Chen, Fangzheng Feng, Haoyi Zhou, Runhua Xu</p>
</li>
<li class="">
<p><strong>institution:</strong> Beihang University, Huazhong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23485" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23485</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Bane-Elvin/AAAI2026-FRoD" target="_blank" rel="noopener noreferrer" class="">https://github.com/Bane-Elvin/AAAI2026-FRoD</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FRoD, a novel PEFT method that combines hierarchical joint decomposition with rotational degrees of freedom for full-rank updates. 2. Introduces a globally shared basis and sparse, learnable perturbations to enhance expressiveness and efficiency beyond low-rank constraints. 3. Demonstrates that FRoD matches full fine-tuning accuracy on 20 benchmarks while using only 1.72% of trainable parameters and achieves faster convergence.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e211fe6dc2d8e782c731fff29ba32c9fe2a1f958b475d5931d50f6e8fd04fdb8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e211fe6dc2d8e782c731fff29ba32c9fe2a1f958b475d5931d50f6e8fd04fdb8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the slow convergence and limited capacity of low-rank PEFT methods like LoRA. It proposes FRoD, a method that enables full-rank updates via a shared basis and sparse perturbations, achieving faster convergence. The method matches full fine-tuning accuracy on diverse benchmarks while using only a tiny fraction of parameters.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [model selection, capability-cost frontier, constrained optimization, deployment-aware leaderboards, compliance trade-offs]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vassilis Digalakis Jr, Ramayya Krishnan, Gonzalo Martin Fernandez, Agni Orfanoudaki</p>
</li>
<li class="">
<p><strong>institution:</strong> Boston University, Carnegie Mellon University, Universitat Politècnica de Catalunya, Oxford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23487" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23487</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ML Compass, a framework that treats AI model selection as constrained optimization over a capability-cost frontier to bridge the gap between capability leaderboards and deployment decisions. 2. Characterizes optimal model configurations theoretically, showing a three-regime structure in internal measures and deriving comparative statics for budget, regulation, and technology changes. 3. Implements a practical pipeline that extracts internal measures, estimates an empirical frontier, learns task-specific utility, and validates with case studies in conversational and healthcare settings.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c28f58754a80430c57b0351355d200ab9fbfde8dd5fafc1b0d5caf4dd85bbb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c28f58754a80430c57b0351355d200ab9fbfde8dd5fafc1b0d5caf4dd85bbb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the gap between AI model capability rankings and real-world deployment decisions by introducing ML Compass, a framework that formulates model selection as constrained optimization over a capability-cost frontier. It combines theoretical analysis of optimal configurations with an implementation pipeline for recommendation, validated in conversational and healthcare case studies. The framework shows that deployment-aware rankings can differ significantly from capability-only leaderboards, clarifying trade-offs between capability, cost, and compliance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [communication &amp; networking], [URLLC, Link Adaptation, Device Scheduling, Deep Reinforcement Learning, Bayesian Optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wei Gao, Paul Zheng, Peng Wu, Yulin Hu, Anke Schmeink</p>
</li>
<li class="">
<p><strong>institution:</strong> Wuhan University, RWTH Aachen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23493" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23493</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a joint link adaptation and device scheduling design for multi-device URLLC IIoT networks under imperfect CSI, aiming to maximize total transmission rate under strict BLER constraints. 2. Introduces a novel Bayesian Optimization-driven Twin Delayed Deep Deterministic Policy Gradient (BO-TD3) method to adaptively determine device serving order and MCS based on outdated CQI. 3. Develops a BO-based training mechanism to address issues of error sample imbalance and TD3 parameter sensitivity, improving convergence speed and learning reliability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa25685331ccee6042c70838d3438022f95490a2cc609beba627f444cba8cd7c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa25685331ccee6042c70838d3438022f95490a2cc609beba627f444cba8cd7c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of joint link adaptation and device scheduling in URLLC IIoT networks with imperfect channel state information. It proposes a novel deep reinforcement learning method (BO-driven TD3) that adaptively selects the device serving order and modulation schemes, enhanced by Bayesian Optimization for faster and more reliable training. Simulation results show the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Trustworthy Machine Learning under Distribution Shifts</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [trustworthy machine learning], [distribution shift, robustness, explainability, adaptability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhuo Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Sydney</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23524" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23524</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a systematic framework for studying Trustworthy Machine Learning by categorizing three common types of distribution shifts (Perturbation, Domain, Modality). 2. Rigorously investigates trustworthiness through three key aspects: Robustness, Explainability, and Adaptability. 3. Aims to provide effective solutions and fundamental insights to enhance critical ML problems like efficiency and safety under distribution shifts.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8153efa2ea5f1ee68438cc93b22e23dd850b2f55b7ebc99a5374a4a32aea0bb4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8153efa2ea5f1ee68438cc93b22e23dd850b2f55b7ebc99a5374a4a32aea0bb4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This thesis addresses the core problem of distribution shift, which limits the reliability and trustworthiness of AI systems. The research proposes a framework that studies three types of distribution shifts and evaluates solutions through the lenses of robustness, explainability, and adaptability. The goal is to develop more reliable, versatile, and responsible machine learning models that can generalize effectively under real-world distribution shifts.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] EEG-based Graph-guided Domain Adaptation for Robust Cross-Session Emotion Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [affective computing], [domain adaptation, graph regularization, EEG, emotion recognition, cross-session]</p>
</li>
<li class="">
<p><strong>authors:</strong> Maryam Mirzaei, Farzaneh Shayegh, Hamed Narimani</p>
</li>
<li class="">
<p><strong>institution:</strong> Based on author names and content, specific institution not provided. Could be inferred from typical academic affiliations in this field, but not explicitly stated in the given text.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23526" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23526</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed EGDA, a novel framework integrating domain adaptation with graph-based regularization for cross-session EEG emotion recognition. 2. Introduced a method to jointly align both marginal and conditional distributions while preserving the intrinsic data structure. 3. Demonstrated the discriminative power of the Gamma frequency band and identified critical brain regions (central-parietal and prefrontal) for emotion recognition.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97864260ef21b4d340e353b5b1666e9df5860104730e726e9067386c78cadc72_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97864260ef21b4d340e353b5b1666e9df5860104730e726e9067386c78cadc72_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of cross-session EEG emotion recognition by proposing EGDA, a framework that reduces distribution discrepancies through joint marginal and conditional alignment while using graph regularization to preserve data structure. Experiments on the SEED-IV dataset show EGDA outperforms baselines, achieving robust accuracies. The analysis further identifies the Gamma band and specific brain regions as key for reliable emotion recognition.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] VL-RouterBench: A Benchmark for Vision-Language Model Routing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [vision-language model routing, benchmark, cost-accuracy trade-off, model selection, evaluation protocol]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhehao Huang, Baijiong Lin, Jingyuan Zhang, Jingying Wang, Yuhang Liu, Ning Lu, Tao Li, Xiaolin Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23562" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23562</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/K1nght/VL-RouterBench" target="_blank" rel="noopener noreferrer" class="">https://github.com/K1nght/VL-RouterBench</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes VL-RouterBench, the first systematic and reproducible benchmark for evaluating vision-language model (VLM) routing systems. 2. Constructs a large-scale evaluation foundation with quality and cost matrices over 519,180 sample-model pairs from 17 models and 14 datasets. 3. Introduces a comprehensive evaluation protocol that jointly measures accuracy, cost, and throughput, and uses a ranking score based on the harmonic mean for fair comparison across router configurations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/885d9087464eedead5301ad4cd041923ddee6d5773371e117abbd30fc4ae4f09_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/885d9087464eedead5301ad4cd041923ddee6d5773371e117abbd30fc4ae4f09_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces VL-RouterBench, a benchmark to systematically evaluate routing systems for vision-language models. It constructs matrices of quality and cost from extensive inference logs and uses a ranking score to compare routers. The evaluation shows current routers achieve significant gains but still fall short of an ideal Oracle, indicating room for improvement in router design.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Distribution-Free Process Monitoring with Conformal Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [anomaly detection], [Conformal Prediction, Statistical Process Control, Control Charts, Anomaly Detection, Quality Management]</p>
</li>
<li class="">
<p><strong>authors:</strong> Christopher Burger</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Mississippi</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23602" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23602</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A hybrid framework integrating Conformal Prediction&#x27;s distribution-free guarantees into Statistical Process Control (SPC). 2. Conformal-Enhanced Control Charts that visualize process uncertainty and enable proactive signals like &#x27;uncertainty spikes&#x27;. 3. Conformal-Enhanced Process Monitoring that reframes multivariate control as a formal anomaly detection problem using an intuitive p-value chart.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b25b4ce8de4803a0d1cc60a16d7221589651a9a271d92f85fb5b6426127e8b4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b25b4ce8de4803a0d1cc60a16d7221589651a9a271d92f85fb5b6426127e8b4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of traditional Statistical Process Control (SPC), which relies on often-violated statistical assumptions, by proposing a hybrid framework that integrates distribution-free Conformal Prediction. The method introduces two novel applications: enhanced control charts for visualizing uncertainty and a p-value chart for formal anomaly detection. The framework provides a more robust and statistically rigorous approach to quality control while maintaining the interpretability of classic methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [transfer learning], [Le Cam Distortion, Deficiency Distance, Directional Simulability, Unsupervised Domain Adaptation, Negative Transfer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Deniz Akdemir</p>
</li>
<li class="">
<p><strong>institution:</strong> None (Institution not specified in provided content)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23617" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23617</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a decision-theoretic framework for robust transfer learning based on Le Cam&#x27;s theory, replacing symmetric invariance with directional simulability. 2. Introduces Le Cam Distortion, quantified by the Deficiency Distance, as a rigorous upper bound for transfer risk. 3. Demonstrates the framework&#x27;s effectiveness across diverse experiments (genomics, vision, RL), showing it prevents source degradation and catastrophic negative transfer where traditional methods fail.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bd736028263c7eaaaaecae78f0df59f633374608f59295b1185c8385eea1e5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bd736028263c7eaaaaecae78f0df59f633374608f59295b1185c8385eea1e5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a flaw in standard Unsupervised Domain Adaptation, which can cause harmful &quot;negative transfer&quot; by forcing invariance between unequally informative domains. It proposes a new framework based on Le Cam&#x27;s theory, using directional simulability and a metric called Le Cam Distortion to enable safe transfer without degrading the source domain. Experiments show this method successfully prevents information loss and catastrophic failure in safety-critical applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Regret-Based Federated Causal Discovery with Unknown Interventions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [causal discovery, unknown interventions, differential privacy, Φ-CPDAG, regret-based]</p>
</li>
<li class="">
<p><strong>authors:</strong> Federico Baldo, Charles K. Assaad</p>
</li>
<li class="">
<p><strong>institution:</strong> Sorbonne Université, INSERM, Institut Pierre Louis d&#x27;Epidémiologie et de Santé Publique</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23626" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23626</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes I-PERI, a novel federated causal discovery algorithm that handles unknown client-level interventions, 2. Introduces the Φ-Markov Equivalence Class (Φ-CPDAG), a tighter equivalence class derived from structural differences across clients, 3. Provides theoretical guarantees on convergence and privacy-preserving properties (differential privacy).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373e2e9b4041d9efb71fbe2d1095901813d7f2551cdfe37d40d79c04d7aa235d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373e2e9b4041d9efb71fbe2d1095901813d7f2551cdfe37d40d79c04d7aa235d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses federated causal discovery where client data are subject to unknown, heterogeneous interventions, a common real-world scenario overlooked by prior work. It proposes the I-PERI algorithm, which first recovers the union CPDAG and then orients additional edges by exploiting intervention-induced structural differences across clients, resulting in a tighter equivalence class called the Φ-CPDAG. Theoretical and empirical results demonstrate the algorithm&#x27;s effectiveness and privacy guarantees.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Memorization in 3D Shape Generation: An Empirical Study</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D shape generation], [memorization, diffusion models, latent vector-set, evaluation framework, data leakage]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shu Pu, Boya Zeng, Kaichen Zhou, Mengyu Wang, Zhuang Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Princeton University, Harvard University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23628" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23628</a></p>
</li>
<li class="">
<p><strong>code:</strong> github.com/zlab-princeton/3d_mem</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a novel evaluation framework to quantify memorization in 3D generative models. 2. Applied the framework to benchmark and quantify memorization in existing 3D generation methods. 3. Conducted controlled experiments to identify how data and modeling factors (e.g., modality, guidance scale, augmentation) influence memorization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca470d9a610f6e7172776f63b7bfed02850fdb4a843786976b699c63c87febc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca470d9a610f6e7172776f63b7bfed02850fdb4a843786976b699c63c87febc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether 3D shape generative models memorize training data. The authors design an evaluation framework to measure memorization and conduct experiments with a latent vector-set diffusion model. They find memorization depends on data modality and diversity, peaks at moderate guidance, and can be reduced with longer latent vectors and rotation augmentation without harming quality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [multi-agent systems, hierarchical agents, bandit optimization, software engineering agents, SWE-bench]</p>
</li>
<li class="">
<p><strong>authors:</strong> Iris Xu, Guangtao Zeng, Zexue He, Charles Jin, Aldo Pareja, Dan Gutfreund, Chuang Gan, Zhang-Wei Hong</p>
</li>
<li class="">
<p><strong>institution:</strong> Massachusetts Institute of Technology, MIT-IBM Watson AI Lab, Stanford University, University of Massachusetts Amherst</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23631" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23631</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/iamxjy/BOAD-SWE-Agent" target="_blank" rel="noopener noreferrer" class="">https://github.com/iamxjy/BOAD-SWE-Agent</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulates the automatic discovery of effective hierarchical multi-agent systems for software engineering as a multi-armed bandit (MAB) problem, enabling efficient exploration under limited budgets. 2. Proposes the BOAD framework, which uses bandit optimization to coordinate specialized sub-agents (e.g., for localization, editing, validation) and attribute credit within a team. 3. Demonstrates that automatically discovered hierarchical agents outperform single-agent and manually designed multi-agent systems on challenging, out-of-distribution SWE benchmarks, including achieving second place on SWE-bench-Live with a 36B model.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67084d40c24e3869f47efa4b52c7ec1479016d613997e911c6730d7e7684254f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67084d40c24e3869f47efa4b52c7ec1479016d613997e911c6730d7e7684254f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the poor generalization of single-agent LLMs on long-horizon, out-of-distribution software engineering tasks by proposing a hierarchical multi-agent system. The core method, BOAD, automatically discovers effective agent hierarchies by formulating the search as a multi-armed bandit optimization problem. The results show that this approach significantly improves performance on SWE benchmarks, surpassing larger models like GPT-4 and Claude.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [educational ai], [generative AI, fine-tuning, randomized controlled trial, Socratic questioning, pedagogical instruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> LearnLM Team Google, Eedi, Albert Wang, Aliya Rysbek, Andrea Huber, Anjali Nambiar, Anna Kenolty, Ben Caulfield, Beth Lilley-Draper, Bibi Groot, Brian Veprek, Chelsea Burdett, Claire Willis, Craig Barton, Digory Smith, George Mu, Harriet Walters, Irina Jurenka, Iris Hulls, James Stalley-Moores, Jonathan Caton, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Liam McCafferty, Lucy Dalton, Markus Kunesch, Pauline Malubay, Rachel Kidson, Rich Wells, Sam Wheeler, Sara Wiltberger, Shakir Mohamed, Simon Woodhead, Vasco Brazão</p>
</li>
<li class="">
<p><strong>institution:</strong> Google, Eedi</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23633" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23633</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a rigorous, in-classroom exploratory RCT to evaluate the safety and efficacy of a generative AI tutor (LearnLM) in a real educational setting. 2. Demonstrated that a pedagogically fine-tuned AI model can reliably draft instructional content, with human tutors approving 76.4% of its messages with minimal or no edits. 3. Showed that AI-supported tutoring led to student performance at least equivalent to human-only tutoring, with a significant 5.5 percentage point improvement in solving novel problems on subsequent topics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b430e7c78534d660126208f226397ed95756e540bade56276301199a0114bc76_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b430e7c78534d660126208f226397ed95756e540bade56276301199a0114bc76_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether generative AI can scale effective one-to-one tutoring. The authors integrated LearnLM, a pedagogically fine-tuned AI model, into a math tutoring platform and conducted a randomized controlled trial where human tutors supervised its outputs. The results show that LearnLM was a reliable tutor, and students using it performed as well as or better than those with human tutors alone, suggesting AI can deliver effective, individualized learning support at scale.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Simultaneous Approximation of the Score Function and Its Derivatives by Deep Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [score function, approximation theory, deep neural networks, curse of dimensionality, Ornstein-Uhlenbeck process]</p>
</li>
<li class="">
<p><strong>authors:</strong> Konstantin Yakovlev, Nikita Puchkin</p>
</li>
<li class="">
<p><strong>institution:</strong> HSE University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23643" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23643</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Presents a theory for simultaneous approximation of the score function and its derivatives, extending beyond the first-order setting. 2. Derives approximation error bounds that are free from the curse of dimensionality. 3. Relaxes the common assumption of bounded data support, enabling handling of distributions with low-dimensional structure and unbounded support.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3404ba95328f082dc11706309fc0e30ca110b3f842a24921698b46a1065bfda6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3404ba95328f082dc11706309fc0e30ca110b3f842a24921698b46a1065bfda6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper develops a theoretical framework for using deep neural networks to approximate the score function and its derivatives simultaneously. The method relaxes the typical bounded support assumption and provides error bounds that avoid the curse of dimensionality. The main conclusion is that this theory enables more efficient handling of complex data distributions, which is crucial for improving the convergence of diffusion and ODE-based generative models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Random Controlled Differential Equations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time-series learning], [controlled differential equations, random features, signature kernels, reservoir computing, rough paths]</p>
</li>
<li class="">
<p><strong>authors:</strong> Francesco Piatti, Thomas Cass, William F. Turner</p>
</li>
<li class="">
<p><strong>institution:</strong> Imperial College London</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23670" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23670</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/FrancescoPiatti/RandomSigJax" target="_blank" rel="noopener noreferrer" class="">https://github.com/FrancescoPiatti/RandomSigJax</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A training-efficient framework combining random features with Controlled Differential Equations (CDEs) to create continuous-time reservoirs for time-series learning. 2. Two novel variants: Random Fourier CDEs (RF-CDEs) for kernel-free RBF approximation and Random Rough DEs (R-RDEs) for stable, efficient modeling of rough-path inputs. 3. Theoretical proof that these models induce the RBF-lifted and rough signature kernels in the infinite-width limit, unifying random-feature reservoirs, continuous-time architectures, and path-signature theory.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68926f52841c63ea8f7ccc6a25444c327f8f05d4f014a21ea65330e8b1d0af5a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68926f52841c63ea8f7ccc6a25444c327f8f05d4f014a21ea65330e8b1d0af5a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces a fast and scalable framework for time-series learning by using large, randomly parameterized Controlled Differential Equations (CDEs) as continuous-time reservoirs, with only a linear readout layer trained. Two specific model variants, RF-CDEs and R-RDEs, are proposed and shown to approximate powerful signature kernels. The methods achieve competitive or state-of-the-art performance on benchmarks, offering a practical alternative to explicit signature computations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] End-to-End Test-Time Training for Long Context</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [test-time training, meta-learning, sliding-window attention, continual learning, long-context modeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Arnuv Tandon, Karan Dalal, Xinhao Li, Daniel Koceja, Marcel Rød, Sam Buchanan, Xiaolong Wang, Jure Leskovec, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin, Jed McCaleb, Yejin Choi, Yu Sun</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University, UC Berkeley, UC San Diego, Astera Institute, NVIDIA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23675</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulates long-context modeling as a continual learning problem, enabling a standard sliding-window Transformer to learn at test time via next-token prediction, 2. Uses meta-learning during training to optimize the model&#x27;s initialization for efficient test-time learning, 3. Achieves scaling performance comparable to full-attention Transformers while maintaining constant inference latency like RNNs, resulting in significant speedups for long contexts.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a044c036c50d93caf06213291a7e5d53aecb58cc761c7738c635ddb4234a64d3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a044c036c50d93caf06213291a7e5d53aecb58cc761c7738c635ddb4234a64d3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an end-to-end test-time training method for long-context language modeling. It uses a standard sliding-window attention Transformer that learns continuously at test time via next-token prediction, with its initialization optimized via meta-learning during training. The method matches the scaling performance of full-attention Transformers while offering constant inference latency, making it 2.7x faster for 128K contexts.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Eliciting Behaviors in Multi-Turn Conversations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [llm evaluation], [behavior elicitation, multi-turn conversation, online methods, dynamic benchmarks, test case generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jing Huang, Shujian Zhang, Lun Wang, Andrew Hard, Rajiv Mathews, John Lambert</p>
</li>
<li class="">
<p><strong>institution:</strong> Google DeepMind, Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23701" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23701</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an analytical framework categorizing behavior elicitation methods into three families based on their interaction with the target model (prior knowledge, offline, online). 2. Introduces a generalized multi-turn formulation for online behavior elicitation methods, unifying single-turn and multi-turn settings. 3. Demonstrates the superior efficiency of online methods in discovering failure cases in multi-turn conversations compared to static benchmarks, advocating for a shift to dynamic evaluation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afe3305263c3b509bdd6e846cce6501d101c0ecedb788ef025ac0c9405a28103_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afe3305263c3b509bdd6e846cce6501d101c0ecedb788ef025ac0c9405a28103_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the problem of efficiently eliciting specific behaviors from large language models in multi-turn conversational settings. It introduces a framework for categorizing existing elicitation methods and proposes a generalized online method for multi-turn interactions. The key finding is that online methods can discover many more failure cases with few queries than static benchmarks, highlighting the need for dynamic evaluation approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] UniFi: Combining Irregularly Sampled CSI from Diverse Communication Packets and Frequency Bands for Wi-Fi Sensing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [Integrated Sensing and Communication (ISAC), Channel State Information (CSI), Attention Model, Irregular Sampling, Wi-Fi Sensing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gaofeng Dong, Kang Yang, Mani Srivastava</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Los Angeles (UCLA)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22143" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22143</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes UniFi, the first Wi-Fi ISAC framework that eliminates intrusive packet injection by exploiting irregularly sampled CSI from diverse communication packets across multiple bands. 2. Introduces a CSI sanitization pipeline to harmonize heterogeneous packets and a time-aware attention model that learns directly from non-uniform CSI sequences. 3. Presents CommCSI-HAR, the first dataset with irregularly sampled CSI from real-world dual-band communication traffic.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d5e6f4970d32933635a3174be0b8ba23c4b996e18b04bd75310812bd1071737_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d5e6f4970d32933635a3174be0b8ba23c4b996e18b04bd75310812bd1071737_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents UniFi, a Wi-Fi sensing framework that solves the problem of communication degradation caused by high-rate probing packets. It achieves this by directly using irregularly sampled Channel State Information from existing communication traffic across multiple frequency bands, combined with a novel sanitization pipeline and a time-aware attention model. Evaluations show that UniFi achieves state-of-the-art sensing accuracy while fully preserving communication throughput.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Training AI Co-Scientists Using Rubric Rewards</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [research plan generation, self-grading, rubric rewards]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shashwat Goel, Rishi Hazra, Dulhan Jayalath, Timon Willi, Parag Jain, William F. Shen, Ilias Leontiadis, Francesco Barbieri, Yoram Bachrach, Jonas Geiping, Chenxi Whitehouse</p>
</li>
<li class="">
<p><strong>institution:</strong> Meta Superintelligence Labs, ELLIS Institute Tübingen, Max Planck Institute for Intelligent Systems, University of Oxford, University of Cambridge</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23707" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23707</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A scalable method to automatically extract research goals and goal-specific grading rubrics from existing papers to build a training corpus. 2. A reinforcement learning framework with self-grading, where a frozen initial model acts as the grader using rubrics, enabling unsupervised improvement. 3. Demonstration of significant performance gains (12-22% relative improvement) and cross-domain generalization (e.g., to medical research) validated by human experts and frontier model juries.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/360be253dbca068ab35e1e7dcf794f5e43ae1d6fd7478850692d4a8ffc057d14_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/360be253dbca068ab35e1e7dcf794f5e43ae1d6fd7478850692d4a8ffc057d14_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of training language models to generate high-quality, constraint-following research plans. The proposed method uses reinforcement learning with self-grading, where rubrics automatically extracted from research papers provide reward signals. The approach shows significant improvements in plan quality and generalizes across domains like machine learning and medicine, validated by human expert preference.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] EEG-to-Voice Decoding of Spoken and Imagined speech Using Non-Invasive EEG</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [brain-computer interface], [EEG-to-Voice, mel-spectrogram, domain adaptation, automatic speech recognition, language model correction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hanbeot Park, Yunjeong Cho, Hunhee Kim</p>
</li>
<li class="">
<p><strong>institution:</strong> Pukyong National University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22146" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22146</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a direct, open-loop EEG-to-Voice reconstruction pipeline that generates mel-spectrograms from EEG without requiring dynamic time warping or explicit temporal alignment. 2. Applied transfer learning-based domain adaptation by pretraining a subject-specific generator on spoken speech and adapting it to imagined speech. 3. Integrated a minimal language model-based correction module to reduce ASR errors while preserving semantic structure, improving linguistic accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30fda90c01417e82377bfcdd82830a196b29afd74925e2009f1a1a1d02d4d2bd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30fda90c01417e82377bfcdd82830a196b29afd74925e2009f1a1a1d02d4d2bd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a direct EEG-to-Voice paradigm to reconstruct speech from non-invasive EEG signals for both spoken and imagined speech. The method uses a subject-specific generator to produce mel-spectrograms, followed by a vocoder and ASR, and employs domain adaptation from spoken to imagined speech. The results demonstrate the feasibility of open-loop speech reconstruction without explicit temporal alignment, with stable acoustic and linguistic performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Machine Learning-Based Basil Yield Prediction in IoT-Enabled Indoor Vertical Hydroponic Farms</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [indoor vertical hydroponics, IoT sensors, LSTM, DNN, Linear Regression]</p>
</li>
<li class="">
<p><strong>authors:</strong> Emna Bouzid, Noura Baccar, Kamran Iqbal, Yassine Chaouch, Fares Ben Youssef, Amine Regayeg, Sarra Toumi, Houda Nsir, Amina Mseddi, Leila Costelle</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Arkansas, Little Rock; Mediterranean Institute of Technology, South Mediterranean University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22151" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22151</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a prediction system for basil yield in IoT-enabled indoor vertical hydroponic farms using ML models. 2. Conducted a comparative performance analysis of Linear Regression, LSTM, and DNN models, evaluating accuracy, execution time, and RAM usage. 3. Identified DNN as offering an optimal balance between computational efficiency (speed/RAM) and high prediction accuracy (98%), making it suitable for real-world, resource-conscious deployment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db5f20edab19e2fc847a47e72931db82045d6fc345183bcd4be1eb31da0f25e0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db5f20edab19e2fc847a47e72931db82045d6fc345183bcd4be1eb31da0f25e0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses water scarcity in agriculture by proposing a machine learning-based system to predict basil yield in IoT-enabled indoor vertical hydroponic farms. It compares Linear Regression, LSTM, and DNN models using sensor data, finding that DNN provides a good trade-off between high accuracy (98%) and computational efficiency, making it suitable for practical deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Complete Anatomy of the Madden-Julian Oscillation Revealed by Artificial Intelligence</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [climate informatics], [similarity-preserving representation, latent space clustering, physics-coherent monitoring]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiao Zhou, Yuze Sun, Jie Wu, Xiaomeng Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, National Climate Centre, China Meteorological Administration</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22144" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22144</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced an &quot;AI-for-theory&quot; paradigm using a deep learning model (PhysAnchor-MJO-AE) to learn a latent representation where distance corresponds to physical-feature similarity for the MJO. 2. Objectively discovered the first complete six-phase anatomical map of the MJO life cycle, isolating two long-hypothesized transitional phases. 3. Constructed a new physics-coherent monitoring framework that decouples location and intensity, drastically reducing spurious propagation and convective misplacement compared to classical methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b09901ad992d27215117f20984faf17d508a192bf9010cc39bd8b74553ff543_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b09901ad992d27215117f20984faf17d508a192bf9010cc39bd8b74553ff543_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of objectively defining the life cycle of the Madden-Julian Oscillation (MJO) by introducing an &quot;AI-for-theory&quot; paradigm. It develops a deep learning model to learn a similarity-preserving latent representation, enabling clustering that reveals a complete six-phase anatomy of the MJO. The derived new monitoring framework significantly outperforms the classical index, demonstrating AI&#x27;s role as a discovery tool for complex systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Neural ocean forecasting from sparse satellite-derived observations: a case-study for SSH dynamics and altimetry data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [spatiotemporal forecasting], [4DVarNet, U-Net, sequence-to-sequence, sea level anomaly, neural forecast]</p>
</li>
<li class="">
<p><strong>authors:</strong> Daria Botvynko, Pierre Haslée, Lucile Gaultier, Bertrand Chapron, Clement de Boyer Montégut, Anass El Aouni, Julien Le Sommer, Ronan Fablet</p>
</li>
<li class="">
<p><strong>institution:</strong> IMT Atlantique, Ifremer, CNRS, Mercator Ocean International</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22152" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22152</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Adapts U-Net and 4DVarNet architectures for short-term forecasting of ocean dynamics from sparse satellite data. 2. Formulates the forecasting task as a sequence-to-sequence mapping using partial SLA snapshots to predict future full-field maps. 3. Demonstrates that the end-to-end neural framework outperforms an operational baseline, especially in high-variability regions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b91a25a42a5dc1b5739ae5689c604765502ed07062eadaa473e043ec5a0d288f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b91a25a42a5dc1b5739ae5689c604765502ed07062eadaa473e043ec5a0d288f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an end-to-end deep learning framework for 7-day forecasting of sea surface dynamics using sparse satellite altimetry data. It adapts U-Net and 4DVarNet models to perform sequence-to-sequence mapping from partial observations to full-field forecasts. The results show the neural model outperforms an operational ocean forecast product, demonstrating the feasibility of neural forecasting for operational oceanography under data-sparse conditions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Sampling with Shielded Langevin Monte Carlo Using Navigation Potentials</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [constrained sampling], [Langevin Monte Carlo, navigation functions, constrained sampling, non-convex support, adaptive temperature]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nicolas Zilberstein, Santiago Segarra, Luiz Chamon</p>
</li>
<li class="">
<p><strong>institution:</strong> Rice University, École Polytechnique (Institut Polytechnique de Paris)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22153" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22153</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces shielded Langevin Monte Carlo (LMC) for sampling from distributions with non-convex supports defined by convex sets with convex holes. 2. Incorporates a navigation function-inspired approach using a spatially adaptive temperature and repulsive drift to keep samples within feasible regions. 3. Demonstrates effectiveness through experiments on 2D Gaussian mixture and MIMO symbol detection, showing advantages over unconstrained methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b94714cf81960de10c530b580016be757aee25e63d9344efa57a771bb15c9bc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b94714cf81960de10c530b580016be757aee25e63d9344efa57a771bb15c9bc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes shielded Langevin Monte Carlo, a constrained sampling method that uses navigation potentials to sample from unnormalized target distributions over punctured (non-convex) supports. It modifies the Langevin diffusion with adaptive temperature and repulsive drift to avoid holes. Experiments on Gaussian mixtures and MIMO detection show it outperforms unconstrained sampling.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PaperNet: Efficient Temporal Convolutions and Channel Residual Attention for EEG Epilepsy Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [brain-computer interface (BCI)], [temporal convolution, residual attention, recurrent networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Shahriar Sajid, Abhijit Kumar Ghosh, Fariha Nusrat</p>
</li>
<li class="">
<p><strong>institution:</strong> Rajshahi University of Engineering &amp; Technology, BRAC University, University of Asia Pacific</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22172" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22172</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed PaperNet, a compact hybrid architecture combining temporal convolutions, channel-wise residual attention, and a lightweight bidirectional recurrent block for EEG classification. 2. Demonstrated high performance (macro-F1 0.96) on the BEED dataset with only ~0.6M parameters under a subject-independent protocol. 3. Provided interpretability through channel-wise attention weights to reveal electrode relevance and validated efficiency for deployment on resource-constrained systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42ceecec9d77194ecb3fce40cd41e394fe9fe473136aa06f3cec099aa5631ac0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42ceecec9d77194ecb3fce40cd41e394fe9fe473136aa06f3cec099aa5631ac0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces PaperNet, a lightweight deep learning model that integrates temporal convolutions, channel residual attention, and a bidirectional recurrent block for efficient EEG epilepsy detection. It achieves a macro-F1 score of 0.96 on the BEED dataset with only about 0.6 million parameters, showing balanced performance across classes. The results indicate that combining temporal filtering, channel reweighting, and recurrent context modeling can deliver strong classification without high computational cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On Fibonacci Ensembles: An Alternative Approach to Ensemble Learning Inspired by the Timeless Architecture of the Golden Ratio</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [ensemble learning], [Fibonacci weighting, Rao-Blackwell optimization, variance reduction, recursive ensemble, orthogonalization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ernest Fokoué</p>
</li>
<li class="">
<p><strong>institution:</strong> Rochester Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22284" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22284</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Fibonacci Ensembles, a novel ensemble learning framework using normalized Fibonacci weights optimized via orthogonalization and Rao-Blackwellization for systematic variance reduction. 2. Proposes a second-order recursive ensemble dynamic inspired by the Fibonacci sequence to enhance representational depth beyond classical boosting. 3. Develops a General Weighting Theory that unifies various ensemble methods (bagging, boosting, stacking, etc.) under a single mathematical framework as distributional operators.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3aac35c4c829c693f4e9607fedf9124e05465f66f615b3d6ff040b6b1d9348a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3aac35c4c829c693f4e9607fedf9124e05465f66f615b3d6ff040b6b1d9348a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Fibonacci Ensembles, a new ensemble learning method inspired by the Fibonacci sequence, which uses mathematically optimized Fibonacci weights and a recursive dynamic to reduce variance and improve model depth. Experimental results on one-dimensional regression show it can match or outperform uniform averaging and integrates effectively with orthogonal Rao-Blackwellization. The work suggests Fibonacci ensembles offer a natural and interpretable design within ensemble theory.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A review of NMF, PLSA, LBA, EMA, and LCA with a focus on the identifiability issue</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [matrix factorization], [nonnegative matrix factorization, identifiability, latent class analysis, probabilistic latent semantic analysis, end-member analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qianqian Qi, Peter G. M. van der Heijden</p>
</li>
<li class="">
<p><strong>institution:</strong> Hangzhou Dianzi University, Utrecht University, University of Southampton</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22282" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22282</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Highlights the similarities among five popular matrix factorization models (LBA, LCA, EMA, PLSA, NMF) that are often presented separately across different fields. 2. Proves a unified identifiability condition, showing that the solution uniqueness for LBA, EMA, LCA, and PLSA is equivalent to the uniqueness of the NMF solution. 3. Provides a brief review of algorithms for these models and illustrates their application with a social science time budget dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369a6018ccc708c5d8368b9a1290021cb1b1e9d92785fa247417c9aeb167a164_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369a6018ccc708c5d8368b9a1290021cb1b1e9d92785fa247417c9aeb167a164_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper reviews and unifies five nonnegative matrix factorization models (NMF, PLSA, LBA, EMA, LCA) from different disciplines, focusing on the identifiability issue. It proves that the uniqueness of solutions for LBA, EMA, LCA, and PLSA is equivalent to the uniqueness of the NMF solution. The work clarifies model similarities, reviews algorithms, and demonstrates application with a real-world dataset.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A General Weighting Theory for Ensemble Learning: Beyond Variance Reduction via Spectral and Geometric Structure</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [ensemble learning], [weighting theory, spectral complexity, approximation geometry, bias-variance decomposition, constrained quadratic program]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ernest Fokoué</p>
</li>
<li class="">
<p><strong>institution:</strong> Rochester Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22286" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22286</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Develops a general weighting theory for ensembles that moves beyond variance reduction, formalizing ensembles as linear operators with geometric and spectral constraints. 2. Derives a refined bias-variance-approximation decomposition showing how structured weights can outperform uniform averaging by reshaping approximation geometry and redistributing spectral complexity. 3. Provides a unified theoretical framework that subsumes classical averaging, stacking, and recent Fibonacci-based ensembles, showing optimal weights arise from constrained quadratic programs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a91a59418696aa33fbf07981fc6c57647153631a58f387a01a431ae28237ebb8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a91a59418696aa33fbf07981fc6c57647153631a58f387a01a431ae28237ebb8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a new theoretical framework for ensemble learning that explains its effectiveness beyond the traditional variance-reduction argument, particularly for stable base learners. The method formalizes ensembles as linear operators and shows how structured, non-uniform weighting can optimize performance by managing spectral complexity and approximation geometry. The main conclusion is that the principal role of aggregation for low-variance learners is the redistribution of spectral complexity, establishing a foundation for structure-driven ensemble design.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [transformer interpretability], [cross-entropy, gradient dynamics, attention mechanism, expectation-maximization, Bayesian inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra</p>
</li>
<li class="">
<p><strong>institution:</strong> Dream Sports, Columbia University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22473" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22473</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Derived an advantage-based routing law and a responsibility-weighted update rule for attention scores and values under cross-entropy training. 2. Showed that the coupled gradient dynamics induce a positive feedback loop that behaves like a two-timescale Expectation-Maximization (EM) procedure. 3. Demonstrated that these gradient dynamics sculpt the low-dimensional manifolds necessary for Bayesian inference, linking optimization to geometry and function.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed145ec9892ca4b4f8b91e5c78948ac6b81399c20bcafdccd4cb429e92da2aed_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed145ec9892ca4b4f8b91e5c78948ac6b81399c20bcafdccd4cb429e92da2aed_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes how cross-entropy training shapes the internal geometry of transformer attention heads. By deriving first-order gradient dynamics, it shows that attention score and value updates form a positive feedback loop analogous to an EM algorithm. The core conclusion is that this gradient flow sculpts the Bayesian manifolds that enable in-context probabilistic reasoning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Integrating Wide and Deep Neural Networks with Squeeze-and-Excitation Blocks for Multi-Target Property Prediction in Additively Manufactured Fiber Reinforced Composites</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-target regression], [squeeze-and-excitation blocks, wide-and-deep neural networks, Latin Hypercube Sampling, SHAP analysis, multi-input multi-target learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Behzad Parvaresh, Rahmat K. Adesunkanmi, Adel Alaeddini</p>
</li>
<li class="">
<p><strong>institution:</strong> Southern Methodist University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22397" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22397</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a data-efficient multi-input, multi-target learning approach integrating Latin Hypercube Sampling (LHS) with a squeeze-and-excitation wide and deep neural network (SE-WDNN) for predicting mechanical and manufacturing properties of additively manufactured fiber-reinforced composites. 2. Demonstrated superior performance of SE-WDNN over baseline models (e.g., feedforward neural networks, XGBoost) with the lowest overall test error (MAPE=12.33%) and statistically significant improvements for several target variables. 3. Provided interpretability through SHAP analysis, identifying reinforcement strategy as the major influence on mechanical performance, enabling guided parameter selection balancing mechanical behavior and manufacturing metrics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46886a511640f1fda32bb5caad6d94ff9e4208332562a065cf26f5a070434085_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46886a511640f1fda32bb5caad6d94ff9e4208332562a065cf26f5a070434085_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of predicting multiple properties in additively manufactured fiber-reinforced composites, where performance is sensitive to process and material parameters. The authors propose a sample-efficient method combining Latin Hypercube Sampling with a novel squeeze-and-excitation wide and deep neural network (SE-WDNN) to jointly predict mechanical and manufacturing properties. The model outperforms several baseline machine learning models, achieving the lowest test error, and SHAP analysis reveals that reinforcement strategy is the most influential factor, demonstrating the approach&#x27;s effectiveness for interpretable, multi-target prediction in this domain.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Uncertainty-Aware Flow Field Reconstruction Using SVGP Kolmogorov-Arnold Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [uncertainty quantification], [sparse variational Gaussian processes, Kolmogorov-Arnold networks, flow reconstruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Y. Sungtaek Ju</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Los Angeles</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22426" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22426</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel machine learning framework (SVGP-KAN) for uncertainty-aware flow field reconstruction, combining sparse variational Gaussian processes with Kolmogorov-Arnold network topology. 2. Enables principled epistemic uncertainty quantification, extending classical methods like Linear Stochastic Estimation (LSE) and Spectral Analysis Modal Methods (SAMM). 3. Provides a systematic evaluation demonstrating that the method achieves accuracy comparable to established techniques while offering well-calibrated uncertainty estimates that reliably indicate prediction quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78057a4908248b84f2c93949508c1a8ed187c23a15c17c9cb4d97da3da80d1a6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78057a4908248b84f2c93949508c1a8ed187c23a15c17c9cb4d97da3da80d1a6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a machine learning framework called SVGP-KAN for reconstructing time-resolved flow fields from sparse measurements. The method combines sparse variational Gaussian processes with Kolmogorov-Arnold networks to provide accurate reconstructions along with principled uncertainty estimates. The results show the framework achieves comparable accuracy to classical methods while offering reliable uncertainty quantification, which is valuable for experimental design in periodic flows.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Likelihood-Preserving Embeddings for Statistical Inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [statistical inference], [likelihood-preserving embeddings, likelihood-ratio distortion, Hinge Theorem, approximate sufficient statistics, surrogate MLE]</p>
</li>
<li class="">
<p><strong>authors:</strong> Deniz Akdemir</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly provided; inferred from email domain as independent researcher or unspecified institution.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22638" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22638</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Likelihood-Ratio Distortion metric and the Hinge Theorem, establishing it as the necessary and sufficient condition for preserving likelihood-based inference. 2. Proves an impossibility result for universal likelihood preservation, motivating model-class-specific guarantees. 3. Provides a constructive framework using neural networks as approximate sufficient statistics with explicit bounds linking training loss to inferential guarantees.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aff5ca31934e9408fcf0755ba3d0be2604e2c3a6692b27396516614ac67f2b0a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aff5ca31934e9408fcf0755ba3d0be2604e2c3a6692b27396516614ac67f2b0a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that modern machine learning embeddings often destroy the geometric structure needed for classical likelihood-based statistical inference. It proposes a theory of likelihood-preserving embeddings, centered on controlling the Likelihood-Ratio Distortion, and proves that this control is necessary and sufficient to preserve tests, Bayes factors, and MLEs. The main conclusion is that with this framework, neural network embeddings can be made compatible with classical inference workflows under specific, provable conditions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Machine learning models for predicting catastrophe bond coupons using climate data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [financial machine learning], [catastrophe bonds, climate indicators, extremely randomized trees, gradient boosting, risk pricing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Julia Kończal, Michał Balcerek, Krzysztof Burnecki</p>
</li>
<li class="">
<p><strong>institution:</strong> Wrocław University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22660" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22660</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel integration of large-scale climate indicators (e.g., ONI, NAO, SSTs) into the prediction of catastrophe bond coupon rates. 2. Systematically compares the performance of linear regression against advanced tree-based ensemble methods (RF, GBM, ERT, XGBoost) for this financial prediction task. 3. Demonstrates that including climate variables improves predictive accuracy across all models, with Extremely Randomized Trees achieving the best performance, quantifying the influence of climate variability on CAT bond pricing.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad97e42c90e08944209aa776783ca45b8fd5114532eaa6a8db48fee77ac0f8e2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad97e42c90e08944209aa776783ca45b8fd5114532eaa6a8db48fee77ac0f8e2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the use of machine learning models to predict catastrophe bond coupons by incorporating climate data. The authors combine traditional financial features with climate indicators and compare models like linear regression, random forest, and gradient boosting. The results show that climate variables improve prediction accuracy, with extremely randomized trees performing best, indicating that climate variability significantly impacts bond pricing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [medical audio classification], [Audio Spectrogram Transformer, Sharpness-Aware Minimization, ICBHI 2017, class imbalance, loss landscape]</p>
</li>
<li class="">
<p><strong>authors:</strong> Atakan Işık, Selin Vulga Işık, Ahmet Feridun Işık, Mahşuk Taylan</p>
</li>
<li class="">
<p><strong>institution:</strong> Başkent University, Gaziantep University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22564" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22564</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel framework integrating the Audio Spectrogram Transformer (AST) with Sharpness-Aware Minimization (SAM) for robust respiratory sound classification. 2. Implements a weighted sampling strategy to effectively handle the severe class imbalance present in medical datasets like ICBHI 2017. 3. Achieves state-of-the-art performance on the ICBHI 2017 dataset, with a particular focus on improving sensitivity for reliable clinical screening.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0861fcb0d50b762d97367d04dce9ca920f2ffca74cd5112df95b11652972a9b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0861fcb0d50b762d97367d04dce9ca920f2ffca74cd5112df95b11652972a9b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of respiratory sound classification, such as data scarcity and class imbalance, by enhancing the Audio Spectrogram Transformer with Sharpness-Aware Minimization (SAM) to find flatter minima for better generalization. The method also employs weighted sampling and achieves a new state-of-the-art score of 68.10% and a sensitivity of 68.31% on the ICBHI 2017 dataset, demonstrating improved robustness for clinical applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Nonlinear Dynamical Modeling of Human Intracranial Brain Activity with Flexible Inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [computational neuroscience], [DFINE, state-space models, intracranial EEG, neural forecasting, brain-computer interfaces]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kiarash Vaziri, Lucine L. Oganesian, HyeongChan Jo, Roberto M.C. Vera, Charles Y. Liu, Brian Lee, Maryam M. Shanechi</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Southern California</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22785" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22785</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Extended the DFINE framework, originally for intracortical recordings, to model multisite human intracranial EEG (iEEG) signals. 2. Demonstrated that DFINE significantly outperforms linear state-space models (LSSMs) and matches or exceeds the accuracy of GRU models in forecasting future neural activity. 3. Showed that DFINE handles missing observations more robustly than baseline models, highlighting its flexible inference capability for practical BCI applications.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b25c55145e75d86ed37939fcb3e028529910a649ed86c8bd2fdb2e8fb91c496_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b25c55145e75d86ed37939fcb3e028529910a649ed86c8bd2fdb2e8fb91c496_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper extends the DFINE framework to model nonlinear dynamics in human intracranial EEG (iEEG) data. DFINE combines neural networks with a linear state-space model backbone to enable accurate neural forecasting and robust handling of missing data. The results show DFINE outperforms linear models, matches or beats GRU performance, and is particularly effective in high gamma bands, making it a promising tool for next-generation brain-computer interfaces.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Causal-Policy Forest for End-to-End Policy Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [causal inference], [policy learning, causal forest, conditional average treatment effect (CATE), end-to-end learning, random forests]</p>
</li>
<li class="">
<p><strong>authors:</strong> Masahiro Kato</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Tokyo</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22846" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22846</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Establishes an equivalence between maximizing policy value and minimizing MSE for CATE under a specific regression model, providing a theoretical foundation. 2. Proposes the causal-policy forest, a novel end-to-end algorithm that modifies the widely-used causal forest for direct policy learning. 3. Integrates policy training steps more tightly than prior methods, avoiding separate nuisance parameter estimation and improving computational efficiency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2b4476f0b9f739e0d89d20df3fd68607b0631baf53a2956bd05175daf10d348_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2b4476f0b9f739e0d89d20df3fd68607b0631baf53a2956bd05175daf10d348_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an end-to-end algorithm for policy learning in causal inference, called the causal-policy forest. It modifies the causal forest method by leveraging a theoretical equivalence between policy value maximization and CATE estimation. The method unifies policy training steps, is computationally efficient, and bridges the gap between policy learning and CATE estimation in practice.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A first-order method for nonconvex-strongly-concave constrained minimax optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [optimization theory], [minimax optimization, augmented Lagrangian method, first-order method, operation complexity, nonconvex-strongly-concave]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhaosong Lu, Sanyou Mei</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Minnesota, The Hong Kong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22909" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22909</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a first-order augmented Lagrangian method for solving nonconvex-strongly-concave constrained minimax problems. 2. Develops a new first-order method to solve the resulting unconstrained minimax subproblems by leveraging the strong concavity structure. 3. Establishes an improved operation complexity of O(ε^{-3.5} log ε^{-1}) for finding an ε-KKT solution, which is a factor of ε^{-0.5} better than the previous best-known result.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8900c67d50fc273f41601bb4bbe900a26bc3aacdb1b31495591bc5c452297c76_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8900c67d50fc273f41601bb4bbe900a26bc3aacdb1b31495591bc5c452297c76_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of nonconvex-strongly-concave constrained minimax optimization. The authors propose a novel first-order augmented Lagrangian method, where the subproblems are solved by a specially designed first-order algorithm. The main result is that their method achieves an improved operation complexity of O(ε^{-3.5} log ε^{-1}) for finding an approximate KKT solution.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Deep Learning for the Multiple Optimal Stopping Problem</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning / optimal stopping], [multiple optimal stopping, dynamic programming principle, neural network approximation, high-dimensional problems, American basket options]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mathieu Laurière, Mehdi Talbi</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Center for Data Science; NYU-ECNU Institute of Mathematical Sciences at NYU Shanghai; NYU Shanghai; Laboratoire de Probabilités, Statistiques et Modélisation, Université Paris-Cité</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22961" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22961</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel deep learning framework combining the Dynamic Programming Principle with neural networks to solve high-dimensional multiple optimal stopping problems. 2. Provides theoretical error analysis for both the discrete-time problem (neural network training error) and continuous problems (discretization error). 3. Demonstrates the method&#x27;s efficiency and scalability through numerical experiments on high-dimensional American basket options and nonlinear utility maximization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ef52bc9da77a7dcb529e7b4b3c239b993aeb4f5fc7464ce8d4f6e1c468ee677_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ef52bc9da77a7dcb529e7b4b3c239b993aeb4f5fc7464ce8d4f6e1c468ee677_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a deep learning framework to solve the challenging multiple optimal stopping problem in high dimensions. The method combines the Dynamic Programming Principle with neural network approximation of the value function. Numerical experiments show it is an efficient and scalable solution for problems like pricing American basket options.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Risk-Averse Learning with Varying Risk Levels</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [online convex optimization], [Conditional Value-at-Risk (CVaR), dynamic regret, zeroth-order optimization, risk-level variation, first-order optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Siyi Wang, Zifan Wang, Karl H. Johansson</p>
</li>
<li class="">
<p><strong>institution:</strong> KTH Royal Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22986" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22986</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a novel risk-level variation metric to capture the dynamics of changing risk preferences in online optimization. 2. Develops risk-averse learning algorithms for both first-order and zeroth-order information settings under a limited sampling budget. 3. Provides dynamic regret bounds for the proposed algorithms, analyzing their performance in terms of function variation, risk-level variation, and sample count.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19b18e34aeb85bbf659ba6da5ac559cf4534627ec075752ac91c1f532838981e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19b18e34aeb85bbf659ba6da5ac559cf4534627ec075752ac91c1f532838981e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles risk-averse online optimization in non-stationary environments where both the cost functions and the desired risk level can change over time. The authors propose new algorithms for first-order and zeroth-order settings that use Conditional Value-at-Risk (CVaR) and analyze their dynamic regret, showing adaptability to changing conditions. Numerical experiments validate the effectiveness of the proposed methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [simulation-based inference], [Bayesian adaptive design, amortized inference, diffusion models, sequential experimental design, policy learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Niels Bracher, Lars Kühmichel, Desi R. Ivanova, Xavier Intes, Paul-Christian Bürkner, Stefan T. Radev</p>
</li>
<li class="">
<p><strong>institution:</strong> Rensselaer Polytechnic Institute, TU Dortmund University, University of Oxford</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22999" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22999</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces JADAI, a framework that jointly amortizes Bayesian adaptive design and inference by training a policy, history, and inference network end-to-end., 2. Proposes a generic loss function that aggregates incremental reductions in posterior error across sequential experiments., 3. Instantiates the inference network with diffusion-based posterior estimators to handle high-dimensional and multimodal posteriors at each experimental step.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a357d49f23f5fdffd9539d05904d86150c3c7775033f4847b56afac3375ad8e6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a357d49f23f5fdffd9539d05904d86150c3c7775033f4847b56afac3375ad8e6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces JADAI, a framework that jointly learns to optimize experimental designs and perform Bayesian inference in a sequential setting. It trains a policy network, a history network, and a diffusion-based inference network end-to-end to minimize posterior error. The method achieves superior or competitive performance on standard adaptive design benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Deep Learning for Art Market Valuation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-modal learning], [multi-modal deep learning, visual embeddings, Grad-CAM, hedonic regression, repeated-sales dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jianping Mei, Michael Moses, Jan Waelty, Yucheng Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> Cheung Kong Graduate School of Business (CKGSB), University of Zurich, Swiss Finance Institute, Art Market Consultancy</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23078" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23078</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces and benchmarks multi-modal deep learning models that fuse tabular data (artist, history) with visual embeddings from artwork images for art market valuation. 2. Demonstrates that visual content provides a distinct and economically significant predictive contribution, especially for fresh-to-market works lacking prior transaction history. 3. Provides interpretability analyses using Grad-CAM and embedding visualizations to show models attend to compositional and stylistic visual cues.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes using multi-modal deep learning to improve art market valuation by incorporating visual content from artwork images alongside traditional tabular data. It finds that while artist identity and history are most predictive overall, visual features provide crucial value for first-time sales where historical data is absent. The results show deep learning offers new insights for valuation, particularly in the most challenging scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Federated Learning With L0 Constraint Via Probabilistic Gates For Sparsity</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [L0 regularization, probabilistic gates, communication efficiency, model sparsity, federated stochastic gradient descent]</p>
</li>
<li class="">
<p><strong>authors:</strong> Krishna Harsha Kovelakuntla Huthasana, Alireza Olama, Andreas Lundell</p>
</li>
<li class="">
<p><strong>institution:</strong> Åbo Akademi University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23071" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23071</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel federated learning method that enforces an L0 constraint on model parameters using probabilistic gates and their continuous relaxation to achieve target sparsity. 2. Derives the L0 constrained stochastic minimization objective from an entropy maximization problem of the stochastic gates. 3. Demonstrates that the method can achieve high target sparsity (down to ρ=0.005) under data and client heterogeneity with minimal loss in statistical performance, outperforming magnitude pruning-based methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/259d06fa8e807f154e153891f40b44308796040886ed51e218b65ee4e67a8c9c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/259d06fa8e807f154e153891f40b44308796040886ed51e218b65ee4e67a8c9c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of poor generalizability and communication inefficiency in Federated Learning due to overly dense models. It proposes a method to enforce L0 sparsity constraints via probabilistic gates, deriving the objective from entropy maximization and implementing it with federated stochastic gradient descent. The method is shown to be communication-efficient and achieves high target sparsity with better statistical performance than pruning-based baselines on synthetic and real datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] QSAR-Guided Generative Framework for the Discovery of Synthetically Viable Odorants</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models], [variational autoencoder, QSAR, molecular generation, Fréchet ChemNet Distance, retrosynthesis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tim C. Pearce, Ahmed Ibrahim</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Leicester, University of Cambridge</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23080" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23080</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel generative AI framework combining a VAE with a QSAR model to design novel odorant molecules from limited training data., 2. Demonstration of effective latent space structuring for odor likelihood, enabling exploration of novel chemical scaffolds beyond simple derivatization., 3. Comprehensive validation showing generated molecules are syntactically valid, unique, thermodynamically stable, and synthetically viable.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2f78de871dd5cdb6d0663be15fa9ec9b8b77d5cae24344545c68979b5c02eaf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2f78de871dd5cdb6d0663be15fa9ec9b8b77d5cae24344545c68979b5c02eaf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a generative AI framework that combines a variational autoencoder (VAE) with a quantitative structure-activity relationship (QSAR) model to design novel odorant molecules. The method structures the VAE&#x27;s latent space based on odor probability, enabling the generation of valid, unique, and synthetically viable candidate molecules from a limited training set. The results show the model successfully explores novel chemical space, producing stable candidates with practical synthesis routes.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Why Machine Learning Models Systematically Underestimate Extreme Values II: How to Fix It with LatentNN</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [statistical machine learning], [attenuation bias, latent variable, neural networks, measurement error, joint likelihood]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuan-Sen Ting</p>
</li>
<li class="">
<p><strong>institution:</strong> The Ohio State University, Max-Planck-Institut für Astronomie</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23138" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23138</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/tingyuansen/LatentNN" target="_blank" rel="noopener noreferrer" class="">https://github.com/tingyuansen/LatentNN</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that neural networks suffer from attenuation bias, a systematic underestimation of extreme values due to input measurement errors. 2. Proposes LatentNN, a method that generalizes the latent variable solution from linear regression to neural networks by jointly optimizing network parameters and latent input values. 3. Validates the method&#x27;s effectiveness in reducing bias across various scenarios, including low signal-to-noise astronomical data, and defines its effective operational regime.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e1e58f31518d154aacb0668496d22b5dba87a170b019457f35d0fd0bfb1e03b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e1e58f31518d154aacb0668496d22b5dba87a170b019457f35d0fd0bfb1e03b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of attenuation bias, where neural networks systematically underestimate extreme values due to noisy input measurements. It introduces LatentNN, a method that treats true inputs as latent variables and jointly optimizes them with the network parameters by maximizing the joint data likelihood. The results show that LatentNN effectively reduces this bias, especially in the low signal-to-noise regimes common in astronomy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] An Inference-Based Architecture for Intent and Affordance Saturation in Decision-Making</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Kullback-Leibler divergence, decision paralysis, intent selection, affordance selection, hierarchical decision process]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wendyam Eric Lionel Ilboudo, Saori C Tanaka</p>
</li>
<li class="">
<p><strong>institution:</strong> Nara Institute of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23144" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23144</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a computational account of decision paralysis as convergence failure in a hierarchical decision process, separating intent and affordance selection. 2. Formalizes decision commitment as inference under a mixture of reverse-KL (mode-seeking) and forward-KL (mode-covering) objectives. 3. Demonstrates through simulations that forward-KL-biased inference reproduces key features of decision inertia and shutdown, framing autism as an extreme regime of this general decision-making continuum.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae0c1bd96570e940c6fa7d72cbfcec334b1a94d288ce774a384e5c60b2bfe206_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae0c1bd96570e940c6fa7d72cbfcec334b1a94d288ce774a384e5c60b2bfe206_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses decision paralysis by proposing a hierarchical inference-based model that separates intent and affordance selection. Commitment is formalized using a mixture of reverse-KL and forward-KL divergence objectives, where a bias towards forward-KL leads to slow, heavy-tailed response times and distinct failure modes. The model reproduces features of decision inertia and suggests autism represents an extreme case on this decision-making continuum.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Clipped Gradient Methods for Nonsmooth Convex Optimization under Heavy-Tailed Noise: A Refined Analysis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [stochastic optimization], [gradient clipping, heavy-tailed noise, nonsmooth convex optimization, convergence analysis, Freedman&#x27;s inequality]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zijian Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23178" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23178</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provided a refined analysis for Clipped SGD under heavy-tailed noise, achieving faster high-probability convergence rates that depend on a novel &quot;generalized effective dimension&quot; term. 2. Extended the refined analysis to convergence in expectation, obtaining new rates that break previously known lower bounds and proving their optimality by matching newly established lower bounds. 3. Established new lower bounds for both high-probability and in-expectation convergence, completing the theoretical landscape and confirming the optimality of the new in-expectation rates.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f14888c84379a942fea9e71d6d6a8df252ea20840f1e922a872a2fa3ee4897b2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f14888c84379a942fea9e71d6d6a8df252ea20840f1e922a872a2fa3ee4897b2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper refines the theoretical analysis of Clipped Stochastic Gradient Descent (Clipped SGD) for nonsmooth convex optimization under heavy-tailed gradient noise. By improving the use of Freedman&#x27;s inequality and providing finer bounds for clipping error, the authors derive faster high-probability convergence rates and new optimal in-expectation rates that surpass previous lower bounds. The work also establishes matching lower bounds, demonstrating the optimality of the proposed analysis for convergence in expectation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Persistent Homology via Finite Topological Spaces</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [topological data analysis], [persistent homology, finite topological spaces, posets, crosscut complexes, stability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Selçuk Kayacan</p>
</li>
<li class="">
<p><strong>institution:</strong> Bahçeşehir University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23348" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23348</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a functorial framework for persistent homology using filtrations of finite topological spaces and posets, bypassing the need for inclusion relations between simplicial complexes. 2. Demonstrates that standard simplifications at the poset level preserve persistent invariants. 3. Proves the stability of the resulting persistence diagrams under metric perturbations in a density-based instantiation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66f9ee5417a1e46f255a691f585eeb3b163bd4b47edaafbe115e358a54f4c884_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66f9ee5417a1e46f255a691f585eeb3b163bd4b47edaafbe115e358a54f4c884_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes a new functorial framework for persistent homology that starts from a finite metric space and constructs a filtration of finite topological spaces, then functorially maps these to posets and simplicial complexes via crosscut constructions. This approach decouples the metric from the homological analysis and does not require inclusion maps between complexes. The authors show that poset-level simplifications preserve persistent invariants and prove the stability of the resulting persistence diagrams.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Probabilistic Modelling is Sufficient for Causal Inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [causal inference], [probabilistic modelling, causal inference, do-operator, structural causal models, Bayesian networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bruno Mlodozeniec, David Krueger, Richard E. Turner</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Cambridge, Max Planck Institute for Intelligent Systems, MILA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23408" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23408</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that standard probabilistic modelling is sufficient for answering causal inference questions without requiring special causal frameworks. 2. Provides concrete examples showing how causal problems (interventional and counterfactual) can be solved by &quot;writing down the probability of everything&quot;. 3. Reinterprets established causal tools (e.g., do-operator, do-calculus) as &quot;syntactic sugar&quot; emerging from standard probabilistic modelling, clarifying their utility.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42538108be020614ac69c6d340cfeef4400c649311210c2f7081e50dc3d98ef5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42538108be020614ac69c6d340cfeef4400c649311210c2f7081e50dc3d98ef5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper argues that causal inference questions can be fully addressed using standard probabilistic modelling and inference, without needing specialized causal tools or notation. The core method is to &quot;write down the probability of everything&quot; to model and solve both interventional and counterfactual problems. The authors conclude that causal-specific frameworks are not fundamentally necessary but can be seen as convenient abstractions built upon probabilistic foundations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [alpha screening, large language models, reinforcement learning, factor investing, economic reasoning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zuoyou Jiang, Li Zhao, Rui Sun, Ruohan Sun, Zhongjian Li, Jing Li, Daxin Jiang, Zuo Bai, Cheng Hua</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, StepFun, FinStep</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23515" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23515</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/FinStep-AI/Alpha-R1" target="_blank" rel="noopener noreferrer" class="">https://github.com/FinStep-AI/Alpha-R1</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. 2. Introduces a method for LLMs to reason over factor logic and real-time news to evaluate alpha relevance under changing market conditions. 3. Demonstrates that the model consistently outperforms benchmarks and shows improved robustness to alpha decay across multiple asset pools.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d921912985e0858276fe1088914641df9c33c30f5de309733b2244c86c21e75e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d921912985e0858276fe1088914641df9c33c30f5de309733b2244c86c21e75e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of alpha decay in non-stationary financial markets by proposing Alpha-R1, a reasoning model trained with reinforcement learning. It uses a large language model to process factor logic and news, selectively activating factors based on contextual economic relevance. Empirical results show it outperforms benchmark strategies and is more robust to signal decay.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Adaptive Fusion Graph Network for 3D Strain Field Prediction in Solid Rocket Motor Grains</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [graph U-Net, adaptive pooling, feature fusion, strain field prediction, solid rocket motor]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiada Huang, Hao Ma, Zhibin Shen, Yizhou Qiao, Haiyang Li</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Defense Technology, Zhengzhou University of Aeronautics</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23443" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23443</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GrainGNet, an adaptive graph network with an adaptive pooling dynamic node selection mechanism to preserve key mechanical features in critical structural regions. 2. Utilizes feature fusion to transmit deep features and enhance the model&#x27;s representational capacity for 3D strain field prediction. 3. Demonstrates significant performance improvements, including a 62.8% reduction in mean squared error and a sevenfold training efficiency gain over a baseline graph U-Net, with particular accuracy in high-strain regions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a35459d2c7214afb29d74bb1f279ddd56dc621982ef3910ac226653dcfc465f1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a35459d2c7214afb29d74bb1f279ddd56dc621982ef3910ac226653dcfc465f1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes GrainGNet, an adaptive fusion graph network, to predict the 3D strain field in solid rocket motor grains, addressing the computational expense of traditional simulations. The model uses adaptive pooling and feature fusion to accurately capture high-strain regions. It achieves a 62.8% reduction in mean squared error and improved training efficiency compared to baseline methods, offering a high-fidelity approach for structural safety evaluation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A general framework for deep learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [statistical learning theory], [deep neural networks, Bernstein-type inequality, excess risk bound, minimax optimality, mixing processes]</p>
</li>
<li class="">
<p><strong>authors:</strong> William Kengne, Modou Wade</p>
</li>
<li class="">
<p><strong>institution:</strong> Université Jean Monnet, CY Cergy Paris Université</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23425" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23425</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a general theoretical framework for deep learning that unifies analysis for data satisfying a generalized Bernstein-type inequality, encompassing independent and various dependent (mixing) observations. 2. Introduces two novel estimators: a Non-Penalized Deep Neural Network (NPDNN) and a Sparse-Penalized Deep Neural Network (SPDNN) estimator. 3. Establishes minimax optimal (up to logarithmic factors) convergence rates for the expected excess risk of both estimators on Hölder smooth and composition function classes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4efb1121ddfae593e4dedab080501366fad55274c1b149910f149a1920a0b6e2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4efb1121ddfae593e4dedab080501366fad55274c1b149910f149a1920a0b6e2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper develops a general theoretical framework for analyzing deep neural network estimators in settings including nonparametric regression and classification. It proposes two estimators (NPDNN and SPDNN) and derives upper bounds for their expected excess risk for data satisfying a generalized Bernstein-type inequality, covering independent and various dependent data processes. The main conclusion is that both proposed estimators achieve minimax optimal convergence rates in many classical settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] From geometry to dynamics: Learning overdamped Langevin dynamics from sparse observations with geometric constraints</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [stochastic system identification], [overdamped Langevin dynamics, sparse observations, geometric constraints, stochastic control, path augmentation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dimitra Maoutsa</p>
</li>
<li class="">
<p><strong>institution:</strong> Technical University of Berlin</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23566" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23566</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A new framework that reconciles geometric and temporal perspectives by reformulating inference as a stochastic control problem. 2. A method using geometry-driven path augmentation, guided by the system&#x27;s invariant density, to reconstruct trajectories without assuming specific parametric models. 3. Demonstrating accurate recovery of stochastic dynamics from extremely undersampled data, outperforming existing methods in synthetic benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7a01c554437b016c9e156f0cb5f7047dd8973704b2623e3d218e1b9c76975c0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7a01c554437b016c9e156f0cb5f7047dd8973704b2623e3d218e1b9c76975c0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of learning stochastic dynamics from sparse temporal observations. It proposes a new framework that uses geometry-driven path augmentation within a stochastic control formulation to infer the underlying laws without parametric assumptions. The method successfully recovers overdamped Langevin dynamics from highly undersampled data, outperforming existing approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Nonstationarity-Complexity Tradeoff in Return Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [financial machine learning], [non-stationarity, model selection, adaptive window selection, return prediction, tournament procedure]</p>
</li>
<li class="">
<p><strong>authors:</strong> Agostino Capponi, Chengpiao Huang, J. Antonio Sidaoui, Kaizheng Wang, Jiacheng Zou</p>
</li>
<li class="">
<p><strong>institution:</strong> Columbia University, Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23596" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23596</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and formalizes the nonstationarity-complexity tradeoff in return prediction, where complex models reduce misspecification but require longer, more non-stationary training windows. 2. Proposes a novel model selection method that jointly optimizes model class and training window size using an adaptive tournament procedure evaluated on non-stationary validation data. 3. Provides theoretical analysis showing the method balances misspecification error, estimation variance, and non-stationarity, and demonstrates its empirical superiority with significant performance gains in out-of-sample prediction and trading strategy returns, especially during recessions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07919419a5b2bc9758efcadd74b91c11fcdca1529edf541c429c2694c06ddde0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07919419a5b2bc9758efcadd74b91c11fcdca1529edf541c429c2694c06ddde0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of predicting stock returns in non-stationary environments by identifying a tradeoff between model complexity and non-stationarity. It proposes a new model selection method that jointly chooses the model and its training window via an adaptive tournament. The method outperforms standard benchmarks, particularly during economic recessions, and generates higher cumulative returns for a trading strategy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Calibrated Multi-Level Quantile Forecasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [online learning, forecasting], [quantile forecasting, calibration, online learning, adversarial robustness, no-regret guarantee]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tiffany Ding, Isaac Gibbs, Ryan J. Tibshirani</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Berkeley</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23671" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23671</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Multi-Level Quantile Tracker (MultiQT), a lightweight online method that wraps any existing forecaster to guarantee multi-level quantile calibration against adversarial distribution shifts. 2. Provides theoretical guarantees including calibration and a no-regret property ensuring asymptotic performance is not worse than the base forecaster. 3. Ensures the corrected forecasts are properly ordered across different quantile levels.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/133e23e4bb898bd730a1ed52ef1e6f20bce16c1d2910d09105ad3e344368fe6f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/133e23e4bb898bd730a1ed52ef1e6f20bce16c1d2910d09105ad3e344368fe6f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of producing reliable multi-level quantile forecasts that are calibrated. It proposes MultiQT, an online wrapper method that guarantees calibration and proper ordering of forecasts even under adversarial conditions, without asymptotically worsening the base forecaster&#x27;s performance. Experiments show it significantly improves calibration in epidemic and energy forecasting tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Bellman Calibration for V-Learning in Offline Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Bellman calibration, off-policy evaluation, value iteration, doubly robust estimator, Markov decision process]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lars van der Laan, Nathan Kallus</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Washington, Netflix, Cornell University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23694" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23694</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Iterated Bellman Calibration, a model-agnostic, post-hoc procedure for calibrating value predictions in infinite-horizon MDPs. 2. Adapts classical calibration methods (histogram, isotonic) to the dynamic, counterfactual setting using a doubly robust pseudo-outcome for off-policy data. 3. Provides finite-sample guarantees for calibration and prediction without requiring Bellman completeness or realizability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9300a417035535b3fec586f3ad8f9e10dae8a084d794d1d413f42e5ba2b05d37_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9300a417035535b3fec586f3ad8f9e10dae8a084d794d1d413f42e5ba2b05d37_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Iterated Bellman Calibration, a post-hoc method to improve the accuracy of long-term value predictions in offline reinforcement learning. The method repeatedly regresses fitted Bellman targets onto a model&#x27;s predictions, adapting classical calibration techniques to handle off-policy data. The analysis shows the approach provides finite-sample guarantees for calibrated predictions under weak assumptions, without needing Bellman completeness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] INSIGHT: Spatially resolved survival modelling from routine histology crosslinked with molecular profiling reveals prognostic epithelial-immune axes in stage II/III colorectal cancer</strong></p>
<ul>
<li class=""><strong>tags:</strong> [cv], [computational pathology], [graph neural network, survival prediction, spatial transcriptomics, colorectal cancer, histology]</li>
<li class=""><strong>authors:</strong> Piotr Keller, Mark Eastwood, Zedong Hu, Aimée Selten, Ruqayya Awan, Gertjan Rasschaert, Sara Verbandt, Vlad Popovici, Hubert Piessevaux, Hayley T Morris, Petros Tsantoulis, Thomas Alexander McKee, André D&#x27;Hoore, Cédric Schraepen, Xavier Sagaert, Gert De Hertogh, Sabine Tejpar, Fayyaz Minhas</li>
<li class=""><strong>institution:</strong> KU Leuven, University of Oxford, University of Cambridge, University of Manchester, University of Bristol, University of Edinburgh, University of Glasgow, University of Sheffield, University of Southampton, University of Warwick, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strath</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22262" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22262</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e16c74c6d57e6b8f3cec00c4cc6267b2b66595c314bba0f371e27e2f86f25458_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e16c74c6d57e6b8f3cec00c4cc6267b2b66595c314bba0f371e27e2f86f25458_w640_q70.webp</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2026-01-01">2026-01-01<a href="#2026-01-01" class="hash-link" aria-label="Direct link to 2026-01-01" title="Direct link to 2026-01-01" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv260101] Network Traffic Analysis with Process Mining: The UPSIDE Case Study</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [network traffic analysis], [process mining, Petri nets, unsupervised characterization, network traffic classification, interpretability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Francesco Vitale, Paolo Palmiero, Massimiliano Rak, Nicola Mazzocca</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Naples Federico II, University of Campania Luigi Vanvitelli</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23718" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23718</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel process mining-based method for the unsupervised characterization of states from gaming network traffic data. 2. Encodes the identified network states into interpretable process models (Petri nets) for explainable analysis. 3. Demonstrates the method&#x27;s capability to classify network traffic to identify different video games being played, achieving good accuracy and model coherence.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aef2d73934ec0b931bbb0dbae5c9be217f30aa949e9142a6391ac35afe9d92a9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aef2d73934ec0b931bbb0dbae5c9be217f30aa949e9142a6391ac35afe9d92a9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a process mining method to analyze gaming network traffic for explainable modeling and classification. The method performs unsupervised state characterization, encodes states into interpretable Petri nets, and classifies traffic to identify specific games. Results on the UPSIDE case study show the approach can effectively model network behavior with high coherence and specificity while maintaining good classification accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] A Survey of AI Methods for Geometry Preparation and Mesh Generation in Engineering Simulation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [hpc], [computational geometry], [mesh generation, geometry preparation, CAD-to-mesh, machine learning, large language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Steven Owen, Nathan Brown, Nikos Chrisochoides, Rao Garimella, Xianfeng Gu, Franck Ledoux, Na Lei, Roshan Quadros, Navamita Ray, Nicolas Winovich, Yongjie Jessica Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Sandia National Laboratories, Old Dominion University, Los Alamos National Laboratory, New York University / Stony Brook University, CEA, Dalian University of Technology, Carnegie Mellon University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23719" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23719</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Surveys the application of AI/ML methods to automate and improve key steps in the CAD-to-mesh pipeline, such as part classification, mesh quality prediction, and defeaturing. 2. Reviews AI techniques for enhancing unstructured/block-structured meshing, volumetric parameterization, and parallel mesh generation. 3. Examines emerging tools like reinforcement learning and large language models for scripting automation in meshing workflows.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c3d8c73084ddd952c2e0c9fd20e1b7fe1d87bb02c884cf6327ca47e6ec442eb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c3d8c73084ddd952c2e0c9fd20e1b7fe1d87bb02c884cf6327ca47e6ec442eb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This survey paper reviews how artificial intelligence and machine learning are being applied to address bottlenecks in geometry preparation and mesh generation for engineering simulation. It explores a range of methods, from quality prediction to automation with large language models, concluding that AI serves as an assistive technology to extend traditional tools and highlights key challenges for future data-driven workflows.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Governing Cloud Data Pipelines with Agentic AI</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [policy-aware control, bounded AI agents, adaptive resource reconfiguration, schema reconciliation, automated failure recovery]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aswathnarayan Muthukrishnan Kirubakaran, Adithya Parthasarathy, Nitin Saksena, Ram Sekhar Bodala, Akshay Deshpande, Suhas Malempati, Shiva Carimireddy, Abhirup Mazumder</p>
</li>
<li class="">
<p><strong>institution:</strong> IEEE, Independent Researcher, Albertsons, Amtrak, Cato</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23737" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23737</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A production-oriented control plane architecture for policy-aware agentic management of cloud data pipelines. 2. Detailed agent workflows for monitoring, optimization, and schema management. 3. An evaluation demonstrating significant improvements in recovery time, operational cost, and reduction of manual intervention.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5297c02d67a11ab83e576eb218227051a192108c8ce2adf60d62e9fbdb7e355_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5297c02d67a11ab83e576eb218227051a192108c8ce2adf60d62e9fbdb7e355_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Agentic Cloud Data Engineering, a control architecture that integrates bounded AI agents to autonomously govern cloud data pipelines by analyzing telemetry and enforcing declarative policies. The method enables adaptive actions like resource reconfiguration and automated recovery. Experimental results show it reduces recovery time by up to 45%, lowers costs by ~25%, and cuts manual intervention by over 70% compared to static orchestration.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] A Comprehensive Study of Deep Learning Model Fixing Approaches</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [se], [software testing and debugging], [deep learning model fixing, empirical study, robustness, fairness, backward compatibility]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hanmo You, Zan Wang, Zishuo Dong, Luanqi Mo, Jianjun Zhao, Junjie Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Tianjin University, Kyushu University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23745" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23745</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a large-scale empirical study evaluating 16 state-of-the-art DL model fixing approaches across model-level, layer-level, and neuron-level categories. 2. Comprehensively assessed the approaches not only on fixing effectiveness but also on their impact on critical properties like robustness, fairness, and backward compatibility. 3. Provided key findings and insights for industry and academia, such as model-level approaches having superior fixing effectiveness and the trade-off between fixing performance and maintaining other model properties.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2da70efdc2c67bda59610794db06383b27cdc3ff0d3c9ab3e06c8ec8a0fac3a1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2da70efdc2c67bda59610794db06383b27cdc3ff0d3c9ab3e06c8ec8a0fac3a1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper conducts a comprehensive empirical study on 16 deep learning model fixing approaches. It evaluates their effectiveness and impact on properties like robustness and fairness, finding that model-level approaches are most effective but no single approach excels in all aspects. The study concludes that future research should focus on mitigating the side effects of model fixing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] A Review of Diffusion-based Simulation-Based Inference: Foundations and Applications in Non-Ideal Data Scenarios</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [simulation-based inference], [diffusion models, score matching, posterior sampling, misspecification, Schrodinger bridge]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haley Rosso, Talea Mayo</p>
</li>
<li class="">
<p><strong>institution:</strong> Emory University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23748" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23748</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a comprehensive review of diffusion models as a framework for simulation-based inference (SBI), connecting mathematical foundations to practical applications. 2. Analyzes the comparative advantages of diffusion-based SBI over methods like normalizing flows, particularly in addressing robustness under non-ideal data conditions. 3. Synthesizes specialized methods for handling challenges like model misspecification, unstructured observations, and missing data, highlighting inference-time prior adaptation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7b2b583a1f5706dab9104c469ca95816c45b56fef4f982f0a27368fe696141d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7b2b583a1f5706dab9104c469ca95816c45b56fef4f982f0a27368fe696141d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This review paper explores the use of diffusion models for simulation-based inference, a method for learning posterior distributions when likelihood functions are intractable. It examines how diffusion models, through conditional score matching, offer a flexible and robust framework, especially for scientific data with issues like model misspecification and missingness. The main conclusion is that diffusion-based SBI provides significant advantages in non-ideal data scenarios, though it introduces trade-offs such as iterative sampling costs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Coordinate Matrix Machine: A Human-level Concept Learning to Classify Very Similar Documents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [one-shot learning], [Coordinate Matrix Machine, structural intelligence, Green AI, lazy learning, glass-box model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amin Sadri, M Maruf Hossain</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated (email domains are personal: gmail.com)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23749" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23749</a></p>
</li>
<li class="">
<p><strong>code:</strong> GitHub Repository (URL not fully specified in provided text)</p>
</li>
<li class="">
<p><strong>contributions:</strong> Proposes the Coordinate Matrix Machine (CM^2) for one-shot document classification, Introduces a structural coordinate-based approach as an alternative to semantic vectorization, Designs a &quot;Green AI&quot; model optimized for CPU use with inherent explainability</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3acd9104b41dbc3c7f9d1597d31b940424a078e93beb2b2b21007c741209b006_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3acd9104b41dbc3c7f9d1597d31b940424a078e93beb2b2b21007c741209b006_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of human-level concept learning, where machines require many examples to learn a concept. It proposes the Coordinate Matrix Machine (CM^2), a purpose-built model that learns document structures to classify very similar documents using only one sample per class. The method is presented as a &quot;Green AI&quot; solution that outperforms traditional models while being computationally efficient and explainable.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Generalized Regularized Evidential Deep Learning Models: Theory and Comprehensive Evaluation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [uncertainty quantification], [Evidential Deep Learning, Subjective Logic, Activation Functions, Regularization, Learning Dynamics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Deep Shankar Pandey, Hyomin Choi, Qi Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> Rochester Institute of Technology, InterDigital</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23753" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23753</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Theoretically characterizes the activation-dependent &quot;learning-freeze&quot; behavior in evidential deep learning models, where gradients vanish in low-evidence regions. 2. Designs a general family of activation functions and corresponding evidential regularizers to enable consistent evidence updates across different activation regimes. 3. Empirically validates the proposed theory and method through extensive experiments on multiple benchmark classification, few-shot classification, and blind face restoration tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fae8a70028f83294a153c8fd9b9083e99f87bf5f04f2fb8d64ce2fbab74beb82_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fae8a70028f83294a153c8fd9b9083e99f87bf5f04f2fb8d64ce2fbab74beb82_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies and theoretically analyzes a &quot;learning-freeze&quot; problem in Evidential Deep Learning (EDL) models caused by specific activation functions. To solve this, the authors propose a generalized family of activation functions and regularizers. Extensive experiments show the proposed method improves learning dynamics and effectiveness across various tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Geometric Scaling of Bayesian Inference in LLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [interpretability], [Bayesian inference, geometric scaling, attention mechanism, value manifolds, predictive entropy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra</p>
</li>
<li class="">
<p><strong>institution:</strong> Dream Sports, Columbia University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23752" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23752</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that production-grade LLMs (Pythia, Phi-2, Llama-3, Mistral) preserve a geometric substrate (low-dimensional value manifolds) similar to that enabling exact Bayesian inference in small, controlled &quot;wind-tunnel&quot; models. 2. Shows that the dominant axis of last-layer value representations strongly correlates with predictive entropy, and domain-restricted prompts collapse the structure into the same low-dimensional manifolds. 3. Through targeted interventions on the entropy-aligned axis, reveals that this geometry is a privileged readout of uncertainty rather than a singular computational bottleneck for Bayesian-like behavior.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e11ae0519509ba1ae43d1087dfc56ed0f799df57a2ee4fe7c4a6a7f20eb11c3f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e11ae0519509ba1ae43d1087dfc56ed0f799df57a2ee4fe7c4a6a7f20eb11c3f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether the geometric structures that enable exact Bayesian inference in small, controlled transformer models persist in large-scale production language models. The authors find that models like Llama-3 and Mistral organize their value representations along an entropy-correlated axis, forming similar low-dimensional manifolds. They conclude that modern LLMs preserve this geometric substrate for approximate Bayesian updates, though it acts more as a readout mechanism than a sole computational bottleneck.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] HINTS: Extraction of Human Insights from Time-Series Without External Sources</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [self-supervised learning, opinion dynamics, attention mechanism, latent factor extraction, residual analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sheo Yon Jhin, Noseong Park</p>
</li>
<li class="">
<p><strong>institution:</strong> KAIST</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23755" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23755</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes HINTS, a novel self-supervised framework that extracts latent human factors (e.g., sentiment, influence) endogenously from time series residuals without requiring external data sources like news or social media. 2. Introduces the use of the Friedkin-Johnsen (FJ) opinion dynamics model as a structural inductive bias to model evolving social influence, memory, and bias patterns within the time series data. 3. Demonstrates that integrating the extracted human factors as an attention map into a state-of-the-art backbone model consistently improves forecasting accuracy across multiple datasets and provides interpretable insights aligned with real-world events.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a97eb229421de0a13cd23a1f8c66b8e7b1ac081ecf63d3e2a393fa375ac5f65_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a97eb229421de0a13cd23a1f8c66b8e7b1ac081ecf63d3e2a393fa375ac5f65_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high cost of using external data to model human factors in time series forecasting. It proposes HINTS, a self-supervised learning framework that extracts latent human insights directly from time series residuals using an opinion dynamics model as inductive bias. The method improves forecasting accuracy and provides interpretable factors aligned with real events, validated on nine real-world and benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Drift-Based Dataset Stability Benchmark</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [communication &amp; networking], [concept drift, dataset stability, traffic classification, benchmark, feature weights]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dominik Soukup, Richard Plný, Daniel Vašata, Tomáš Čejka</p>
</li>
<li class="">
<p><strong>institution:</strong> Czech Technical University in Prague, CESNET a.l.e.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23762" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23762</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel methodology for evaluating dataset stability based on concept drift detection and ML feature weights. 2. A benchmark workflow for comparing datasets and identifying their weak points. 3. A demonstration and initial benchmark of the framework on the CESNET-TLS-Year22 dataset, showing its use for dataset optimization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc4badd4590db607d7056063e44a30285afee3c4bd25f3f6b231fbc0932dd8de_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc4badd4590db607d7056063e44a30285afee3c4bd25f3f6b231fbc0932dd8de_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of model degradation in network traffic classification due to data/concept drift. It proposes a new framework that uses a concept drift detection method enhanced with ML feature weights to benchmark dataset stability. The method is demonstrated on a real-world TLS dataset, providing insights for dataset optimization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Exploring Cumulative Effects in Survival Data Using Deep Learning Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [survival analysis], [cumulative exposure, time-dependent data, deep learning, interpretability, Cox proportional hazards]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kang-Chung Yang, Shinsheng Yuan</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Statistical Science, Academia Sinica</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23764" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23764</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces CENNSurv, a novel deep learning approach for modeling the cumulative effects of time-dependent exposures on survival outcomes., 2. Addresses the scalability limitations of conventional spline-based methods by offering a more efficient approach suitable for large datasets., 3. Provides interpretable insights into cumulative exposure patterns, a feature often overlooked by existing neural network-based survival analysis methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c5684128b891f6cf0f0d31c2656cc3c3eda012e286475475646b128c847e73_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c5684128b891f6cf0f0d31c2656cc3c3eda012e286475475646b128c847e73_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces CENNSurv, a deep learning method designed to model the complex cumulative effects of time-varying exposures in survival data. It overcomes scalability issues of traditional methods and provides interpretable patterns. Evaluations on real-world datasets demonstrate its ability to uncover both long-term lagged associations and short-term critical shifts prior to an event.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Learning Coupled System Dynamics under Incomplete Physical Constraints and Missing Data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [physics-informed machine learning], [coupled systems, sparsity regularization, multitask learning, partial differential equations, mesh-free sampling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Esha Saha, Hao Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Alberta</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23761" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23761</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes MUSIC, a novel sparsity-induced multitask neural network framework for learning coupled system dynamics when physics constraints and data are incomplete and mutually exclusive. 2. Introduces a method that integrates partial physical constraints with data-driven learning using mesh-free sampling and sparsity regularization for model compression and efficiency. 3. Demonstrates the framework&#x27;s effectiveness on complex solutions (e.g., shock waves) under data-scarce and noisy conditions, outperforming non-sparse baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0e3d79b9b98f61c6847aeaba2b3e63b7fa984963e6f3f20b8ab17794c98a542_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0e3d79b9b98f61c6847aeaba2b3e63b7fa984963e6f3f20b8ab17794c98a542_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of modeling coupled dynamical systems where the governing equation is known for only one variable and data is available for another. It proposes MUSIC, a sparsity-regularized multitask neural network that integrates these partial constraints with data to recover full system solutions. The method shows improved accuracy and efficiency in learning complex solutions under scarce and noisy data conditions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Neural Optimal Design of Experiment for Inverse Problems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [inverse problems], [optimal experimental design, neural reconstruction, sparsity by design, single-level optimization, sensor placement]</p>
</li>
<li class="">
<p><strong>authors:</strong> John E. Darges, Babak Maboudi Afkham, Matthias Chung</p>
</li>
<li class="">
<p><strong>institution:</strong> Emory University, University of Oulu</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23763" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23763</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel single-level optimization framework (NODE) for optimal experimental design, avoiding the computational complexity of classical bilevel formulations. 2. Introduces direct optimization of continuous design variables (e.g., sensor locations) to enforce sparsity inherently, eliminating the need for l1 regularization and tuning. 3. Demonstrates the framework&#x27;s effectiveness on analytical, image-based (MNIST), and real-world (sparse-view CT) inverse problems, showing improved reconstruction accuracy over baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6af595776f7b085ae6680908c7f2c444ddf88c3d25cdb5c27eba1e5fe6a41cbe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6af595776f7b085ae6680908c7f2c444ddf88c3d25cdb5c27eba1e5fe6a41cbe_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces Neural Optimal Design of Experiments (NODE), a learning-based framework that jointly trains a neural reconstruction model and optimizes continuous experimental design variables (like sensor locations) in a single loop. This approach enforces sparsity directly, avoids complex bilevel optimization, and reduces computational cost. NODE is validated on multiple inverse problem benchmarks, consistently outperforming baseline methods in reconstruction accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] A Granular Grassmannian Clustering Framework via the Schubert Variety of Best Fit</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [subspace clustering], [Schubert Variety, Grassmann Manifold, Linde-Buzo-Grey (LBG), Subspace Clustering, Geometric Learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Karim Salta, Michael Kirby, Chris Peterson</p>
</li>
<li class="">
<p><strong>institution:</strong> Colorado State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23766" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23766</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the concept of a trainable prototype called a Schubert Variety of Best Fit (SVBF) for representing clusters of subspaces. 2. Integrates the SVBF prototype into the Linde-Buzo-Grey (LBG) clustering pipeline to create the SVBF-LBG algorithm. 3. Demonstrates improved cluster purity on synthetic, image, spectral, and video action data compared to methods using subspace means.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a73e8fbe969f250567092a96ad55fca5bd7133b8ca34f30feedb918976b1faf5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a73e8fbe969f250567092a96ad55fca5bd7133b8ca34f30feedb918976b1faf5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a new subspace clustering method that uses a geometric prototype called a Schubert Variety of Best Fit (SVBF) instead of a simple subspace mean. The SVBF is integrated into the Linde-Buzo-Grey algorithm, resulting in an SVBF-LBG framework that shows improved clustering performance on various data types while preserving mathematical structure for analysis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Enabling Physical AI at the Edge: Hardware-Accelerated Recovery of System Dynamics</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [FPGA acceleration, model recovery, hardware-software co-design, GRU, Neural ODE]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bin Xu, Ayan Banerjee, Sandeep Gupta</p>
</li>
<li class="">
<p><strong>institution:</strong> Arizona State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23767" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23767</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed MERINDA, a hardware-friendly FPGA-accelerated framework for model recovery that replaces Neural ODEs with a formulation combining GRU-based discretized dynamics, dense inverse-ODE layers, sparsity-driven dropout, and lightweight solvers. 2. Designed the framework for streaming parallelism, enabling critical computational kernels to be fully parallelized on FPGA hardware. 3. Demonstrated transformative efficiency gains over GPU implementations, including 114x lower energy, 28x smaller memory footprint, and 1.68x faster training while maintaining state-of-the-art accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e286e0d5a9672c23140099a1b5fd5c7ad7e56f56cb1c276735170b95ad29fd47_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e286e0d5a9672c23140099a1b5fd5c7ad7e56f56cb1c276735170b95ad29fd47_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of deploying physical AI for model recovery on resource-constrained edge devices, where state-of-the-art methods using Neural ODEs are inefficient. The authors propose MERINDA, an FPGA-accelerated framework that uses a hardware-friendly architecture to replace expensive Neural ODE components. The results show that MERINDA achieves substantial improvements in energy, memory, and speed over GPU implementations while matching model recovery accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Uncovering Discrimination Clusters: Quantifying and Explaining Systematic Fairness Violations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [algorithmic fairness], [discrimination clustering, individual fairness, hybrid verification, SMT solver, MILP solver]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ranit Debnath Akash, Ashish Kumar, Verya Monjezi, Ashutosh Trivedi, Gang, Saeid Tizpaz-Niari</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Illinois Chicago, University of Colorado Boulder, Pennsylvania State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23769" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23769</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced the concept of &quot;discrimination clustering&quot; as a generalization of individual fairness to uncover systematic bias patterns. 2. Proposed HyFair, a hybrid technique combining formal symbolic analysis (SMT/MILP) and randomized search for both certification and violation discovery. 3. Developed a novel explanation method to generate interpretable, decision-tree-style artifacts for inputs exhibiting high discrimination.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05da1223f6a1c9a93634f021d221ac5175d00dee272ded0b8782834941db9c55_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05da1223f6a1c9a93634f021d221ac5175d00dee272ded0b8782834941db9c55_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a limitation in individual fairness, which only detects isolated unfairness, and proposes the concept of &quot;discrimination clustering&quot; to uncover systematic bias patterns. It introduces HyFair, a hybrid method combining formal verification and randomized search to detect these clusters and generate explanations. Experiments show HyFair outperforms existing fairness verification and explanation methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Safety-Biased Policy Optimisation: Towards Hard-Constrained Reinforcement Learning via Trust Regions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [safe reinforcement learning], [constrained MDP, trust region policy optimization, natural policy gradient, safety gymnasium, hard constraints]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ankit Kanwar, Dominik Wagner, Luke Ong</p>
</li>
<li class="">
<p><strong>institution:</strong> Sony Corporation, Nanyang Technological University (NTU Singapore)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23770" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23770</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Safety-Biased Trust Region Policy Optimisation (SB-TRPO), a new algorithm for hard-constrained RL that adaptively biases policy updates towards safety while seeking reward improvement. 2. Introduces a trust-region update using a convex combination of the natural policy gradients of cost and reward to ensure a fixed fraction of optimal cost reduction per step. 3. Provides a theoretical guarantee of local progress towards safety and demonstrates superior balance of safety and task performance on Safety Gymnasium benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ea6c2a84f6cc7b4f3144847fc78a84736f7247b99f773ed23dd1861f2ff0760_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ea6c2a84f6cc7b4f3144847fc78a84736f7247b99f773ed23dd1861f2ff0760_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of reinforcement learning under hard safety constraints, where existing methods struggle to avoid violations without sacrificing reward. It proposes SB-TRPO, an algorithm that performs trust-region updates by combining reward and cost gradients to bias updates towards safety. Experiments show that SB-TRPO achieves a better balance of safety and task completion than state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] A Survey on Graph Neural Networks for Fraud Detection in Ride Hailing Platforms</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [anomaly detection], [Graph Neural Networks (GNNs), Fraud Detection, Class Imbalance, Fraudulent Camouflage, Ride-Hailing Platforms]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kanishka Hewageegana, Janani Harischandra, Nipuna Senanayake, Gihan Danansuriya, Kavindu Hapuarachchi, Pooja Illangarathne</p>
</li>
<li class="">
<p><strong>institution:</strong> Informatics Institute of Technology, Rajarata University, University of Sri Jayewardenepura</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23777" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23777</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a structured overview and comparison of existing Graph Neural Network (GNN) architectures and methodologies for fraud detection in ride-hailing platforms. 2. Highlights and analyzes key challenges in the domain, specifically class imbalance and fraudulent camouflage, within the ride-hailing ecosystem. 3. Identifies significant methodological progress and research gaps, calling for further exploration into real-world applicability and technical improvements.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65fc7f6ff0330c22a6dc35373a15256baac5eef984a9100cce967991d93a1e67_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65fc7f6ff0330c22a6dc35373a15256baac5eef984a9100cce967991d93a1e67_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This survey investigates the use of Graph Neural Networks (GNNs) for detecting fraud in ride-hailing platforms. It analyzes and compares various GNN models, focusing on their effectiveness in handling complex relational data and challenges like class imbalance. The paper concludes by identifying progress and gaps in the field, advocating for more research on real-world applications and technical enhancements.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Prompt-Induced Over-Generation as Denial-of-Service: A Black-Box Attack-Side Benchmark</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [adversarial attacks on llms], [denial-of-service, over-generation, black-box attack, evolutionary search, reinforcement learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Manu, Yi Guo, Jo Plested, Tim Lynar, Kanchana Thilakarathna, Nirhoshan Sivaroopan, Jack Yang, Wangli Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> Western Sydney University, University of New South Wales Canberra, The University of Sydney, University of Wollongong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23779" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23779</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a black-box, query-only benchmark for evaluating prompt-induced denial-of-service attacks on LLMs. 2. Proposes two novel prompt-only attackers: an evolutionary search method (EOGen) and a goal-conditioned reinforcement learning method (RL-GOAL). 3. Defines the Over-Generation Factor (OGF) as a key metric to quantify attack success and characterize model vulnerability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3002b15ce3957d88befc241c59f1be73a9e49f4e8ad4c345e9d472f11883059e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3002b15ce3957d88befc241c59f1be73a9e49f4e8ad4c345e9d472f11883059e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of denial-of-service attacks on large language models via prompt-induced over-generation. It proposes a standardized black-box benchmark and two automated attack methods, EOGen and RL-GOAL, to find adversarial prefixes that delay model termination. The results show that the RL-GOAL attacker is particularly effective at forcing models to generate excessively long outputs, highlighting a significant vulnerability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] FineFT: Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [ensemble reinforcement learning, selective update, variational autoencoder, high-frequency trading, risk management]</p>
</li>
<li class="">
<p><strong>authors:</strong> Molei Qin, Xinyu Cai, Yewen Li, Haochong Xia, Chuqiao Zong, Shuo Sun, Xinrun Wang, Bo An</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanyang Technological University, Singapore Management University, Hong Kong University of Science and Technology (Guangzhou)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23773" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23773</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A selective update mechanism for ensemble Q-learners using ensemble TD errors to stabilize training and improve convergence in high-leverage environments. 2. A risk-aware filtering and routing mechanism that uses VAEs to model market state dynamics and identify agent capability boundaries, enabling dynamic policy selection to mitigate risk. 3. A novel three-stage ensemble RL framework (FineFT) that integrates stable training and risk management, demonstrating superior profitability and over 40% risk reduction in crypto futures trading.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c036ba975592c2c3be9068e742ccd28ed5b9722ff62085fbcc37e9f3627fe370_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c036ba975592c2c3be9068e742ccd28ed5b9722ff62085fbcc37e9f3627fe370_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes FineFT, a three-stage ensemble reinforcement learning framework designed to address the challenges of high leverage and unseen market states in futures trading. The method uses selective updates for stable training and VAEs for risk-aware policy routing, achieving higher profitability and significantly lower risk compared to state-of-the-art baselines in high-frequency crypto futures experiments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] TabMixNN: A Unified Deep Learning Framework for Structural Mixed Effects Modeling on Tabular Data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [statistical machine learning], [mixed-effects models, deep learning, tabular data, interpretability, structural equation models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Deniz Akdemir</p>
</li>
<li class="">
<p><strong>institution:</strong> Not specified in provided content (email domain is gmail)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23787" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23787</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A modular three-stage deep learning framework (mixed-effects encoder, backbone architectures, outcome heads) that synthesizes classical mixed-effects modeling with neural networks for tabular data. 2. Key innovations including an R-style formula interface, support for DAG constraints for causal learning, SPDE kernels for spatial modeling, and comprehensive interpretability tools. 3. A unified interface that maintains the interpretability and theoretical grounding of classical models while leveraging the representational power of deep learning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1e0b6c8d4704d8d650a7573d4312c5032944f0b87d311b6d8a7f5554e250498_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1e0b6c8d4704d8d650a7573d4312c5032944f0b87d311b6d8a7f5554e250498_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces TabMixNN, a PyTorch-based deep learning framework designed to handle hierarchical tabular data by combining classical mixed-effects models with neural network architectures. It supports diverse tasks like regression and classification through a modular design and emphasizes interpretability. The framework provides a unified tool for complex data analysis while preserving the strengths of traditional statistical models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Zero-Trust Agentic Federated Learning for Secure IIoT Defense Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [Zero-Trust Architecture, SHAP-weighted aggregation, TPM-based attestation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Samaresh Kumar Singh, Joyjit Roy, Martin So</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researchers (based on provided affiliations: IEEE Senior Member in Texas, IEEE Member in Texas, Independent Researcher in Canada)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23809" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23809</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1) Proposed a hierarchical edge-fog-cloud zero-trust federated learning architecture for trusted agent participation. 2) Introduced a novel SHAP-weighted aggregation algorithm for explainable Byzantine detection in non-IID environments. 3) Integrated TPM-based cryptographic attestation and on-device adversarial training into a defense-in-depth framework.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/baa785fc442fcbc6a80214c4fdc6361e67a8e34e5a9bb6f5dd8fb34baf21bb68_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/baa785fc442fcbc6a80214c4fdc6361e67a8e34e5a9bb6f5dd8fb34baf21bb68_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses security vulnerabilities in Federated Learning for Industrial IoT by proposing ZTA-FL, a framework combining zero-trust agent authentication, explainable Byzantine-resilient aggregation, and on-device adversarial training. It demonstrates high detection accuracy and robustness against attacks on intrusion detection benchmarks while reducing communication overhead.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Improved Bounds for Private and Robust Alignment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [preference learning], [private alignment, robust alignment, uniform convergence, log loss, adversarial corruption]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenqian Weng, Yi He, Xingyu Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> Wayne State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23816" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23816</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Showed that standard private MLE-type log loss can achieve near-optimal rates for private alignment, contrary to prior belief. 2. Demonstrated that existing offline algorithms for joint privacy-and-corruption provide stronger guarantees than previously known, leading to improved bounds for corruption-only settings. 3. Presented the first set of theoretical results for private and robust online alignment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07d766123762454b64f45852c98c882b263a3fe86efdee6b0c39b70b0d888215_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07d766123762454b64f45852c98c882b263a3fe86efdee6b0c39b70b0d888215_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the theoretical alignment of language models under privacy constraints and adversarial corruption. It shows that a standard MLE-style log loss can achieve near-optimal rates for private alignment and provides improved bounds for joint private-and-robust settings, including the first online results, enabled by new uniform convergence guarantees.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] MS-SSM: A Multi-Scale State Space Model for Efficient Sequence Modeling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [sequence modeling], [state-space models, multi-scale dependencies, linear recurrences, long-range modeling, hierarchical tasks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mahdi Karami, Ali Behrouz, Peilin Zhong, Razvan Pascanu, Vahab Mirrokni</p>
</li>
<li class="">
<p><strong>institution:</strong> Google Research, Google DeepMind</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23824" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23824</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a multi-scale SSM framework that processes sequence dynamics across multiple resolutions with specialized state-space dynamics. 2. Proposed an input-dependent scale-mixer for dynamic information fusion across different resolutions. 3. Demonstrated consistent performance improvements over prior SSM-based models on benchmarks including Long Range Arena, hierarchical reasoning, time series classification, and image recognition.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8a72c3eef1e96a4a3f749cd5d5056117d6d9f67369231cfaf5ce87b8f9bd943_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8a72c3eef1e96a4a3f749cd5d5056117d6d9f67369231cfaf5ce87b8f9bd943_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes MS-SSM, a multi-scale state-space model that captures both fine-grained and coarse patterns in sequences to address the limited memory and multi-scale dependency issues of traditional SSMs. It introduces specialized dynamics per resolution and a dynamic scale-mixer, leading to enhanced memory efficiency and long-range modeling. Experiments show it outperforms prior SSM models across various tasks while maintaining computational efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Deep learning methods for inverse problems using connections between proximal operators and Hamilton-Jacobi equations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [inverse problems], [proximal operators, Hamilton-Jacobi equations, deep learning architectures, prior learning, nonconvex priors]</p>
</li>
<li class="">
<p><strong>authors:</strong> Oluwatosin Akande, Gabriel P. Langlois, Akwum Onwunta</p>
</li>
<li class="">
<p><strong>institution:</strong> Lehigh University, University of Illinois Urbana-Champaign</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23829" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23829</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes leveraging connections between proximal operators and Hamilton-Jacobi PDEs to develop novel deep learning architectures for learning priors in inverse problems. 2. Introduces a method to learn the prior directly without needing to invert it after training. 3. Demonstrates the efficiency of the proposed method through numerical results in high-dimensional settings.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0a12124651668ce14c5fbfa24bee53d6d2566efb05706aeb5d6359b3d85bff6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0a12124651668ce14c5fbfa24bee53d6d2566efb05706aeb5d6359b3d85bff6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of solving ill-posed inverse problems by developing a novel deep learning approach that leverages the connection between proximal operators and Hamilton-Jacobi PDEs to directly learn the prior. The method avoids the need to invert the prior after training and is shown to be efficient in high-dimensional scenarios through numerical experiments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Exploiting the Prior of Generative Time Series Imputation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series imputation], [Schrodinger Bridge, generative model, diffusion model, expert prior, compositional priors]</p>
</li>
<li class="">
<p><strong>authors:</strong> YuYang Miao, Chang Li, Zehua Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Imperial College London, University of Science and Technology of China, Tsinghua University, Shengshu AI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23832" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23832</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Bridge-TS, a novel generative time series imputation method that builds a data-to-data generation process. 2. Introduces the concept of an &quot;expert prior&quot;, using a pretrained transformer to provide a deterministic, informative starting point for the generation process. 3. Explores &quot;compositional priors&quot;, combining estimations from multiple pretrained models to further enhance the imputation process.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e54cafc350b1a81442b6b33a9d58319eeae7b34b2cbcfba860bed3f8138ed8af_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e54cafc350b1a81442b6b33a9d58319eeae7b34b2cbcfba860bed3f8138ed8af_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of limited accuracy in generative time series imputation methods, which stems from using uninformative priors like Gaussian noise. It proposes Bridge-TS, a method that uses informative priors from pretrained models to guide a data-to-data generation process. Experiments show Bridge-TS achieves state-of-the-art imputation accuracy on benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [adversarial robustness], [mechanistic interpretability, attention layers, adversarial examples, LLM evaluation, token substitution]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kaustubh Dhole</p>
</li>
<li class="">
<p><strong>institution:</strong> Emory University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23837" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23837</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel adversarial example generation method that exploits intermediate attention layer token distributions, contrasting with prompt-based or gradient-based attacks. 2. Introduces two specific attention-based generation techniques: attention-based token substitution and attention-based conditional generation. 3. Empirically demonstrates that such adversarial examples can degrade performance on an evaluation task (argument quality assessment) while maintaining semantic similarity, highlighting both the promise and limitations (e.g., grammatical degradation) of the approach.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85f39e8eb9d1e9534ca2c7e95f4e08380c10c2d3b58ec0a3a896f0767b75fdd8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85f39e8eb9d1e9534ca2c7e95f4e08380c10c2d3b58ec0a3a896f0767b75fdd8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a new method to generate adversarial examples by extracting token predictions from the intermediate attention layers of LLMs, leveraging their iterative refinement property. The approach is used to stress-test LLM-based evaluation pipelines, showing it can cause performance drops on an argument quality task while preserving semantics, though grammatical issues can arise. The findings illustrate the potential and current constraints of using internal model representations for adversarial testing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [adaptive prompting, context window, open-domain QA, retrieval-augmented generation, LLM ignorance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dingmin Wang, Ji Ma, Shankar Kumar</p>
</li>
<li class="">
<p><strong>institution:</strong> Google Research, University of Oxford</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23836" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23836</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an adaptive prompting strategy for RAG that splits retrieved information into smaller chunks for sequential processing, mitigating the noise from irrelevant information in long contexts. 2. Demonstrates experimentally that this strategy matches or outperforms standard prompting on open-domain QA datasets while using fewer tokens. 3. Identifies and analyzes a key failure mode where LLMs generate incorrect answers instead of declining when information is insufficient, highlighting a critical area for future research.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58d02669f63e2ba5d0171fc84f89e87cc22d595344fc20e61769b4288b009ef5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58d02669f63e2ba5d0171fc84f89e87cc22d595344fc20e61769b4288b009ef5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that longer context windows in Retrieval-Augmented Generation (RAG) introduce irrelevant information, degrading LLM performance. It proposes an adaptive prompting strategy that processes retrieved text in smaller, sequential chunks, achieving comparable accuracy with lower token usage. The study concludes that a major source of error is the LLM&#x27;s tendency to generate wrong answers rather than admit ignorance, pointing to the need for improved refusal capabilities.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Integrating Domain Knowledge for Financial QA: A Multi-Retriever RAG Approach with LLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [question answering], [Retrieval-Augmented Generation (RAG), SecBERT, financial numerical reasoning, multi-retriever, few-shot learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yukun Zhang, Stefan Elbl Droguett, Samyak Jain</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23848" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23848</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a multi-retriever RAG system that retrieves both external domain knowledge (e.g., financial definitions) and internal question contexts to improve financial QA. 2. Demonstrated that domain-specific training with the SecBERT encoder significantly boosts performance, allowing a neural symbolic model to surpass a strong baseline. 3. Showed that a prompt-based LLM generator achieves state-of-the-art performance with a &gt;7% improvement, highlighting the enhanced few-shot numerical reasoning of latest LLMs and the trade-off between hallucination and external knowledge gains.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de021f75daa09442db831a9d2d84064bf5381a47f4cb0fc792e2cfd3bfbd128b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de021f75daa09442db831a9d2d84064bf5381a47f4cb0fc792e2cfd3bfbd128b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses errors in financial numerical QA by proposing a multi-retriever RAG system that retrieves external financial knowledge and internal context. The best model, using domain-specific training and a prompt-based LLM, achieves state-of-the-art results, though still below human expert performance, and reveals a trade-off between hallucination and knowledge gains.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Flow Matching Neural Processes</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models], [neural processes, flow matching, ODE solver, conditional sampling, stochastic processes]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hussen Abu Hamad, Dan Rosenbaum</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Haifa</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23853" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23853</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new Neural Process model based on the flow matching generative modeling paradigm. 2. Enables sampling from conditional distributions using an ODE solver without auxiliary conditioning methods. 3. Provides a controllable trade-off between accuracy and computational cost via the ODE solver steps.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c876d5d150fc117d2b987aac4cc9a3bf9b34f3e670cf528e12b73266c4f2dab4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c876d5d150fc117d2b987aac4cc9a3bf9b34f3e670cf528e12b73266c4f2dab4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Flow Matching Neural Processes, a new model that integrates flow matching into the neural process framework for learning stochastic processes. The method allows for efficient and simple conditional sampling using an ODE solver. The authors demonstrate that their model outperforms previous state-of-the-art neural process methods on several benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [language model evaluation], [epistemic robustness, semantic compression, adversarial fabrication, two-system cognitive model, comprehension integrity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rahul Baxi</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher (affiliation inferred from email domain: alumni.cmu.edu, Carnegie Mellon University)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23850" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23850</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Drill-Down and Fabricate Test (DDFT), a novel protocol for measuring epistemic robustness in language models under stress from semantic compression and adversarial fabrication. 2. Proposes a two-system cognitive model (Semantic System and Epistemic Verifier) to explain and analyze LLM behavior. 3. Provides empirical evidence that epistemic robustness is orthogonal to model scale and architecture, identifying error detection as the critical bottleneck.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c853b521a9f8bb0173c42ffdb79e01c42db6066203b6aa0c5e838c2f6a78f18f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c853b521a9f8bb0173c42ffdb79e01c42db6066203b6aa0c5e838c2f6a78f18f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a gap in current language model evaluations, which fail to measure how robustly models maintain factual knowledge under stress. It introduces the Drill-Down and Fabricate Test (DDFT) to measure epistemic robustness by applying semantic compression and adversarial fabrication. The key finding is that epistemic robustness is not predicted by model size or architecture but by a model&#x27;s internal verification mechanisms, challenging assumptions about scaling and reliability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Trellis: Learning to Compress Key-Value Memory in Attention Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [KV cache compression, bounded memory, online gradient descent, recurrent compression, long-context]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mahdi Karami, Ali Behrouz, Praneeth Kacham, Vahab Mirrokni</p>
</li>
<li class="">
<p><strong>institution:</strong> Google Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23852" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23852</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Trellis, a novel Transformer architecture that replaces the standard unbounded KV cache with a fixed-size memory, enabling bounded memory usage. 2. Proposes a trainable two-pass recurrent compression mechanism that dynamically compresses new key-value pairs into the fixed memory at test time. 3. Leverages an online gradient descent procedure with a forget gate to recursively update the compressed memory, learning to retain important contextual information from long sequences.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/889c9150f49160406df5713209e4165753466bc69bac6ffa78b28c214caa5c7d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/889c9150f49160406df5713209e4165753466bc69bac6ffa78b28c214caa5c7d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the quadratic complexity and unbounded memory growth of the KV cache in Transformers by proposing Trellis, an architecture with a fixed-size memory and a learned recurrent compression mechanism. The method uses online gradient descent with a forget gate to dynamically update the compressed memory during inference. Experiments show Trellis outperforms baselines, with increasing gains on longer sequences, demonstrating its potential for efficient long-context modeling.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Yggdrasil: Bridging Dynamic Speculation and Static Runtime for Latency-Optimal Tree-Based LLM Decoding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [speculative decoding, tree-based decoding, latency optimization, compiler-friendly execution, static runtime]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yue Guan, Changming Yu, Shihan Fang, Weiming Hu, Zaifeng Pan, Zheng Wang, Zihan Liu, Yangjie Zhou, Yufei Ding, Minyi Guo, Jingwen Leng</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, Shanghai Qizhi Institute, University of California, San Diego</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23858" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23858</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces an equal-growth tree structure for speculative decoding that is compatible with static graph compilers. 2. Proposes a latency-aware optimization objective for draft selection, moving beyond simple average accepted length. 3. Designs a stage-based scheduling mechanism to reduce runtime overhead.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13cf851f76b89c86dd0ecc981628839d1ca0ba1772a6b9b893ec9677720b6be0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13cf851f76b89c86dd0ecc981628839d1ca0ba1772a6b9b893ec9677720b6be0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a performance mismatch between dynamic speculative decoding algorithms and static runtime systems. It proposes Yggdrasil, a co-designed system that uses a context-aware tree drafting structure and compiler-friendly execution to achieve latency-optimal speculative decoding. The system supports unmodified LLMs and achieves up to 3.98x speedup over state-of-the-art baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Security Without Detection: Economic Denial as a Primitive for Edge and IoT Defense</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [IoT Security], [Economic Denial Security, Stackelberg Game, Cost Asymmetry, Computational Puzzles]</p>
</li>
<li class="">
<p><strong>authors:</strong> Samaresh Kumar Singh, Joyjit Roy</p>
</li>
<li class="">
<p><strong>institution:</strong> IEEE (Inferred from author affiliations as IEEE members; specific institutional affiliation not provided in the excerpt)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23849" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23849</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed the Economic Denial Security (EDS) framework, a detection-independent defense that exploits the defender&#x27;s environmental control to impose economic infeasibility on attackers., 2. Formally modeled EDS as a Stackelberg game, deriving optimal parameters and proving that the composition of its four mechanisms yields superlinear (2.1x) cost amplification., 3. Demonstrated practical efficacy with a lightweight (&lt;12KB) implementation, validated on a 20-device IoT testbed and against IoT-23 malware, showing significant attack slowdown, cost asymmetry, and improved mitigation rates.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53020dd5fb969c1980dd7f764afe5f97440c1ef68620bdcd3383e97bf39600fc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53020dd5fb969c1980dd7f764afe5f97440c1ef68620bdcd3383e97bf39600fc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the failure of detection-based security in resource-constrained IoT/edge environments. It proposes Economic Denial Security (EDS), a framework that uses mechanisms like computational puzzles and bandwidth taxation to make attacks economically infeasible by amplifying attacker costs. The method is proven to be lightweight, effective in significantly slowing attacks and reducing success rates, and provides a detection-independent layer of defense.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Probing the Limits of Compressive Memory: A Study of Infini-Attention in Small-Scale Pretraining</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [Infini-attention, compressive memory, small language models (SLMs), long-context extrapolation, pretraining]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ruizhe Huang, Kexuan Zhang, Yihao Fang, Baifeng Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> Huawei Technologies Canada Co., Ltd.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23862" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23862</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/RRaAy-H/nanotron-infini" target="_blank" rel="noopener noreferrer" class="">https://github.com/RRaAy-H/nanotron-infini</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Replaced standard attention in a 300M-parameter LLaMA model with Infini-attention to study compressive memory behavior under short-sequence pretraining. 2. Analyzed the training dynamics of SLMs with Infini-attention, revealing characteristics like loss fluctuations, gradient volatility, and early-layer memory concentration. 3. Demonstrated that Infini-attention improves long-context extrapolation over a baseline model, with supervised fine-tuning further boosting performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af598a1dcd8b2b6a3dec75fe7434942b519517dea235cfb006c3cf73881444fd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af598a1dcd8b2b6a3dec75fe7434942b519517dea235cfb006c3cf73881444fd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether the Infini-attention mechanism, which combines local attention with compressive memory, can enhance long-context capabilities in Small Language Models (SLMs) during small-scale pretraining. The authors empirically study a 300M-parameter LLaMA model equipped with Infini-attention and find it improves long-context retrieval accuracy over a baseline, despite some degradation over very long sequences. The conclusion is that architectural memory like Infini-attention is beneficial for achieving robust long-context performance in SLMs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Lifelong Domain Adaptive 3D Human Pose Estimation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human pose estimation], [lifelong domain adaptation, catastrophic forgetting, generative adversarial network, pose-aware knowledge, temporal-aware knowledge]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qucheng Peng, Hongfei Xue, Pu Wang, Chen Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Central Florida, University of North Carolina at Charlotte</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23860" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23860</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel lifelong domain adaptation task for 3D Human Pose Estimation, addressing the challenge of non-stationary target pose datasets. 2. Introduces an innovative GAN framework with 3D pose generators, a 2D pose discriminator, and a 3D pose estimator to mitigate domain shifts and align poses. 3. Constructs a novel 3D pose generator paradigm that integrates pose-aware, temporal-aware, and domain-aware knowledge to enhance adaptation and alleviate catastrophic forgetting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbe905e18ac9835b4aae0dbb155169c447150fbd0e28ac454c7cc8a56bb7251e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbe905e18ac9835b4aae0dbb155169c447150fbd0e28ac454c7cc8a56bb7251e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a lifelong domain adaptation framework for 3D human pose estimation to handle non-stationary target data distributions. The method uses a novel GAN-based framework with a knowledge-integrated 3D pose generator to adapt to new domains while preventing catastrophic forgetting of previous ones. Experiments show the approach achieves superior performance on diverse domain adaptive 3D HPE datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Max-Entropy Reinforcement Learning with Flow Matching and A Case Study on LQR</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [max-entropy reinforcement learning, flow-based policy, flow matching, soft actor-critic, linear quadratic regulator]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuyang Zhang, Yang Hu, Bo Dai, Na Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Harvard University, Georgia Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23870" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23870</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a variant of the Soft Actor-Critic (SAC) algorithm that uses flow-based models to parameterize the policy, enhancing expressiveness. 2. Introduces an online variant of flow matching called Importance Sampling Flow Matching (ISFM) for policy updates using samples from a user-specified distribution instead of the unknown target. 3. Provides a theoretical analysis of ISFM, characterizing how the choice of sampling distribution impacts learning efficiency, and validates the method with a case study on max-entropy Linear Quadratic Regulator (LQR) problems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d43d8f718c30e1679c2274346eea229b95864ab6207cedce0e09dc784832028_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d43d8f718c30e1679c2274346eea229b95864ab6207cedce0e09dc784832028_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of simple policy approximations in max-entropy reinforcement learning by proposing a new SAC variant that uses expressive flow-based policies. The method employs a novel online flow matching technique (ISFM) for efficient policy updates and demonstrates its effectiveness by learning the optimal action distribution in max-entropy LQR problems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [Transformer, Mamba, Knowledge Distillation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tin Hoang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Surrey</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23898" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23898</a></p>
</li>
<li class="">
<p><strong>code:</strong> github.com/Tin-Hoang/solar-timeseries-forecasting</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a comprehensive benchmark of ten deep learning architectures for short-term solar irradiance forecasting, identifying the Transformer as the best-performing model. 2. Used SHAP analysis to reveal and contrast the distinct temporal reasoning patterns of different architectures (e.g., Transformer&#x27;s recency bias vs. Mamba&#x27;s periodic dependency). 3. Demonstrated that Knowledge Distillation can effectively compress the high-performance Transformer model, reducing its size by 23.5% while improving accuracy for edge deployment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8685dbd2386bb45fd799c168962048f6a243c0b8c836a5be5978ff64d0c2e184_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8685dbd2386bb45fd799c168962048f6a243c0b8c836a5be5978ff64d0c2e184_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper benchmarks ten deep learning models for 1-hour ahead solar irradiance forecasting in Ho Chi Minh City. The Transformer model achieved the highest accuracy, and the study used explainable AI to analyze model behavior and successfully compressed the model via Knowledge Distillation for efficient edge deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Rethinking Dense Linear Transformations: Stagewise Pairwise Mixing (SPM) for Near-Linear Training in Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [Stagewise Pairwise Mixers, dense linear layers, near-linear complexity, compositional inductive bias, drop-in replacement]</p>
</li>
<li class="">
<p><strong>authors:</strong> Peter Farag</p>
</li>
<li class="">
<p><strong>institution:</strong> SP Cloud &amp; Technologies Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23905" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23905</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Stagewise Pairwise Mixers (SPM), a structured linear operator that replaces dense matrices with a composition of sparse pairwise-mixing stages, achieving near-linear time and parameter complexity. 2. Derives complete forward and backward expressions for two parameterizations: an orthogonal norm-preserving rotation-based variant and a fully general 2×2 mixing variant. 3. Demonstrates that SPM reduces wall-clock cost and improves accuracy on structured learning problems while maintaining competitive performance on real-world benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6296322c9e11128e2315b979afc9e75bc26d003cffef0deb0d439140a648751a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6296322c9e11128e2315b979afc9e75bc26d003cffef0deb0d439140a648751a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high computational and parametric cost of dense linear layers in neural networks by introducing Stagewise Pairwise Mixers (SPM), a structured operator that composes sparse pairwise-mixing stages to achieve near-linear complexity. SPM serves as a drop-in replacement for dense layers, offering exact closed-form computations and an explicit compositional inductive bias. Proof-of-concept experiments show substantial reductions in training cost and improved generalization on structured tasks while retaining competitive performance on benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Constraint Breeds Generalization: Temporal Dynamics as an Inductive Bias</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [neuromorphic computing], [temporal inductive bias, dissipative dynamics, spiking neural networks, generalization, phase-space analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xia Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Technische Universität München (Georg Nemetschek Institute, Munich Data Science Institute)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23916" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23916</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes that physical constraints (like metabolic budgets) act not as limitations but as a temporal inductive bias that promotes generalization in neural systems. 2. Reveals through phase-space analysis that proper dissipative dynamics compress the solution space and align with spectral bias to abstract invariant features, unlike expansive dynamics. 3. Empirically demonstrates across multiple tasks (classification, reconstruction, RL) that a critical &quot;transition&quot; regime of dynamical constraints maximizes generalization capability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b28592acb015fa66de83fc46eda200fa0018a58a31fbfacdf9dd0fced988bac9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b28592acb015fa66de83fc46eda200fa0018a58a31fbfacdf9dd0fced988bac9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper argues that physical constraints, often seen as limitations, can serve as a beneficial temporal inductive bias for generalization in neural networks. It analyzes signal propagation to show that dissipative dynamics compress phase space and abstract features, a principle implemented using Spiking Neural Networks. Experiments across various tasks confirm that properly constrained temporal dynamics maximize generalization, suggesting a new direction for robust AI development.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Interactive Machine Learning: From Theory to Scale</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [interactive machine learning], [active learning, contextual bandits, model selection, sequential decision making, partial feedback]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yinglun Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Wisconsin–Madison</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23924" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23924</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed computationally efficient active learning algorithms that achieve exponential label savings without requiring low-noise assumptions., 2. Introduced the first efficient, general-purpose contextual bandit algorithms whose performance guarantees are independent of the action space size., 3. Provided the first tight characterizations of the fundamental cost of model selection in sequential decision-making settings.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d3c8fc2dd2145126e7321fa8de265f0c2652dbd2dc7df8c387585634498e335_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d3c8fc2dd2145126e7321fa8de265f0c2652dbd2dc7df8c387585634498e335_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This dissertation addresses the high cost of data labeling and trial-and-error in machine learning by developing new algorithms for interactive learning. It proposes statistically optimal and computationally efficient methods for active learning, contextual bandits with large action spaces, and model selection under partial feedback. The work advances the theoretical foundations of interactive learning and provides guidance for its deployment in large-scale, real-world applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Statistical Guarantees in the Search for Less Discriminatory Algorithms</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [algorithmic fairness], [model multiplicity, optimal stopping, disparate impact, statistical guarantees, less discriminatory algorithms]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chris Hays, Ben Laufer, Solon Barocas, Manish Raghavan</p>
</li>
<li class="">
<p><strong>institution:</strong> MIT, Cornell University, Microsoft Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23943" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23943</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formalizes the search for less discriminatory algorithms (LDAs) as an optimal stopping problem, providing a statistical framework to define a &quot;good-faith effort&quot; in model development. 2. Proposes an adaptive stopping algorithm that yields a high-probability upper bound on the potential gains from continued search, allowing developers to certify the sufficiency of their exploration. 3. Provides a flexible framework where developers can incorporate stronger assumptions about the model distribution to obtain correspondingly stronger statistical bounds, validated on real-world datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/323b958070f176d29545b148d49cdc3b6141db385ed752500b67c6d9556a8c78_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/323b958070f176d29545b148d49cdc3b6141db385ed752500b67c6d9556a8c78_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of how firms can demonstrate a good-faith effort to find less discriminatory algorithms. It proposes an adaptive stopping algorithm based on optimal stopping theory, which provides statistical guarantees on the potential benefits of further search. The method allows developers to certify that their search for fairer models was sufficient, as validated on credit, employment, and housing datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] DivQAT: Enhancing Robustness of Quantized Convolutional Neural Networks against Model Extraction Attacks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [Quantization Aware Training, Model Extraction Attack, Quantized Convolutional Neural Networks, Edge Device, Robustness]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kacem Khaled, Felipe Gohring de Magalhães, Gabriela Nicolescu</p>
</li>
<li class="">
<p><strong>institution:</strong> Polytechnique Montreal</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23948" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23948</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DivQAT, a novel algorithm to train quantized CNNs that integrates a defense mechanism directly into the Quantization Aware Training process to enhance robustness against model extraction attacks. 2. Demonstrates that the proposed technique effectively defends against model extraction attacks without compromising the model&#x27;s accuracy, as validated on benchmark vision datasets. 3. Shows that combining the proposed quantization technique with other defense mechanisms improves their effectiveness compared to using traditional QAT.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c08371f06267ec2fffb37ff15727bbbcfaf1e05de4f88d1541883445740a65d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c08371f06267ec2fffb37ff15727bbbcfaf1e05de4f88d1541883445740a65d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the vulnerability of quantized Convolutional Neural Networks to model extraction attacks. It proposes DivQAT, a novel training algorithm that modifies the quantization process to integrate a defense mechanism directly, enhancing model robustness. The method is shown to be effective against attacks without harming accuracy and can improve other defenses when combined.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Improved Balanced Classification with Theoretically Grounded Loss Functions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [imbalanced classification], [balanced loss, surrogate loss, H-consistency, logit-adjusted, class-weighted]</p>
</li>
<li class="">
<p><strong>authors:</strong> Corinna Cortes, Mehryar Mohri, Yutao Zhong</p>
</li>
<li class="">
<p><strong>institution:</strong> Google Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23947" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23947</a></p>
</li>
<li class="">
<p><strong>contributions:</strong>  1. Introduces two new surrogate loss families for balanced classification: Generalized Logit-Adjusted (GLA) and Generalized Class-Aware weighted (GCA) losses. 2. Provides a comprehensive theoretical analysis showing GCA losses have stronger H-consistency guarantees (scaling as 1/√p_min) than GLA losses (scaling as 1/p_min) in imbalanced settings. 3. Empirically demonstrates that both GCA and GLA losses outperform standard class-weighted and Logit-Adjusted losses, with GLA slightly better on common benchmarks and GCA better in highly imbalanced scenarios.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03edb44855299a7da70383f03cc9676cdeb3330caf1fa4f5212538cf5a33b248_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03edb44855299a7da70383f03cc9676cdeb3330caf1fa4f5212538cf5a33b248_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses class imbalance in multi-class classification by proposing two theoretically grounded surrogate loss families: Generalized Logit-Adjusted (GLA) and Generalized Class-Aware weighted (GCA) losses. The theoretical analysis shows GCA offers stronger consistency guarantees, especially for imbalanced data. Experiments confirm both losses outperform existing methods, with each excelling in different imbalance settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Physics-informed Graph Neural Networks for Operational Flood Modeling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [physics-informed neural networks, graph neural networks, flood modeling, curriculum learning, message-passing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Carlo Malapad Acosta, Herath Mudiyanselage Viraj Vidura Herath, Jia Yu Lim, Abhishek Saha, Sanka Rasnayaka, Lucy Marshall</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Singapore, The University of Sydney, Delft University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23964" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23964</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/acostacos/dual_flood_gnn" target="_blank" rel="noopener noreferrer" class="">https://github.com/acostacos/dual_flood_gnn</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DUALFloodGNN, a novel GNN architecture that embeds physical constraints at both global and local scales through explicit loss terms. 2. Introduces a model that jointly predicts water volume at nodes and flow along edges using a shared message-passing framework. 3. Enhances autoregressive inference performance via multi-step loss training with dynamic curriculum learning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c6bbddb2010c8550ce3ea4a09f24c04abc0993a7ee2a722f960fc0852c4f049_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c6bbddb2010c8550ce3ea4a09f24c04abc0993a7ee2a722f960fc0852c4f049_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high computational cost of physics-based flood models by proposing DUALFloodGNN, a physics-informed graph neural network architecture. The model incorporates physical constraints into its loss function and uses a multi-step training strategy with curriculum learning. It achieves improved prediction accuracy for hydrologic variables while maintaining high computational efficiency compared to existing GNN models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [hypergraph memory, multi-step reasoning, global sense-making, long-context modeling, retrieval-augmented generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chulun Zhou, Chunkang Zhang, Guoxin Yu, Fandong Meng, Jie Zhou, Wai Lam, Mo Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> The Chinese University of Hong Kong, WeChat AI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23959" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23959</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Encyclomen/HGMem" target="_blank" rel="noopener noreferrer" class="">https://github.com/Encyclomen/HGMem</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes HGMem, a novel hypergraph-based memory mechanism that models memory as a dynamic structure with higher-order interactions, moving beyond passive storage. 2. Addresses the limitation of existing multi-step RAG memory in capturing complex relational structures and providing strong guidance for subsequent reasoning steps. 3. Demonstrates through extensive experiments that the method consistently improves multi-step RAG performance and substantially outperforms strong baselines on global sense-making tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/259f66b1c3afc451216d5a69cb56a5a72ee4244c5fa02941603de2fbd4afc261_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/259f66b1c3afc451216d5a69cb56a5a72ee4244c5fa02941603de2fbd4afc261_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of static, passive memory in multi-step RAG systems, which leads to fragmented reasoning in long-context tasks. It proposes HGMem, a dynamic hypergraph-based memory mechanism that captures high-order correlations among facts to form an integrated knowledge structure for stronger reasoning guidance. The method is shown to consistently and substantially outperform baseline systems across diverse global sense-making tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Exploring the Potential of Spiking Neural Networks in UWB Channel Estimation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [Spiking Neural Networks (SNNs), Ultra-Wide Band (UWB), Channel Estimation, Liquid State Machine (LSM), Neuromorphic Computing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Youdong Zhang, Xu He, Xiaolin Meng</p>
</li>
<li class="">
<p><strong>institution:</strong> Southeast University (SEU)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23975" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23975</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a fully unsupervised Spiking Neural Network (SNN) solution for UWB channel estimation, addressing the resource constraints of edge devices. 2. Employs a Liquid State Machine (LSM) with fixed synaptic weights to extract spiking representations from UWB features, sidestepping typical SNN training difficulties. 3. Demonstrates that the proposed SNN approach achieves competitive accuracy (80%) compared to supervised deep learning methods while offering drastically reduced model complexity and suitability for neuromorphic deployment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6223f5cf13e59469e26c765b0854ea31ff37f76d9f1c9137587f99a5a743dcc0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6223f5cf13e59469e26c765b0854ea31ff37f76d9f1c9137587f99a5a743dcc0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high computational cost of deep learning-based UWB channel estimation on resource-constrained edge devices by proposing a fully unsupervised Spiking Neural Network (SNN) solution. The method uses a Liquid State Machine to process encoded UWB features and achieves 80% test accuracy, comparable to supervised methods, while being inherently more efficient and suitable for neuromorphic hardware.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Causify DataFlow: A Framework For High-performance Machine Learning Stream Computing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [streaming machine learning, directed acyclic graph (DAG), point-in-time idempotency, temporal tiling, causality enforcement]</p>
</li>
<li class="">
<p><strong>authors:</strong> Giacinto Paolo Saggese, Paul Smith</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated. Could be inferred from author names and arXiv submission, but no clear affiliation is provided in the given content.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23977" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23977</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A unified DAG-based execution model with point-in-time idempotency, ensuring identical model behavior in batch and streaming modes without code changes. 2. Automatic causality enforcement by tracking knowledge time across transformations, eliminating future-peeking bugs. 3. Flexible temporal and feature dimension tiling, allowing models to operate at different frequencies and memory profiles via configuration alone.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4105add65c1e7a9960d7e21b6bda977778c41365cfff18c04d16a1af3773b5c4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4105add65c1e7a9960d7e21b6bda977778c41365cfff18c04d16a1af3773b5c4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents DataFlow, a framework for building high-performance ML systems on streaming time-series data. It uses a DAG-based model with point-in-time idempotency to bridge the gap between batch prototyping and streaming production, ensuring causality and reproducibility. The framework demonstrates effectiveness in domains like financial trading and IoT analytics.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Information-Theoretic Quality Metric of Low-Dimensional Embeddings</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [dimensionality reduction], [low-dimensional embeddings, information-theoretic metric, singular-value spectrum, stable rank, neighborhood preservation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sebastián Gutiérrez-Bernal, Hector Medel Cobaxin, Abiel Galindo González</p>
</li>
<li class="">
<p><strong>institution:</strong> Tecnologico de Monterrey</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23981" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23981</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Entropy Rank Preservation Measure (ERPM), a novel local quality metric for embeddings based on Shannon entropy and stable rank of neighborhood matrices. 2. Provides an information-theoretic perspective on embedding quality, directly assessing information preservation rather than just geometric or distance distortions. 3. Demonstrates that ERPM complements existing metrics by identifying neighborhoods with severe information loss, offering a more comprehensive assessment for information-sensitive applications.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30121111f7039384030734c0f1c119a147b1f9d2605df8260026b1ba34b6b2dd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30121111f7039384030734c0f1c119a147b1f9d2605df8260026b1ba34b6b2dd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes a new information-theoretic metric called ERPM to evaluate the quality of low-dimensional embeddings by measuring changes in uncertainty via the singular-value spectrum of neighborhood matrices. It shows that ERPM correlates strongly with geometric measures but identifies local discrepancies, complementing existing distance-based and geometric metrics. The conclusion is that ERPM enables a more thorough assessment of embeddings, especially for applications sensitive to information loss like early-warning indicators.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Assured Autonomy: How Operations Research Powers and Orchestrates Generative AI Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [flow-based generative models, adversarial robustness, optimal transport]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tinglong Dai, David Simchi-Levi, Michelle Xiao Wu, Yao Xie</p>
</li>
<li class="">
<p><strong>institution:</strong> Johns Hopkins University, Massachusetts Institute of Technology, Purdue University, Georgia Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23978" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23978</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a conceptual framework for &quot;assured autonomy&quot; to address the fragility of stochastic generative models in operational domains. 2. Identifies flow-based generative models as a key approach for enabling auditability and constraint-aware generation. 3. Formulates operational safety through an adversarial robustness lens to account for worst-case perturbations and unmodeled risks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc5a435448048829b572c2ce3ff1418aa28bf4adcf98e17ff1ed80a690a1b51a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc5a435448048829b572c2ce3ff1418aa28bf4adcf98e17ff1ed80a690a1b51a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the &quot;autonomy paradox&quot; where more autonomous GenAI systems require stronger formal constraints. It proposes a framework grounded in Operations Research, combining flow-based generative models for deterministic control and adversarial robustness for safety. This shifts OR&#x27;s role from solver to system architect, defining a research agenda for assured autonomy in critical domains.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] MeLeMaD: Adaptive Malware Detection via Chunk-wise Feature Selection and Meta-Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [malware detection], [Model-Agnostic Meta-Learning (MAML), Chunk-wise Feature Selection (CFSGB), Gradient Boosting]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ajvad Haneef K, Karan Kuwar Singh, Madhu Kumar S D</p>
</li>
<li class="">
<p><strong>institution:</strong> National Institute of Technology Calicut</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23987" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23987</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed MeLeMaD, a novel malware detection framework leveraging Model-Agnostic Meta-Learning (MAML) for adaptability and generalization. 2. Introduced a novel Chunk-wise Feature Selection based on Gradient Boosting (CFSGB) technique to handle large-scale, high-dimensional datasets efficiently. 3. Demonstrated state-of-the-art performance on benchmark datasets (CIC-AndMal2020, BODMAS) and a custom dataset (EMBOD), achieving high accuracy and robustness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1509044c1c0bdfbb4a075c799e3c0cafd035f0b7c579548b445cf04caee2975d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1509044c1c0bdfbb4a075c799e3c0cafd035f0b7c579548b445cf04caee2975d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes MeLeMaD, a novel malware detection framework that combines a new chunk-wise feature selection method (CFSGB) with meta-learning (MAML) to improve adaptability and efficiency on large-scale datasets. It achieves high accuracy on benchmark and custom datasets, outperforming existing state-of-the-art approaches and demonstrating robustness against evolving threats.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [mechanistic interpretability], [sparse auto-encoder, reasoning vectors, chain-of-thought]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhenyu Zhang, Shujian Zhang, John Lambert, Wenxuan Zhou, Zhangyang Wang, Mingqing Chen, Andrew Hard, Rajiv Mathews, Lun Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Google DeepMind, The University of Texas at Austin</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23988" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23988</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes RISE, an unsupervised framework using sparse auto-encoders (SAEs) to discover &quot;reasoning vectors&quot; that encode distinct reasoning behaviors from step-level LLM activations. 2. Demonstrates that these discovered vectors correspond to interpretable behaviors (e.g., reflection, backtracking) and can be used for targeted intervention to controllably steer the reasoning process without retraining. 3. Shows SAEs can uncover novel, human-undefined reasoning behaviors and structural properties, such as controlling response confidence, highlighting the potential of unsupervised latent discovery.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6bf740af692f8a7498e3cb43c54db55a7bd4f6005d4c48bc0e225978738bf9a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6bf740af692f8a7498e3cb43c54db55a7bd4f6005d4c48bc0e225978738bf9a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of interpreting the internal reasoning process of large language models (LLMs). It proposes RISE, an unsupervised framework that uses sparse auto-encoders to discover disentangled &quot;reasoning vectors&quot; from chain-of-thought activations. The method enables the identification, visualization, and controllable intervention of specific reasoning behaviors, revealing novel insights beyond supervised analysis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] RepetitionCurse: Measuring and Understanding Router Imbalance in Mixture-of-Experts LLMs under DoS Stress</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [Mixture-of-Experts, Router Imbalance, Denial-of-Service, Expert Parallelism, Adversarial Prompt]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ruixuan Huang, Qingyue Wang, Hantao Huang, Yudong Gao, Dong Chen, Shuai Wang, Wei Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> HKUST, NTU</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23995" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23995</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies a novel DoS vulnerability in MoE LLMs where adversarial inputs can cause severe routing concentration and load imbalance during inference. 2. Proposes RepetitionCurse, a low-cost, black-box, and model-agnostic attack method that uses simple repetitive token patterns to exploit the router&#x27;s universal flaw. 3. Empirically demonstrates significant performance degradation (e.g., 3.063x latency increase on Mixtral-8x7B), highlighting a critical risk to service-level agreements for real-world MoE deployments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d78c253b115346789599383601182c2e6e0cd34050dd3dfd9f1541a370776561_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d78c253b115346789599383601182c2e6e0cd34050dd3dfd9f1541a370776561_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies a denial-of-service vulnerability in Mixture-of-Experts LLMs, where adversarial prompts can manipulate the router to concentrate all tokens on a few experts, creating severe load imbalance. The authors propose RepetitionCurse, a simple black-box attack using repetitive token patterns to exploit this flaw. Their method significantly increases inference latency, demonstrating a critical risk to the availability of MoE-based services.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Tracing the Heart&#x27;s Pathways: ECG Representation Learning from a Cardiac Conduction Perspective</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [self-supervised learning], [electrocardiogram (ECG), self-supervised learning (SSL), cardiac conduction, sparse attention, hierarchical diagnosis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tan Pan, Yixuan Sun, Chen Jiang, Qiong Gao, Rui Sun, Xingmeng Zhang, Zhenqi Yang, Limei Han, Yixiu Liang, Yuan Cheng, Kaiyu Guo</p>
</li>
<li class="">
<p><strong>institution:</strong> Fudan University, Shanghai Academy of Artificial Intelligence for Science</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24002" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24002</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Ashespt/CLEAR-HUG" target="_blank" rel="noopener noreferrer" class="">https://github.com/Ashespt/CLEAR-HUG</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies a key limitation in prior ECG self-supervised learning (eSSL) methods: they overlook inherent heartbeat differences rooted in cardiac conduction and neglect the sequential logic of clinical ECG diagnosis. 2. Proposes a novel two-stage framework (CLEAR-HUG), where the first stage (CLEAR) is an eSSL model that uses a sparse attention mechanism to reconstruct signals by treating each heartbeat as a distinct entity, capturing subtle conduction variations. 3. Introduces a Hierarchical lead-Unified Group head (HUG) for the downstream diagnosis stage, which mirrors the clinical workflow from heartbeats to leads to lead combinations, aligning model patterns with expert guidelines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4316ae9f56d525621d461087803f69e54c99676d3863c02d5df1d1ad32dab6a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4316ae9f56d525621d461087803f69e54c99676d3863c02d5df1d1ad32dab6a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes CLEAR-HUG, a two-stage framework for ECG representation learning. The method first uses a self-supervised model (CLEAR) with sparse attention to learn from cardiac conduction variations, then applies a hierarchical diagnosis head (HUG) aligned with clinical guidelines. Experiments across six tasks show a 6.84% performance improvement, validating its effectiveness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Beyond Hallucinations: A Composite Score for Measuring Reliability in Open-Source Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [llm evaluation], [reliability, calibration, robustness, uncertainty quantification, composite score]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rohit Kumar Salla, Manoj Saravanan, Shrikar Reddy Kota</p>
</li>
<li class="">
<p><strong>institution:</strong> Virginia Tech</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24058" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24058</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/rohitsalla/CRS.git" target="_blank" rel="noopener noreferrer" class="">https://github.com/rohitsalla/CRS.git</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A unified reliability metric (CRS) integrating calibration, robustness, and uncertainty. 2. A large-scale evaluation of ten open-source LLMs on five QA datasets. 3. The demonstration that CRS provides stable model rankings and uncovers hidden failure modes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98ff5e45b3d4957a7de510123e5e0280e7ee39117d9ff73439f058e6965ebfab_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98ff5e45b3d4957a7de510123e5e0280e7ee39117d9ff73439f058e6965ebfab_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the fragmented evaluation of Large Language Model (LLM) reliability by proposing the Composite Reliability Score (CRS), a unified metric that integrates calibration, robustness, and uncertainty quantification. Through experiments on ten open-source LLMs, the authors show that CRS provides consistent model rankings and reveals trade-offs between reliability dimensions. The main conclusion is that the most dependable LLM systems balance accuracy, robustness, and calibrated uncertainty.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Hyperspherical Graph Representation Learning via Adaptive Neighbor-Mean Alignment and Uniformity</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph representation learning], [hyperspherical embedding, neighbor-mean alignment, sampling-free uniformity, entropy-guided balancing, graph neural networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rui Chen, Junjun Guo, Hongbin Wang, Yan Xiang, Yantuan Xian, Zhengtao Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> Kunming University of Science and Technology, Yunnan Key Laboratory of Artificial Intelligence</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24062" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24062</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes HyperGRL, a unified framework for hyperspherical graph representation learning using two adversarially coupled objectives: neighbor-mean alignment and sampling-free uniformity. 2. Introduces an entropy-guided adaptive balancing mechanism to dynamically regulate the interplay between alignment and uniformity without manual hyperparameter tuning. 3. Demonstrates superior performance on node classification, clustering, and link prediction tasks, achieving improvements over existing methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e0d10e35c66ca8482e3324110ac5fded6eefbaab8a774c81cba99d854efe694_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e0d10e35c66ca8482e3324110ac5fded6eefbaab8a774c81cba99d854efe694_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes HyperGRL, a novel framework for graph representation learning that embeds nodes on a hypersphere using neighbor-mean alignment and a sampling-free uniformity objective, stabilized by an adaptive balancing mechanism. The method avoids complex negative sampling and hyperparameter tuning, addressing issues like over-smoothing and training instability. Experiments show HyperGRL outperforms existing methods on standard graph learning tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from Cognitive Behaviors to Low-Level Patterns</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [large language models], [supervised fine-tuning, reinforcement learning, reasoning decomposition, meta-probing, generalization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haoyue Bai, Yiyou Sun, Wenjie Hu, Shi Qiu, Maggie Ziyu Huan, Peiyang Song, Robert Nowak, Dawn Song</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Wisconsin, Madison; University of California, Berkeley; University of Pennsylvania; California Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24063" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24063</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a novel benchmark that decomposes reasoning into atomic core skills (e.g., calculation, simulation) for fine-grained analysis. 2. Proposed a meta-probing framework to track model behavioral profiles across different training stages. 3. Provided a combined analysis linking high-level cognitive skill changes to low-level statistical patterns, revealing that RL tuning preserves reasoning skills better than SFT.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e9fab646e78db9be62f127bfa42a9acf6be78552ea9847e5873b10c18802eaa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e9fab646e78db9be62f127bfa42a9acf6be78552ea9847e5873b10c18802eaa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates why LLMs generalize differently after SFT versus RL tuning. It proposes a new benchmark to decompose reasoning into core skills and a meta-probing framework to analyze model behavior, finding that RL-tuned models maintain more stable reasoning abilities while SFT models tend to overfit to surface patterns.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Time-varying Mixing Matrix Design for Energy-efficient Decentralized Federated Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [decentralized federated learning, mixing matrix, energy consumption, time-varying topology, wireless networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xusheng Zhang, Tuan Nguyen, Ting He</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Oxford, Pennsylvania State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24069" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24069</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel convergence theorem for DFL that allows for arbitrarily time-varying mixing matrices, providing theoretical justification for dynamic communication topologies. 2. A multi-phase design framework for mixing matrices that activates time-varying communication topologies to trade off per-iteration energy consumption and convergence rate. 3. An optimization approach that minimizes the maximum per-node energy consumption until convergence, explicitly considering the broadcast nature of wireless communications.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/090a96d0adae430f081f3c2325be19fc983de3cc1e88b07fcc83e4ab4e983d30_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/090a96d0adae430f081f3c2325be19fc983de3cc1e88b07fcc83e4ab4e983d30_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of high energy consumption in Decentralized Federated Learning (DFL) for wireless networks by designing time-varying mixing matrices. The proposed method introduces a multi-phase framework that dynamically adjusts communication topologies to balance energy use across nodes and trade off per-iteration cost with convergence speed. The evaluation shows the solution effectively combines the low energy of sparse topologies with the fast convergence of dense ones.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Multi-Scenario Highway Lane-Change Intention Prediction: A Temporal Physics-Informed Multi-Modal Framework</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [autonomous driving], [lane-change intention prediction, physics-informed AI, temporal embeddings, class imbalance, LightGBM]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiazhao Shi, Ziyu Wang, Yichen Lin, Shoufeng Lu</p>
</li>
<li class="">
<p><strong>institution:</strong> New York University, Wake Forest University, Nanjing Tech University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24075" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24075</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a hybrid Temporal Physics-Informed AI (TPI-AI) framework that fuses deep temporal embeddings from a Bi-LSTM encoder with physics-inspired interaction features for lane-change intention prediction. 2. Introduced imbalance-aware optimization techniques, including resampling/weighting and fold-wise threshold calibration, to improve minority-class reliability. 3. Demonstrated robust multi-scenario generalization by evaluating on two large-scale, heterogeneous highway datasets (highD and exiD) with location-based splits and outperforming standalone baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/689b7843d7cd1396bf4f4df0cdc13d3a94ef4d76e2d2ca0c9948486895c4e482_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/689b7843d7cd1396bf4f4df0cdc13d3a94ef4d76e2d2ca0c9948486895c4e482_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes TPI-AI, a hybrid framework combining Bi-LSTM temporal embeddings with physics-informed interaction features for highway lane-change intention prediction. It addresses challenges like class imbalance and noisy data, achieving high macro-F1 scores on two drone-based datasets. The results show that integrating physics cues with learned representations yields robust performance across diverse highway scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Random Multiplexing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [wireless communication], [random multiplexing, AMP detection, power allocation, replica optimality, constrained capacity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lei Liu, Yuhao Chi, Shunqi Huang, Zhaoyang Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Xidian University, Japan Advanced Institute of Science and Technology (JAIST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24087" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24087</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a random multiplexing technique decoupled from physical channel structures, enabling application to arbitrary norm-bounded and spectrally convergent channel matrices. 2. Introduces a low-complexity cross-domain memory AMP (CD-MAMP) detector and derives optimal power allocations to minimize BER and maximize constrained capacity. 3. Investigates the optimal coding principle and proves the replica constrained-capacity optimality of the CD-MAMP detector for random multiplexing systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad2f6d7e30422bda561811ec881c0aa17f79a2f4e7bf13203955c43c9c8fab17_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad2f6d7e30422bda561811ec881c0aa17f79a2f4e7bf13203955c43c9c8fab17_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a random multiplexing technique to overcome the limitations of traditional and emerging multiplexing schemes (like OFDM and OTFS) which rely on specific channel structures. The method decouples from the physical channel, uses a random transform to create an input-isotropic equivalent channel, and employs a low-complexity AMP-type detector to achieve near-optimal performance for arbitrary norm-bounded channels. The authors validate the approach with theoretical analysis and numerical results, demonstrating its robustness and versatility in dynamic wireless environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Training a Huggingface Model on AWS Sagemaker (Without Tears)</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [AWS SageMaker, Hugging Face, MLOps, cloud computing, Jupyter as a Service]</p>
</li>
<li class="">
<p><strong>authors:</strong> Liling Tan</p>
</li>
<li class="">
<p><strong>institution:</strong> (Institution not explicitly stated in provided content. Based on author name and context, likely independent researcher or affiliation not listed on first page.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24098" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24098</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a centralized, comprehensive guide to train a Hugging Face model on AWS SageMaker, addressing fragmented documentation. 2. Bridges the knowledge gap between local Jupyter Notebook development and cloud-based training on SageMaker. 3. Aims to democratize cloud adoption for researchers lacking on-premise computing resources by lowering the platform&#x27;s learning curve.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e9d3b4e38e89877a6b18e1df5bad45a785b69e26e421db2b8a658f22bdff539_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e9d3b4e38e89877a6b18e1df5bad45a785b69e26e421db2b8a658f22bdff539_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This demo paper addresses the steep learning curve and fragmented documentation that hinder researchers from using AWS SageMaker to train Hugging Face models. It proposes a centralized guide to bridge the gap between local and cloud-based development workflows. The main conclusion is that this approach can democratize cloud adoption, enabling more researchers to train models without extensive on-premise resources.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Enhancing LLM Planning Capabilities through Intrinsic Self-Critique</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [planning], [self-critique, few-shot learning, many-shot learning, iterative refinement, planning benchmarks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bernd Bohnet, Pierre-Alexandre Kamienny, Hanie Sedghi, Dilan Gorur, Pranjal Awasthi, Aaron Parisi, Kevin Swersky, Rosanne Liu, Azade Nova, Noah Fiedel</p>
</li>
<li class="">
<p><strong>institution:</strong> Google DeepMind</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24103" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24103</a></p>
</li>
<li class="">
<p><strong>contributions:</strong>  1. Proposes an intrinsic self-critique method for LLMs to improve their own planning outputs without external verifiers. 2. Demonstrates significant performance gains on established planning benchmarks (Blocksworld, Logistics, Mini-grid) over strong baselines. 3. Shows the method&#x27;s applicability across different models and datasets, achieving new state-of-the-art results for the considered model class.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1646fd60ae3c2dbada7b54640bd9b507a010326fc6e75dd48733e4e2612eeefd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1646fd60ae3c2dbada7b54640bd9b507a010326fc6e75dd48733e4e2612eeefd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces an intrinsic self-critique approach where LLMs iteratively critique and refine their own plans. The method, building upon few-shot and many-shot learning, significantly improves planning performance on benchmarks like Blocksworld without needing external verification. The results set a new state-of-the-art, demonstrating that self-critique can effectively enhance LLM planning capabilities.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Autoregressivity in the Latent Space of a GP-VAE Language Model: An Empirical Ablation Study</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [language modeling], [GP-VAE, latent autoregression, ablation study, Gaussian process, variational autoencoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yves Ruffenach</p>
</li>
<li class="">
<p><strong>institution:</strong> Conservatoire National des Arts et Métiers</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24102" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24102</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducts a systematic ablation study to isolate and analyze the role of latent autoregression in a GP-VAE language model., 2. Demonstrates that latent autoregression leads to latent trajectories more aligned with the Gaussian-process prior and with greater long-horizon stability., 3. Provides empirical evidence that sequential structure can be effectively carried by latent dynamics even when using a non-autoregressive decoder.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bed10d31b6309110f5cec704462d295e7459d74c493da4d130689dc4d04af954_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bed10d31b6309110f5cec704462d295e7459d74c493da4d130689dc4d04af954_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes the role of latent autoregression in a GP-VAE language model through an ablation study. It compares models with and without latent autoregression, finding that latent autoregression organizes long-range structure, leading to more stable and prior-aligned latent trajectories. The study shows latent dynamics can carry sequential structure even with a non-autoregressive decoder.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] OptRot: Mitigating Weight Outliers via Data-Free Rotations for Post-Training Quantization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [post-training quantization, weight outliers, rotation, GPTQ, data-free]</p>
</li>
<li class="">
<p><strong>authors:</strong> Advait Gadhikar, Riccardo Grazzi, James Hensman</p>
</li>
<li class="">
<p><strong>institution:</strong> CISPA Helmholtz Center for Information Security, Microsoft Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24124" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24124</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes OptRot, a data-free method that learns fusible rotations by minimizing a principled, cheap proxy objective (element-wise fourth power of weights) to reduce weight outliers for quantization. 2. Demonstrates that OptRot outperforms existing rotation methods (Hadamard, SpinQuant, OSTQuant) for weight quantization and improves W4A8 activation quantization. 3. Introduces OptRot+, a data-dependent variant that incorporates activation covariance information for further performance gains, while highlighting a trade-off between weight and activation quantization in the W4A4 setting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/137fb0c1b0206d10c607db973da90e5733c1ed86f7a8360cfe91f146000affae_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/137fb0c1b0206d10c607db973da90e5733c1ed86f7a8360cfe91f146000affae_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of quantizing Large Language Models (LLMs) by mitigating weight outliers. It proposes OptRot, a data-free method that learns efficient rotations to minimize a proxy for weight quantization error, and shows it outperforms existing techniques for weight and W4A8 activation quantization. The work also introduces an enhanced data-dependent variant and reveals a performance trade-off in more aggressive quantization settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Colorful Pinball: Density-Weighted Quantile Regression for Conditional Guarantee of Conformal Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [uncertainty quantification], [conformal prediction, conditional coverage, quantile regression, density-weighted pinball loss, three-headed network]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qianyi Chen, Bo Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24139" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24139</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Derivation of a novel density-weighted pinball loss as a sharp surrogate objective for quantile regression to improve conditional coverage in conformal prediction. 2. Proposal of a three-headed quantile network architecture that estimates the necessary density weights via finite differences using auxiliary quantiles. 3. Provision of a theoretical analysis with exact non-asymptotic guarantees for the excess risk and demonstration of significant conditional coverage improvements on real-world datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0a2f8de6a5fa6ef2df8d7f5895f2503ae4000f24fff02f3e88840fcb1d0f27f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0a2f8de6a5fa6ef2df8d7f5895f2503ae4000f24fff02f3e88840fcb1d0f27f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of achieving reliable conditional coverage in conformal prediction. The authors propose a new method that refines quantile regression using a density-weighted pinball loss and a three-headed network to estimate the weights, leading to improved conditional coverage guarantees. Experiments show the method significantly enhances conditional coverage performance on diverse datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] GARDO: Reinforcing Diffusion Models without Reward Hacking</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [reward hacking, diffusion models, regularization, mode collapse, online RL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haoran He, Yuxiao Ye, Jie Liu, Jiajun Liang, Zhiyong Wang, Ziyang Yuan, Xintao Wang, Hangyu Mao, Pengfei Wan, Ling Pan</p>
</li>
<li class="">
<p><strong>institution:</strong> Hong Kong University of Science and Technology, Kuaishou Technology, CUHK MMLab, The University of Edinburgh</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24138" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24138</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://tinnerhrhe.github.io/gardo_project" target="_blank" rel="noopener noreferrer" class="">https://tinnerhrhe.github.io/gardo_project</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed GARDO, a framework with gated regularization that selectively penalizes high-uncertainty samples to mitigate reward hacking efficiently., 2. Introduced an adaptive regularization mechanism that periodically updates the reference model to align with the online policy, enabling effective exploration., 3. Designed a diversity-aware reward amplification strategy to encourage mode coverage and prevent diversity collapse during RL fine-tuning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df8973aa0f222e89b818973c0c7ef576738632b0095b29ac1f837f1a83f47f9b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df8973aa0f222e89b818973c0c7ef576738632b0095b29ac1f837f1a83f47f9b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of reward hacking in RL-fine-tuned diffusion models, where optimizing imperfect proxy rewards degrades real image quality and diversity. The authors propose GARDO, a framework featuring gated, adaptive regularization and diversity-aware optimization to prevent overfitting, maintain exploration, and enhance diversity. Experiments show GARDO effectively mitigates reward hacking and improves generation diversity without sacrificing sample efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Paired Seed Evaluation: Statistical Reliability for Learning-Based Simulators</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [experimental methodology], [paired evaluation, variance reduction, statistical power, random seeds, learning-based simulators]</p>
</li>
<li class="">
<p><strong>authors:</strong> Udit Sharma</p>
</li>
<li class="">
<p><strong>institution:</strong> IIT Kharagpur</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24145" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24145</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formalizes a paired seed evaluation design for learning-based simulators, where competing systems are evaluated under identical random seeds to induce matched realizations of stochastic components. 2. Analyzes the statistical structure of comparative evaluation, showing that this design provides strict variance reduction and tighter confidence intervals when outcomes are positively correlated at the seed level. 3. Empirically demonstrates that seed-level correlations are typically large and positive, leading to order-of-magnitude efficiency gains in statistical power and effective sample size compared to independent evaluation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22d4813c6d5407b2660a9657493b8a66ce92b409d971e7a9ac715b6c94922b60_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22d4813c6d5407b2660a9657493b8a66ce92b409d971e7a9ac715b6c94922b60_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high variance in evaluating learning-based simulators by proposing a paired seed evaluation design. This method evaluates competing systems under identical random seeds, reducing variance when outcomes are correlated. The main conclusion is that this approach is weakly dominant, improving statistical reliability with efficiency gains when correlation exists and reverting to standard evaluation without loss when it does not.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Deep Global Clustering for Hyperspectral Image Segmentation: Concepts, Applications, and Open Challenges</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [hyperspectral image segmentation], [deep global clustering, memory-efficient segmentation, unsupervised disease detection, multi-objective loss balancing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yu-Tang Chang, Pin-Wei Chen, Shih-Fang Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> National Taiwan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24172" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24172</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/b05611038/HSI_global_clustering" target="_blank" rel="noopener noreferrer" class="">https://github.com/b05611038/HSI_global_clustering</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Deep Global Clustering (DGC), a conceptual framework for memory-efficient hyperspectral image segmentation that learns global clustering from local patches without pre-training., 2. Demonstrated the framework&#x27;s ability to achieve background-tissue separation and unsupervised disease detection on a leaf disease dataset with high efficiency (training &lt;30 min on consumer hardware)., 3. Identified and analyzed the key challenge of optimization instability due to multi-objective loss balancing, positioning the work as intellectual scaffolding for future principled solutions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c5ca8fa4968e4f11184d8faa275b2f2edacdd6e6374072e519e722e16016a06_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c5ca8fa4968e4f11184d8faa275b2f2edacdd6e6374072e519e722e16016a06_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the computational bottleneck in hyperspectral image (HSI) analysis by proposing Deep Global Clustering (DGC), a memory-efficient framework that learns global segmentation from local patches without pre-training. It successfully demonstrates background-tissue separation and unsupervised disease detection on agricultural data. However, the main conclusion is that while the design philosophy is promising, the framework suffers from optimization instability due to loss balancing, requiring more principled solutions for stable implementation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Guiding a Diffusion Transformer with the Internal Dynamics of Itself</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [Internal Guidance, Diffusion Transformer, Classifier-Free Guidance, Sampling Guidance, Denoising Diffusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xingyu Zhou, Qifan Li, Xiaobin Hu, Hai Chen, Shuhang Gu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China, National University of Singapore, Sun Yat-sen University, North China Institute of Computer Systems Engineering</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24176" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24176</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Internal Guidance (IG), a novel sampling guidance strategy that uses intermediate-layer outputs within a Diffusion Transformer to improve generation quality. 2. Introduces an auxiliary supervisory signal at an intermediate layer during training and extrapolates outputs during sampling, requiring no extra training, degradation strategies, or additional sampling steps. 3. Demonstrates state-of-the-art performance on ImageNet 256x256, achieving an FID of 1.19 when combined with CFG, and shows significant improvements in training efficiency and generation quality across various baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/605c83917fcbd68353ebfb53e0da152ab446f909b59cdfd81ee3bd6c6023a44f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/605c83917fcbd68353ebfb53e0da152ab446f909b59cdfd81ee3bd6c6023a44f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the issue of standard classifier-free guidance (CFG) causing over-simplified or distorted samples in diffusion models. It proposes Internal Guidance (IG), a simple method that adds auxiliary supervision to an intermediate layer during training and extrapolates outputs during sampling. The method significantly improves generation quality and efficiency, achieving state-of-the-art FID scores on ImageNet.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Micro-Macro Tensor Neural Surrogates for Uncertainty Quantification in Collisional Plasma</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [hpc], [uncertainty quantification], [tensor neural networks, micro-macro decomposition, asymptotic-preserving methods, variance-reduced Monte Carlo, physics-informed neural networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wei Chen, Giacomo Dimarco, Lorenzo Pareschi</p>
</li>
<li class="">
<p><strong>institution:</strong> Xiamen University, University of Ferrara</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24205" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24205</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A variance-reduced Monte Carlo framework for UQ in the Vlasov-Poisson-Landau system that uses neural network surrogates to replace costly collision term evaluations. 2. A generalization of separable physics-informed neural networks (SPINN) into a class of tensor neural networks based on an anisotropic micro-macro decomposition to reduce model complexity and the curse of dimensionality. 3. The calibration of a VPFP surrogate model and the design of an asymptotic-preserving SPINN to increase correlation with the high-fidelity VPL model, ensuring correct recovery of limiting systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b3b7038e3ab35211c6d097f5ecf8a93c74f1b92887ec66726a8e7618764cb3d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b3b7038e3ab35211c6d097f5ecf8a93c74f1b92887ec66726a8e7618764cb3d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of efficient uncertainty quantification in collisional plasma simulations, which is hindered by high computational cost and dimensionality. The authors propose a method that couples a high-fidelity solver with inexpensive neural network surrogates based on tensor networks and a micro-macro decomposition. The results show their framework achieves substantial variance reduction, accurate statistics with fewer samples, and lower computational time compared to standard Monte Carlo.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Medical Image Classification on Imbalanced Data Using ProGAN and SMA-Optimized ResNet: Application to COVID-19</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [imbalanced data, progressive generative adversarial network (ProGAN), slime mould algorithm (SMA), ResNet, synthetic data generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sina Jahromi, Farshid Hajati, Alireza Rezaee, Javaher Nourian</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Tehran, University of New England</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24214" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24214</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Progressive GAN (ProGAN) to generate synthetic medical images to address data imbalance. 2. Introduces a weighted approach for combining synthetic and real data before classification. 3. Employs the Slime Mould Algorithm (SMA), a multi-objective meta-heuristic, to optimize the hyper-parameters of the ResNet classifier.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95bbc93b1f6719a8c5ffdfe98638f4844e331a5a71715b6698a65f669e17f927_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95bbc93b1f6719a8c5ffdfe98638f4844e331a5a71715b6698a65f669e17f927_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the problem of imbalanced medical image data, particularly for COVID-19 detection from chest X-rays. The proposed method uses a Progressive GAN to generate synthetic data and a weighted combination of real and synthetic data, with a ResNet classifier optimized by the Slime Mould Algorithm. The model achieved high accuracy (95.5% for 4-class, 98.5% for 2-class) on an imbalanced dataset, demonstrating its effectiveness for pandemic-related medical image classification.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] MotivNet: Evolving Meta-Sapiens into an Emotionally Intelligent Foundation Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [facial expression recognition], [facial emotion recognition, generalization, foundation model, masked autoencoder, downstream task]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rahul Medicharla, Alper Yilmaz</p>
</li>
<li class="">
<p><strong>institution:</strong> The Ohio State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24231" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24231</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/OSUPCVLab/EmotionFromFaceImages" target="_blank" rel="noopener noreferrer" class="">https://github.com/OSUPCVLab/EmotionFromFaceImages</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes MotivNet, a novel FER model built on the Meta-Sapiens foundation model to achieve strong generalization without cross-domain training. 2. Defines and applies three criteria (benchmark performance, model similarity, data similarity) to validate a new downstream task for the Sapiens foundation model. 3. Demonstrates that the proposed approach achieves competitive performance across diverse datasets, making FER more viable for real-world, in-the-wild applications.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81d3ca85dac4626b7644912941beba01e188b0d41eefb51c00fdb16cd74177b8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81d3ca85dac4626b7644912941beba01e188b0d41eefb51c00fdb16cd74177b8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces MotivNet, a facial emotion recognition model that uses the Meta-Sapiens foundation model as a backbone to achieve strong generalization across datasets without requiring cross-domain training. The authors validate MotivNet as a suitable downstream task for Sapiens using specific criteria and show it achieves competitive performance. The work aims to make FER more robust and applicable in real-world scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Early Prediction of Sepsis using Heart Rate Signals and Genetic Optimized LSTM Algorithm</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [Genetic Algorithm, LSTM, Wearable Device, Transfer Learning, Heart Rate Signals]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alireza Rafiei, Farshid Hajati, Alireza Rezaee, Amirhossien Panahi, Shahadat Uddin</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Tehran, University of New England, The University of Sydney</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24253" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24253</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed and optimized four machine learning models (LGB, MLP, LSTM, LSTM-FCN) for early sepsis prediction specifically for deployment on wearable devices. 2. Used a genetic algorithm to refine the model architectures, optimizing for performance, computational complexity, and memory requirements suitable for wearables. 3. Extended the prediction window from one hour to four hours using transfer learning, demonstrating adaptability for longer-term forecasting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1495386bf6867b81b1a66ce95759268f2ed495c711dc3b4668583fc8a9c26b95_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1495386bf6867b81b1a66ce95759268f2ed495c711dc3b4668583fc8a9c26b95_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes using genetic algorithm-optimized machine learning models to predict sepsis early by analyzing heart rate data from wearable devices. The models are designed for computational efficiency on wearables and were extended to a four-hour prediction window via transfer learning. The results show promise for enabling early sepsis detection outside of intensive care settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Deep Reinforcement Learning for Solving the Fleet Size and Mix Vehicle Routing Problem</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Fleet Size and Mix Vehicle Routing Problem (FSMVRP), deep reinforcement learning (DRL), Markov Decision Process (MDP), fleet-and-route integrated policy network (FRIPN), remaining graph embedding]</p>
</li>
<li class="">
<p><strong>authors:</strong> Pengfu Wan, Jiawei Chen, Gangyan Xu</p>
</li>
<li class="">
<p><strong>institution:</strong> The Hong Kong Polytechnic University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24251" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24251</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulates the Fleet Size and Mix Vehicle Routing Problem (FSMVRP) as a Markov Decision Process (MDP) for a deep reinforcement learning approach. 2. Proposes a novel policy network (FRIPN) that integrates fleet composition and routing decisions into a single model. 3. Introduces specialized input embeddings, including a remaining graph embedding, to enhance decision-making for vehicle employment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3969891b48accce35280355d106951820196973739958b782a307a2a3df23aa3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3969891b48accce35280355d106951820196973739958b782a307a2a3df23aa3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a deep reinforcement learning method to solve the complex Fleet Size and Mix Vehicle Routing Problem (FSMVRP). The core innovation is a policy network called FRIPN that jointly decides on fleet composition and routing. Experiments show the method is computationally efficient and scalable, producing near-optimal solutions quickly, especially for large-scale problems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Joint Selection for Large-Scale Pre-Training Data via Policy Gradient-based Mask Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [data selection, policy gradient, mask learning, quality-diversity trade-off, FineWeb]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ziqing Fan, Yuqiao Xian, Yan Sun, Li Shen</p>
</li>
<li class="">
<p><strong>institution:</strong> ByteDance Seed, Shanghai Jiao Tong University, University of Sydney, Sun Yat-sen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24265" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24265</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/ByteDance-Seed/DATAMASK" target="_blank" rel="noopener noreferrer" class="">https://github.com/ByteDance-Seed/DATAMASK</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces DATAMASK, a novel joint learning framework for large-scale pre-training data selection that simultaneously optimizes quality and diversity metrics. 2. Formulates data selection as a mask learning problem and solves it efficiently using policy gradient-based optimization with acceleration enhancements, reducing selection time by 98.9% compared to greedy algorithms. 3. Creates and releases FineWeb-Mask, a high-quality and diverse 10% subset of the 15-trillion-token FineWeb dataset, which significantly improves model performance (e.g., +3.2% on a 1.5B model) across diverse tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02fd504349b8c0bb1934304d821a648ed4ca490b2f41e136f9b7a6220f39d5d2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02fd504349b8c0bb1934304d821a648ed4ca490b2f41e136f9b7a6220f39d5d2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of efficiently selecting high-quality and diverse data for large-scale LLM pre-training, where traditional methods are costly and suboptimal. It proposes DATAMASK, a policy gradient-based framework that learns optimal data masks to jointly optimize quality and diversity, drastically speeding up selection. The resulting curated dataset, FineWeb-Mask, leads to significant performance gains in pre-trained models, demonstrating the framework&#x27;s effectiveness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Empower Low-Altitude Economy: A Reliability-Aware Dynamic Weighting Allocation for Multi-modal UAV Beam Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [reliability-aware dynamic weighting, cross-modal contrastive learning, semantic-aware beam prediction, low-altitude UAV, multi-modal learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haojin Li, Anbang Zhang, Chen Sun, Chenyuan Feng, Kaiqian Qu, Tony Q. S. Quek, Haijun Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology Beijing, Sony China Research Laboratory, Shandong University, Southeast University, University of Exeter, Singapore University of Technology and Design</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24324" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24324</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a reliability-aware dynamic weighting scheme that adaptively allocates contributions across different modalities (e.g., visual, posture, geospatial) based on their instantaneous reliability, moving beyond fixed-weight approaches. 2. Introduces a semantic-aware multi-modal beam prediction framework (SaM²B) that uses cross-modal contrastive learning to align multi-source representations into a shared semantic space, enhancing robustness to noise and distribution shifts. 3. Validates the proposed SaM²B framework on real-world low-altitude UAV datasets, demonstrating superior performance over baseline methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1fa219c5db4bfd0eeac8ee93679e8eba9ceb6e2e5ce7336d45afc6686f1d8162_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1fa219c5db4bfd0eeac8ee93679e8eba9ceb6e2e5ce7336d45afc6686f1d8162_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of unreliable beam prediction in multi-modal UAV communications caused by static weighting and modal misalignment. It proposes SaM²B, a framework that uses reliability-aware dynamic weighting and cross-modal contrastive learning to adaptively fuse modalities and align their semantics. Experiments on real-world datasets show SaM²B outperforms existing baseline methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] MaRCA: Multi-Agent Reinforcement Learning for Dynamic Computation Allocation in Large-Scale Recommender Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [Multi-Agent Reinforcement Learning, Centralized Training with Decentralized Execution (CTDE), Model Predictive Control (MPC), Dynamic Computation Allocation, Recommender Systems]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wan Jiang, Xinyi Zang, Yudong Zhao, Yusi Zou, Yunfei Lu, Junbo Tong, Yang Liu, Ming Li, Jiani Shi, Xin Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> JD.com, Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24325" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24325</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes MaRCA, a multi-agent reinforcement learning framework that models recommender system stages as cooperative agents for end-to-end computation resource allocation. 2. Introduces an AutoBucket TestBench for accurate computation cost estimation in large-scale systems. 3. Designs a Model Predictive Control (MPC)-based Revenue-Cost Balancer to proactively forecast traffic loads and adjust the revenue-cost trade-off.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5648d9986b292c2db26fc1ddb4325c3fc0f14c6516ddecf792ceaf524e365c4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5648d9986b292c2db26fc1ddb4325c3fc0f14c6516ddecf792ceaf524e365c4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of dynamic computation allocation in large-scale, multi-stage recommender systems under resource constraints. It proposes MaRCA, a multi-agent reinforcement learning framework that uses Centralized Training with Decentralized Execution (CTDE) and integrates a Model Predictive Control-based balancer to optimize revenue. The system was deployed on a major e-commerce platform, handling hundreds of billions of daily requests and achieving a 16.67% revenue uplift using existing resources.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Tubular Riemannian Laplace Approximations for Bayesian Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [bayesian deep learning], [Laplace approximation, Riemannian geometry, uncertainty quantification, Bayesian neural networks, model calibration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rodrigo Pereira David</p>
</li>
<li class="">
<p><strong>institution:</strong> National Institute of Metrology, Technology and Quality (Inmetro)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24381" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24381</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Tubular Riemannian Laplace (TRL) approximation, a novel method that models the posterior as a probabilistic tube following low-loss valleys induced by functional symmetries. 2. Proposes using a Fisher/Gauss-Newton metric to separate prior-dominated tangential uncertainty from data-dominated transverse uncertainty, adapting to the anisotropic, curved loss surfaces of deep models. 3. Demonstrates empirically that TRL achieves calibration comparable to Deep Ensembles on ResNet-18 (CIFAR-10/100) at a fraction (1/5) of the training cost, bridging single-model efficiency with ensemble-grade reliability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbef449cd138ae804891e277de76405797dfc3e78511bcb03bf13e56823c9746_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbef449cd138ae804891e277de76405797dfc3e78511bcb03bf13e56823c9746_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the poor calibration of traditional Euclidean Laplace approximations in Bayesian Neural Networks. It proposes the Tubular Riemannian Laplace (TRL) approximation, which models the posterior as a tube using a Riemannian metric to better capture parameter space geometry. The method achieves excellent uncertainty calibration on image classification tasks, matching Deep Ensembles&#x27; reliability with significantly lower computational cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [cross-view geo-localization], [visual reasoning, reinforcement learning, contrastive learning, cross-view alignment, visual planning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Soham Pahari, M. Srinivas</p>
</li>
<li class="">
<p><strong>institution:</strong> UPES (School of Computer Science), NIT Warangal (Department of CS&amp;E)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24404" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24404</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel visual reasoning paradigm called Geo-Consistent Visual Planning and a framework named ViReLoc for planning and localization using only visual representations. 2. Introduces a method that learns spatial and geometric dependencies through step-by-step visual inference optimized with reinforcement learning objectives. 3. Integrates contrastive learning and adaptive feature interaction to align ground and aerial perspectives and reduce viewpoint differences.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d4164fa0c19f567cf79df4a59a8129e5b520a00f0d6456c7d4f5649d4a580da_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d4164fa0c19f567cf79df4a59a8129e5b520a00f0d6456c7d4f5649d4a580da_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of text-based reasoning in spatial tasks by proposing ViReLoc, a visual reasoning framework for ground-to-aerial localization and route planning. The method uses reinforcement learning and contrastive learning to perform inference directly in the visual domain without relying on GPS. Experiments show improved spatial reasoning and cross-view retrieval, establishing visual reasoning as a secure complementary approach for navigation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Efficient Inference for Inverse Reinforcement Learning and Dynamic Discrete Choice Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [inverse reinforcement learning, dynamic discrete choice, semiparametric inference, debiased machine learning, efficient influence function]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lars van der Laan, Aurelien Bibaut, Nathan Kallus</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Washington, Netflix Research, Cornell University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24407" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24407</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a semiparametric framework for debiased inverse reinforcement learning that enables statistically efficient inference for reward-dependent functionals. 2. Showed that the log-behavior policy acts as a pseudo-reward that identifies policy value differences and, with normalization, the reward itself, formalizing these as smooth functionals. 3. Constructed automatic debiased machine-learning estimators that allow flexible nonparametric nuisance estimation while achieving √n-consistency, asymptotic normality, and semiparametric efficiency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f071bfe838a84ec68caefb3e8f32ddce9a94446707278cc78a1f8f23d2b1cdcf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f071bfe838a84ec68caefb3e8f32ddce9a94446707278cc78a1f8f23d2b1cdcf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper develops a unified semiparametric framework for inference in inverse reinforcement learning and dynamic discrete choice models. The method leverages the log-behavior policy as a pseudo-reward and constructs debiased machine learning estimators, enabling flexible nonparametric estimation while providing statistical guarantees like asymptotic normality and efficiency. The framework bridges classical econometric inference with modern machine learning tools for sequential decision-making problems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Adaptive Learning Guided by Bias-Noise-Alignment Diagnostics</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [adaptive learning], [bias-noise-alignment, diagnostic-driven adaptation, temporal-difference error, stabilized optimizer, actor-critic]</p>
</li>
<li class="">
<p><strong>authors:</strong> Akash Samanta, Sheldon Williamson</p>
</li>
<li class="">
<p><strong>institution:</strong> Ontario Tech University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24445" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24445</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel diagnostic-driven adaptive learning framework that decomposes error evolution into bias, noise, and alignment components. 2. Derives and instantiates the framework across multiple learning paradigms, including supervised optimization, actor-critic RL, and learned optimizers. 3. Establishes theoretical stability guarantees and bounded updates for the proposed diagnostic-driven methods under standard assumptions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fffef5a11b6873e7029659f1544c8ea507323fdf48462b3c7caeee1ffd63e30_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fffef5a11b6873e7029659f1544c8ea507323fdf48462b3c7caeee1ffd63e30_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of unstable and slow learning in nonstationary environments by proposing a new framework that models error evolution through bias, noise, and alignment diagnostics. The method uses these online-computed diagnostics to guide and stabilize learning in optimization, reinforcement learning, and meta-learning. The work provides a unifying, interpretable foundation for reliable adaptation in dynamic settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Sparse classification with positive-confidence data in high dimensions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [weakly-supervised learning], [Pconf classification, sparse regularization, Lasso, SCAD, MCP]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tien Mai, Mai Anh Nguyen, Trung Nghia Nguyen</p>
</li>
<li class="">
<p><strong>institution:</strong> Norwegian Institute of Public Health, Seoul National University, Rutgers University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24443" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24443</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel sparse-penalization framework for high-dimensional Positive-Confidence (Pconf) classification, bridging weak supervision and high-dimensional statistics. 2. Introduces estimators using both convex (Lasso) and non-convex (SCAD, MCP) penalties to address shrinkage bias and improve feature recovery. 3. Establishes theoretical error bounds for the L1-regularized estimator and develops an efficient proximal gradient algorithm to solve the composite objective.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d4d37b85ea25099bdc6c46c973123d1cadddc0454b10411668cb1a47bc23596_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d4d37b85ea25099bdc6c46c973123d1cadddc0454b10411668cb1a47bc23596_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of performing sparse classification in high-dimensional settings using only positive samples with confidence scores (Pconf data). It proposes a new framework using Lasso, SCAD, and MCP penalties for variable selection and develops an efficient algorithm. The method achieves performance comparable to fully supervised approaches, effectively bridging weak supervision and high-dimensional learning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Generative forecasting with joint probability models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [probabilistic forecasting], [joint probability distribution, generative model, uncertainty quantification, chaotic dynamical systems, Wasserstein drift]</p>
</li>
<li class="">
<p><strong>authors:</strong> Patrick Wyrod, Ashesh Chattopadhyay, Daniele Venturi</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California Santa Cruz</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24446" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24446</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Reframes forecasting as a fully generative problem by learning the joint probability distribution of system states over temporal windows and obtaining forecasts via marginalization. 2. Introduces a general, model-agnostic training and inference framework for joint generative forecasting. 3. Proposes three complementary uncertainty quantification metrics (ensemble variance, short-horizon autocorrelation, cumulative Wasserstein drift) to assess forecast robustness without ground truth.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95523ae23ff02d01f3cc01323f495f300ee24988a99409698e6d43daf9c09a82_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95523ae23ff02d01f3cc01323f495f300ee24988a99409698e6d43daf9c09a82_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a new generative forecasting method that learns the joint probability distribution of system states over time windows, rather than just next-step predictions. This approach better captures temporal dependencies and dynamics, leading to improved short-term predictions and more accurate long-term statistical behavior for chaotic systems like Lorenz-63 and Kuramoto-Sivashinsky, as demonstrated by several uncertainty metrics.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Privacy-Preserving Semantic Communications via Multi-Task Learning and Adversarial Perturbations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [semantic communications], [min-max optimization, adversarial perturbations, multi-task learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yalin E. Sagduyu, Tugba Erpek, Aylin Yener, Sennur Ulukus</p>
</li>
<li class="">
<p><strong>institution:</strong> Nexcepta, The Ohio State University, University of Maryland</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24452" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24452</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A deep learning-based semantic communication framework that jointly supports multiple receiver tasks (e.g., inference and reconstruction) while explicitly limiting semantic information leakage to an eavesdropper. 2. Formulation of the privacy problem as an iterative min-max optimization, where the legitimate transmitter-receiver pair is trained to degrade an adaptive eavesdropper&#x27;s semantic inference performance. 3. Introduction of an auxiliary adversarial perturbation layer that superimposes a crafted signal on the transmitted waveform to degrade eavesdropper performance, even when the legitimate link is not co-trained against it.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25e9c394c6a473f54b07d7337b43fb693f3ebef1de80e7b275ea3c91df03de11_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25e9c394c6a473f54b07d7337b43fb693f3ebef1de80e7b275ea3c91df03de11_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses privacy leakage in semantic communications, where task-optimized representations can be exploited by eavesdroppers. The proposed method uses a min-max adversarial training framework and an auxiliary perturbation layer to protect semantic information. Evaluations on image datasets show the approach significantly reduces eavesdropper inference accuracy without harming legitimate receiver performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Spectral and Spatial Graph Learning for Multispectral Solar Image Compression</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image compression], [graph neural network, multispectral image compression, spectral graph embedding, spatial graph attention, learned image compression]</p>
</li>
<li class="">
<p><strong>authors:</strong> Prasiddha Siwakoti, Atefeh Khoshkhahtinat, Piyush M. Mehta, Barbara J. Thompson, Michael S. F. Kirk, Daniel da Silva</p>
</li>
<li class="">
<p><strong>institution:</strong> West Virginia University, NASA Goddard Space Flight Center</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24463" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24463</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/agyat4/sgraph" target="_blank" rel="noopener noreferrer" class="">https://github.com/agyat4/sgraph</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed an Inter-Spectral Windowed Graph Embedding (iSWGE) module to model inter-band relationships by representing spectral channels as graph nodes with learned edges. 2. Introduced a Windowed Spatial Graph Attention and Convolutional Block Attention (WSGA-C) module to reduce spatial redundancy and emphasize fine-scale structures. 3. Developed a learned image compression framework tailored for multispectral solar imagery, achieving improved spectral fidelity and reconstruction quality on the SDOML dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22141561baf730c3986a8299115f064f9a3f82996264d04ebeefffad89a18be1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22141561baf730c3986a8299115f064f9a3f82996264d04ebeefffad89a18be1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of compressing high-volume multispectral solar imagery for space missions. It proposes a learned compression framework that uses two novel graph-based modules to model spectral and spatial dependencies. The method demonstrates improved performance in preserving spectral information and reconstruction quality compared to strong baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] HOLOGRAPH: Active Causal Discovery via Sheaf-Theoretic Alignment of Large Language Model Priors</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [causal discovery], [sheaf theory, large language models, natural gradient descent, algebraic latent projection, presheaf]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hyunjun Kim</p>
</li>
<li class="">
<p><strong>institution:</strong> Korea Advanced Institute of Science and Technology (KAIST), École Polytechnique Fédérale de Lausanne (EPFL)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24478" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24478</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/hyunjun1121/holograph" target="_blank" rel="noopener noreferrer" class="">https://github.com/hyunjun1121/holograph</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A sheaf-theoretic framework formalizing LLM-guided causal discovery as a presheaf satisfaction problem. 2. A natural gradient descent algorithm on the belief manifold for principled optimization. 3. The introduction of Algebraic Latent Projection to handle hidden confounders.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68eaf3f0002711b7e42d725e2a453cb23c5db4c9ca0b3a10f0290cfce9779f2e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68eaf3f0002711b7e42d725e2a453cb23c5db4c9ca0b3a10f0290cfce9779f2e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces HOLOGRAPH, a framework that uses sheaf theory to formally integrate Large Language Model priors for causal discovery, addressing issues of coherence and hidden confounders. It proposes novel methods like Algebraic Latent Projection and natural gradient optimization. The approach provides a rigorous mathematical foundation and shows competitive performance, while analysis reveals a failure of the Locality axiom in larger graphs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [world models], [joint-embedding predictive architecture, representation space planning, model-based reinforcement learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Basile Terver, Tsung-Yen Yang, Jean Ponce, Adrien Bardes, Yann LeCun</p>
</li>
<li class="">
<p><strong>institution:</strong> Meta FAIR, INRIA Paris, Ecole normale supérieure/PSL, New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24497" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24497</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/facebookresearch/jepa-wms" target="_blank" rel="noopener noreferrer" class="">https://github.com/facebookresearch/jepa-wms</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a comprehensive characterization and study of Joint-Embedding Predictive Architecture World Models (JEPA-WMs) for physical planning. 2. Systematically investigates the impact of model architecture, training objective, and planning algorithm on planning success in simulated and real-world robotic tasks. 3. Combines the findings to propose a new model that outperforms established baselines (DINO-WM and V-JEPA-2-AC) in navigation and manipulation tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3be3154d136e165197c022f619cd1d45cd62098d7f202059d7110d6335e67c44_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3be3154d136e165197c022f619cd1d45cd62098d7f202059d7110d6335e67c44_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the key factors for successful physical planning using Joint-Embedding Predictive World Models (JEPA-WMs). It conducts a systematic study of architectural and algorithmic choices within this family of methods and proposes a new model that achieves superior performance on navigation and manipulation tasks compared to existing baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Can Small Training Runs Reliably Guide Data Curation? Rethinking Proxy-Model Practice</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [proxy models, data curation, hyperparameter tuning, learning rate, pretraining]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiachen T. Wang, Tong Wu, Kaifeng Lyu, James Zou, Dawn Song, Ruoxi Jia, Prateek Mittal</p>
</li>
<li class="">
<p><strong>institution:</strong> Princeton University, Tsinghua University, Stanford University, UC Berkeley, Virginia Tech</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24503" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24503</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies a critical flaw in the standard proxy-model evaluation protocol, showing that using a fixed training configuration for all data recipes leads to unreliable conclusions that can flip with minor hyperparameter changes. 2. Proposes a simple and effective patch to the protocol: training proxy models with reduced learning rates, which preserves the relative performance ranking of data recipes and correlates strongly with fully-tuned large-scale training. 3. Provides theoretical justification for the proposed method by proving it preserves dataset ordering for random-feature models, and validates it empirically across 23 data recipes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0253b877f5ec90fd3f09b9d1a8f7d0967a88407fbbd25eb1e8a8bfcdf23081c4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0253b877f5ec90fd3f09b9d1a8f7d0967a88407fbbd25eb1e8a8bfcdf23081c4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that the standard practice of using small proxy models with identical hyperparameters to evaluate data recipes is unreliable because optimal training configurations are data-dependent. To fix this, the authors propose training proxy models with reduced learning rates, a simple change that makes small-scale experiment rankings strongly correlate with those from fully-tuned large-scale LLM pretraining. This method is theoretically justified and empirically validated, dramatically improving the reliability of data curation guidance from small training runs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Generalising E-prop to Deep Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [biologically plausible learning algorithms], [E-prop, eligibility traces, credit assignment, recurrent neural networks, backpropagation through time]</p>
</li>
<li class="">
<p><strong>authors:</strong> Beren Millidge</p>
</li>
<li class="">
<p><strong>institution:</strong> Zyphra</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24506" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24506</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Extends the E-prop framework to handle arbitrarily deep networks, enabling credit assignment across both time and depth. 2. Derives a novel recursion relationship across depth that generalizes eligibility traces to deeper layers. 3. Demonstrates an online learning algorithm capable of training deep recurrent networks without backpropagation through time.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6eda8a1b73fb9e52a5baf437a42dfe8578438c0dde5dfa6d166ba856f037f47_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6eda8a1b73fb9e52a5baf437a42dfe8578438c0dde5dfa6d166ba856f037f47_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the biological implausibility of Backpropagation Through Time (BPTT) for training recurrent neural networks. It proposes an extension of the E-prop algorithm to deep networks, enabling online credit assignment across both time and depth. The main conclusion is that this method allows for the training of deep recurrent networks without BPTT.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] A Graph Neural Network with Auxiliary Task Learning for Missing PMU Data Reconstruction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [graph neural network, auxiliary task learning, missing data reconstruction, spatial-temporal dependencies, low-rank property]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bo Li, Zijun Chen, Haiwang Zhong, Di Cao, Guangchun Ruan</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University (inferred from IEEE affiliations and common institutional patterns for authors Bo Li, Haiwang Zhong, Di Cao, Guangchun Ruan)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24542" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24542</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a K-hop GNN that operates directly on the subgraph of observable PMU nodes, enabling learning under incomplete system observability. 2. Designs an auxiliary learning framework with two complementary GNNs: a spatial-temporal GNN for reconstruction and an auxiliary GNN for unsupervised online learning using the low-rank property of data. 3. Demonstrates that the method dynamically leverages low-rank properties across the architecture to achieve robustness and self-adaptation, showing superior performance under high missing rates and incomplete observability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0bb99f00deaaada2673c4fa2deba197e3de14c9c3164c0d055378a9290049033_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0bb99f00deaaada2673c4fa2deba197e3de14c9c3164c0d055378a9290049033_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of reconstructing missing data from Phasor Measurement Units (PMUs) in power systems, which is critical for grid monitoring. The proposed method uses a Graph Neural Network with Auxiliary Task Learning, combining a spatial-temporal GNN for reconstruction with an auxiliary GNN for online adaptation using data&#x27;s low-rank properties. The results show the method is robust under high missing rates and incomplete system observability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] More Than Bits: Multi-Envelope Double Binary Factorization for Extreme Quantization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [extreme quantization, double binary factorization, low-bit LLM, post-training quantization, binary matrix multiplication]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuma Ichikawa, Yoshihiko Fujisawa, Yudai Fujimoto, Akira Sakai, Katsuki Fujisawa</p>
</li>
<li class="">
<p><strong>institution:</strong> Fujitsu Limited, RIKEN Center for AIP, Institute of Science Tokyo, Tokai University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24545" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24545</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Multi-Envelope Double Binary Factorization (MDBF), which replaces the single magnitude envelope in DBF with a rank-l envelope to enhance magnitude expressiveness while maintaining a shared binary sign carrier. 2. Introduced a closed-form initialization and an alternating refinement method to effectively optimize the MDBF parameters. 3. Demonstrated that MDBF improves perplexity and zero-shot accuracy over prior binary formats on LLaMA and Qwen models at matched bit budgets while preserving the same efficient inference primitive.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e17ec329eb54b789cd97cf9a3fc6db67786908aca3eb86af65de80d0797eb12_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e17ec329eb54b789cd97cf9a3fc6db67786908aca3eb86af65de80d0797eb12_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the performance saturation of Double Binary Factorization (DBF) in extreme low-bit quantization of LLMs, where a single magnitude envelope limits expressiveness. It proposes Multi-Envelope DBF (MDBF), which uses multiple envelope components to allocate more expressivity to magnitudes while keeping binary sign matrices shared. Experiments on LLaMA and Qwen families show MDBF outperforms previous binary formats in accuracy and perplexity at the same bit rate without changing the inference primitive.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] From Perception to Punchline: Empowering VLM with the Art of In-the-wild Meme</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal generation], [vision-language models, chain-of-thought, reinforcement learning from human feedback]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xueyan Li, Yingyi Xue, Mengjie Jiang, Qingzi Zhu, Yazhe Niu</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Artificial Intelligence Laboratory, Xi&#x27;an Jiaotong University, Columbia University, The Chinese University of Hong Kong MMLab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24555" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24555</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hierarchical, multi-path Chain-of-Thought (CoT) method to enhance reasoning diversity for meme generation. 2. Introduces a group-wise pairwise reward model trained on memes sharing the same template to robustly capture subjective human humor preferences. 3. Develops a group-wise reinforcement learning optimization framework with a theoretical guarantee for monotonic improvement, enabling better alignment with human preferences.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51d7b92e3cbe88fcb46458e3ce7a303a30ccee8230eb6865946f2c654b707c34_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51d7b92e3cbe88fcb46458e3ce7a303a30ccee8230eb6865946f2c654b707c34_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces HUMOR, a framework for generating humorous memes. It uses a hierarchical multi-path Chain-of-Thought to guide reasoning and a group-wise reward model with RL for preference alignment. Experiments show it improves reasoning diversity, alignment, and overall meme quality in VLMs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] CPR: Causal Physiological Representation Learning for Robust ECG Analysis under Distribution Shifts</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robust machine learning], [causal representation learning, adversarial robustness, electrocardiogram (ECG), structural causal model (SCM), smooth adversarial perturbations (SAP)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shunbo Jia, Caizhi Liao</p>
</li>
<li class="">
<p><strong>institution:</strong> Macau University of Science and Technology, Shenzhen University of Advanced Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24564" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24564</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Causal Physiological Representation Learning (CPR), a novel framework that integrates a Physiological Structural Prior into a causal disentanglement model for ECG analysis. 2. Models ECG generation via a Structural Causal Model (SCM) to enforce a structural intervention that strictly separates invariant pathological features from non-causal artifacts. 3. Demonstrates that CPR achieves certified robustness comparable to Randomized Smoothing while maintaining single-pass inference efficiency, offering a superior trade-off for real-time clinical applications.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ba48c31785d4bc725e8bd6077843ed01150da361f64874d3d85d9a541b0d692_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ba48c31785d4bc725e8bd6077843ed01150da361f64874d3d85d9a541b0d692_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the vulnerability of deep learning ECG diagnostic models to smooth adversarial perturbations. The authors propose Causal Physiological Representation Learning (CPR), a method that uses a causal disentanglement framework with a physiological prior to separate robust pathological features from artifacts. The results show that CPR provides strong adversarial robustness with efficient inference, outperforming existing defense methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [chain-of-thought reasoning, attention heads, test-time intervention, computational efficiency, reasoning steering]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhenyu Zhang, Xiaoxia Wu, Zhongzhu Zhou, Qingyang Wu, Yineng Zhang, Pragaash Ponnusamy, Harikaran Subbaraj, Jue Wang, Shuaiwen Leon Song, Ben Athiwaratkun</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Texas at Austin, Together AI, University of Sydney</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24574" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24574</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/togethercomputer/CREST" target="_blank" rel="noopener noreferrer" class="">https://github.com/togethercomputer/CREST</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identified specialized attention heads in LLMs that correlate with distinct cognitive reasoning behaviors (e.g., verification, backtracking). 2. Proposed CREST, a training-free method for Cognitive REasoning Steering at Test-time, which involves offline calibration to find steering vectors and inference-time rotation to suppress unproductive reasoning. 3. Demonstrated that CREST improves reasoning accuracy and reduces token usage across diverse benchmarks, offering a pathway to faster and more reliable LLM inference.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7de1babf1bde06083c19ff84ab24d16f7280ee2cd3e28ae548f08be7f95a5882_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7de1babf1bde06083c19ff84ab24d16f7280ee2cd3e28ae548f08be7f95a5882_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the inefficiency and instability of long chain-of-thought reasoning in LLMs, which leads to high latency and alternating underthinking/overthinking. The authors propose CREST, a training-free method that identifies and steers specific attention heads at test-time to suppress unproductive cognitive behaviors. The method improves accuracy by up to 17.5% and reduces token usage by 37.6%, enabling faster and more reliable reasoning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] 3D Semantic Segmentation for Post-Disaster Assessment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D semantic segmentation], [3D point clouds, Structure-from-Motion (SfM), Multi-View Stereo (MVS), Fast Point Transformer (FPT), Point Transformer v3 (PTv3)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nhut Le, Maryam Rahnemoonfar</p>
</li>
<li class="">
<p><strong>institution:</strong> Lehigh University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24593" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24593</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Constructed a specialized 3D dataset for post-disaster assessment using UAV footage and 3D reconstruction techniques (SfM/MVS). 2. Evaluated state-of-the-art 3D semantic segmentation models (FPT, PTv3, OA-CNNs) on this new dataset. 3. Identified significant limitations of existing models in disaster-stricken environments, highlighting the need for new techniques and benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7f40d3c2e21cef6917feeb765394fae515f763b5c2f2f5b374f921e3a8ac1ba_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7f40d3c2e21cef6917feeb765394fae515f763b5c2f2f5b374f921e3a8ac1ba_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of specialized datasets for 3D semantic segmentation in post-disaster scenarios by constructing a new 3D point cloud dataset from UAV footage of Hurricane Ian. The authors evaluated several state-of-the-art models on this dataset and found their performance to be significantly limited, demonstrating an urgent need for improved methods and benchmarks tailored to disaster environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [hierarchical compression, compression-aware scaling law, decoupled µP parametrization, concept space, adaptive semantic boundaries]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xingwei Qu, Shaowen Wang, Zihao Huang, Kai Hua, Fan Yin, Rui-Jie Zhu, Jundong Zhou, Qiyang Min, Zihao Wang, Yizhi Li, Tianyu Zhang, He Xing, Zheng Zhang, Yuxuan Song, Tianyu Zheng, Zhiyuan Zeng, Chenghua Lin, Ge Zhang, Wenhao Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> ByteDance Seed, University of Manchester, Mila - Quebec AI Institute, Tsinghua University, M-A-P</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24617" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24617</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns variable-length semantic concepts end-to-end and shifts computation from tokens to a compressed concept space for more efficient reasoning. 2. Introduced the first compression-aware scaling law that disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. 3. Developed a decoupled µP parametrization for stable training of the heterogeneous architecture, supporting zero-shot hyperparameter transfer across model widths and compression regimes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dff9c61b225b5a86d535cfc752b13f390ae301473e649284a6815c3eaf80b24_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dff9c61b225b5a86d535cfc752b13f390ae301473e649284a6815c3eaf80b24_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the inefficiency of uniform token-level computation in LLMs by proposing Dynamic Large Concept Models (DLCM), which learns adaptive semantic concepts and reallocates compute to a higher-capacity reasoning backbone in a compressed concept space. This approach achieves a +2.69% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] AutoFed: Manual-Free Federated Traffic Prediction via Personalized Prompt</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [personalized federated learning, prompt learning, traffic prediction, non-IID data, hyper-parameter tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zijian Zhao, Yitong Shang, Sen Li</p>
</li>
<li class="">
<p><strong>institution:</strong> The Hong Kong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24625" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24625</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/RS2002/AutoFed" target="_blank" rel="noopener noreferrer" class="">https://github.com/RS2002/AutoFed</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes AutoFed, a novel Personalized Federated Learning (PFL) framework for traffic prediction that eliminates the need for manual hyper-parameter tuning. 2. Introduces a federated representor with a client-aligned adapter to distill local data into a compact, globally shared prompt matrix, inspired by prompt learning. 3. Demonstrates through extensive experiments that AutoFed consistently achieves superior performance across diverse real-world traffic prediction scenarios.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e572784e0dff554ff14fce989febaa7869bdfb488d60b6fd86ebd7e98cdf0bf7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e572784e0dff554ff14fce989febaa7869bdfb488d60b6fd86ebd7e98cdf0bf7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes AutoFed, a manual-free Personalized Federated Learning framework for traffic prediction that uses a client-aligned adapter to generate a shared prompt matrix, enabling knowledge sharing while preserving local specificity. Experiments on real-world datasets show that AutoFed achieves superior performance without requiring manual hyper-parameter tuning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] AI-Driven Acoustic Voice Biomarker-Based Hierarchical Classification of Benign Laryngeal Voice Disorders from Sustained Vowels</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [medical audio classification], [hierarchical classification, acoustic biomarkers, mel-spectrograms, voice disorders, sustained vowels]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mohsen Annabestani, Samira Aghadoost, Anais Rameau, Olivier Elemento, Gloria Chia-Yi Chiang</p>
</li>
<li class="">
<p><strong>institution:</strong> Weill Cornell Medicine</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24628" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24628</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel three-stage hierarchical machine learning framework for voice disorder classification that mirrors clinical triage workflows, integrating deep spectral features with interpretable acoustic biomarkers. 2. The proposed system outperforms flat multi-class classifiers and state-of-the-art pre-trained self-supervised audio models (HuBERT, HeAR) on the task of classifying benign laryngeal disorders from sustained vowels. 3. Demonstrates the potential of combining deep learning representations with clinically interpretable features to enhance transparency and alignment for scalable, non-invasive vocal health screening and monitoring.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a53018a32ff94f55161f7c3e6f57843d9d248636c6146e968b0832ca7fdb34b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a53018a32ff94f55161f7c3e6f57843d9d248636c6146e968b0832ca7fdb34b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a hierarchical AI framework to classify benign laryngeal voice disorders from short, sustained vowel recordings. The method uses a three-stage pipeline combining CNN-derived mel-spectrogram features with interpretable acoustic biomarkers, outperforming standard multi-class and pre-trained audio models. The results highlight the framework&#x27;s potential as a scalable tool for early voice disorder screening and diagnostic triage.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] A Scalable Framework for logP Prediction: From Terabyte-Scale Data Integration to Interpretable Ensemble Modeling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [cluster infrastructure], [byte-offset indexing, ensemble modeling, SHAP analysis, heteroskedasticity, stratified modeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Malikussaid, Septian Caesar Floresko, Ade Romadhony, Isman Kurniawan, Warih Maharani, Hilal Hudan Nuha</p>
</li>
<li class="">
<p><strong>institution:</strong> Telkom University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24643" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24643</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a novel computational infrastructure for terabyte-scale data integration, achieving a 740-fold speedup in processing time using a byte-offset indexing architecture. 2. Conducted a comprehensive analysis revealing the multivariate nature of lipophilicity, identifying molecular weight as the most important global predictor via SHAP analysis, despite its weak bivariate correlation. 3. Proposed and validated a stratified modeling strategy (specialized models for drug-like vs. extreme molecules) that achieved optimal predictive performance, demonstrating the competitiveness of well-curated descriptor-based ensemble models with graph neural networks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad416775f59ba17a28f8c4aa4f177b114e9e8b7b5d9ab0210d586ba595ee51e6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad416775f59ba17a28f8c4aa4f177b114e9e8b7b5d9ab0210d586ba595ee51e6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a scalable framework for predicting molecular lipophilicity (logP). It introduces a high-performance data integration infrastructure and employs tree-based ensemble models with a stratified strategy, achieving robust prediction accuracy and providing insights into key molecular descriptors. The work shows that carefully engineered traditional machine learning models can remain competitive with advanced neural architectures for this task.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robot navigation], [hybrid motion planning, deep reinforcement learning, entity-aware reward, graph-based global planner, collision avoidance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yury Kolomeytsev, Dmitry Golembiovsky</p>
</li>
<li class="">
<p><strong>institution:</strong> Lomonosov Moscow State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24651" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24651</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes HMP-DRL, a hybrid framework integrating a graph-based global planner with a local DRL policy via checkpoints. 2. Introduces an entity-aware reward structure for the local planner to ensure social compliance by adjusting safety based on agent type. 3. Validates the method in a realistic simulation, showing superior performance in success rate, collision rate, and time to goal.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6963a6e6a0df2bf9e5857d6154c70386d9271eb85763359a237ce12c4881bec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6963a6e6a0df2bf9e5857d6154c70386d9271eb85763359a237ce12c4881bec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes HMP-DRL, a hybrid motion planning framework that combines a graph-based global planner for long-range pathfinding with a local Deep Reinforcement Learning policy for reactive, socially-compliant navigation. The method uses checkpoints to integrate the global path and an entity-aware reward function to dynamically adjust to different moving agents. Experiments in realistic simulation show it outperforms other methods in key navigation metrics, enhancing safety and reliability in complex environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] HeteroHBA: A Generative Structure-Manipulating Backdoor Attack on Heterogeneous Graphs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [adversarial machine learning], [heterogeneous graph neural networks, backdoor attack, adaptive instance normalization, maximum mean discrepancy, node classification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Honglin Gao, Lan Zhao, Junhao Ren, Xiang Li, Gaoxi Xiao</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24665" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24665</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A generative backdoor attack framework (HeteroHBA) for heterogeneous graphs that selects influential trigger attachment points and synthesizes diverse trigger features/connections. 2. A stealthiness enhancement method combining Adaptive Instance Normalization (AdaIN) and Maximum Mean Discrepancy (MMD) loss to align trigger features with benign statistics. 3. A bilevel optimization objective that jointly maximizes attack success rate and preserves clean model accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4ba54f6485be7540924a3b9c9f4089fbeb975b730fbabfbde211ba1bfb4ba09c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4ba54f6485be7540924a3b9c9f4089fbeb975b730fbabfbde211ba1bfb4ba09c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes HeteroHBA, a generative backdoor attack method for heterogeneous graph neural networks (HGNNs) that manipulates graph structure and node features to stealthily poison the model. The method uses saliency-based screening and distribution alignment techniques to improve attack effectiveness and stealth. Experiments show HeteroHBA achieves higher attack success than baselines while maintaining clean accuracy, demonstrating significant security risks in heterogeneous graph learning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Mobility-Assisted Decentralized Federated Learning: Convergence Analysis and A Data-Driven Approach</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [decentralized federated learning, user mobility, convergence analysis, data heterogeneity, wireless networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Reza Jahani, Md Farhamdur Reza, Richeng Jin, Huaiyu Dai</p>
</li>
<li class="">
<p><strong>institution:</strong> North Carolina State University, Zhejiang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24694" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24694</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Established the convergence of Decentralized Federated Learning (DFL) in sparse networks under user mobility, theoretically showing that even random movement can boost performance. 2. Proposed a novel DFL framework that utilizes mobile users with data-distribution-aware induced mobility patterns to enhance information propagation. 3. Provided extensive empirical validation and a comprehensive analysis of how network parameters influence DFL performance in mobile settings.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcaaf30fa2b4fe4f882f45ad096166cc0d22979181161bedc0b1504a8bf7549b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcaaf30fa2b4fe4f882f45ad096166cc0d22979181161bedc0b1504a8bf7549b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the role of user mobility in improving Decentralized Federated Learning (DFL) performance in sparse, heterogeneous networks. It proposes a data-driven DFL framework where mobile users follow induced trajectories to enhance information flow. Theoretical and experimental results show that mobility, even when random, significantly boosts DFL convergence and performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Causal Discovery with Mixed Latent Confounding via Precision Decomposition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [causal discovery], [latent confounding, precision matrix decomposition, DAG learning, identifiability, deconfounding]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amir Asiaee, Samhita Pal, James O&#x27;quinn, James P. Long</p>
</li>
<li class="">
<p><strong>institution:</strong> Vanderbilt University Medical Center, Johns Hopkins University, MD Anderson Cancer Center</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24696" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24696</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed DCL-DECOR, a modular pipeline using precision matrix decomposition to separate pervasive from local latent confounding. 2. Provided identifiability results characterizing recoverable causal structure under mixed confounding. 3. Demonstrated improved directed edge recovery in synthetic experiments over baseline methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0fdc733393b17ba1fd45e5626a557d39d8072f5494f4f2325aa8b588ae2f0ee_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0fdc733393b17ba1fd45e5626a557d39d8072f5494f4f2325aa8b588ae2f0ee_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses causal discovery in linear Gaussian systems with mixed latent confounding, where some confounders affect many variables and others only a few. It introduces DCL-DECOR, a method that decomposes the precision matrix to isolate pervasive confounders and then applies a correlated-noise DAG learner to recover the causal graph. Experiments show it consistently improves edge recovery compared to applying DAG learning directly to confounded data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Nested Learning: The Illusion of Deep Learning Architectures</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [learning theory], [nested learning, in-context learning, continual learning, associative memory, self-modifying model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, Vahab Mirrokni</p>
</li>
<li class="">
<p><strong>institution:</strong> Google Research (inferred from authors Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni, who are affiliated with Google)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24695" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24695</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Expressive Optimizers: Shows gradient-based optimizers are associative memory modules and proposes more expressive variants with deeper memory and learning rules. 2. Self-Modifying Learning Module: Presents a sequence model that learns to modify itself by learning its own update algorithm. 3. Continuum Memory System: Introduces a new memory formulation generalizing long/short-term memory, which is combined with the self-modifying model to create &quot;Hope&quot;, a continual learning module.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aac4e338b76a10773a96b12a072d809a81c99a79e63d8b40927cb078da7b7fdb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aac4e338b76a10773a96b12a072d809a81c99a79e63d8b40927cb078da7b7fdb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes a new learning paradigm called Nested Learning (NL), which frames machine learning models as nested optimization problems. This view explains the emergence of in-context learning and is used to design more expressive optimizers, a self-modifying model, and a new memory system, culminating in a continual learning module named &quot;Hope&quot; that shows promising results on various tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] BandiK: Efficient Multi-Task Decomposition Using a Multi-Bandit Framework</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-task learning], [multi-armed bandit, negative transfer, auxiliary task selection, multi-bandit framework, drug-target interaction]</p>
</li>
<li class="">
<p><strong>authors:</strong> András Millinghoffer, András Formanek, András Antos, Péter Antal</p>
</li>
<li class="">
<p><strong>institution:</strong> Budapest University of Technology and Economics, E-Group ICT Software Zrt., KU Leuven</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24708" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24708</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A three-stage method (BandiK) for efficient auxiliary task subset selection in multi-task learning, 2. Reduction of candidate auxiliary sets from exponential to linear complexity using pairwise transfer estimations, 3. A novel multi-bandit framework that exploits semi-overlapping arms across tasks to improve computational efficiency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e49fedc6c9b07cd38435ef8daff0ba16207d6088f86e049c77cd192822bd2cf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e49fedc6c9b07cd38435ef8daff0ba16207d6088f86e049c77cd192822bd2cf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces BandiK, a three-stage method using multi-armed bandits to efficiently select beneficial auxiliary task subsets in multi-task learning, reducing computational cost by estimating pairwise transfers and leveraging a multi-bandit structure. It is validated on a drug-target interaction benchmark, showing scalable performance for complex multi-task scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] FPGA Co-Design for Efficient N<!-- -->:M<!-- --> Sparse and Quantized Model Inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [N<!-- -->:M<!-- --> structured pruning, 4-bit quantization, systolic array, FPGA accelerator, hardware-software co-design]</p>
</li>
<li class="">
<p><strong>authors:</strong> Fen-Yu Hsieh, Yun-Chang Teng, Ding-Yong Hong, Jan-Jan Wu</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Information Science, Academia Sinica</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24713" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24713</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an automation framework and unified pipeline for applying N<!-- -->:M<!-- --> structured pruning and 4-bit integer quantization to compress LLMs. 2. Presents a hardware-software co-design method that generates a custom systolic-array-based FPGA accelerator for efficient inference. 3. Demonstrates the synergy of fine-grained sparsity and quantization, achieving significant reductions in storage and latency while offering flexibility beyond fixed hardware sparsity patterns.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a594f5eb4f19e15f5f182ee786cc270613c6a3d07553a78731a54b9a3ae90ea_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a594f5eb4f19e15f5f182ee786cc270613c6a3d07553a78731a54b9a3ae90ea_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high computational and memory demands of LLMs by proposing a hardware-software co-design framework. The method combines N<!-- -->:M<!-- --> structured pruning and 4-bit quantization to compress models, and implements a custom FPGA accelerator for efficient inference. The results show significant reductions in storage and latency, demonstrating the effectiveness of the approach for deployable LLM inference.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] From Trial to Deployment: A SEM Analysis of Traveler Adoptions to Fully Operational Autonomous Taxis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [human-computer interaction, transportation systems], [Structural Equation Modeling, autonomous taxis, user adoption, survey, latent constructs]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yutong Cai, Hua Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Singapore University of Technology and Design, Hefei University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24767" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24767</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducts a study on actual user behavior of a fully operational autonomous taxi service (Baidu Apollo Robotaxi in Wuhan), moving beyond hypothetical scenarios. 2. Identifies and validates six key latent psychological constructs (Trust &amp; Policy Support, Cost Sensitivity, Performance, Behavioral Intention, Lifestyle, Education) influencing adoption using Structural Equation Modeling. 3. Provides empirical evidence that Cost Sensitivity and Behavioral Intention are the strongest positive predictors of adoption frequency, offering insights for real-world policy and service deployment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8981785dc74c2d0e8bd1e9be7c18041b4ff904b4eac18913982db535cbd97bd4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8981785dc74c2d0e8bd1e9be7c18041b4ff904b4eac18913982db535cbd97bd4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study investigates the factors influencing traveler adoption of fully operational autonomous taxis by analyzing survey data from actual users of Baidu&#x27;s Apollo Robotaxi service in Wuhan, China. Using Structural Equation Modeling on 336 valid responses, the research identifies key psychological constructs and finds that Cost Sensitivity and Behavioral Intention are the strongest predictors of adoption frequency. The findings offer empirical support for policymaking and service design to scale autonomous taxi deployments in urban settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [monocular depth estimation], [adversarial attack, physics-in-the-loop optimization, sep-CMA-ES]</p>
</li>
<li class="">
<p><strong>authors:</strong> Takeru Kusakabe, Yudai Hirose, Mashiho Mukaida, Satoshi Ono</p>
</li>
<li class="">
<p><strong>institution:</strong> Kagoshima University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24792" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24792</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a projection-based adversarial attack method for monocular depth estimation models, using projected light as the perturbation. 2. Employs physics-in-the-loop (PITL) optimization to design perturbations in real-world environments, accounting for device specifications and disturbances. 3. Utilizes a distributed covariance matrix adaptation evolution strategy (sep-CMA-ES) for effective black-box optimization to generate adversarial examples.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5085ff109d1d570d16fe1fa964d0eb5cc890f0ebaed8dcfe0c3cdaa01d4f00bd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5085ff109d1d570d16fe1fa964d0eb5cc890f0ebaed8dcfe0c3cdaa01d4f00bd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a physical adversarial attack method for monocular depth estimation models. The method projects perturbation light onto a target object and uses physics-in-the-loop optimization with a distributed evolution strategy to create adversarial examples. Experiments confirmed the attack&#x27;s success, causing depth misestimations that made parts of objects disappear from the scene.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Self-Supervised Neural Architecture Search for Multimodal Deep Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [neural architecture search, self-supervised learning, multimodal fusion, contrastive learning, gradient-based search]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shota Suzuki, Satoshi Ono</p>
</li>
<li class="">
<p><strong>institution:</strong> Kagoshima University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24793" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24793</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a self-supervised learning (SSL) method for neural architecture search (NAS) specifically for multimodal deep neural networks. 2. Applies SSL comprehensively to both the architecture search and model pretraining processes, eliminating the need for labeled data during search. 3. Demonstrates that the method can successfully design network architectures from unlabeled training data, achieving performance comparable to supervised NAS methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a6c1a495cc476572529c11ecd2d19b6e7849693fc7a5c6941d8e99e91599cc5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a6c1a495cc476572529c11ecd2d19b6e7849693fc7a5c6941d8e99e91599cc5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that neural architecture search (NAS) for multimodal deep neural networks typically requires large amounts of labeled data. The authors propose a self-supervised learning method that uses contrastive learning to perform NAS without labeled data. Experimental results show the method can successfully design effective multimodal network architectures using only unlabeled data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Nonlinear Noise2Noise for Efficient Monte Carlo Denoiser Training</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image denoising], [Noise2Noise, Monte Carlo denoising, high dynamic range, tone mapping, Jensen gap]</p>
</li>
<li class="">
<p><strong>authors:</strong> Andrew Tinits, Stephen Mann</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Waterloo</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24794" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24794</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identified that certain nonlinear functions can be applied to noisy targets in Noise2Noise training without introducing significant bias. 2. Developed a theoretical framework to analyze the effects of nonlinearities and described a class of functions with minimal bias. 3. Demonstrated the method&#x27;s effectiveness for training Monte Carlo denoisers on HDR images using only noisy data, achieving results comparable to models trained with clean references.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a7ef23248abb9edf960ef0ea8dac0c52ee12d9db03ab8dd602dfd8c83c62645_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a7ef23248abb9edf960ef0ea8dac0c52ee12d9db03ab8dd602dfd8c83c62645_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of Noise2Noise training where applying nonlinear functions to noisy targets introduces bias. The authors propose a theoretical framework to identify low-bias nonlinearities and apply this to denoise high dynamic range Monte Carlo renderings using tone mapping. Their method, trained only on noisy data, achieves performance close to models trained with clean reference images.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Gradient Descent as Implicit EM in Distance-Based Neural Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [machine learning theory], [gradient descent, expectation-maximization, log-sum-exp, distance-based models, probabilistic inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alan Oursland</p>
</li>
<li class="">
<p><strong>institution:</strong> Unknown (Inferred from arXiv identifier only; no explicit affiliation provided)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24780" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24780</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a direct derivation showing that for objectives with a log-sum-exp structure, the gradient with respect to a distance is exactly the negative posterior responsibility, an algebraic identity. 2. Demonstrates that gradient descent on such objectives implicitly performs expectation-maximization, embedding inference within the optimization process. 3. Unifies learning in unsupervised mixture modeling, attention mechanisms, and supervised classification under a single mechanism, explaining observed Bayesian behaviors as a necessary consequence of objective geometry.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dedc1f2ebe4cae6339387562e130f21772d7b403da5771f698e488286b8cffce_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dedc1f2ebe4cae6339387562e130f21772d7b403da5771f698e488286b8cffce_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper shows that gradient descent on objectives with a log-sum-exp structure (common in neural networks) is algebraically equivalent to performing expectation-maximization, where the gradient directly corresponds to posterior responsibilities. This finding unifies learning across unsupervised, attention-based, and supervised regimes, explaining probabilistic behaviors like soft clustering as a fundamental property of the objective, not an emergent phenomenon.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] LeanCat: A Benchmark Suite for Formal Category Theory in Lean (Part I: 1-Categories)</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [theorem proving], [formal verification, category theory, benchmark, Lean, large language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rongge Xu, Hui Dai, Yiming Fu, Jiedong Jiang, Tianjiao Nie, Hongwei Wang, Junkai Wang, Holiverse Yang, Jiatong Yang, Zhi-Hao Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Southern University of Science and Technology, Westlake University, Xi&#x27;an Jiaotong-Liverpool University, The Chinese University of Hong Kong, Yanqi Lake Beijing Institute of Mathematical Sciences and Applications (BIMSA)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24796" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24796</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/sciencraft/LeanCat" target="_blank" rel="noopener noreferrer" class="">https://github.com/sciencraft/LeanCat</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces LeanCat, a benchmark for formal category theory in Lean, designed to stress-test abstraction and library-mediated reasoning. 2. Presents a curated dataset of 100 tasks with topic families and difficulty tiers, created via an LLM-assisted human grading process. 3. Demonstrates the benchmark&#x27;s utility by evaluating models and the LeanBridge method, showing current AI capabilities and providing a checkpoint for tracking progress.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4bb17bb33fa46697baca7f3b3a6262916453dd0a4bf5f92ff26cebdd7d681ffe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4bb17bb33fa46697baca7f3b3a6262916453dd0a4bf5f92ff26cebdd7d681ffe_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces LeanCat, a benchmark for formalizing category theory in Lean to better evaluate AI&#x27;s ability for abstract, library-based reasoning in mathematics. It presents a curated set of 100 tasks and evaluates models, finding low success rates, especially on harder problems, while showing that retrieval-augmented methods like LeanBridge can improve performance. The benchmark serves as a compact checkpoint for tracking progress in research-level formal theorem proving.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] DTI-GP: Bayesian operations for drug-target interactions using deep kernel Gaussian processes</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [drug-target interaction prediction], [Gaussian processes, deep kernel learning, Bayesian inference, drug-target interaction, Bayesian precedence matrix]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bence Bolgár, András Millinghoffer, Péter Antal</p>
</li>
<li class="">
<p><strong>institution:</strong> None</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24810" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24810</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed DTI-GP, a deep kernel Gaussian process architecture for drug-target interaction prediction that combines neural embeddings with Bayesian inference. 2. Introduced novel Bayesian operations, including classification with rejection, top-K selection, and ranking, enabled by sampling from the predictive distribution. 3. Demonstrated superior performance over state-of-the-art methods and enabled new evaluation metrics like a Bayesian accuracy-confidence enrichment score.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5db80a2ae688e29179cef03704df858a81feee85a09984daebbb301bc3ea93a0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5db80a2ae688e29179cef03704df858a81feee85a09984daebbb301bc3ea93a0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the need for precise probabilistic predictions in drug-target interaction (DTI) tasks. It proposes DTI-GP, a method that integrates deep neural embeddings for drugs and proteins with a Gaussian process module to enable scalable Bayesian inference. The approach outperforms existing methods and facilitates novel operations like confidence-aware rejection and ranking.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Unregularized Linear Convergence in Zero-Sum Game from Preference Feedback</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Nash equilibrium, Optimistic Multiplicative Weights Update, duality gap, last-iterate convergence, non-transitive preferences]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shulun Chen, Runlong Zhou, Zihan Zhang, Maryam Fazel, Simon S. Du</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, University of Washington, HKUST</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24818" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24818</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides the first convergence guarantee for Optimistic Multiplicative Weights Update (OMWU) in the Nash Learning from Human Feedback (NLHF) setting, showing it achieves last-iterate linear convergence to the original Nash equilibrium after a burn-in phase when a full-support NE exists. 2. Removes the prior assumption of Nash equilibrium uniqueness required by related work. 3. Identifies a novel marginal convergence behavior where the probability of rarely played actions grows exponentially from small values, leading to an exponentially better dependence on instance-dependent constants.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad0c58b07c97ac061247ceb0efd9cdb7a7663982811f67d347d7d126d158bebc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad0c58b07c97ac061247ceb0efd9cdb7a7663982811f67d347d7d126d158bebc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of aligning LLMs with non-transitive human preferences by framing it as a zero-sum game in the NLHF framework. It proposes using the unregularized Optimistic Multiplicative Weights Update (OMWU) algorithm and proves it achieves last-iterate linear convergence to the original Nash equilibrium without requiring uniqueness assumptions. The theoretical findings are supported by experiments on tabular and neural policy classes, demonstrating the method&#x27;s potential for LLM alignment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Discovering Coordinated Joint Options via Inter-Agent Relative Dynamics</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent reinforcement learning], [option discovery, graph Laplacian, state abstraction, coordination, Fermat state]</p>
</li>
<li class="">
<p><strong>authors:</strong> Raul D. Steleac, Mohan Sridharan, David Abel</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Edinburgh, Google DeepMind</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24827" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24827</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel joint-state abstraction method that compresses the state space while preserving information needed for discovering strongly coordinated behaviors. 2. Introduces the concept of a &quot;Fermat state&quot; (a fictitious state of maximal team alignment) and a &quot;spreadness&quot; measure to quantify team-level misalignment. 3. Employs a neural graph Laplacian estimator on this representation to derive options that capture state synchronization patterns between agents.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80fbcbdbf3ff73a3709f40a0baeb20e2f256323634f984656081dbac35380c30_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80fbcbdbf3ff73a3709f40a0baeb20e2f256323634f984656081dbac35380c30_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of discovering coordinated, temporally extended actions (options) in multi-agent systems, where the joint state space grows exponentially. The proposed method introduces a joint-state abstraction based on agent state synchronization, using a &quot;Fermat state&quot; and &quot;spreadness&quot; measure, and then applies a neural graph Laplacian estimator to discover options. The resulting options demonstrate stronger downstream coordination capabilities compared to existing methods in multi-agent domains.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] AODDiff: Probabilistic Reconstruction of Aerosol Optical Depth via Diffusion-based Bayesian Inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [diffusion-based Bayesian inference, corruption-aware training, decoupled annealing posterior sampling, uncertainty quantification, spatiotemporal prior]</p>
</li>
<li class="">
<p><strong>authors:</strong> Linhao Fan, Hongqiang Fang, Jingyang Dai, Yong Jiang, Qixing Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology of China, National Institute of Standards and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24847" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24847</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes AODDiff, a probabilistic reconstruction framework using diffusion-based Bayesian inference to learn a spatiotemporal prior for AOD fields. 2. Introduces a corruption-aware training strategy to learn the prior from naturally incomplete data, eliminating the need for complete training data. 3. Employs a decoupled annealing posterior sampling strategy to effectively integrate heterogeneous observations as constraints for flexible task adaptation without retraining.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9c1c9e0ed0baf5b503de97c6a15b2d7601232aba8c5c2f231a32ee44d046afcf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9c1c9e0ed0baf5b503de97c6a15b2d7601232aba8c5c2f231a32ee44d046afcf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes AODDiff, a diffusion-based Bayesian inference framework for probabilistically reconstructing Aerosol Optical Depth fields. It learns a spatiotemporal prior from incomplete data and uses a novel sampling strategy to integrate observations for tasks like downscaling and inpainting. Experiments show it maintains high fidelity and enables uncertainty quantification.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Characterization of Transfer Using Multi-task Learning Curves</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-task learning], [multi-task learning curves, task affinity grouping, transfer effects, inductive inference, foundation models]</p>
</li>
<li class="">
<p><strong>authors:</strong> András Millinghoffer, Bence Bolgár, Péter Antal</p>
</li>
<li class="">
<p><strong>institution:</strong> Budapest University of Technology and Economics</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24866" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24866</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes using multi-task learning curves (approximating performance over varying sample sizes) as a fundamental method to characterize transfer effects, complementing gradient-based training analysis. 2. Describes an efficient method to approximate these multi-task learning curves, analogous to the Task Affinity Grouping method. 3. Compares statistical (learning curve) and computational (training-based) approaches to transfer, finding the former has lower compute cost while the latter has better power and broader applicability, and demonstrates its utility on a drug-target interaction dataset and for analyzing foundation models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7a18c81d007e0d8e8e143a6ef66be8aaaafd4edddc20571b598040702aaa801_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7a18c81d007e0d8e8e143a6ef66be8aaaafd4edddc20571b598040702aaa801_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes characterizing transfer effects in machine learning by analyzing multi-task learning curves, which model inductive performance as sample size varies, rather than solely through gradient updates during training. It introduces an efficient method to approximate these curves and compares this statistical approach to traditional computational methods. The results show that learning curves effectively capture multi-task learning effects and can delineate transfer in foundation models, offering a complementary and more computationally efficient perspective.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] mHC: Manifold-Constrained Hyper-Connections</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [Hyper-Connections, residual connection, identity mapping, manifold constraint, training stability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhenda Xie, Yixuan Wei, Huanqi Cao, Chenggang Zhao, Chengqi Deng, Jiashi Li, Damai Dai, Huazuo Gao, Jiang Chang, Liang Zhao, Shangyan Zhou, Zhean Xu, Zhengyan Zhang, Wangding Zeng, Shengding Hu, Yuqing Wang, Jingyang Yuan, Lean Wang, Wenfeng Liang</p>
</li>
<li class="">
<p><strong>institution:</strong> DeepSeek-AI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24880" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24880</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Manifold-Constrained Hyper-Connections (mHC), a framework that projects the residual connection space onto a specific manifold to restore the identity mapping property compromised by Hyper-Connections (HC). 2. Incorporates rigorous infrastructure optimization to address the memory access overhead and ensure training efficiency. 3. Demonstrates that mHC enables effective large-scale training with tangible performance improvements and superior scalability, offering a flexible and practical extension of HC.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7219c6945df5dfb5231231a93ccf8e3cf155e38527f2c4071501eaae05a8b7ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7219c6945df5dfb5231231a93ccf8e3cf155e38527f2c4071501eaae05a8b7ac_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that Hyper-Connections (HC), while improving performance, lose the identity mapping property of standard residual connections, leading to training instability and memory overhead. To solve this, the authors propose Manifold-Constrained Hyper-Connections (mHC), which projects HC&#x27;s connection space onto a manifold to restore identity mapping and includes infrastructure optimizations. Empirical results show mHC is effective for scalable training, offering better performance and stability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] PRISM: A hierarchical multiscale approach for time series forecasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [hierarchical modeling, multiscale decomposition, wavelet transform, tree-based partitioning, multivariate forecasting]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zihao Chen, Alexandre Andre, Wenrui Ma, Ian Knight, Sergey Shuvaev, Eva Dyer</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Pennsylvania</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24898" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24898</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/nerdslab/prism" target="_blank" rel="noopener noreferrer" class="">https://github.com/nerdslab/prism</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes PRISM, a novel hierarchical forecasting method using a learnable tree-based partitioning of time series signals. 2. Introduces a joint time-frequency decomposition (e.g., wavelets) at each tree level to extract and aggregate scale-specific features. 3. Demonstrates a lightweight and flexible framework that outperforms state-of-the-art methods on benchmark datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a39698d43f8debac5a07c7c8b7b2c886add93fdb385fed95a191b3257708859_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a39698d43f8debac5a07c7c8b7b2c886add93fdb385fed95a191b3257708859_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of forecasting time series with multiscale features by proposing PRISM, a method that uses a learnable tree structure to hierarchically partition the signal and apply time-frequency transforms at each level. This approach jointly captures global trends and local dynamics, leading to improved forecasting accuracy. Experiments show that PRISM outperforms existing state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Spectral Graph Neural Networks for Cognitive Task Classification in fMRI Connectomes</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [neuroimaging analysis], [Graph Neural Network, Spectral Convolution, Graph Fourier Transform, fMRI Connectome, Cognitive Classification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Debasis Maji, Arghya Banerjee, Debaditya Barman</p>
</li>
<li class="">
<p><strong>institution:</strong> Visva-Bharati, Institute of Engineering &amp; Management</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24901" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24901</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/gnnplayground/SpectralBrainGNN" target="_blank" rel="noopener noreferrer" class="">https://github.com/gnnplayground/SpectralBrainGNN</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed SpectralBrainGNN, a novel spectral convolution framework for brain network analysis. 2. Applied Graph Fourier Transform (GFT) computed via normalized Laplacian eigendecomposition to model functional connectivity. 3. Demonstrated state-of-the-art cognitive task classification performance (96.25% accuracy) on the HCP-Task dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef6720522974da991b45f55d20e9895bdadedeae152a749a83e04947607b62bd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef6720522974da991b45f55d20e9895bdadedeae152a749a83e04947607b62bd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses cognitive task classification from fMRI brain networks. It proposes SpectralBrainGNN, a spectral graph neural network model based on Graph Fourier Transforms, to capture complex connectivity patterns. The method achieves high classification accuracy on a standard dataset, showing its effectiveness for decoding brain states.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Frequent subgraph-based persistent homology for graph classification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph representation learning], [persistent homology, frequent subgraph mining, graph neural networks, graph filtration, topological data analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xinyang Chen, Amaël Broustet, Guoting Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Harbin Institute of Technology, Shenzhen; Université de Lille, Laboratoire Painlevé (CNRS UMR 8524); Great Bay University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24917" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24917</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a novel Frequent Subgraph Filtration (FSF) method to generate frequency-based persistent homology features for graphs. 2. Developed two graph classification frameworks: an FPH-based machine learning model (FPH-ML) and a hybrid framework integrating FPH with GNNs (FPH-GNNs). 3. Provided theoretical analysis and experimental validation showing performance improvements over baseline methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/027326f2cb71708e2df7a0cf125f5d22c8db2742b7d53533a738ac9ae412b01d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/027326f2cb71708e2df7a0cf125f5d22c8db2742b7d53533a738ac9ae412b01d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of existing persistent homology methods on graphs, which rely on simple filtrations and miss recurring structural patterns. The authors propose a Frequent Subgraph Filtration (FSF) to extract richer topological features and integrate them into both traditional ML and GNN models for graph classification. Experiments show that the proposed methods achieve competitive or superior accuracy, with the hybrid FPH-GNN framework yielding significant performance gains over standard GNN backbones.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Adaptive Dependency-aware Prompt Optimization Framework for Multi-Step LLM Pipeline</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [prompt optimization, multi-step LLM pipeline, Shapley value, text-gradient estimation, dependency modeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Minjun Zhao, Xinyu Zhang, Shuai Zhang, Deyang Li, Ruifeng Shi</p>
</li>
<li class="">
<p><strong>institution:</strong> Huawei Poisson Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24933" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24933</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ADOPT, a framework that explicitly models the dependency between each LLM step and the final task outcome for precise text-gradient estimation. 2. Decouples textual gradient estimation from gradient updates, reducing complex multi-prompt optimization to flexible single-prompt optimization steps. 3. Employs a Shapley-based mechanism to adaptively allocate optimization resources across different pipeline steps.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0120693b7eaf05e640c60779fef913238cda72212cee4971942ac26a248c12d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0120693b7eaf05e640c60779fef913238cda72212cee4971942ac26a248c12d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of jointly optimizing prompts in multi-step LLM pipelines, where missing step-level supervision and inter-step dependencies make optimization difficult. It proposes ADOPT, an Adaptive Dependency-aware Prompt Optimization framework that models step dependencies for precise gradient estimation and uses a Shapley-based resource allocation mechanism. Experiments show ADOPT is effective and robust, consistently outperforming existing prompt optimization methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Iterative Deployment Improves Planning Skills in LLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [iterative deployment, implicit reward, data curation, planning, fine-tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Augusto B. Corrêa, Yoav Gelberg, Luckeciano C. Melo, Ilia Shumailov, André G. Pereira, Yarin Gal</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Oxford, AI Sequrity Company, UFRGS</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24940" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24940</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that iterative deployment and fine-tuning on curated user data significantly improves LLM planning skills, including emergent generalization to longer plans. 2. Provides a theoretical analysis showing iterative deployment effectively implements an outer-loop reinforcement learning process with an implicit reward function. 3. Highlights the AI safety implications of this implicit training regime and positions it as an alternative to explicit RL training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8da35d1ea681d386cec51c012c9f81bb54c6876b6c1e632e59874f77690cd1a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8da35d1ea681d386cec51c012c9f81bb54c6876b6c1e632e59874f77690cd1a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper shows that repeatedly deploying LLMs and fine-tuning them on curated data from previous deployments significantly improves their planning capabilities. This process is analyzed as an implicit form of reinforcement learning, which raises safety concerns due to the undefined reward function and offers an alternative training paradigm based on data curation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [information retrieval], [relevance assessment, benchmark, long-tail, visual salience, e-commerce]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chenji Lu, Zhuo Chen, Hui Zhao, Zhenyi Wang, Pengjie Wang, Jian Xu, Bo Zheng</p>
</li>
<li class="">
<p><strong>institution:</strong> Taobao &amp; Tmall Group of Alibaba</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24943" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24943</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes RAIR, a comprehensive Chinese benchmark for e-commerce relevance assessment derived from real-world scenarios. 2. Establishes a standardized evaluation framework with universal rules to address the lack of standardized metrics. 3. Introduces a dataset with three specialized subsets (general, long-tail hard, visual salience) to evaluate fundamental, challenging, and multimodal capabilities.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01d0a7f153d6f84b77a35da0a0f62dec9a8af10bfb23f1a8a481697233cbe992_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01d0a7f153d6f84b77a35da0a0f62dec9a8af10bfb23f1a8a481697233cbe992_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes RAIR, a rule-aware benchmark for e-commerce search relevance assessment, to address the lack of complex and standardized evaluation datasets. It introduces a comprehensive dataset with three subsets to test different model capabilities. Experiments on 14 models show RAIR is challenging, with GPT-5 performing best, and it serves as a new industry benchmark.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Lyapunov certificates, exponential stability, multi-step learning, actor-critic, maximum entropy RL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yongwei Zhang, Yuanzhe Xing, Quan Quan, Zhikun She</p>
</li>
<li class="">
<p><strong>institution:</strong> Beihang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24955" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24955</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel framework (MSACL) that integrates exponential stability theory with maximum entropy RL via multi-step Lyapunov certificate learning, using off-policy data to learn certificates that satisfy theoretical stability conditions. 2. Introduces Exponential Stability Labels (ESL) and a λ-weighted aggregation mechanism to effectively balance the bias-variance trade-off in multi-step learning. 3. Guides policy optimization with a stability-aware advantage function to ensure the learned policy promotes rapid Lyapunov descent, achieving provable stability and robustness under simple rewards.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea102a46402567fc13871b6bc5f72e6c07e79e6ee87e8349aee1c18c8fc9627e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea102a46402567fc13871b6bc5f72e6c07e79e6ee87e8349aee1c18c8fc9627e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes MSACL, a model-free reinforcement learning framework that ensures provable exponential stability by learning Lyapunov certificates from multi-step data and guiding policy optimization with a stability-aware advantage. It demonstrates superior performance over baseline and state-of-the-art Lyapunov-based RL methods across six benchmarks, achieving rapid convergence and robustness with simple rewards. The work establishes a link between Lyapunov theory and actor-critic frameworks for verifiably safe control.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] ProDM: Synthetic Reality-driven Property-aware Progressive Diffusion Model for Coronary Calcium Motion Correction in Non-gated Chest CT</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [diffusion models, motion correction, synthetic data, coronary artery calcium, non-gated CT]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xinran Gong, Gorkem Durak, Halil Ertugrul Aktas, Vedat Cicek, Jinkui Hao, Ulas Bagci, Nilay S. Shah, Bo Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> Northwestern University, Georgia Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24948" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24948</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A CAC motion simulation data engine that synthesizes realistic non-gated CT acquisitions from gated CTs for supervised training without paired data. 2. A property-aware learning strategy that incorporates calcium-specific priors via a differentiable consistency loss to preserve lesion integrity. 3. A progressive correction scheme that gradually reduces motion artifacts across diffusion steps to enhance stability and calcium fidelity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd53987de700ad7024fad5ea5d53da57ecb375707cda1bff594c7264ec4ca118_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd53987de700ad7024fad5ea5d53da57ecb375707cda1bff594c7264ec4ca118_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes ProDM, a diffusion model framework to correct motion artifacts in coronary calcium lesions from non-gated chest CT scans. The method uses a synthetic data engine for training, incorporates calcium-specific priors, and applies progressive correction. Experiments show it improves scoring accuracy, lesion fidelity, and risk stratification compared to baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Semi-overlapping Multi-bandit Best Arm Identification for Sequential Support Network Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-armed bandits], [semi-overlapping multi-bandit, best arm identification, sequential support network learning, GapE algorithm, sample complexity]</p>
</li>
<li class="">
<p><strong>authors:</strong> András Antos, András Millinghoffer, Péter Antal</p>
</li>
<li class="">
<p><strong>institution:</strong> Budapest University of Technology and Economics, E-Group ICT Software Zrt.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24959" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24959</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new pure-exploration model called the semi-overlapping multi-bandit (SOMMAB) for Sequential Support Network Learning (SSNL)., 2. Develops a generalized GapE algorithm for the SOMMAB setting., 3. Derives new exponential error bounds that improve the best-known constant in the exponent and scale linearly with the degree of overlap, showing sample complexity gains from shared evaluations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1ccf2e1312507d052da8a3c6c2b6fb042432d3f13e5efa2572b5d2cd1dff292_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1ccf2e1312507d052da8a3c6c2b6fb042432d3f13e5efa2572b5d2cd1dff292_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a new framework called Sequential Support Network Learning (SSNL) and models it as a semi-overlapping multi-bandit (SOMMAB) problem, where a single evaluation provides feedback to multiple bandits. The authors develop a generalized GapE algorithm for SOMMABs and prove new, improved error bounds that demonstrate significant sample-complexity reductions due to structural overlap.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Efficiently Estimating Data Efficiency for Language Model Fine-tuning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [fine-tuning], [data efficiency, gradient cosine similarity, low-confidence examples, fine-tuning scaling, annotation cost]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gyung Hyun Je, Colin Raffel</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Toronto</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24991" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24991</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/r-three/dataefficiency" target="_blank" rel="noopener noreferrer" class="">https://github.com/r-three/dataefficiency</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a concrete metric to quantify a task&#x27;s data efficiency for LLM fine-tuning. 2. Proposed a method using gradient cosine similarity of low-confidence examples to predict data efficiency from a small number of samples. 3. Validated the approach on diverse tasks, achieving 8.6% prediction error and significantly reducing unnecessary annotations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cd5fa15acf63fdcf1cd6c628775da72e936b364cb45a18adddcde2e329ce8c48_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cd5fa15acf63fdcf1cd6c628775da72e936b364cb45a18adddcde2e329ce8c48_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of unknown data efficiency for fine-tuning LLMs, which leads to costly annotation cycles. It proposes predicting data efficiency using gradient cosine similarity from low-confidence examples, based on a small labeled set. The method achieves low prediction error and can eliminate hundreds of unnecessary annotations per task.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Attribution-Guided Distillation of Matryoshka Sparse Autoencoders</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [sparse autoencoders, mechanistic interpretability, gradient attribution, feature distillation, matryoshka]</p>
</li>
<li class="">
<p><strong>authors:</strong> Cristina P. Martin-Linares, Jonathan P. Ling</p>
</li>
<li class="">
<p><strong>institution:</strong> Johns Hopkins University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24975" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24975</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Distilled Matryoshka Sparse Autoencoders (DMSAEs), a novel training pipeline for distilling a compact, reusable core of consistent features from sparse autoencoders. 2. Proposes an iterative, attribution-guided distillation cycle that uses gradient × activation to select the most useful features based on their contribution to next-token loss. 3. Demonstrates empirically that the distilled core improves SAEBench metrics and enables the transfer of consistent latent features across different sparsity levels.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a691c55852370e5366257008c9914761d773cede53c49ece8c170d5653fcbac4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a691c55852370e5366257008c9914761d773cede53c49ece8c170d5653fcbac4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of feature redundancy and inconsistency in sparse autoencoders (SAEs) used for mechanistic interpretability. It proposes Distilled Matryoshka Sparse Autoencoders (DMSAEs), a method that iteratively distills a core set of useful features using gradient-based attribution. The results show that this approach yields a stable, compact feature set that improves performance and can be transferred across training runs and sparsity levels.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [embodied vision-language reasoning], [low-light vision, embodied question answering, vision-language models, image enhancement, benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yohan Park, Hyunwoo Ha, Wonjun Jo, Tae-Hyun Oh</p>
</li>
<li class="">
<p><strong>institution:</strong> Korea Advanced Institute of Science and Technology (KAIST), Pohang University of Science and Technology (POSTECH)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24985" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24985</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces DarkEQA, the first benchmark for evaluating Embodied Question Answering (EQA) under multi-level, physics-based low-light conditions. 2. Features a physically faithful degradation pipeline that models illumination drop and sensor noise in linear RAW space, followed by an ISP-inspired renderer. 3. Systematically evaluates and reveals the limitations of state-of-the-art VLMs and the effectiveness of Low-Light Image Enhancement (LLIE) models as pre-processors in this challenging scenario.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f144c0ff2baabb069f605975462919cef76b3f54919a8c9db67dab0432973003_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f144c0ff2baabb069f605975462919cef76b3f54919a8c9db67dab0432973003_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies a gap in evaluating Vision-Language Models (VLMs) for embodied agents under low-light conditions and proposes DarkEQA, a new benchmark that simulates realistic dark environments. The benchmark uses a physics-based image degradation model to test VLM robustness and the utility of image enhancement techniques. The evaluation reveals significant performance drops in VLMs under low-light, highlighting a critical area for improvement in robust embodied AI.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Diffusion Language Models are Provably Optimal Parallel Samplers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [diffusion language models, parallel sampling, chain-of-thought, remasking, revision]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haozhe Jiang, Nika Haghtalab, Lijie Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Berkeley</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.25014" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.25014</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formalized a model of parallel sampling and proved that DLMs with CoT can simulate any parallel sampling algorithm with an optimal number of sequential steps. 2. Showed that enabling remasking or revision with CoT allows DLMs to simulate any parallel sampling algorithm with optimal space complexity. 3. Established a strict expressivity gap, proving DLMs with revision or remasking are strictly more expressive than those without.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43c65e3d030ce4b9471215a4735f2217f9be018da8e7b5ecc092a62d1394440b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43c65e3d030ce4b9471215a4735f2217f9be018da8e7b5ecc092a62d1394440b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper provides a theoretical foundation for the efficiency of Diffusion Language Models (DLMs) as parallel samplers. It proves that DLMs augmented with chain-of-thought reasoning can simulate any parallel sampling algorithm with optimal sequential steps and, when further equipped with token remasking or revision, also achieve optimal space complexity. The results theoretically justify DLMs as highly efficient parallel samplers and advocate for enabling revision capabilities in such models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Convergence of the generalization error for deep gradient flow methods for PDEs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scientific machine learning], [deep gradient flow methods, generalization error, partial differential equations, neural networks, approximation error]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chenguang Liu, Antonis Papapantoleon, Jasper Rou</p>
</li>
<li class="">
<p><strong>institution:</strong> (Inferred from author names and typical affiliations in the field; specific institutions not listed on the first page. Could be academic institutions like universities.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.25017" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.25017</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a rigorous mathematical framework for analyzing the generalization error of Deep Gradient Flow Methods (DGFMs) for solving PDEs. 2. Proves that the approximation error for PDE solutions using neural networks converges to zero as the network width increases. 3. Derives and analyzes the gradient flow dynamics in the wide network limit, showing the training error converges as training time increases.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ade90edaf9660be616e42f9a752fe8dcdd594d87db2454f913bca0a983f2d8bc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ade90edaf9660be616e42f9a752fe8dcdd594d87db2454f913bca0a983f2d8bc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper establishes a mathematical foundation for Deep Gradient Flow Methods (DGFMs) used to solve high-dimensional PDEs. It decomposes the generalization error into approximation and training components, proving both converge to zero as the number of neurons and training time go to infinity. The main conclusion is that DGFMs are theoretically sound, with provable convergence of the total error.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning from human feedback (RLHF)], [preference strength, reward modeling, sample efficiency, utility difference, Pearson Distance Correlation (PDC)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Timo Kaufmann, Yannick Metz, Daniel Keim, Eyke Hüllermeier</p>
</li>
<li class="">
<p><strong>institution:</strong> LMU Munich, University of Konstanz</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.25023" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.25023</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. ResponseRank, a novel method that robustly learns preference strength by leveraging locally valid relative strength signals. 2. Empirical evidence of improved sample efficiency and robustness across diverse tasks. 3. The Pearson Distance Correlation (PDC), a novel metric that isolates cardinal utility learning from ordinal accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/89532a897b8fa7db270c20a989bfbc8848f6809665ba966305a08daa55266fce_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/89532a897b8fa7db270c20a989bfbc8848f6809665ba966305a08daa55266fce_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the limitation of standard RLHF, which only captures the direction of a preference but not its strength. It proposes ResponseRank, a method that learns preference strength by ranking responses using relative differences in noisy proxy signals (like response times) within local strata. The method demonstrates improved sample efficiency and robustness across synthetic, language modeling, and RL control tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Generative Classifiers Avoid Shortcut Solutions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models], [generative classifiers, spurious correlations, distribution shift, diffusion models, autoregressive models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alexander C. Li, Ananya Kumar, Deepak Pathak</p>
</li>
<li class="">
<p><strong>institution:</strong> Carnegie Mellon University, Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.25034" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.25034</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/alexlioralexli/generative-classifiers" target="_blank" rel="noopener noreferrer" class="">https://github.com/alexlioralexli/generative-classifiers</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that generative classifiers (using class-conditional generative models) inherently avoid shortcut learning by modeling all features, not just spurious ones. 2. Shows that generative classifiers achieve state-of-the-art performance on multiple image and text distribution shift benchmarks without specialized techniques. 3. Provides a theoretical analysis in a Gaussian toy setting to explain the inductive biases and data conditions favoring generative classifiers.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f744eff83ad768a9dc5e431ef5b2d98baefe24c12d01fc980aa2fa92c3c21c65_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f744eff83ad768a9dc5e431ef5b2d98baefe24c12d01fc980aa2fa92c3c21c65_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of discriminative classifiers learning spurious shortcuts that fail under distribution shift. It proposes using generative classifiers, which model p(x|y), and finds they avoid shortcuts and achieve state-of-the-art robustness on standard benchmarks without needing specialized training tricks. The main conclusion is that generative classifiers offer a simple and effective alternative for building more robust models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Reliable and Resilient Collective Communication Library for LLM Training and Serving</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [communication &amp; networking], [fault-tolerant collective communication, multi-NIC failover, connection migration, bandwidth-aware load redistribution, resilient collective algorithms]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wei Wang, Nengneng Yu, Sixian Xiong, Zaoxing Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Maryland, College Park</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.25059" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.25059</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/r2cc-project/R-2CCL" target="_blank" rel="noopener noreferrer" class="">https://github.com/r2cc-project/R-2CCL</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A fault-tolerant communication library (R²CCL) that provides lossless, low-overhead failover by exploiting multi-NIC hardware. 2. Techniques including rapid connection migration, bandwidth-aware load redistribution, and resilient collective algorithms to maintain progress under network failures. 3. Demonstrated high robustness to NIC failures with minimal overhead (&lt;1% for training, &lt;3% for inference) and significant performance improvements over baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd037366831b8135233f491feff11095f79c30662f36cb517794f6588542c452_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd037366831b8135233f491feff11095f79c30662f36cb517794f6588542c452_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents R²CCL, a fault-tolerant collective communication library designed to handle network faults in large-scale LLM training and serving. It achieves low-overhead recovery through techniques like rapid connection migration and bandwidth-aware load redistribution. Evaluation shows it incurs minimal performance overhead and significantly outperforms existing solutions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] On the geometry and topology of representations: the manifolds of modular addition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [mechanistic interpretability], [modular addition, manifold hypothesis, topological analysis, circuit universality, attention mechanisms]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gabriela Moisescu-Pareja, Gavin McCracken, Harley Wiltzer, Vincent Létourneau, Colin Daniels, Doina Precup, Jonathan Love</p>
</li>
<li class="">
<p><strong>institution:</strong> McGill University, Mila, Université de Montréal, Leiden University, Google DeepMind</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.25060" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.25060</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that different neural network architectures (uniform vs. learnable attention) learn topologically and geometrically equivalent representations for modular addition, refuting prior claims of disparate circuits. 2. Introduces a methodology that studies learned representations as collective manifolds rather than interpreting individual neurons, applying tools from topology. 3. Provides statistical analysis across hundreds of circuits to show the similarity of learned modular addition algorithms under common deep learning paradigms, supporting the universality hypothesis.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17fc5efae344878642fe3a2ab7985eeb71b49efef00cf839b1bb65714a390090_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17fc5efae344878642fe3a2ab7985eeb71b49efef00cf839b1bb65714a390090_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper challenges the claim that different neural network architectures learn fundamentally different circuits for modular addition. By analyzing learned representations as manifolds using topological methods, the authors show that both uniform and trainable attention architectures implement the same underlying algorithm with equivalent representations. The work provides statistical evidence supporting the universality hypothesis in mechanistic interpretability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Many Minds from One Model: Bayesian Transformers for Population Intelligence</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [Bayesian Transformers, Variational Inference, Population Diversity, Normalization Layers, Wisdom of Crowds]</p>
</li>
<li class="">
<p><strong>authors:</strong> Diji Yang, Yi Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California Santa Cruz</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.25063" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.25063</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Population Bayesian Transformers (B-Trans), a method to convert a standard LLM into a Bayesian model by treating normalization layer biases as stochastic variables with a Gaussian variational approximation, enabling diverse model sampling from a single weight set. 2. Introduces sequence-level noise freezing to maintain temporal coherence within each sampled model instance&#x27;s generation, ensuring consistent behavior across tokens. 3. Demonstrates that aggregating predictions from a population of sampled B-Trans instances enhances exploration and decision-making, leading to superior semantic diversity and task performance in zero-shot generation, RLVR, and RL without labels.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae0113a2c263c7e9a33e3b5ce49ac7afb88b3a3baeb7fd88c121fac5ef4b745b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae0113a2c263c7e9a33e3b5ce49ac7afb88b3a3baeb7fd88c121fac5ef4b745b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the lack of diversity and exploration in deterministic LLMs by proposing B-Trans, which transforms a standard LLM into a Bayesian model by making normalization biases stochastic. This allows sampling diverse &quot;minds&quot; from one model, and aggregating their predictions improves performance and semantic variety in reasoning tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Scaling Open-Ended Reasoning to Predict the Future</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [language model forecasting], [open-ended forecasting, reinforcement learning, retrieval-augmented generation, calibration, Qwen3]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping</p>
</li>
<li class="">
<p><strong>institution:</strong> Max Planck Institute for Intelligent Systems, ELLIS Institute Tübingen, Tübingen AI Center, University of Tübingen</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.25070" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.25070</a></p>
</li>
<li class="">
<p><strong>code:</strong> /github (URL implied from first page content)</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A fully automated pipeline to synthesize a large-scale dataset (OpenForesight) for training language models on open-ended forecasting questions from news events. 2. A specialized forecasting system integrating retrieval and an improved RL reward function, trained on Qwen3, which prevents future information leakage. 3. The OpenForecaster 8B model, which demonstrates that specialized training improves accuracy, calibration, and consistency, matching larger proprietary models, with calibration benefits generalizing to other benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7433331ffccb9fb0f52db33a75b12ae808ae87c5410037274fcfdc5f22b3505_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7433331ffccb9fb0f52db33a75b12ae808ae87c5410037274fcfdc5f22b3505_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of training language models for open-ended future prediction. The authors propose an automated method to generate a large forecasting dataset from news and train a specialized model (OpenForecaster 8B) using retrieval and an improved RL reward. Their final model matches the performance of much larger proprietary models, showing improved prediction accuracy, calibration, and consistency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Coordinated Humanoid Manipulation with Choice Policies</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [imitation learning], [humanoid robot, teleoperation, choice policy, multimodal behavior, whole-body coordination]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haozhi Qi, Yen-Jen Wang, Toru Lin, Brent Yi, Yi Ma, Koushil Sreenath, Jitendra Malik</p>
</li>
<li class="">
<p><strong>institution:</strong> UC Berkeley</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.25072" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.25072</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://choice-policy.github.io" target="_blank" rel="noopener noreferrer" class="">https://choice-policy.github.io</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A modular teleoperation interface that decomposes humanoid control into intuitive submodules (e.g., hand-eye coordination, locomotion) for efficient, high-quality data collection. 2. The Choice Policy, a novel imitation learning architecture that generates multiple candidate actions and learns to score them, enabling fast inference and effective modeling of multimodal behaviors. 3. Empirical validation on real-world tasks (dishwasher loading, whiteboard wiping) showing superior performance over diffusion policies and behavior cloning, and highlighting the critical role of hand-eye coordination.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8fc3a4a5fc6ec972fc1d7ab23cbd63d6c1c8efc8d326140cc34f70ea5a5cb65_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8fc3a4a5fc6ec972fc1d7ab23cbd63d6c1c8efc8d326140cc34f70ea5a5cb65_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the challenge of achieving robust whole-body coordination for humanoid robots in unstructured environments. It proposes a system combining a modular teleoperation interface for data collection with a novel &quot;Choice Policy&quot; for imitation learning, which scores multiple candidate actions. Experiments on real-world tasks demonstrate that this approach outperforms baseline methods and that hand-eye coordination is crucial for success in long-horizon manipulation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Spike-Timing-Dependent Plasticity for Bernoulli Message Passing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [computational neuroscience], [spike-timing-dependent plasticity, Bayesian inference, message passing, factor graphs, spiking neural networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sepideh Adamiat, Wouter M. Kouw, Bert de Vries</p>
</li>
<li class="">
<p><strong>institution:</strong> TU Eindhoven, Lazy Dynamics B.V.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23728" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23728</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Bridging Bayesian inference and spike-based neural computation by designing spiking neural networks for Bernoulli message passing. 2. Employing spike-timing-dependent plasticity (STDP), a biologically plausible Hebbian learning rule, to train these networks. 3. Demonstrating the approach&#x27;s versatility by applying it to a factor graph example from coding theory for signal transmission over an unreliable channel.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a449542e05c85b78a1c2c36d2343549e26e8c17a9edfccf34c692fd2512f710c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a449542e05c85b78a1c2c36d2343549e26e8c17a9edfccf34c692fd2512f710c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper bridges Bayesian inference and spike-based neural activity by designing spiking neural networks that perform message passing for Bernoulli variables. The networks are trained using the biologically plausible spike-timing-dependent plasticity rule. The results show the network&#x27;s performance matches the true numerical solution, and the method is demonstrated on a coding theory problem.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Fitted Q Evaluation Without Bellman Completeness via Stationary Weighting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [off-policy evaluation, fitted Q-evaluation, Bellman completeness, stationary distribution, density ratio]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lars van der Laan, Nathan Kallus</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Washington, Netflix, Cornell University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23805" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23805</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identified the fundamental norm mismatch causing FQE&#x27;s reliance on Bellman completeness, 2. Proposed a simple fix by reweighting regression steps with an estimated stationary density ratio, 3. Provided strong evaluation guarantees without requiring realizability or Bellman completeness, avoiding geometric error blow-up.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0334a3bea1166b5e2cbda1d446e2561bf91e6e45af4da23da7bb2759aa9a5043_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0334a3bea1166b5e2cbda1d446e2561bf91e6e45af4da23da7bb2759aa9a5043_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the fragility of Fitted Q-evaluation (FQE) in off-policy reinforcement learning, which traditionally requires the strong assumption of Bellman completeness. The authors propose a simple modification to FQE by reweighting each regression step using an estimate of the stationary density ratio, aligning the optimization with the contractive norm of the Bellman operator. This enables robust policy evaluation guarantees even when the function class is not Bellman complete, maintaining the practicality of regression-based methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Quantum Error Mitigation with Attention Graph Transformers for Burgers Equation Solvers on NISQ Hardware</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [quantum error mitigation, attention graph neural network, NISQ hardware, Burgers equation, zero-noise extrapolation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Seyed Mohamad Ali Tousi, Adib Bazgir, Yuwen Zhang, G. N. DeSouza</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Missouri</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23817" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23817</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A hybrid quantum-classical framework for solving the viscous Burgers equation on NISQ hardware, using the Cole-Hopf transformation and Trotterized quantum circuits. 2. The creation of a large parametric dataset of noisy, ZNE-corrected, hardware, and classical solutions with circuit metadata for data-driven error mitigation. 3. A novel attention-based graph neural network model that ingests circuit features and noisy outputs to predict error-mitigated solutions, outperforming ZNE alone.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57bb32110f5a354755f846f1b5a7ab41ac3fc4a701e97b9b25486f2bab0c72eb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57bb32110f5a354755f846f1b5a7ab41ac3fc4a701e97b9b25486f2bab0c72eb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a hybrid quantum-classical framework enhanced with a learned error mitigation model to solve the Burgers equation on noisy quantum hardware. The method uses an attention graph neural network trained on a dataset of noisy quantum simulations to predict corrected solutions. The results show the learned model consistently reduces errors beyond standard zero-noise extrapolation techniques.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] A Test of Lookahead Bias in LLM Forecasts</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [machine learning for finance], [lookahead bias, pre-training data detection, forecast accuracy, Lookahead Propensity (LAP), statistical test]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhenyu Gao, Wenxi Jiang, Yutong Yan</p>
</li>
<li class="">
<p><strong>institution:</strong> The Chinese University of Hong Kong (CUHK Business School, Department of Finance)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23847" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23847</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel statistical test to detect lookahead bias in LLM-generated economic forecasts. 2. Introduces the concept of Lookahead Propensity (LAP), a metric estimating the likelihood a prompt was in the model&#x27;s training data. 3. Demonstrates the test&#x27;s application on real-world forecasting tasks (stock returns and capital expenditures) to assess forecast validity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3862b50bacdf9663a9e884b32c3a91e6601a609ccd21f8f622e4d85c8d4b3dee_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3862b50bacdf9663a9e884b32c3a91e6601a609ccd21f8f622e4d85c8d4b3dee_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper develops a statistical test to detect lookahead bias in LLM forecasts by correlating forecast accuracy with a new metric called Lookahead Propensity (LAP), which estimates if a prompt was in the training data. The method is applied to financial forecasting tasks, providing a cost-efficient tool to assess the reliability of LLM-generated predictions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Energy-Tweedie: Score meets Score, Energy meets Energy</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [Tweedie&#x27;s formula, energy score, elliptical distributions, score estimation, denoising]</p>
</li>
<li class="">
<p><strong>authors:</strong> Andrej Leban</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Michigan</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23818" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23818</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Extends Tweedie&#x27;s identity beyond the exponential family to a broad class of noising distributions (energy models/elliptical distributions). 2. Derives a fundamental identity connecting the Stein score of the noisy marginal to the (path-) derivative of the energy score. 3. Proposes a practical score estimation method based on this identity, using samples from the denoising posterior.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ff6ceee3e2856f05799a33cb8c1de1d291f58a0c5ebc90275070d02dddc961d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ff6ceee3e2856f05799a33cb8c1de1d291f58a0c5ebc90275070d02dddc961d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper connects the concepts of denoising and score estimation by extending Tweedie&#x27;s formula to elliptical distributions and deriving a new identity linking the energy score&#x27;s derivative to the Stein score. This allows for new applications in score estimation, noise parameter estimation, and enables the use of energy score models with a wider variety of noising distributions in diffusion model samplers.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Tensor Computing Interface: An Application-Oriented, Lightweight Interface for Portable High-Performance Tensor Network Applications</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [compiler &amp; ir], [tensor networks, high-performance computing, portable interface, tensor linear-algebra, Cytnx]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rong-Yang Sun, Tomonori Shirakawa, Hidehiko Kohshiro, D. N. Sheng, Seiji Yunoki</p>
</li>
<li class="">
<p><strong>institution:</strong> California State University Northridge, RIKEN</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23917" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23917</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Tensor Computing Interface (TCI), a lightweight, application-oriented API for framework-independent tensor network applications. 2. Provides a well-defined type system and a minimal, expressive set of core functions abstracting tensor objects and operations. 3. Demonstrates through numerical experiments that TCI enables seamless code migration across heterogeneous platforms while maintaining performance comparable to native implementations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa77bed89109740ba89e21e2beb0128aa40959ecc783f216c3104fb3a4a59038_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa77bed89109740ba89e21e2beb0128aa40959ecc783f216c3104fb3a4a59038_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the lack of portability in tensor network applications due to framework-specific dependencies. It proposes the Tensor Computing Interface (TCI), a lightweight API that abstracts tensor operations, enabling developers to write portable, high-performance code. The authors demonstrate that TCI-based applications can be migrated across different hardware and software backends without sacrificing performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] A multimodal Transformer for InSAR-based ground deformation forecasting with cross-site generalization across Europe</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [remote sensing image analysis], [InSAR, Transformer, ground deformation forecasting, cross-site generalization, multimodal learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wendong Yao, Binhua Huang, Soumyabrata Dev</p>
</li>
<li class="">
<p><strong>institution:</strong> ADAPT SFI Research Centre, University College Dublin</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23906" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23906</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a novel multimodal patch-based Transformer architecture for InSAR-based ground deformation nowcasting, integrating displacement snapshots with static kinematic indicators and temporal encodings. 2. Demonstrated superior performance of the proposed model over baseline models (CNN-LSTM, STGCN) on a test tile in eastern Ireland, achieving high accuracy (RMSE=0.90mm, R²=0.97). 3. Showcased strong cross-site generalization by training on one tile and applying the model without fine-tuning to five unseen European tiles, maintaining high performance (R²≥0.93) across diverse deformation patterns.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/36024dc5598b92b146557714c9e66eeb494b793bdcfaacb98b73cf6725cc8bfa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/36024dc5598b92b146557714c9e66eeb494b793bdcfaacb98b73cf6725cc8bfa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of forecasting ground deformation from InSAR time series data. It proposes a multimodal Transformer model that combines recent displacement maps with kinematic indicators and temporal features to predict the next displacement epoch. The model achieves high accuracy and demonstrates strong generalization across different geographic sites in Europe without requiring retraining.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Assessing generative modeling approaches for free energy estimates in condensed matter</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models], [free energy estimation, normalizing flows, generative modeling, Jarzynski equality, molecular simulation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Maximilian Schebek, Jiajun He, Emil Hoffmann, Yuanqi Du, Frank Noé, Jutta Rogal</p>
</li>
<li class="">
<p><strong>institution:</strong> Freie Universität Berlin, University of Cambridge, Cornell University, Microsoft Research AI for Science, Rice University, Flatiron Institute</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23930" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23930</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Systematic review and benchmarking of generative-model-based methods for free energy estimation in condensed matter systems., 2. Evaluation of discrete and continuous normalizing flows, FEAT, and the escorted Jarzynski equality on coarse-grained ice and Lennard-Jones solids., 3. Provides a quantitative framework comparing accuracy, data efficiency, cost, and scalability to guide method selection.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e09c407083c9249376083548ea23d622f173cbe2b1df29b4bfcf42731dca47e3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e09c407083c9249376083548ea23d622f173cbe2b1df29b4bfcf42731dca47e3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the computationally expensive challenge of estimating free energy differences in molecular simulations by systematically benchmarking generative modeling approaches like normalizing flows. It evaluates these methods on condensed-matter systems to assess their trade-offs in efficiency, accuracy, and scalability. The main conclusion provides a quantitative framework for selecting effective free energy estimation strategies in condensed-phase systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Stationary Reweighting Yields Local Convergence of Soft Fitted Q-Iteration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [fitted Q-iteration, entropy regularization, stationary distribution, Bellman operator, offline RL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lars van der Laan, Nathan Kallus</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Washington, Netflix, Cornell University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23927" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23927</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identified a geometric mismatch causing instability in soft FQI, showing the soft Bellman operator is contractive in the stationary norm of the soft-optimal policy, not the behavior norm. 2. Proposed stationary-reweighted soft FQI, a method that reweights regression updates using the current policy&#x27;s stationary distribution to restore contraction. 3. Provided a theoretical analysis proving local linear convergence under function approximation with damped weight-estimation errors and suggested a continuation approach for global convergence via temperature annealing.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a133d7f0a68e796e5601b9dd17517ab3deea2ceb5f9dce008c265e4c615a1b43_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a133d7f0a68e796e5601b9dd17517ab3deea2ceb5f9dce008c265e4c615a1b43_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the instability of entropy-regularized fitted Q-iteration (soft FQI) under function approximation and distribution shift in offline reinforcement learning. The authors propose a new method, stationary-reweighted soft FQI, which reweights updates using the policy&#x27;s stationary distribution to restore local contraction. They prove local linear convergence and suggest that global convergence can be achieved by gradually reducing the softmax temperature.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Fundamental limits for weighted empirical approximations of tilted distributions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [Monte Carlo simulation, rare event estimation], [exponential tilting, self-normalized importance sampling, rare event simulation, scaling limits, weighted empirical approximations]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sarvesh Ravichandran Iyer, Himadri Mandal, Dhruman Gupta, Rushil Gupta, Agniv Bandhyopadhyay, Achal Bassamboo, Varun Gupta, Sandeep Juneja</p>
</li>
<li class="">
<p><strong>institution:</strong> Ashoka University, Indian Statistical Institute, Kolkata, Tata Institute of Fundamental Research, Northwestern University, The University of Utah</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23979" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23979</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a sharp asymptotic characterization of the accuracy of a self-normalized importance sampler for tilted distributions. 2. Establishes a fundamental dichotomy in sample complexity: polynomial scaling for bounded random vectors vs. super-polynomial scaling for unbounded ones. 3. Analyzes the efficiency of data-driven tilting when the underlying distribution is unknown but samples are available.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2b2078a5f12e67b3138f44c52f21fc16fdfbf4166041f6980ae84c0a56a3c52_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2b2078a5f12e67b3138f44c52f21fc16fdfbf4166041f6980ae84c0a56a3c52_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes the efficiency of generating samples from a tilted distribution when only samples from the original, unknown distribution are available, using a self-normalized importance sampling method. It provides a precise characterization of the estimator&#x27;s accuracy based on sample size and tilt degree. The key finding is a dichotomy in sample complexity: it grows polynomially for bounded random vectors but super-polynomially for unbounded ones.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Implicit geometric regularization in flow matching via density weighted Stein operators</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models], [flow matching, density weighting, Stein metric, Sobolev regularization, continuous normalizing flows]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shinto Eguchi</p>
</li>
<li class="">
<p><strong>institution:</strong> The Institute of Statistical Mathematics</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23956" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23956</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes γ-Flow Matching (γ-FM), a density-weighted variant of Flow Matching that aligns regression geometry with the underlying probability flow to address inefficiencies in high-dimensional void regions., 2. Introduces a Dynamic Density-Weighting strategy that estimates the target density from training particles, enabling dynamic downweighting of the loss in void regions without requiring explicit density evaluation., 3. Theoretically establishes that γ-FM minimizes transport cost on a statistical manifold with the γ-Stein metric and shows it induces implicit Sobolev regularization, leading to smoother vector fields and improved sampling efficiency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51edeed5e04fe354ae4150f7b6f2ac284b3b71da9bc820ece3883dd4198be9ce_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51edeed5e04fe354ae4150f7b6f2ac284b3b71da9bc820ece3883dd4198be9ce_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies that standard Flow Matching is inefficient in high dimensions due to unweighted regression over low-density &quot;void&quot; regions. To solve this, it proposes γ-Flow Matching, a method that dynamically weights the regression loss based on estimated target density, aligning the geometry with the probability flow. The approach theoretically connects to the γ-Stein metric, induces implicit regularization, and empirically improves vector field smoothness and sampling efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Constructive Approximation of Random Process via Stochastic Interpolation Neural Network Operators</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [function approximation], [stochastic interpolation, neural network operators, sigmoidal functions, mean square approximation, modulus of continuity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sachin Saini, Uaday Singh</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology Roorkee</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24106" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24106</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Constructed a novel class of Stochastic Interpolation Neural Network Operators (SINNOs) with random coefficients., 2. Established theoretical properties of SINNOs, including boundedness, interpolation accuracy, and approximation capabilities in mean square, probability, and path-wise senses., 3. Provided quantitative error estimates for the approximation using the modulus of continuity of the stochastic processes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f705010d1796a2048a2e675bb4dc8e1bca921170079aa88325df4b4644ffd64a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f705010d1796a2048a2e675bb4dc8e1bca921170079aa88325df4b4644ffd64a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Stochastic Interpolation Neural Network Operators (SINNOs) to approximate random processes. The method constructs operators with random coefficients and sigmoidal activations, proving their theoretical approximation properties and providing error bounds. The results demonstrate SINNOs&#x27; effectiveness for approximating stochastic processes, with potential applications like COVID-19 case prediction.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Policy Mirror Descent with Temporal Difference Learning: Sample Complexity under Online Markov Data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [policy mirror descent, temporal difference learning, sample complexity, Markov decision process, policy optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenye Li, Hongxu Chen, Jiacai Liu, Ke Wei</p>
</li>
<li class="">
<p><strong>institution:</strong> Fudan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24056" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24056</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes two novel algorithms (Expected TD-PMD and Approximate TD-PMD) that combine policy mirror descent with TD learning under online Markovian sampling. 2. Establishes an <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mo stretchy="false">{</mo><mo>~</mo></mover><mi>O</mi><mo stretchy="false">}</mo><mo stretchy="false">(</mo><msup><mi>ε</mi><mo stretchy="false">{</mo></msup><mo>−</mo><mn>2</mn><mo stretchy="false">}</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tilde\{O\}(\varepsilon^\{-2\})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2369em;vertical-align:-0.25em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9869em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mopen">{</span></span><span style="top:-3.669em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">~</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.25em"><span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mclose">}</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">ε</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mopen mtight">{</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">2</span><span class="mclose">})</span></span></span></span> sample complexity for achieving average-time <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ε</mi></mrow><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">ε</span></span></span></span>-optimality with a constant step size. 3. Improves sample complexity to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>ε</mi><mo stretchy="false">{</mo></msup><mo>−</mo><mn>2</mn><mo stretchy="false">}</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(\varepsilon^\{-2\})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">ε</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mopen mtight">{</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">2</span><span class="mclose">})</span></span></span></span> for last-iterate <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ε</mi></mrow><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">ε</span></span></span></span>-optimality using adaptive policy update step sizes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e127c9df69a866d0fceeb04827980cde5bc823d8ff492633bf924e6720d9ce1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e127c9df69a866d0fceeb04827980cde5bc823d8ff492633bf924e6720d9ce1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the sample complexity of policy mirror descent combined with temporal difference learning under online Markovian data. It introduces two algorithms, Expected TD-PMD and Approximate TD-PMD, and proves they achieve <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mo stretchy="false">{</mo><mo>~</mo></mover><mi>O</mi><mo stretchy="false">}</mo><mo stretchy="false">(</mo><msup><mi>ε</mi><mo stretchy="false">{</mo></msup><mo>−</mo><mn>2</mn><mo stretchy="false">}</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tilde\{O\}(\varepsilon^\{-2\})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2369em;vertical-align:-0.25em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9869em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mopen">{</span></span><span style="top:-3.669em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">~</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.25em"><span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mclose">}</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">ε</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mopen mtight">{</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">2</span><span class="mclose">})</span></span></span></span> sample complexity for average-time optimality, which is further refined to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>ε</mi><mo stretchy="false">{</mo></msup><mo>−</mo><mn>2</mn><mo stretchy="false">}</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(\varepsilon^\{-2\})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">ε</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mopen mtight">{</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">2</span><span class="mclose">})</span></span></span></span> for last-iterate optimality with adaptive step sizes.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Quantitative Understanding of PDF Fits and their Uncertainties</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [neural tangent kernel], [Parton Distribution Functions, Neural Tangent Kernel, training dynamics, uncertainty quantification, stochastic gradient descent]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amedeo Chiefa, Luigi Del Debbio, Richard Kenway</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Edinburgh, Higgs Centre for Theoretical Physics</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24116" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24116</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a theoretical framework based on the Neural Tangent Kernel (NTK) to analyze the training dynamics of neural networks in PDF fits. 2. Derived an analytical description of neural network evolution during training, clarifying the role of NN architecture and the impact of experimental data. 3. Provided a quantitative description of how uncertainties propagate from data to the fitted function by describing the evolution of the covariance of the NN output.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/283b144a92809766c93247cb89d39262cf21156f386f26a3c176b21ddd101f6d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/283b144a92809766c93247cb89d39262cf21156f386f26a3c176b21ddd101f6d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper develops a theoretical framework using the Neural Tangent Kernel to analytically model the training dynamics of neural networks used for fitting Parton Distribution Functions. The method provides a quantitative understanding of how the network learns and how uncertainties propagate from data, serving as a diagnostic tool to assess the robustness of PDF fitting methodologies. The findings also offer a testbed for applying machine learning theory, showing deviations from the simple &quot;lazy training&quot; regime.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Variational Quantum Brushes</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [quantum computing for art], [variational quantum algorithms, quantum geometric control, variational eigensolver, computational art, quantum brushes]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jui-Ting Lu, Henrique Ennes, Chih-Kang Huang, Ali Abbassi</p>
</li>
<li class="">
<p><strong>institution:</strong> Université de Lorraine, CNRS, Inria, Université de Technologie de Troyes, Orange Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24173" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24173</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/moth-quantum/QuantumBrush" target="_blank" rel="noopener noreferrer" class="">https://github.com/moth-quantum/QuantumBrush</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a mathematical framework for quantum brushes based on variational quantum algorithms. 2. Implements the &quot;Steerable&quot; brush, which uses quantum geometric control theory to merge two images. 3. Implements the &quot;Chemical&quot; brush, which mimics variational eigensolvers to evolve colors on a canvas.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d8a1ae2b1cfdee3a098d127a86fa773993bb3eab697781cc52c1e25f3f449b7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d8a1ae2b1cfdee3a098d127a86fa773993bb3eab697781cc52c1e25f3f449b7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces two new quantum brushes for computational art, built on variational quantum algorithms. The &quot;Steerable&quot; brush merges artworks using quantum geometric control, while the &quot;Chemical&quot; brush evolves colors by mimicking molecular energy estimation. The implementations are open-source and compatible with existing quantum brush software.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Score-based sampling without diffusions: Guidance from a simple and modular scheme</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [score-based sampling, strongly log concave, modular scheme]</p>
</li>
<li class="">
<p><strong>authors:</strong> M. J. Wainwright</p>
</li>
<li class="">
<p><strong>institution:</strong> Massachusetts Institute of Technology (MIT)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24152" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24152</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a modular scheme that reduces score-based sampling to a sequence of &quot;nice&quot; sampling problems, specifically those defined by strongly log concave (SLC) distributions. 2. Shows how to design forward trajectories such that both the terminal and backward conditional distributions are SLC, enabling the use of any high-accuracy SLC sampler. 3. Establishes novel theoretical guarantees for both uni-modal and multi-modal densities, achieving ε-accuracy with polynomial dependence on log(1/ε) and √d dimension dependence.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18db1a89bccabdb56e6dc374b810b4ddafad4b8325763f60f894ada6c6f8926c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18db1a89bccabdb56e6dc374b810b4ddafad4b8325763f60f894ada6c6f8926c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of score-based sampling, which typically relies on approximate score functions. It introduces a modular method that transforms the sampling problem into a short sequence of simpler, strongly log-concave sampling sub-problems. This approach allows leveraging existing high-accuracy samplers and provides theoretical guarantees for efficient and accurate sampling from complex densities.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Topological Spatial Graph Coarsening</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [topological data analysis], [spatial graph coarsening, persistent diagrams, triangle-aware graph filtration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Anna Calissano, Etienne Lasalle</p>
</li>
<li class="">
<p><strong>institution:</strong> University College London, Nantes Université, École Centrale Nantes, CNRS</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24327" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24327</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a topological spatial graph coarsening method that balances graph reduction with topological feature preservation. 2. Introduces a triangle-aware graph filtration to adapt persistent diagrams (a topological descriptor) for spatial graphs. 3. Demonstrates that the method is parameter-free and equivariant under rotations, translations, and scaling.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06067063c056f1d3d0e7b1438bde8daf5524598eca6b311651201d86b9e87d88_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06067063c056f1d3d0e7b1438bde8daf5524598eca6b311651201d86b9e87d88_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of reducing spatial graphs while preserving their topological structure. It proposes a method that collapses short edges and uses a novel triangle-aware graph filtration to guide the coarsening process. The approach is shown to significantly reduce graph size while maintaining key topological information.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Fast reconstruction-based ROI triggering via anomaly detection in the CYGNO optical TPC</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [anomaly detection, autoencoder, online data reduction, optical TPC, ROI triggering]</p>
</li>
<li class="">
<p><strong>authors:</strong> F. D. Amaro, R. Antonietti, E. Baracchini, L. Benussi, C. Capoccia, M. Caponero, L. G. M. de Carvalho, G. Cavoto, I. A. Costa, A. Croce, M. D&#x27;Astolfo, G. D&#x27;Imperio, G. Dho, E. Di Marco, J. M. F. dos Santos, D. Fiorina, F. Iacoangeli, Z. Islam, E. Kemp, H. P. Lima Jr., G. Maccarrone, R. D. P. Mano, D. J. G. Marques, G. Mazzitelli, P. Meloni, A. Messina, V. Monno, C. M. B. Monteiro, R. A. Nobrega, G. M. Oppedisano, I. F. Pains, E. Paoletti, F. Petrucci, S. Piacentini, D. Pierluigi, D. Pinci, F. Renga, A. Russo, G. Saviano, P. A. O. C. Silva, N. J. Spooner, R. Tesauro, S. Tomassini, D. Tozzi</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Coimbra, Gran Sasso Science Institute, INFN (Istituto Nazionale di Fisica Nucleare), Sapienza University of Rome</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24290" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24290</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed an unsupervised, reconstruction-based anomaly detection method using a convolutional autoencoder trained exclusively on pedestal (noise) images for fast ROI extraction in optical TPCs. 2. Demonstrated the critical impact of the training objective design through a controlled comparison of two autoencoder configurations on real data. 3. Achieved high performance, retaining 93.0% of signal intensity while discarding 97.8% of image area with an inference time of ~25 ms per frame, establishing a transparent, detector-agnostic baseline for online data reduction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07a2394c8601a3636fcb96b0e78aa474c0a406733284a194c5cbd05434e20b37_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07a2394c8601a3636fcb96b0e78aa474c0a406733284a194c5cbd05434e20b37_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of real-time data selection from large images in optical Time Projection Chambers (TPCs). It proposes an unsupervised anomaly detection method using a pedestal-trained convolutional autoencoder to quickly identify and extract Regions of Interest (ROIs) from particle tracks. The results show the method is highly effective for online data reduction, with performance heavily dependent on the careful design of the training objective.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Deep Learning in Geotechnical Engineering: A Critical Assessment of PINNs and Operator Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scientific machine learning], [physics-informed neural networks, deep operator networks, graph network simulators, automatic differentiation, geotechnical engineering]</p>
</li>
<li class="">
<p><strong>authors:</strong> Krishna Kumar</p>
</li>
<li class="">
<p><strong>institution:</strong> None (Institution not provided in the given content. Author is Krishna Kumar, but no affiliation or email domain is shown.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24365" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24365</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A critical empirical comparison of PINNs, DeepONet, and GNS against traditional solvers for canonical geotechnical problems, revealing severe performance and accuracy limitations. 2. Demonstrates the efficacy of automatic differentiation through traditional solvers for rapid and accurate inverse parameter estimation in geotechnical engineering. 3. Provides practical recommendations for the application of deep learning in geotechnical contexts, emphasizing the importance of site-based validation and the limited envelope where neural networks are viable.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4f4b161baea59d1dc122276a25b7891aac2248e8367a3b0e01bacce08a12baa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4f4b161baea59d1dc122276a25b7891aac2248e8367a3b0e01bacce08a12baa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper critically evaluates deep learning methods like PINNs, DeepONet, and GNS for geotechnical engineering simulations. It finds these methods are often orders of magnitude slower and less accurate than traditional solvers, and fail when extrapolating. The authors recommend using automatic differentiation for inverse problems and reserving neural networks only for specific, well-bounded cases where traditional solvers are prohibitively expensive.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] OptiVote: Non-Coherent FSO Over-the-Air Majority Vote for Communication-Efficient Distributed Federated Learning in Space Data Centers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [over-the-air computation, free-space optical communication, signSGD, majority vote, pulse-position modulation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Anbang Zhang, Chenyuan Feng, Wai Ho Mow, Jia Ye, Shuaishuai Guo, Geyong Min, Tony Q. S. Quek</p>
</li>
<li class="">
<p><strong>institution:</strong> Shandong University, University of Exeter, The Hong Kong University of Science and Technology, Chongqing University, Singapore University of Technology and Design</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24334" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24334</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes OptiVote, a non-coherent FSO AirComp framework that integrates signSGD with majority-vote aggregation and PPM to eliminate the need for precise phase synchronization in space environments. 2. Develops an importance-aware, CSI-free dynamic power control scheme to mitigate aggregation bias caused by heterogeneous FSO channels without extra signaling. 3. Provides theoretical analysis of aggregate error probability and convergence guarantees, and demonstrates superior communication efficiency and learning accuracy through experiments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72d34a686b6aa623eca2a7d8853b5d062ec21148534d26149a8be732a2526af4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72d34a686b6aa623eca2a7d8853b5d062ec21148534d26149a8be732a2526af4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of communication-efficient federated learning in space data centers by proposing OptiVote, a robust non-coherent free-space optical over-the-air computation framework. It combines signSGD with majority-vote aggregation and pulse-position modulation to perform aggregation without precise phase synchronization, and includes a power control scheme to handle channel heterogeneity. The method is shown to outperform baselines in both communication efficiency and learning accuracy for distributed intelligence in space.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Implicit score matching meets denoising score matching: improved rates of convergence and log-density Hessian estimation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [score matching, implicit score matching, denoising score matching, Fisher divergence, Gagliardo-Nirenberg inequality]</p>
</li>
<li class="">
<p><strong>authors:</strong> Konstantin Yakovlev, Anna Markovich, Nikita Puchkin</p>
</li>
<li class="">
<p><strong>institution:</strong> HSE University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24378" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24378</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proved that implicit score matching can adapt to the intrinsic dimension of low-dimensional data distributions and achieve the same convergence rates as denoising score matching. 2. Demonstrated that both implicit and denoising score matching enable estimation of log-density Hessians without the curse of dimensionality via simple differentiation. 3. Provided theoretical justification for the convergence of ODE-based samplers used in generative diffusion models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09412dadfda9474a45ce50ae36a846be163fb7cca6813c986c3a6c13f5ba9670_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09412dadfda9474a45ce50ae36a846be163fb7cca6813c986c3a6c13f5ba9670_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the estimation of the score function using implicit and denoising score matching. It proves that implicit score matching matches the convergence rates of denoising score matching for low-dimensional data and that both methods can efficiently estimate log-density Hessians, thereby justifying the convergence of ODE-based samplers in diffusion models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Virasoro Symmetry in Neural Network Field Theories</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [theoretical machine learning], [Neural Network Field Theories, Virasoro symmetry, conformal field theory, stress-energy tensor, super-Virasoro]</p>
</li>
<li class="">
<p><strong>authors:</strong> Brandon Robinson</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Amsterdam</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24420" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24420</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. First construction of a Neural Network Field Theory (NN-FT) that encodes the full Virasoro symmetry of a 2d CFT, achieved via a specific &quot;Log-Kernel Network&quot; architecture and prior distribution. 2. Extension of the framework to include super-Virasoro symmetry by constructing neural realizations of a Majorana Fermion and an N=(1,1) scalar multiplet. 3. Development of boundary NN-FTs that preserve (super-)conformal symmetry using the method of images, demonstrating the robustness of the framework.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0e2e1fe141fac45bc31d96a59d9378b6ce343fa02fab3e24378bc7554b1789c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0e2e1fe141fac45bc31d96a59d9378b6ce343fa02fab3e24378bc7554b1789c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the lack of local, infinite-dimensional conformal symmetry (Virasoro symmetry) in typical Neural Network Field Theories (NN-FTs). It proposes a specific neural architecture, the Log-Kernel Network, to construct a 2d free boson theory with a local stress-energy tensor, enabling the emergence of Virasoro algebra. The work is validated numerically, extended to super-Virasoro symmetry, and further generalized to boundary theories preserving conformal invariance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Towards mechanistic understanding in a data-driven weather model: internal activations reveal interpretable physical features</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [mechanistic interpretability], [sparse autoencoders, feature discovery, model intervention, GraphCast, interpretability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Theodore MacMillan, Nicholas T. Ouellette</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24440" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24440</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Adapted interpretability tools from Large Language Models (specifically sparse autoencoders) to analyze a large-scale data-driven weather model (GraphCast). 2. Discovered that the model&#x27;s internal activations correspond to interpretable physical features like tropical cyclones and atmospheric rivers. 3. Demonstrated causal probing via feature interventions, showing physically consistent modifications to model predictions (e.g., altering hurricane evolution).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0540d3e6e8dd4531270c37251313fe5f6b7b7ef8f4792bfb500db7d9f5d7ba6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0540d3e6e8dd4531270c37251313fe5f6b7b7ef8f4792bfb500db7d9f5d7ba6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the interpretability of data-driven physics models like GraphCast by applying mechanistic interpretability techniques from LLMs. Using sparse autoencoders, the authors discover that the model&#x27;s internal features correspond to meaningful physical phenomena. They further show that targeted interventions on these features lead to interpretable and physically consistent changes in the model&#x27;s forecasts, advancing the trustworthiness of such models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Improving the stability of the covariance-controlled adaptive Langevin thermostat for large-scale Bayesian sampling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [Bayesian sampling], [stochastic gradient Langevin dynamics, covariance-controlled adaptive Langevin, numerical stability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiani Wei, Xiaocheng Shang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Birmingham</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24515" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24515</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a modified CCAdL (mCCAdL) thermostat using the scaling part of the scaling and squaring method and a truncated Taylor series to approximate the exact solution to a key subsystem. 2. Introduced a symmetric splitting method for discretizing the mCCAdL thermostat, replacing the Euler-type discretization used in the original CCAdL. 3. Demonstrated that the mCCAdL thermostat achieves substantial improvement in numerical stability over the original CCAdL and outperforms alternative stochastic gradient methods in accuracy for large-scale applications.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f9577aa049234db392488eb89c746ecf45ccd1d5a0555bdc8f9a7fefd2c23a7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f9577aa049234db392488eb89c746ecf45ccd1d5a0555bdc8f9a7fefd2c23a7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the numerical stability issue in the covariance-controlled adaptive Langevin (CCAdL) thermostat, a method for large-scale Bayesian sampling. The authors propose a modified version (mCCAdL) that uses a more stable numerical approximation scheme and a symmetric splitting integrator. Their experiments show that mCCAdL significantly improves stability and accuracy compared to the original CCAdL and other stochastic gradient methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Probabilistic Computers for Neural Quantum States</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [cluster infrastructure], [probabilistic computing, FPGA, Boltzmann machine, neural quantum states, Monte Carlo sampling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuvro Chowdhury, Jasper Pieterse, Navid Anjum Aadit, Johan H. Mentink, Kerem Y. Camsari</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Santa Barbara, Radboud University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24558" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24558</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Implementation of a probabilistic computer on a custom multi-FPGA cluster to serve as a fast sampler for neural quantum states. 2. Introduction of a dual-sampling algorithm to train deep Boltzmann machines by replacing intractable marginalization with conditional sampling. 3. Demonstration of scaling variational quantum simulations to large system sizes (up to 6400 spins) and deeper architectures, overcoming the MCMC sampling bottleneck.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ef4c4bc51129bbcf4c7afb6bc33c07bd787081538aa3dbb925d18f3bf724567_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ef4c4bc51129bbcf4c7afb6bc33c07bd787081538aa3dbb925d18f3bf724567_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the Monte Carlo sampling bottleneck in scaling neural quantum states for quantum many-body simulations. It proposes combining sparse Boltzmann machine architectures with probabilistic computing hardware implemented on FPGAs and introduces a dual-sampling training algorithm. The results show that this approach enables accurate simulations of large spin systems and deeper models, overcoming a key scaling limitation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] MultiRisk: Multiple Risk Control via Iterative Score Thresholding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [risk control], [test-time filtering, dynamic programming, risk constraints, score thresholding, exchangeability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sunay Joshi, Yan Sun, Hamed Hassani, Edgar Dobriban</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Pennsylvania, New Jersey Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24587" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24587</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formalizes the problem of enforcing multiple risk constraints with user-defined priorities for generative AI systems. 2. Introduces two efficient dynamic programming algorithms (MULTIRISK-BASE and MULTIRISK) for selecting thresholds to control risks. 3. Provides a theoretical analysis showing that MULTIRISK achieves nearly tight simultaneous control of all constraint risks under mild assumptions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51cedb2b9055aea411d20349fafef70fbd282669fb33f5fc821167eb1304019e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51cedb2b9055aea411d20349fafef70fbd282669fb33f5fc821167eb1304019e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes MultiRisk, a framework for controlling multiple risks in generative AI outputs via test-time filtering. It introduces dynamic programming algorithms to set score thresholds, guaranteeing simultaneous risk control by leveraging data exchangeability. The method is validated on an LLM alignment task, showing it can control individual risks close to their target levels.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Robust Bayesian Dynamic Programming for On-policy Risk-sensitive Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [risk-sensitive reinforcement learning, Bayesian dynamic programming, coherent risk measures, robust Markov decision process, convex optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shanyu Han, Yangbo He, Yang Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University, The Chinese University of Hong Kong, Shenzhen</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24580" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24580</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel unified framework for risk-sensitive reinforcement learning (RSRL) that incorporates robustness against transition uncertainty by defining inner and outer coherent risk measures. 2. Develops a Bayesian Dynamic Programming algorithm that alternates posterior updates with value iteration, using a Monte Carlo and convex optimization estimator with strong consistency guarantees. 3. Provides theoretical analysis including convergence, sample complexity, and computational complexity under Dirichlet posterior and CVaR, validated through numerical experiments and an option hedging application.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b966fa69f9b8aafe26700ce5afe83099b3257d5b90314e5d3f668e4079ecdda_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b966fa69f9b8aafe26700ce5afe83099b3257d5b90314e5d3f668e4079ecdda_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces a robust Bayesian framework for on-policy risk-sensitive reinforcement learning that addresses transition uncertainty through coupled inner and outer risk measures. It develops a Bayesian Dynamic Programming algorithm with theoretical guarantees and demonstrates its effectiveness in convergence and robustness via numerical experiments and an option hedging application.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Soliton profiles: Classical Numerical Schemes vs. Neural Network - Based Solvers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scientific computing], [Physics-Informed Neural Networks, Deep Operator Network, Petviashvili method, solitary waves, numerical solvers]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chandler Haight, Svetlana Roudenko, Zhongming Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Wayne State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24634" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24634</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A comprehensive comparative study of classical numerical schemes (e.g., Petviashvili&#x27;s method) and neural network-based solvers (PINNs, operator-learning) for computing soliton profiles in 1D dispersive PDEs. 2. An empirical confirmation that classical methods retain superior accuracy and computational efficiency for single-instance problems in one-dimensional settings. 3. An analysis highlighting the trade-offs of neural network methods, where operator-learning approaches, despite costly training, offer rapid inference and reusability for multi-parameter or real-time applications.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4b9a088c1ebb97139a0da346b37d398fe825a096d76a313979e6fe0f8544c65_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4b9a088c1ebb97139a0da346b37d398fe825a096d76a313979e6fe0f8544c65_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper compares classical numerical solvers and neural network-based methods for finding solitary-wave solutions to nonlinear PDEs like the NLS and KdV equations. It finds that classical methods are more accurate and efficient for single problems, while neural operators are better suited for repeated simulations due to fast inference after training. The core conclusion is that the choice of solver depends on the application context: precision for single instances vs. speed for parameterized families.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Sparse Offline Reinforcement Learning with Corruption Robustness</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [offline reinforcement learning, sparsity, corruption robustness, single-policy concentrability, actor-critic]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nam Phuong Tran, Andi Nika, Goran Radanovic, Long Tran-Thanh, Debmalya Mandal</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Warwick, Max Planck Institute for Software Systems (MPI-SWS)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24768" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24768</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies limitations of integrating sparsity into standard robust offline RL methods like LSVI, showing they can fail due to overly pessimistic bonuses. 2. Proposes novel actor-critic methods with sparse robust estimator oracles that avoid pointwise pessimistic bonuses. 3. Provides the first non-vacuous theoretical guarantees for learning in high-dimensional sparse MDPs under weak (single-policy concentrability) coverage and strong data corruption.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ce133c71cdf619bf6a1597c047c8adc5b10b7d0584bd6af1944718635e12340_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ce133c71cdf619bf6a1597c047c8adc5b10b7d0584bd6af1944718635e12340_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of robust offline reinforcement learning in high-dimensional, sparse Markov Decision Processes where data may be corrupted. The authors propose new actor-critic methods that use sparse robust estimator oracles, avoiding the pitfalls of traditional approaches. Their work provides the first theoretical guarantees showing that learning a near-optimal policy is possible under weak data coverage and strong corruption, where previous methods fail.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] A New Decomposition Paradigm for Graph-structured Nonlinear Programs via Message Passing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [communication &amp; networking], [Message Passing, Jacobi Block Updates, Graph Decomposition, Hypergraph Optimization, Decentralized Optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kuangyu Ding, Marie Maros, Gesualdo Scutari</p>
</li>
<li class="">
<p><strong>institution:</strong> Purdue University, Texas A&amp;M University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24676" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24676</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MP-Jacobi, a novel decentralized framework that combines min-sum message passing with Jacobi block updates for graph-structured nonlinear programs, enabling single-hop communication and convergence on loopy graphs. 2. Establishes global linear convergence rates for strongly convex objectives, providing theoretical guidance on how curvature, coupling strength, and graph partitioning affect scalability. 3. Develops graph-compliant surrogate updates and a hyperedge-splitting scheme to reduce per-iteration computation/communication costs and extend the method to hypergraphs while preserving convergence.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b545e505c1cccda1a2f472afecb206a4b95c552dd11319ac3f007bab097156a8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b545e505c1cccda1a2f472afecb206a4b95c552dd11319ac3f007bab097156a8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes MP-Jacobi, a decentralized algorithm for solving graph-structured nonlinear programs by partitioning the graph into tree clusters and combining min-sum message passing within clusters with Jacobi-style updates for inter-cluster couplings. This design uses only single-hop communication and is proven to converge linearly for strongly convex problems. Experiments show it outperforms decentralized gradient baselines, offering a scalable primitive for graph optimization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Fairness-Aware Insurance Pricing: A Multi-Objective Optimization Approach</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [fairness in machine learning], [multi-objective optimization, NSGA-II, fairness criteria, insurance pricing, Pareto front]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tim J. Boonen, Xinyue Fan, Zixiao Quan</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Hong Kong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24747" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24747</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel multi-objective optimization framework for insurance pricing that jointly optimizes accuracy, group fairness, individual fairness, and counterfactual fairness. 2. Employs the Non-dominated Sorting Genetic Algorithm II (NSGA-II) to generate a diverse Pareto front of trade-off solutions, moving beyond single-objective optimization. 3. Introduces a specific selection mechanism to extract a balanced premium from the Pareto front, demonstrating a consistent and superior compromise compared to single-model approaches.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ea23717323b55541619e16c4dd6ff704d96b756316680eeebec5f447f78a5d2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ea23717323b55541619e16c4dd6ff704d96b756316680eeebec5f447f78a5d2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the trade-offs between accuracy and multiple fairness criteria in machine learning for insurance pricing. It proposes a multi-objective optimization framework using NSGA-II to generate a Pareto front of solutions and a method to select a balanced premium. The results show the proposed method achieves a better compromise than existing single-objective models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Limits of quantum generative models with classical sampling hardness</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [quantum machine learning], [quantum generative models, anticoncentration, barren plateaus, classical simulability, surrogate sampling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sabrina Herbst, Ivona Brandić, Adrián Pérez-Salinas</p>
</li>
<li class="">
<p><strong>institution:</strong> TU Wien, ETH Zurich</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24801" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24801</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Shows that quantum generative models which anticoncentrate (a property linked to classical hardness) are not trainable on average, creating a trade-off between advantage and learnability. 2. Demonstrates that models outputting sparse distributions can be trained, and explores special cases to enhance trainability. 3. Links this trainability trade-off to quantum process verification and identifies that quantum advantage in generative models must stem from sources other than anticoncentration.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db3f019ba893a7df48b4f7eb38a379b3b7ed4612455ba75a7c9068c960c84acc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db3f019ba893a7df48b4f7eb38a379b3b7ed4612455ba75a7c9068c960c84acc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the trainability of quantum generative models, finding a fundamental trade-off: models that output anticoncentrated distributions (which are classically hard to sample from) suffer from untrainable loss landscapes, while models with sparse outputs are trainable. The authors link this to verification and conclude that quantum advantage in generative modeling is still possible but must originate from mechanisms distinct from anticoncentration.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Learning Temporally Consistent Turbulence Between Sparse Snapshots via Diffusion Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [Denoising Diffusion Probabilistic Models (DDPMs), turbulence interpolation, generative surrogate, Kolmogorov Flow, Kelvin-Helmholtz Instability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mohammed Sardar, Małgorzata J. Zimoń, Samuel Draycott, Alistair Revell, Alex Skillen</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Manchester, IBM Research Europe</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24813" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24813</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a conditional DDPM-based method for temporally interpolating coherent turbulent dynamics between sparse, decorrelated flow snapshots. 2. Demonstrates the method as a proof-of-concept generative surrogate on both a 2D Kolmogorov Flow and a 3D Kelvin-Helmholtz Instability case. 3. Evaluates the generated sequences through statistical turbulence analysis, including turbulent kinetic energy spectra and the temporal decay of structures, showing the ability to capture evolving flow statistics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0eee4dacebc613c047d4355fd35c417d91dbabc771afd4a894ccdc60312874e4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0eee4dacebc613c047d4355fd35c417d91dbabc771afd4a894ccdc60312874e4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes using conditional Denoising Diffusion Probabilistic Models (DDPMs) as a generative surrogate to reconstruct statistically accurate turbulent flow sequences between sparse snapshots. The method is demonstrated on 2D and 3D turbulent flow cases. The analysis shows the generated sequences capture key turbulent statistics and the evolution of flow structures.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Are First-Order Diffusion Samplers Really Slower? A Fast Forward-Value Approach</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [diffusion model, training-free acceleration, first-order sampler, forward-value discretization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuchen Jiao, Na Li, Changxiao Cai, Gen Li</p>
</li>
<li class="">
<p><strong>institution:</strong> The Chinese University of Hong Kong, Zhejiang University, University of Michigan</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24927" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24927</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Challenges the prevailing belief that higher-order ODE solvers are inherently faster for DPM sampling, proposing that the placement of DPM evaluations is a crucial, independent design factor. 2. Introduces a novel, training-free first-order sampler that approximates the forward-value evaluation using a cheap one-step lookahead predictor, resulting in a leading discretization error with the opposite sign to DDIM. 3. Provides theoretical guarantees for the sampler&#x27;s approximation of the ideal forward-value trajectory while maintaining first-order convergence, and demonstrates empirical competitiveness with higher-order samplers on standard benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef8de5dcbba9edb9daa0ab1ba8e254668331d9851247da681521a06ad8b83b22_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef8de5dcbba9edb9daa0ab1ba8e254668331d9851247da681521a06ad8b83b22_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper challenges the view that first-order diffusion samplers are inherently slower than higher-order ones. It proposes a new first-order sampler that uses a forward-value approach with a lookahead predictor to better place model evaluations. The method is theoretically sound and empirically matches or outperforms state-of-the-art higher-order samplers on image generation tasks under the same computational budget.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Basic Inequalities for First-Order Optimization with Applications to Statistical Risk Analysis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [optimization theory], [basic inequalities, implicit regularization, gradient descent, mirror descent, statistical risk analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Seunghoon Paik, Kangjie Zhou, Matus Telgarsky, Ryan J. Tibshirani</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Berkeley, Columbia University, New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24999" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24999</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a formal framework of &quot;basic inequalities&quot; that connects implicit and explicit regularization in first-order optimization. 2. Applies the framework to derive new results for mirror descent with Bregman divergence, generalized linear models trained by gradient/exponentiated gradient descent, and randomized predictors. 3. Revisits and refines known results on gradient descent, demonstrating the framework&#x27;s versatility for analyzing training dynamics and prediction risk.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/056dd77c9a8e63bc02d7a61166503bba0d0961e3c1f64d12e92c35803aa0bd9a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/056dd77c9a8e63bc02d7a61166503bba0d0961e3c1f64d12e92c35803aa0bd9a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a framework of &quot;basic inequalities&quot; to analyze first-order optimization algorithms, linking the number of iterations to an effective regularization parameter. The method provides a unified tool to derive statistical risk bounds for algorithms like gradient descent and mirror descent. The main conclusion is that this framework simplifies and generalizes the analysis of implicit regularization across various optimization settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] SymSeqBench: a unified framework for the generation and analysis of rule-based symbolic sequences and datasets</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [sequence learning], [formal language theory, symbolic sequences, benchmark suite, cognitive modeling, sequence processing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Barna Zajzon, Younes Bouhadjar, Maxime Fabre, Felix Schmidt, Noah Ostendorf, Emre Neftci, Abigail Morrison, Renato Duarte</p>
</li>
<li class="">
<p><strong>institution:</strong> Jülich Research Centre, RWTH Aachen University, University of Groningen, University of Coimbra</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24977" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24977</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces SymSeq, a tool for the rigorous generation and analysis of structured symbolic sequences. 2. Introduces SeqBench, a comprehensive benchmark suite of rule-based sequence processing tasks for evaluating AI systems. 3. Provides a unified, domain-agnostic framework (SymSeqBench) based on Formal Language Theory to standardize experiments across cognitive science and AI.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71331410faecaa464fb09419a580962b2cddad2a5e128910f8482dc81e965858_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71331410faecaa464fb09419a580962b2cddad2a5e128910f8482dc81e965858_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces SymSeqBench, a unified software framework combining a symbolic sequence generator/analyzer (SymSeq) and a benchmark suite (SeqBench) for evaluating sequence learning. It is based on Formal Language Theory to provide a domain-agnostic, formal link between computation and cognition. The main conclusion is that this modular, open-source tool offers a versatile and standardized way to investigate sequential structure across diverse fields like psycholinguistics, cognitive psychology, and AI.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2026-01-05T03:17:11.000Z" itemprop="dateModified">Jan 5, 2026</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/cslg/20251222-20251228"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251222-20251228 (cs.LG)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/daily/cslg/20260105-20260111"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20260105-20260111 (cs.LG)</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-29" class="table-of-contents__link toc-highlight">2025-12-29</a></li><li><a href="#2025-12-30" class="table-of-contents__link toc-highlight">2025-12-30</a></li><li><a href="#2026-01-01" class="table-of-contents__link toc-highlight">2026-01-01</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>