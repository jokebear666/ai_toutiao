<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_LG/20251229-20260104" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251229-20260104 (cs.LG) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/cslg/20251229-20260104"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251229-20260104 (cs.LG) | AI头条"><meta data-rh="true" name="description" content="2025-12-29"><meta data-rh="true" property="og:description" content="2025-12-29"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/cslg/20251229-20260104"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cslg/20251229-20260104" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cslg/20251229-20260104" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.LG","item":"https://jokebear666.github.io/ai_toutiao/daily/cslg"},{"@type":"ListItem","position":3,"name":"20251229-20260104 (cs.LG)","item":"https://jokebear666.github.io/ai_toutiao/daily/cslg/20251229-20260104"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.9ae66a68.css">
<script src="/ai_toutiao/assets/js/runtime~main.f5923855.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.f92140b8.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/daily">Daily</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/paper">Paper</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/arxiv-daily"><span title="Arxiv每日论文" class="linkLabel_WmDU">Arxiv每日论文</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Collapse sidebar category &#x27;cs.LG&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cs_LG/20251215-20251221"><span title="20251215-20251221 (cs.LG)" class="linkLabel_WmDU">20251215-20251221 (cs.LG)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cslg/20251222-20251228"><span title="20251222-20251228 (cs.LG)" class="linkLabel_WmDU">20251222-20251228 (cs.LG)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/cslg/20251229-20260104"><span title="20251229-20260104 (cs.LG)" class="linkLabel_WmDU">20251229-20260104 (cs.LG)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csoh"><span title="cs.OH" class="categoryLinkLabel_W154">cs.OH</span></a><button aria-label="Expand sidebar category &#x27;cs.OH&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/daily/cslg"><span>cs.LG</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251229-20260104 (cs.LG)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251229-20260104 (cs.LG)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-29">2025-12-29<a href="#2025-12-29" class="hash-link" aria-label="Direct link to 2025-12-29" title="Direct link to 2025-12-29" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251229] Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [data spaces, cloud-edge continuum, containerized microservices, edge AI, intelligent infrastructure monitoring]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dimitrios Amaxilatis, Themistoklis Sarantakos, Nikolaos Tsironis, Souvik Sengupta, Kostas Ramantas, Jhofre Ojeda</p>
</li>
<li class="">
<p><strong>institution:</strong> Spark Works Ltd., IONOS SE, Iquadrat Informática S.L.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21340" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21340</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A real-world implementation of intelligent infrastructure monitoring within a data space-enabled cloud-edge framework, demonstrating practical integration. 2. Leveraging edge computing, containerized microservices, and interoperable data sharing to address challenges like sensor integration, data privacy, and scalability. 3. Showcasing the training and deployment of AI/ML services directly at the edge for optimized resource use and timely decision-making in smart city applications.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ae37cae2dac445e3682b661f116dd30e5d680105ac5070eb7b48c049ab9aff3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ae37cae2dac445e3682b661f116dd30e5d680105ac5070eb7b48c049ab9aff3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a real-world use case for smart cities, implementing an intelligent climate monitoring system within a cloud-edge continuum framework enabled by data spaces. The method combines edge computing, containerized microservices, and secure data sharing to facilitate localized analytics and AI deployment. The conclusion highlights the transformative potential of integrating AI, edge computing, and data spaces for building efficient and resilient smart city infrastructures.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Physics-Informed Neural Solvers for Periodic Quantum Eigenproblems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [physics-informed machine learning], [physics-informed neural networks, Floquet-Bloch eigenvalue problem, honeycomb lattice, band structure, transfer learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haaris Mian</p>
</li>
<li class="">
<p><strong>institution:</strong> Columbia University (Program in Applied Mathematics, Department of Applied Physics and Applied Mathematics)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21349" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21349</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A physics-informed machine learning framework for solving the Floquet-Bloch eigenvalue problem for particles in 2D periodic potentials, using neural networks to learn Bloch functions and eigenvalues simultaneously. 2. A mesh-free solver that enforces the Schrödinger equation, Bloch periodicity, and normalization via a composite loss function without supervision. 3. Demonstration of transfer learning to adapt the solver from nearly-free to strongly varying potentials, capturing changes in band topology.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18928753702626417cc700c2238d86a4711adfe422a6c1995a68fbcbf3401d4f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18928753702626417cc700c2238d86a4711adfe422a6c1995a68fbcbf3401d4f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This thesis proposes a physics-informed neural network framework to solve quantum eigenproblems for particles in periodic potentials, specifically targeting the honeycomb lattice. The method learns Bloch wavefunctions and their energies by training over the Brillouin zone with a loss function that encodes the governing physics. It is validated against traditional methods and shows promise for exploring band structure topology through transfer learning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Reinforcement Learning Approach to Synthetic Data Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [synthetic data generation, reinforcement learning, proximal policy optimization, privacy, biomedical data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Natalia Espinosa-Dice, Nicholas J. Jackson, Chao Yan, Aaron Lee, Bradley A. Malin</p>
</li>
<li class="">
<p><strong>institution:</strong> Princeton University, Vanderbilt University Medical Center, Washington University in St. Louis</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21395" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21395</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Reframes synthetic data generation (SDG) as a reinforcement learning problem, introducing a novel perspective. 2. Proposes RLSyn, a framework that models the data generator as a stochastic policy optimized via Proximal Policy Optimization with discriminator-derived rewards for stable, data-efficient training. 3. Demonstrates the effectiveness of the RL approach, showing it performs comparably to or better than GANs and diffusion models, especially in data-scarce regimes on biomedical datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75d9e0c3d2cba88654d16f9652042680f0037508065d6d94fba27208a6199969_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75d9e0c3d2cba88654d16f9652042680f0037508065d6d94fba27208a6199969_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes RLSyn, a reinforcement learning framework for generating synthetic biomedical data by modeling the generator as a policy optimized with PPO. It shows that this approach achieves performance comparable to or better than GANs and diffusion models, particularly when training data is limited, offering a stable and data-efficient alternative for privacy-preserving data sharing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Learning to Reconfigure: Using Device Status to Select the Right Constrained Coding Scheme</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [storage systems], [constrained coding, LOCO codes, linear programming, code reconfiguration, two-dimensional magnetic recording (TDMR)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Doğukan Özbayrak, Ahmed Hareedy</p>
</li>
<li class="">
<p><strong>institution:</strong> Middle East Technical University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21396" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21396</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes offline and online learning methods to reconfigure constrained coding schemes based on actual device status, moving beyond predetermined time stamps. 2. Models the reconfiguration problem as a linear programming optimization to maximize storage capacity and/or minimize decoding complexity, proving global optimality. 3. Demonstrates the effectiveness of the approach through experimental results in Two-Dimensional Magnetic Recording (TDMR) systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602c83173fa9054dd0b0d4fea2c1b1c7d235aa475323c43a09847363ee851c20_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602c83173fa9054dd0b0d4fea2c1b1c7d235aa475323c43a09847363ee851c20_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of optimally reconfiguring constrained coding schemes in storage devices as they age. The authors propose offline and online learning methods that fit device status data to polynomial models and formulate the reconfiguration decision as a linear programming problem to maximize capacity or minimize complexity. Experimental results show the proposed method is effective for TDMR systems and can be extended to other storage or transmission systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] kooplearn: A Scikit-Learn Compatible Library of Algorithms for Evolution Operator Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [dynamical systems learning], [Koopman operator, transfer operator, spectral decomposition, scikit-learn API, reduced-order models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Giacomo Turri, Grégoire Pacreau, Giacomo Meanti, Timothée Devergne, Daniel Ordonez, Erfan Mirzaei, Bruno Belucci, Karim Lounici, Vladimir Kostic, Massimiliano Pontil, Pietro Novelli</p>
</li>
<li class="">
<p><strong>institution:</strong> Italian Institute of Technology, École Polytechnique, Inria, University College London</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21409" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21409</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Machine-Learning-Dynamical-Systems/kooplearn" target="_blank" rel="noopener noreferrer" class="">https://github.com/Machine-Learning-Dynamical-Systems/kooplearn</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a unified library implementing linear, kernel, and deep-learning estimators for both discrete-time (Koopman/Transfer) and continuous-time evolution operators. 2. Offers a scikit-learn compatible API for easy integration into existing ML workflows. 3. Includes curated benchmark datasets to support experimentation, reproducibility, and fair algorithm comparison.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6abf4d9f1d18bb64fbf627b5a4cc8d5b0c12b9f7fda20d8f6811df3f96602779_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6abf4d9f1d18bb64fbf627b5a4cc8d5b0c12b9f7fda20d8f6811df3f96602779_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces kooplearn, a Python library for learning evolution operators (like Koopman and transfer operators) from dynamical systems data. It provides a suite of estimators and a scikit-learn compatible interface to enable spectral analysis, reduced-order modeling, and forecasting. The library aims to standardize and facilitate research in data-driven dynamical systems modeling.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [tool-use framework, vision-language model, interpretability, data-efficiency, tool bottleneck model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Christina Liu, Alan Q. Wang, Joy Hsu, Jiajun Wu, Ehsan Adeli</p>
</li>
<li class="">
<p><strong>institution:</strong> California Institute of Technology, Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21414" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21414</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/christinaliu2020/tool-bottleneck-framework" target="_blank" rel="noopener noreferrer" class="">https://github.com/christinaliu2020/tool-bottleneck-framework</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed the Tool Bottleneck Framework (TBF) for medical image understanding, which uses a learned Tool Bottleneck Model (TBM) to compose tool outputs instead of text-based composition. 2. Introduced a strategy for the TBM to handle arbitrary VLM tool selections, enabling flexible and robust prediction. 3. Demonstrated that the framework improves performance, interpretability, and data-efficiency, matching or surpassing deep learning classifiers and other tool-use frameworks, especially in data-limited scenarios.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that existing tool-use frameworks for image understanding perform poorly on medical images due to their reliance on text to compose specialized tools. The authors propose the Tool Bottleneck Framework (TBF), which uses a vision-language model to select tools and a learned neural network (the Tool Bottleneck Model) to fuse their outputs. Evaluations on histopathology and dermatology tasks show TBF performs on par with or better than existing methods, offering gains in data-limited settings and more interpretable predictions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [wireless networking], [Age of Information (AoI), reinforcement learning, freshness optimization, wireless networks, multi-agent systems]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alimu Alibotaiken, Suyang Wang, Oluwaseun T. Ajayi, Yu Cheng</p>
</li>
<li class="">
<p><strong>institution:</strong> Illinois Institute of Technology, California State University, San Bernardino</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21412" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21412</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel taxonomy for Age of Information (AoI) and its variants, categorizing them into native, function-based, and application-oriented families to clarify freshness modeling for B5G/6G systems. 2. Introduces a policy-centric taxonomy for reinforcement learning in freshness-aware networks, consisting of update-control RL, medium-access RL, risk-sensitive RL, and multi-agent RL. 3. Synthesizes recent RL-driven freshness control progress and highlights open challenges like delayed decision processes and cross-layer design to establish a unified foundation for learning-based freshness optimization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b58e6fe193d4299ab0beca4eb949d8b13e21e8198c223c3dd6c0ca64896bbf7d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b58e6fe193d4299ab0beca4eb949d8b13e21e8198c223c3dd6c0ca64896bbf7d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This survey addresses the gap between classical Age of Information (AoI) studies and broad reinforcement learning (RL) discussions in wireless networks by examining RL specifically for freshness optimization. It organizes AoI variants and introduces a policy-centric RL taxonomy to provide a coherent framework for freshness-aware decision-making in next-generation wireless systems. The paper aims to establish a unified foundation for learning-based freshness control and highlights key open challenges for future research.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Scalable Deep Subspace Clustering Network</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [subspace clustering], [landmark-based approximation, self-expression, spectral clustering, linear complexity, convolutional auto-encoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nairouz Mrabah, Mohamed Bouguessa, Sihem Sami</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Quebec at Montreal</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21434" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21434</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a deep subspace clustering framework (SDSNet) that achieves linear O(n) computational complexity, breaking the traditional O(n^3) bottleneck. 2. Introduces a landmark-based approximation to avoid constructing the full n×n affinity matrix, combined with joint optimization of auto-encoder reconstruction and self-expression objectives. 3. Enables direct spectral clustering on factorized representations, integrating convolutional auto-encoders with subspace-preserving constraints for efficient and accurate clustering.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high computational cost (O(n^3)) of traditional subspace clustering methods. It proposes SDSNet, a scalable deep learning framework that uses landmark approximation and joint optimization to achieve linear O(n) complexity. Experiments show SDSNet maintains clustering quality comparable to state-of-the-art methods while being significantly more efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [se], [software testing], [runtime error detection, coverage-guided testing, multi-agent reasoning, large language models, static analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hridya Dhulipala, Xiaokai Rong, Tien N. Nguyen</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Texas at Dallas</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21431" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21431</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Cerberus, a novel predictive, execution-free coverage-guided testing framework that uses LLMs for input generation, coverage prediction, and error detection without code execution. 2. Introduces a two-phase feedback loop that first maximizes code coverage and detects errors, then focuses solely on error detection after coverage is maximized, improving performance over single-phase prompting. 3. Empirically demonstrates that Cerberus outperforms conventional and learning-based testing frameworks for both complete and incomplete code snippets by generating high-coverage test cases more efficiently and discovering more runtime errors.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d41379a4c8476f7ed1a8a02b193c5fe427e6a274d56beccd85313ce47ba5e76_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d41379a4c8476f7ed1a8a02b193c5fe427e6a274d56beccd85313ce47ba5e76_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes Cerberus, a framework that uses Large Language Models (LLMs) to statically detect runtime errors in code snippets without execution. It employs a multi-agent reasoning approach with a two-phase, coverage-guided feedback loop to generate test inputs and predict errors. The evaluation shows Cerberus is more efficient and effective at finding runtime errors than existing testing methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [lossy compression, quality prediction, deep-surrogate, mixture-of-experts, feature-extraction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Khondoker Mirazul Mumenin, Robert Underwood, Dong Dai, Jinzhen Wang, Sheng Di, Zarija Lukić, Franck Cappello</p>
</li>
<li class="">
<p><strong>institution:</strong> University of North Carolina at Charlotte, Argonne National Laboratory, University of Delaware, Lawrence Berkeley National Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21433" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21433</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1) A generalizable surrogate model for predicting compression quality across different compressors, quality metrics, and datasets. 2) A novel two-stage design that decouples expensive feature extraction from lightweight prediction for efficient training and modular inference. 3) A mixture-of-experts design to optimize performance and robustness for time-evolving scientific data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06e831f83754144a92a117a184fdcc51da0584d4163390cf94b34d4175b77a1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06e831f83754144a92a117a184fdcc51da0584d4163390cf94b34d4175b77a1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes DeepCQ, a deep-surrogate framework to efficiently predict the quality of data after lossy compression, which is traditionally computationally expensive to assess. The method uses a two-stage and mixture-of-experts design for generalizability and robustness across different compressors, metrics, and time-evolving datasets. The framework achieves high predictive accuracy (errors under 10%) on real-world applications, enabling informed compression decisions and reducing I/O and computational overhead.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [computational ethics], [moral context, probabilistic clustering, LLM semantics, interpretable prediction, human judgment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Geoffroy Morlat, Marceau Nahon, Augustin Chartouny, Raja Chatila, Ismael T. Freire, Mehdi Khamassi</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Intelligent Systems and Robotics, Sorbonne University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21439" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21439</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An empirically grounded dataset of 300 moral scenarios with human ternary judgments. 2. A reproducible pipeline (COMETH) combining human judgments, probabilistic context learning, and LLM-based semantic abstraction. 3. An interpretable, context-sensitive moral prediction model that outperforms end-to-end LLM prompting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f4abc9f666c2d29dadd77869bddf3f159d0bbc8839c7c0f65bbdb4c29ad40c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f4abc9f666c2d29dadd77869bddf3f159d0bbc8839c7c0f65bbdb4c29ad40c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that moral judgments depend heavily on context. It proposes the COMETH framework, which uses probabilistic clustering on human judgment data and LLM-based semantic abstraction to learn and explain action-specific moral contexts. The main conclusion is that COMETH significantly outperforms direct LLM prompting in aligning with human majority judgments while providing interpretable predictions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [se], [fuzz testing], [initial corpus generation, large language models, multi-agent framework, predictive code coverage, mutation-based fuzzing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hridya Dhulipala, Xiaokai Rong, Aashish Yadavally, Tien N. Nguyen</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Texas at Dallas</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21440" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21440</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FuzzWise, a novel method that integrates initial corpus generation and minimization into a single, streamlined process using an LLM-based multi-agent framework., 2. Introduces a predictive code coverage module (an LLM agent) that assesses new test cases without requiring actual program execution, saving computational resources., 3. Demonstrates empirically that FuzzWise generates a smaller, higher-quality initial corpus that achieves higher code coverage and triggers more runtime errors more efficiently than baseline methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcb8eafec283e39ae04e0274dc4688aced924346193560581e8469f1151507f6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcb8eafec283e39ae04e0274dc4688aced924346193560581e8469f1151507f6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of generating a high-quality initial seed corpus for mutation-based fuzzing. It proposes FuzzWise, a method that uses a multi-agent LLM framework to generate and intelligently select test cases based on predicted coverage without execution. The evaluation shows FuzzWise produces a smaller, more effective corpus that achieves higher coverage and finds more bugs efficiently.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [masked diffusion language models, reinforcement learning, parallel decoding, on-policy optimization, unmasking planner]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shirui Chen, Jiantao Jiao, Lillian J. Ratliff, Banghua Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Washington, University of California, Berkeley</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21446" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21446</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes dUltra, an on-policy RL framework (GRPO-based) for learning efficient unmasking strategies in MDLMs. 2. Introduces a joint optimization scheme for the base diffusion model and a new unmasking planner head using a composite reward. 3. Demonstrates improved accuracy-efficiency trade-off over heuristic and distillation baselines in reasoning and code generation tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f1e67e2dde4b6b9e98e4e3c5574326f0e2d63114afe47049e17c2ae04bb41b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f1e67e2dde4b6b9e98e4e3c5574326f0e2d63114afe47049e17c2ae04bb41b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the slow sampling speed of masked diffusion language models (MDLMs) by proposing dUltra, a reinforcement learning framework that learns an optimal strategy for parallel token unmasking. The method jointly optimizes the diffusion model and a planner head using rewards for correctness, distillation, and step count. The results show dUltra achieves a better trade-off between accuracy and efficiency than existing methods, advancing towards &quot;diffusion supremacy&quot; over autoregressive models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] An Equivariance Toolbox for Learning Dynamics</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [learning theory], [equivariance, Noether&#x27;s theorem, Hessian constraints, learning dynamics, symmetry]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yongyi Yang, Liu Ziyin</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Michigan, Massachusetts Institute of Technology, NTT Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21447" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21447</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a general equivariance framework that unifies first-order constraints (conservation laws, implicit bias) into a single identity. 2. Extended the analysis to second-order, providing structural predictions about curvature, flat/sharp directions, and gradient-Hessian alignment. 3. Generalized classical symmetry analyses from continuous transformations to include general equivariance and discrete transformations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69f03ddf87a64feb67cd904eadbbfc83a393f0b282be28c74e5bfeac489db50b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69f03ddf87a64feb67cd904eadbbfc83a393f0b282be28c74e5bfeac489db50b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper develops a unified equivariance toolbox for analyzing learning dynamics in neural networks. It extends classical symmetry-based analyses to derive coupled first- and second-order constraints, connecting transformation structure to loss landscape geometry. The framework recovers known results and provides new characterizations linking equivariance to modern optimization phenomena.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] RLLaVA: An RL-central Framework for Language and Vision Assistants</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [reinforcement learning, vision-language models, Markov decision process, resource-efficient training, modular framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lei Zhao, Zihao Ma, Boyu Lin, Yuhe Liu, Wenjun Wu, Lei Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Beihang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21450" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21450</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/TinyLoopX/RLLaVA" target="_blank" rel="noopener noreferrer" class="">https://github.com/TinyLoopX/RLLaVA</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a modular RL framework (RLLaVA) that decouples algorithmic logic from model architecture and distributed execution, enabling easy implementation of new RL algorithms. 2. Formulates the joint visual-textual sequential decision of VLMs as a unified Markov Decision Process (MDP), providing a principled foundation for multi-modal RL. 3. Achieves resource-efficient training, enabling end-to-end full-parameter updates for up to 4B-scale models on a single 24GB GPU.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69c1815245107c8b5c717a57c722eeb64e0b9b4b77c5a989f0d2b9f4353b36df_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69c1815245107c8b5c717a57c722eeb64e0b9b4b77c5a989f0d2b9f4353b36df_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces RLLaVA, a lightweight and modular reinforcement learning framework specifically designed for training vision-language models. It decouples RL logic from system components to simplify algorithm development and enables efficient training of large models on common GPUs. Experiments show that models trained with RLLaVA improve over base models and are competitive with other specialized RL frameworks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] CCAD: Compressed Global Feature Conditioned Anomaly Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [anomaly detection], [global feature conditioning, adaptive compression, reconstruction-based anomaly detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiao Jin, Liang Diao, Qixin Xiao, Yifan Hu, Ziqi Zhang, Yuchen Liu, Haisong Gu</p>
</li>
<li class="">
<p><strong>institution:</strong> Columbia University, University of Michigan, University of California at Berkeley, Stevens Institute of Technology, VisionX LLC</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21459" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21459</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/chloeqxq/CCAD" target="_blank" rel="noopener noreferrer" class="">https://github.com/chloeqxq/CCAD</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CCAD, a novel method that synergizes reconstruction-based and representation-based anomaly detection by using compressed global features as a conditioning modality for the reconstruction model. 2. Introduces an adaptive compression mechanism to enhance model generalization and training efficiency. 3. Contributes a reorganized and re-annotated version of the DAGM 2007 dataset to validate the method&#x27;s effectiveness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of anomaly detection in industrial settings with limited anomalous data by proposing CCAD, a method that combines the strengths of reconstruction-based and representation-based approaches. CCAD conditions a reconstruction model on adaptively compressed global features, leading to improved generalization and training efficiency. Experiments show that CCAD outperforms state-of-the-art methods in AUC and achieves faster convergence.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Statistical vs. Deep Learning Models for Estimating Substance Overdose Excess Mortality in the US</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [LSTM, SARIMA, conformal prediction, counterfactual estimation, uncertainty quantification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sukanya Krishna, Marie-Laure Charpignon, Maimuna Majumder</p>
</li>
<li class="">
<p><strong>institution:</strong> Harvard University, Boston Children&#x27;s Hospital, Broad Institute of MIT and Harvard, Massachusetts Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21456" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21456</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a systematic empirical comparison showing LSTM outperforms SARIMA in point estimation and uncertainty calibration for counterfactual mortality projection under regime change. 2. Provided analysis that attention-based models (Seq2Seq, Transformer) underperform in this task due to overfitting to historical means. 3. Developed a reproducible pipeline incorporating conformal prediction intervals and extensive convergence analysis, providing an open-source framework deployable for public health planning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f7ae15bb7d75d1d53ce8df2d4924f6311b8bfce998ef81244521cd5679c4225_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f7ae15bb7d75d1d53ce8df2d4924f6311b8bfce998ef81244521cd5679c4225_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper compares traditional statistical (SARIMA) and deep learning models (LSTM, Seq2Seq, Transformer) for estimating substance overdose excess mortality in the US during the COVID-19 pandemic. The study finds that LSTM provides more accurate and better-calibrated counterfactual estimates than SARIMA, establishing that carefully validated deep learning models can be more reliable for public health planning under structural disruptions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] When Bayesian Tensor Completion Meets Multioutput Gaussian Processes: Functional Universality and Rank Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [tensor decomposition], [Bayesian tensor completion, multioutput Gaussian processes, variational inference, rank learning, functional universality]</p>
</li>
<li class="">
<p><strong>authors:</strong> Siyuan Li, Shikai Fang, Lei Cheng, Feng Yin, Yik-Chung Wu, Peter Gerstoft, Sergios Theodoridis</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, The Chinese University of Hong Kong, Shenzhen, The University of Hong Kong, Technical University of Denmark, University of California, San Diego, Athena R.C.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21486" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21486</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/OceanSTARLab/RR-FBTC" target="_blank" rel="noopener noreferrer" class="">https://github.com/OceanSTARLab/RR-FBTC</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a rank-revealing functional Bayesian tensor completion (RR-FBTC) method that handles tensors with real-valued indices and automatically determines the tensor rank during inference. 2. Establishes the universal approximation property of the model, demonstrating its expressive power for continuous multi-dimensional signals. 3. Derives an efficient variational inference algorithm with closed-form updates for learning the model.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac46f14d99db0300a24117fcc6a1f29a54c4ad4ae1586354da1e156d0b048995_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac46f14d99db0300a24117fcc6a1f29a54c4ad4ae1586354da1e156d0b048995_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of determining the optimal rank in functional tensor decomposition, which is NP-hard. It proposes a new method called RR-FBTC that uses multioutput Gaussian processes to model latent functions, enabling automatic rank learning and functional universality. Experiments show the method is effective and superior to state-of-the-art approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [wearable sensing, actigraphy encoder, projection module, frozen LLM, behavioral summarization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aiwei Zhang, Arvind Pillai, Andrew Campbell, Nicholas C. Jacobson</p>
</li>
<li class="">
<p><strong>institution:</strong> Dartmouth College</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21506" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21506</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs) for free-text generation of daily behavioral summaries. 2. Constructs a novel, large-scale dataset of 54,383 (actigraphy, text) pairs derived from real-world NHANES recordings. 3. Demonstrates superior performance over prompt-based baselines in semantic fidelity and lexical accuracy, with qualitative analysis showing the model captures circadian structure and behavioral transitions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of generating natural language summaries from raw physiological signals like actigraphy. It proposes MotionTeller, a framework that integrates a pretrained actigraphy encoder with a frozen LLM via a projection module. The model, trained on a novel dataset, outperforms baselines in generating fluent, human-centered descriptions of daily behavior.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-view clustering], [incomplete multi-view clustering, missing pattern tree, decision ensemble, knowledge distillation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenyuan Yang, Jie Xu, Hongqing He, Jiangzhang Gan, Xiaofeng Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China, Hainan University, Singapore University of Technology and Design, Guangxi Normal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21510" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21510</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a missing-pattern tree model to group data into decision sets for fully utilizing available multi-view pairs, addressing the pair under-utilization issue. 2. Introduces a multi-view decision ensemble module that uses uncertainty-based weighting to aggregate robust clustering decisions from different sets. 3. Designs an ensemble-to-individual knowledge distillation module to transfer ensemble knowledge to view-specific models, promoting mutual optimization between ensemble and individual modules.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of incomplete multi-view clustering (IMVC) where inconsistent missing patterns hinder performance. It proposes a framework called TreeEIC that groups data by missing pattern, performs clustering within each group, ensembles the results with uncertainty weighting, and uses knowledge distillation to refine individual models. Experiments show TreeEIC achieves state-of-the-art performance and robustness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] First Provable Guarantees for Practical Private FL: Beyond Restrictive Assumptions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [differential privacy, convergence guarantees, partial client participation, local updates, clipping bias]</p>
</li>
<li class="">
<p><strong>authors:</strong> Egor Shulgin, Grigory Malinovsky, Sarit Khirirat, Peter Richtárik</p>
</li>
<li class="">
<p><strong>institution:</strong> King Abdullah University of Science and Technology (KAUST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21521" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21521</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Fed-α-NormEC, the first DP FL framework with provable convergence under standard assumptions (without bounded gradients or heterogeneity). 2. Fully supports practical FL features like multiple local updates and, crucially, partial client participation. 3. Provides theoretical analysis showing how partial client participation is essential for real-world deployment and vital for privacy amplification.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffad28b47b2302842557d1ba4a799ee36a241e4ab2529611ee61b38dd671f76d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffad28b47b2302842557d1ba4a799ee36a241e4ab2529611ee61b38dd671f76d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the gap between theoretical private federated learning methods, which rely on unrealistic assumptions, and practical deployment needs. It proposes Fed-α-NormEC, a new framework that provides provable differential privacy and convergence guarantees while supporting key practical features like local updates and partial client participation. Experiments on private deep learning tasks validate the theoretical findings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Perplexity-Aware Data Scaling Law: Perplexity Landscapes Predict Performance for Continual Pre-training</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [continual pre-training, scaling laws, perplexity, data selection, knowledge gap]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lei Liu, Hao Zhu, Yue Shen, Zhixuan Chu, Jian Wang, Jinjie Gu, Kui Ren</p>
</li>
<li class="">
<p><strong>institution:</strong> Ant Group, Zhejiang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21515" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21515</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel perplexity-aware data scaling law that predicts model test loss from the perplexity landscape of domain data, moving beyond dataset size. 2. Introduces the concept of &quot;perplexity landscapes&quot; to quantify the informational value and knowledge gap of candidate training samples. 3. Enables adaptive selection of high-utility data subsets for Continual Pre-training, improving efficiency and performance by prioritizing informative content and reducing redundancy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d01b1cad6e908309c7e813cb1d98f2c41345aa1e4d78cbdaf163996c5d111b4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d01b1cad6e908309c7e813cb1d98f2c41345aa1e4d78cbdaf163996c5d111b4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the inefficiency of scaling data for Continual Pre-training (CPT) of LLMs, where simply adding more data yields diminishing returns. The authors propose a new scaling law that uses the model&#x27;s perplexity on domain data as a proxy for the knowledge gap, allowing for the predictive selection of optimal training subsets. Experiments show this method consistently identifies high-utility data, leading to superior performance on domain-specific benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-view clustering], [contrastive learning, incomplete multi-view data, noise-robust clustering, graph-guided learning, imputation-free]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hongqing He, Jie Xu, Wenyuan Yang, Yonghua Zhu, Guoqiu Wen, Xiaofeng Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> Guangxi Normal University, University of Electronic Science and Technology of China, Singapore University of Technology and Design, Hainan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21516" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21516</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a global-graph guided contrastive learning strategy to address the rare-paired sample issue in incomplete multi-view data by constructing a global affinity graph to form new sample pairs. 2. Introduces a local-graph weighted contrastive learning mechanism to mitigate the mis-paired sample issue caused by noise, using local neighbors to generate adaptive weights for pair-wise contrast. 3. Integrates these strategies into a unified, imputation-free framework for effective clustering on both incomplete and noisy multi-view data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of rare-paired and mis-paired samples in contrastive learning for multi-view clustering on incomplete and noisy data. It proposes a unified framework combining global-graph guided contrastive learning to explore complementary information and local-graph weighted contrastive learning to adaptively handle noisy pairs. Experiments show the method outperforms state-of-the-art approaches on both incomplete and noisy data settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Generative Actor Critic</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [generative modeling, policy evaluation, latent plan, offline-to-online, actor-critic]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aoyang Qin, Deqian Kong, Wei Wang, Ying Nian Wu, Song-Chun Zhu, Sirui Xie</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Beijing Institute of General Artificial Intelligence (BIGAI), UCLA, Peking University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21527" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21527</a></p>
</li>
<li class="">
<p><strong>code:</strong> github.com/qayqaq/Generative-Actor-Critic</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the Generative Actor Critic (GAC) framework that reframes policy evaluation as learning a generative model of the joint distribution over trajectories and returns, decoupling decision-making. 2. Introduces a specific instantiation using a latent variable model with continuous latent plan vectors and novel inference strategies for exploitation and exploration. 3. Demonstrates strong offline performance and significantly enhanced offline-to-online improvement on benchmarks, even without step-wise rewards.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62b697a3a1c4a1b559c19af7d65fd19d2eed0bb097eccecdd9008a509a0456c0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62b697a3a1c4a1b559c19af7d65fd19d2eed0bb097eccecdd9008a509a0456c0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Generative Actor Critic (GAC), a novel reinforcement learning framework that decouples sequential decision-making by learning a generative model of trajectories and returns and then performing inference on it. It shows strong performance in offline learning and significantly improves when fine-tuned online, even in sparse-reward environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] AVP-Fusion: Adaptive Multi-Modal Fusion and Contrastive Learning for Two-Stage Antiviral Peptide Identification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [bioinformatics], [adaptive gating mechanism, contrastive learning, transfer learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xinru Wen, Weizhong Lin, Xuan Xiao</p>
</li>
<li class="">
<p><strong>institution:</strong> JCI (inferred from email domain <code>jci.edu.cn</code>)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21544" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21544</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a two-stage deep learning framework (AVP-Fusion) for antiviral peptide identification and subclass prediction. 2. Introduces an Adaptive Gating Mechanism to dynamically fuse local (CNN) and global (BiLSTM) sequence features. 3. Employs a contrastive learning strategy with OHEM and BLOSUM62-based data augmentation to sharpen decision boundaries and handle hard samples.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72c58b1c8ae5a53950bfc6d9e8fcca6dc27fc849f514d47d4a2cdff795cb10b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72c58b1c8ae5a53950bfc6d9e8fcca6dc27fc849f514d47d4a2cdff795cb10b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes AVP-Fusion, a two-stage deep learning framework that integrates adaptive feature fusion and contrastive learning for identifying antiviral peptides (AVPs). The method dynamically fuses multi-modal sequence features and uses contrastive learning to improve classification, achieving state-of-the-art accuracy and enabling precise prediction of antiviral activity against specific viral families.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Discovering Sparse Recovery Algorithms Using Neural Architecture Search</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [sparse recovery], [neural architecture search, meta-learning, iterative shrinkage thresholding algorithm, sparse optimization, algorithm discovery]</p>
</li>
<li class="">
<p><strong>authors:</strong> Patrick Yubeaton, Sarthak Gupta, M. Salman Asif, Chinmay Hegde</p>
</li>
<li class="">
<p><strong>institution:</strong> New York University, University of California, Riverside</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21563" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21563</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a meta-learning framework using Neural Architecture Search (NAS) for automated discovery of sparse recovery algorithms. 2. Demonstrates the framework&#x27;s capability to rediscover key elements of ISTA and FISTA from a search space of over 50,000 variables. 3. Shows the framework&#x27;s applicability to various data distributions and algorithms beyond ISTA/FISTA.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49f1d5d0a89c3d52b36f1e443675494175442f0930725fc8a763e525ca05243a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49f1d5d0a89c3d52b36f1e443675494175442f0930725fc8a763e525ca05243a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a meta-learning framework that uses Neural Architecture Search (NAS) to automatically discover sparse recovery algorithms. It successfully rediscovers components of ISTA and FISTA from a large search space and demonstrates generalizability to other algorithms and data distributions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] AnchorGK: Anchor-based Incremental and Stratified Graph Learning Framework for Inductive Spatio-Temporal Kriging</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [spatio-temporal kriging], [graph neural networks, incremental learning, data stratification, anchor locations, incomplete features]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiaobin Ren, Kaiqi Zhao, Katerina Taškova, Patricia Riddle</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Auckland, Harbin Institute of Technology, Shenzhen</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21569" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21569</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/xren451/Spatial-interpolation" target="_blank" rel="noopener noreferrer" class="">https://github.com/xren451/Spatial-interpolation</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces an anchor-based stratification framework to handle sparse spatial distributions and heterogeneous feature availability in sensor networks. 2. Proposes a dual-view graph learning layer that jointly aggregates feature-relevant and location-relevant information to learn stratum-specific representations. 3. Designs an incremental representation mechanism that systematically utilizes all available features across different strata to mitigate feature incompleteness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c757748db9cf59d7295a4cdf698c5e194dc4ae79ec767aa6f7c71c0e78243768_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c757748db9cf59d7295a4cdf698c5e194dc4ae79ec767aa6f7c71c0e78243768_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes AnchorGK, a graph learning framework for inductive spatio-temporal kriging that addresses sparse sensor deployment and incomplete features. It uses anchor-based stratification and a dual-view graph learning layer to model correlations and incrementally integrate features. Experiments show it outperforms existing state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [compiler &amp; ir], [e-graph, term rewriting, phase ordering, NUMA abstraction, auto vectorize]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hui Guo, Qihang Zheng, Chenghai Huo, Dongliang Guo, Haoqi Yang, Yang Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Canaan Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21571" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21571</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/kendryte/nncase" target="_blank" rel="noopener noreferrer" class="">https://github.com/kendryte/nncase</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an end-to-end compilation framework (nncase) that unifies LLM deployment across heterogeneous memory architectures using a NUMA abstraction for a &quot;compile once, adapt everywhere&quot; capability. 2. Introduces an e-graph-based term rewriting engine with equality saturation to mitigate the phase ordering problem and enable global optimization of computation and data movement. 3. Integrates three key automated optimization modules: Auto Vectorize for heterogeneous computing units, Auto Distribution for parallel strategies with communication optimization, and Auto Schedule for on-chip cache locality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a555b38ecd80e8c11606362e5402405bbf0053e11754e06f720d50ce213ee0d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a555b38ecd80e8c11606362e5402405bbf0053e11754e06f720d50ce213ee0d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents nncase, an end-to-end compiler framework designed to tackle the challenge of efficiently deploying large language models on heterogeneous memory architectures. Its core innovation is an e-graph-based rewriting engine that avoids the phase ordering problem, enabling unified optimization across diverse hardware targets. Evaluations show nncase outperforms frameworks like MLC LLM and Intel IPEX, achieving performance close to hand-optimized llama.cpp, demonstrating the viability of automated compilation for high-performance LLM deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Unified Definition of Hallucination, Or: It&#x27;s the World Model, Stupid</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [hallucination detection &amp; evaluation], [hallucination, world modeling, knowledge conflict, benchmark, language model evaluation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Emmy Liu, Varun Gangal, Chelsea Zou, Xiaoqi Huang, Michael Yu, Alex Chang, Zhuofu Tao, Sachin Kumar, Steven Y. Feng</p>
</li>
<li class="">
<p><strong>institution:</strong> Carnegie Mellon University, Stanford University, The Ohio State University, Patronus AI, DegenAI Labs, Independent Researchers</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21577" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21577</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified definition of hallucination as inaccurate internal world modeling that is observable to the user, synthesizing prior definitions. 2. Provides a framework for analyzing hallucinations by varying the reference world model and knowledge conflict policy, clarifying what constitutes a hallucination versus other error types. 3. Outlines plans for a family of benchmarks based on synthetic, fully-specified world models to stress-test and improve the world modeling components of language models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e2cc31121f769cca7464239d4aa27b26c8c4bc903970a4833fbebac56dc9b85_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e2cc31121f769cca7464239d4aa27b26c8c4bc903970a4833fbebac56dc9b85_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper argues that the persistent problem of hallucination in language models stems from inaccurate internal world modeling. It unifies various historical definitions under this core concept and proposes a framework for clearer evaluation. The authors conclude by sketching plans for new benchmarks to rigorously test and improve language models&#x27; world modeling capabilities.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [Schrödinger Bridge, Low-rank Adaptation, Time Series Foundation Models, Financial Forecasting, Generative Refinement]</p>
</li>
<li class="">
<p><strong>authors:</strong> Anthony Bolton, Wuyang Zhou, Zehua Chen, Giorgos Iacovides, Danilo Mandic</p>
</li>
<li class="">
<p><strong>institution:</strong> Imperial College London</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21572" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21572</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes RefineBridge, a novel post-processing module built on a tractable Schrödinger Bridge framework to refine TSFM forecasts. 2. Introduces a paradigm that treats TSFM predictions as a generative prior and learns context-conditioned stochastic transport maps to iteratively improve them. 3. Demonstrates consistent performance improvements over state-of-the-art TSFMs on multiple financial benchmarks, addressing challenges like non-stationarity and noise.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d984f91104f06a2169d1d04626078f4bd0698ade16094c7642b5b47c5a1a6198_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d984f91104f06a2169d1d04626078f4bd0698ade16094c7642b5b47c5a1a6198_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of applying Time Series Foundation Models (TSFMs) to financial forecasting, where data properties like non-stationarity degrade performance. It proposes RefineBridge, a generative refinement module based on Schrödinger Bridges that iteratively transports TSFM predictions toward ground truths. Experiments show RefineBridge consistently enhances TSFM forecasts across various financial benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [imitation learning], [behavior cloning, latent representation, self-supervised learning, sample efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xin Liu, Haoran Li, Dongbin Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21586" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21586</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel, unsupervised framework (BCV-LR) for imitation learning from videos (ILV) that learns latent actions from visual inputs. 2. Introduces an iterative policy improvement loop that aligns pre-trained latent actions with the real action space online, enabling highly sample-efficient learning. 3. Demonstrates state-of-the-art sample efficiency, outperforming existing ILV and RL methods on a wide range of visual control tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c96d71be701998ebf55ef6ed2a0c0a001a306b44f3555239559ced527b17559_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c96d71be701998ebf55ef6ed2a0c0a001a306b44f3555239559ced527b17559_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes BCV-LR, a framework for learning policies from videos without action labels. It uses self-supervised learning to extract latent actions and an iterative alignment process for sample-efficient behavior cloning. The method achieves expert-level performance on many tasks with minimal interaction, showing videos can be highly effective supervision for policy learning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Quantitative Verification of Omega-regular Properties in Probabilistic Programming</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [probabilistic programming and verification], [temporal posterior inference, omega-regular properties, stochastic barrier certificates, Rabin automata, quantitative verification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Peixin Wang, Jianhao Bai, Min Zhang, C.-H. Luke Ong</p>
</li>
<li class="">
<p><strong>institution:</strong> East China Normal University, Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21596" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21596</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Temporal Posterior Inference (TPI), a new framework unifying probabilistic programming with temporal logic to compute posterior distributions over execution traces satisfying omega-regular properties. 2. Develops a novel method for computing rigorous upper and lower bounds on satisfaction probabilities by decomposing Rabin acceptance conditions and constructing sound stochastic barrier certificates. 3. Implements the approach in a prototype tool named TPInfer and demonstrates its effectiveness and efficiency on a suite of benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c111a01f9d5e96a85d9b5c62645dae0f5bb40053d723e34cb57dc7f31554dcda_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c111a01f9d5e96a85d9b5c62645dae0f5bb40053d723e34cb57dc7f31554dcda_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of standard probabilistic program inference, which fails to capture temporal behavior, by proposing Temporal Posterior Inference (TPI). TPI computes posterior distributions over program traces that satisfy omega-regular temporal specifications, using a method based on stochastic barrier certificates to provide quantitative verification bounds. The approach is implemented in the TPInfer tool and shown to be effective for inference over rich temporal properties.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data in Emergency and Critical Care</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [imbalanced classification], [XGBoost, TabNet, TabResNet, Bayesian hyperparameter search, class imbalance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yusuf Brima, Marcellin Atemkeng</p>
</li>
<li class="">
<p><strong>institution:</strong> Osnabrück University, Rhodes University, National Institute for Theoretical and Computational Sciences (NITheCS)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21602" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21602</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Systematic evaluation of robustness and scalability for classical ML and deep learning models on imbalanced clinical tabular data. 2. Introduction of TabResNet, a custom lightweight residual network designed as a computationally efficient alternative to TabNet for real-time clinical use. 3. Evidence-based finding that tree-based ensemble methods (especially XGBoost) offer the most stable and scalable performance for high-stakes, time-sensitive clinical environments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper evaluates the robustness and scalability of machine learning models on imbalanced clinical data from emergency and critical care settings. It proposes TabResNet, a lightweight deep learning alternative to TabNet, and compares it against tree-based methods like XGBoost. The main conclusion is that tree-based ensemble models, particularly XGBoost, provide the most stable performance and computational scalability for practical clinical deployment, outperforming more complex deep learning architectures in these environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Data-Driven Multi-Objective Approach for Predicting Mechanical Performance, Flowability, and Porosity in Ultra-High-Performance Concrete (UHPC)</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [predictive modeling], [XGBoost, SHAP analysis, K-Fold Cross-Validation, Isolation Forest, hyperparameter tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jagaran Chakma, Zhiguang Zhou, Jyoti Chakma, Cao YuSen</p>
</li>
<li class="">
<p><strong>institution:</strong> Tongji University, University of Chittagong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21610" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21610</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a two-stage data-driven framework for UHPC property prediction, integrating model selection, data cleaning (multicollinearity removal, outlier detection), and feature importance analysis. 2. Demonstrated the superior performance of the XGBoost model after rigorous hyperparameter tuning and validation, achieving high accuracy across multiple objectives (mechanical performance, flowability, porosity). 3. Created a practical graphical user interface (GUI) to facilitate the application of the predictive model by material designers, reducing reliance on extensive experimental testing.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/988c9f68c148fb1ef37ff8e292bf9a2cab1878ea08a2f5baee1a1a47f095be23_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/988c9f68c148fb1ef37ff8e292bf9a2cab1878ea08a2f5baee1a1a47f095be23_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a data-driven, multi-objective machine learning framework to predict the mechanical performance, flowability, and porosity of Ultra-High-Performance Concrete (UHPC). The method involves testing 21 algorithms, selecting and tuning XGBoost, and refining the dataset through multicollinearity removal, outlier detection, and SHAP-based feature selection. The final model achieves high prediction accuracy, and a supporting GUI is developed to aid in UHPC mix design, reducing the need for experimental tests.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] MAD-NG: Meta-Auto-Decoder Neural Galerkin Method for Solving Parametric Partial Differential Equations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scientific machine learning], [Neural Galerkin Method, meta-learning, parametric PDEs, space-time decoupling, randomized sparse updates]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qiuqi Li, Yiting Liu, Jin Zhao, Wencan Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> Hunan University, Capital Normal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21633" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21633</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel framework that enhances the Neural Galerkin Method by integrating the Meta-Auto-Decoder paradigm for solving parametric PDEs. 2. Introduces space-time decoupling to enable more stable and efficient time integration compared to full space-time approximations. 3. Employs meta-learning for rapid adaptation to new parameters and randomized sparse updates to reduce computational cost without sacrificing accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8574d601c121a7a53ef19693b53d019c5139f87d5ae2dd641aec204463aa59c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8574d601c121a7a53ef19693b53d019c5139f87d5ae2dd641aec204463aa59c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenges of generalization and efficiency in neural network-based solvers for parametric PDEs. It proposes MAD-NG, a framework that combines the Neural Galerkin Method with meta-learning and space-time decoupling to achieve stable, efficient, and accurate long-term predictions. Numerical experiments show the method performs well in accuracy, robustness, and adaptability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Mechanical Strength Prediction of Steel-Polypropylene Fiber-based High-Performance Concrete Using Hybrid Machine Learning Algorithms</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [materials informatics], [hybrid machine learning, SHAP analysis, uncertainty quantification, strength prediction, high-performance concrete]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jagaran Chakma, Zhiguang Zhou, Badhan Chakma</p>
</li>
<li class="">
<p><strong>institution:</strong> Tongji University, Chongqing Jiaotong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21638" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21638</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed and evaluated three novel hybrid machine learning model families (ET-XGB, RF-LGBM, Transformer-XGB) for predicting the mechanical properties of fiber-reinforced concrete. 2. Conducted a comprehensive evaluation including k-fold cross-validation, hyperparameter optimization, SHAP analysis for interpretability, and uncertainty quantification for robustness assessment. 3. Identified key influential material parameters (fiber aspect ratios, silica fume, steel fiber content) and negative factors (water content, water-binder ratio) on concrete strength through SHAP analysis.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76eefc8193c0d98df6f997d7ae3134d34a448f8aba255f28a39044233c1f3bb8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76eefc8193c0d98df6f997d7ae3134d34a448f8aba255f28a39044233c1f3bb8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This research develops hybrid machine learning models to predict the compressive, flexural, and tensile strength of steel-polypropylene fiber-reinforced high-performance concrete. The study compares three hybrid models (ET-XGB, RF-LGBM, Transformer-XGB) and finds that the ET-XGB model achieved the highest overall accuracy, while SHAP analysis reveals the most influential material factors. The findings confirm that these ML models provide accurate and interpretable tools for optimizing concrete mix design in engineering applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Monte Carlo Tree Search, Upper Confidence Bound, Variance-Aware, Prior-Based Tree Policy, Inverse-RPO]</p>
</li>
<li class="">
<p><strong>authors:</strong> Maximilian Weichart</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Regensburg</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21648" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21648</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Max-We/inverse-rpo" target="_blank" rel="noopener noreferrer" class="">https://github.com/Max-We/inverse-rpo</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Inverse-RPO, a general methodology to systematically derive prior-based UCTs from any prior-free UCB., 2. Applies Inverse-RPO to UCB-V to create two new variance-aware prior-based tree policies., 3. Provides an extension to the mctx library for variance-aware UCTs, showing minimal code changes and improved performance over PUCT in benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c9098504a8a9013ab805fffa4a23f04af76e28f5d8b8a0353e1e7d5583f589_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c9098504a8a9013ab805fffa4a23f04af76e28f5d8b8a0353e1e7d5583f589_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of extending prior-based tree policies in Monte Carlo Tree Search beyond the empirically derived PUCT. The authors propose Inverse-RPO, a principled method to derive prior-based UCTs from any prior-free UCB, and apply it to create variance-aware policies. Their new policies outperform the standard PUCT across multiple benchmarks without added computational cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Causal-HM: Restoring Physical Generative Logic in Multimodal Anomaly Detection via Hierarchical Modulation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [anomaly detection], [multimodal fusion, causal modeling, hierarchical modulation, sensor guidance, unsupervised learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiao Liu, Junchen Jin, Yanjie Zhao, Zhixuan Xing</p>
</li>
<li class="">
<p><strong>institution:</strong> Chongqing University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21650" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21650</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified multimodal UAD framework (Causal-HM) that explicitly models the physical Process→Result dependency to address causal blindness. 2. Introduces a Sensor-Guided CHM Modulation mechanism that uses low-dimensional sensor signals as context to guide high-dimensional audio-visual feature extraction. 3. Designs a Causal-Hierarchical Architecture that enforces a unidirectional generative mapping to identify anomalies violating physical consistency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f0b7760ac76038dd08b0ccd2890cb2628906dcf233282246d08ee584093193_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f0b7760ac76038dd08b0ccd2890cb2628906dcf233282246d08ee584093193_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of causal blindness and the heterogeneity gap in multimodal anomaly detection for industrial processes like welding. It proposes the Causal-HM framework, which models the physical generative logic from process to result modalities using sensor-guided modulation and a causal-hierarchical architecture. The method achieves state-of-the-art performance on a new Weld-4M benchmark, demonstrating its effectiveness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Semantic Codebooks as Effective Priors for Neural Speech Compression</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [speech compression], [semantic codebooks, residual vector quantization (RVQ), HuBERT, FiLM-conditioned decoder, neural audio codec]</p>
</li>
<li class="">
<p><strong>authors:</strong> Liuyang Bai, Weiyi Lu, Li Guo</p>
</li>
<li class="">
<p><strong>institution:</strong> NYU Shanghai</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21653" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21653</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SemDAC, a semantic-aware neural audio codec that uses semantic codebooks as priors for compression., 2. Introduces a design where the first RVQ quantizer is distilled from HuBERT to capture phonetic content, and a FiLM-conditioned decoder uses these semantic tokens., 3. Demonstrates superior performance over baseline DAC in perceptual metrics and ASR (Whisper) WER at significantly lower bitrates.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a14c953c6c23139c4473b8d6e59b36c7615f79f4316119e168deb19de30eced_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a14c953c6c23139c4473b8d6e59b36c7615f79f4316119e168deb19de30eced_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes SemDAC, a neural speech codec that uses semantic codebooks distilled from HuBERT as priors within an RVQ framework to separate phonetic from acoustic information. This method achieves better perceptual quality and lower word error rates for speech recognition at much lower bitrates compared to traditional neural codecs. The results show that semantic priors provide an effective inductive bias for efficient, recognition-friendly speech compression.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Rethinking Output Alignment For 1-bit Post-Training Quantization of Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [1-bit quantization, post-training quantization, output alignment, activation error, large language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dung Anh Hoang, Cuong Pham, Cuong Nguyen, Trung le, Jianfei Cai, Thanh-Toan Do</p>
</li>
<li class="">
<p><strong>institution:</strong> Monash University, University of Surrey</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21651" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21651</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Investigates the failure conditions of naive output-matching in 1-bit LLM quantization, 2. Proposes a novel data-aware PTQ approach that explicitly accounts for activation error accumulation, 3. Demonstrates consistent performance improvements over existing 1-bit PTQ methods with minimal overhead</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0357717d29d733787933b581fbbb292b187b9fbc651fbc0f15bb04ef03e619eb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0357717d29d733787933b581fbbb292b187b9fbc651fbc0f15bb04ef03e619eb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the significant performance degradation in 1-bit post-training quantization (PTQ) of Large Language Models. The authors propose a new data-aware PTQ method that efficiently accounts for activation error accumulation. Their solution is shown to consistently outperform existing 1-bit PTQ approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [deepfake detection], [mechanistic interpretability, sparse autoencoder, forensic manifold analysis, feature selectivity, vision-language model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Subramanyam Sahoo, Jared Junkin</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Berkeley, Johns Hopkins University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21670" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21670</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/SubramanyamSahoo/The-Deepfake-Detective" target="_blank" rel="noopener noreferrer" class="">https://github.com/SubramanyamSahoo/The-Deepfake-Detective</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a novel mechanistic interpretability framework specifically for deepfake detection, combining sparse autoencoder analysis with forensic manifold analysis. 2. Demonstrates that only a small fraction of latent features are actively used in each layer of the model for detection. 3. Shows that geometric properties of the model&#x27;s feature manifold (e.g., intrinsic dimensionality, curvature) vary systematically with different types of deepfake artifacts, linking learned features to specific forensic cues.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the &quot;black box&quot; nature of deepfake detectors by proposing a mechanistic interpretability framework that analyzes a vision-language model&#x27;s internal representations. The method uses sparse autoencoders and a novel forensic manifold analysis to probe how the model&#x27;s features respond to forensic artifacts. The key findings are that detection relies on a sparse set of features and that the geometry of the feature manifold correlates with specific artifact types, providing a step towards more interpretable and robust detectors.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [YOLO-NAS, YOLOv8, perception, autonomous vehicles, custom dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jalal Khan</p>
</li>
<li class="">
<p><strong>institution:</strong> United Arab Emirates University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21673" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21673</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Dictionary-Transform Generative Adversarial Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative adversarial networks], [dictionary learning, transform learning, sparse modeling, adversarial learning, nash equilibrium]</p>
</li>
<li class="">
<p><strong>authors:</strong> Angshul Majumdar</p>
</li>
<li class="">
<p><strong>institution:</strong> Indraprastha Institute of Information Technology Delhi (IIIT-D)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21677" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21677</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces DT-GAN, a fully model-based adversarial framework with a sparse synthesis dictionary generator and an analysis transform discriminator, enabling rigorous theoretical analysis. 2. Proves the adversarial game is well-posed, admits at least one Nash equilibrium, and that equilibrium solutions are identifiable under a sparse generative model. 3. Establishes finite-sample stability and consistency of empirical equilibria, demonstrating reliable convergence and robustness, validated on synthetic data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e368966cab749cb3ef0799abfb9a31d801a73291e7a5d2b4a5b81fd185a9d25_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e368966cab749cb3ef0799abfb9a31d801a73291e7a5d2b4a5b81fd185a9d25_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the theoretical fragility and instability of classical GANs by proposing DT-GAN, a model-based adversarial framework where the generator and discriminator are constrained linear operators (a sparse synthesis dictionary and an analysis transform). The authors prove the framework is well-posed, has identifiable equilibria, and converges reliably, demonstrating that adversarial learning can be made interpretable and provably correct for data with sparse structure.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [spatiotemporal forecasting], [probabilistic forecasting, uncertainty estimation, principal component analysis, road impedance, traffic flow]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haochen Lv, Yan Lin, Shengnan Guo, Xiaowei Mao, Hong Nie, Letian Gong, Youfang Lin, Huaiyu Wan</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Jiaotong University, Aalborg University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21685" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21685</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a dynamic impedance evolution network to model directional traffic transfer patterns driven by congestion and flow variability, revealing causes of uncertainty and enhancing reliability and interpretability. 2. Designs a principal component network to forecast the dominant eigenvectors of future flow covariance, enabling the capture of spatiotemporal uncertainty correlations. 3. Integrates domain-specific transportation theory with spatiotemporal principal component learning for probabilistic traffic flow forecasting, achieving superior performance over existing methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f835b2051524d67481fbf9f4479086a541b29307c2876046f2c3ee4eec60f92_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f835b2051524d67481fbf9f4479086a541b29307c2876046f2c3ee4eec60f92_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes RIPCN, a Road Impedance Principal Component Network for probabilistic traffic flow forecasting. It integrates transportation theory with principal component learning to model the causes of uncertainty and capture spatiotemporal uncertainty correlations. Experimental results show it outperforms existing probabilistic forecasting methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [communication &amp; networking], [multiconnectivity, SAGIN, resource allocation, agentic reinforcement learning, heterogeneous networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abd Ullah Khan, Adnan Shahid, Haejoon Jung, Hyundong Shin</p>
</li>
<li class="">
<p><strong>institution:</strong> Kyung Hee University, Ghent University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21717" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21717</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a comprehensive review of current developments and key implementation challenges in SAGIN-enabled multiconnectivity. 2. Highlights the transformative potential of AI-driven approaches, particularly agentic reinforcement learning, for resource optimization in heterogeneous SAGIN environments. 3. Presents a case study demonstrating that learning-based methods can effectively enhance network performance (latency, capacity) with a moderate trade-off in power consumption.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31a130dffac74abb8ad0616817d555bf857333050b690554853596eda30c2fa7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31a130dffac74abb8ad0616817d555bf857333050b690554853596eda30c2fa7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper reviews the challenges of implementing multiconnectivity in heterogeneous Space-Air-Ground Integrated Networks (SAGIN) and proposes AI-driven solutions, specifically agentic reinforcement learning, for optimal resource allocation. A case study shows these methods significantly improve latency and capacity, albeit with a moderate increase in power consumption as a trade-off. The work concludes by outlining open research problems for realizing efficient SAGIN-enabled multiconnectivity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] An Information Theoretic Perspective on Agentic System Design</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [mutual information, noisy channel, compressor-predictor, on-device AI, information-theoretic]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shizhe He, Avanika Narayan, Ishan S. Khare, Scott W. Linderman, Christopher Ré, Dan Biderman</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21720" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21720</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an information-theoretic framework for analyzing agentic LM systems, viewing the compressor as a noisy channel. 2. Introduces a task-independent estimator of mutual information between context and compression to quantify compression quality. 3. Empirically demonstrates that scaling compressor models is more effective than scaling predictors for performance and cost, enabling efficient on-device compression.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124535642e159e8f7a123525ffbf3cb5f163a7ae4a7876a4d1e71e7e6c885ace_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124535642e159e8f7a123525ffbf3cb5f163a7ae4a7876a4d1e71e7e6c885ace_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the ad-hoc design of agentic LM systems that use a compressor LM to summarize context for a predictor LM. It proposes an information-theoretic framework using mutual information to evaluate compressors, finding that larger compressors are more accurate, concise, and information-dense, making scaling compressors more effective than scaling predictors for cost-efficient performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] HELP: Hierarchical Embodied Language Planner for Household Tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [embodied agent, hierarchical planning, large language model, household tasks, open source LLM]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alexandr V. Korchemnyi, Anatoly O. Onishchenko, Eva A. Bakaeva, Alexey K. Kovalev, Aleksandr I. Panov</p>
</li>
<li class="">
<p><strong>institution:</strong> MIRAI, Cognitive AI Systems Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21723" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21723</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Hierarchical Embodied Language Planner (HELP) architecture using multiple LLM-based agents for decomposing and grounding natural language instructions. 2. Demonstrates the approach on a real-world household task using an embodied agent. 3. Focuses on the use of relatively small, open-source LLMs to enable autonomous deployment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of planning for embodied agents following ambiguous natural language instructions in complex environments. It proposes HELP, a hierarchical planner using multiple LLM-based agents to decompose high-level instructions into grounded, executable subtasks. The method is evaluated on a household task with a real robot, showing the feasibility of using smaller, open-source LLMs for autonomous operation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Model of Causal Explanation on Neural Networks for Tabular Data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [explainable ai], [CENNET, structural causal models, entropy, causal explanation, tabular data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Takashi Isozaki, Masahiro Yamamoto, Atsushi Noda</p>
</li>
<li class="">
<p><strong>institution:</strong> Sony Computer Science Laboratories, Inc., Sony Corporation of America</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21746" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21746</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CENNET, a novel causal explanation method for neural network predictions on tabular data. 2. Introduces a new explanation power index based on entropy for evaluating the proposed method. 3. Demonstrates the method&#x27;s effectiveness by combining structural causal models with neural networks for causal explanations, validated on synthetic and quasi-real data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02e8ba8968728eba560984773ed8ba2b124e42c46e3c839dec8a1ca2a5975ce1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02e8ba8968728eba560984773ed8ba2b124e42c46e3c839dec8a1ca2a5975ce1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of providing causal explanations for neural network predictions on tabular data, where pseudo-correlations can mislead. The authors propose a method called CENNET, which integrates structural causal models with neural networks to generate causal explanations and introduces an entropy-based index to measure explanation power. Experiments on synthetic and quasi-real data show that CENNET effectively provides causal explanations compared to existing methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [continual learning], [entropy scaling, catastrophic forgetting, stability-plasticity dilemma, dynamic feedback, layer-wise control]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hengyi Wu, Zhenyi Wang, Heng Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Maryland, College Park, University of Central Florida</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21743" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21743</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel entropy-aware continual learning method that uses a dynamic feedback mechanism to regulate each layer based on its entropy, addressing underfitting and overfitting. 2. Introduces Self-Adaptive Entropy Scaling, which adaptively adjusts regularization strength per layer to encourage convergence to wider local minima for better generalization. 3. Presents a complementary adaptive training mechanism that modulates layer plasticity based on performance, preserving knowledge in high-performing layers while amplifying updates in underperforming ones.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses catastrophic forgetting in continual learning by proposing a dynamic feedback mechanism that regulates each neural network layer based on its entropy. This entropy-aware method reduces entropy in high-entropy layers to prevent underfitting and increases it in low-entropy layers to prevent overfitting, promoting convergence to wider minima for improved generalization. The approach is general and integrates with existing replay- and regularization-based methods, showing substantial performance gains over state-of-the-art baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Approximation Capabilities of Feedforward Neural Networks with GELU Activations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [neural network approximation theory], [GELU activation, feedforward neural networks, approximation error bounds, derivative approximation, constructive approximation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Konstantin Yakovlev, Nikita Puchkin</p>
</li>
<li class="">
<p><strong>institution:</strong> HSE University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21749" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21749</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Derivation of simultaneous approximation error bounds for a function and all its derivatives up to any prescribed order using GELU networks. 2. Constructive approximation guarantees for elementary operations (multiplication, division, exponential) with control over network size, weight magnitudes, and behavior at infinity. 3. Extension of approximation results to multivariate polynomials and other elementary functions, ensuring global boundedness of higher-order derivatives of the approximators.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af63acd2ab9f3e42763901f23d1762f6658fd25b72daf9e1f1f73e7b30ce1173_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af63acd2ab9f3e42763901f23d1762f6658fd25b72daf9e1f1f73e7b30ce1173_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes the approximation capabilities of feedforward neural networks using the GELU activation function. The authors provide constructive methods to approximate elementary functions and their derivatives, proving simultaneous error bounds for the function and all its higher-order derivatives. The main conclusion is that GELU networks can effectively approximate key operations like multiplication, division, and exponentiation with controlled network complexity and globally bounded derivatives.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Assessing the Effectiveness of Membership Inference on Generative Music</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [membership inference attacks], [membership inference attack (MIA), generative music, MuseGAN, privacy, copyright]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kurtis Chow, Omar Samiullah, Vinesh Sridhar, Hewen Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Irvine</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21762" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21762</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducts the first preliminary study on the effectiveness of membership inference attacks (MIAs) specifically on generative music models., 2. Evaluates several existing MIA techniques on the popular MuseGAN model to assess their performance in this domain., 3. Provides empirical evidence suggesting that generative music data is relatively resilient to known membership inference techniques, aligning with prior findings in generative audio.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09b7176fccbb69c4f0a15bfa6bcbb6ff59b09cf6ac02e0f524334f306ce4041f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09b7176fccbb69c4f0a15bfa6bcbb6ff59b09cf6ac02e0f524334f306ce4041f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether membership inference attacks (MIAs) are effective against generative music models. The authors conduct a preliminary study by applying several existing MIA techniques to the MuseGAN model. Their findings indicate that generative music data is fairly resilient to these known attacks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3d medical image analysis], [Masked Autoencoder, Swin Transformer, Self-Supervised Learning, 3D Vision Transformer, Structural Priority Loss]</p>
</li>
<li class="">
<p><strong>authors:</strong> Evgeny Alves Limarenko, Anastasiia Studenikina</p>
</li>
<li class="">
<p><strong>institution:</strong> Moscow Institute of Physics and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21769" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21769</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed BertsWin, a hybrid architecture combining full BERT-style token masking with Swin Transformer windows to preserve 3D spatial topology during SSL pre-training. 2. Introduced a structural priority loss function to enhance learning. 3. Demonstrated significant acceleration in semantic convergence (5.8x) and a 15-fold reduction in training epochs to reach SOTA fidelity when combined with the GradientConductor optimizer, without increasing computational FLOPs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the difficulty of applying standard Masked Autoencoders to 3D medical images, which lose spatial context. It proposes BertsWin, a hybrid architecture that maintains a full 3D token grid using Swin Transformer windows and a structural loss. The method achieves much faster convergence and state-of-the-art reconstruction fidelity for 3D CBCT scans without extra computational cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Accelerating Scientific Discovery with Autonomous Goal-evolving Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scientific discovery agents], [autonomous goal evolution, bi-level optimization, LLM agents, objective function design, scientific discovery]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuanqi Du, Botao Yu, Tianyu Liu, Tony Shen, Junwu Chen, Jan G. Rittig, Kunyang Sun, Yikun Zhang, Zhangde Song, Bo Zhou, Cassandra Masschelein, Yingze Wang, Haorui Wang, Haojun Jia, Chao Zhang, Hongyu Zhao, Martin Ester, Teresa Head-Gordon, Carla P. Gomes, Huan Sun, Chenru Duan, Philippe Schwaller, Wengong Jin</p>
</li>
<li class="">
<p><strong>institution:</strong> Cornell University, The Ohio State University, Yale University, Simon Fraser University, École Polytechnique Fédérale de Lausanne, University of California Berkeley, Northeastern University, Deep Principle, University of Illinois Chicago, Georgia Institute of Technology, Broad Institute of MIT and Harvard</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21782" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21782</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and addresses the unmet requirement of automating objective function design for scientific discovery agents, moving beyond fixed, imperfect proxies. 2. Proposes the SAGA framework, a novel bi-level architecture where an outer loop of LLM agents evolves objectives and an inner loop optimizes solutions under them. 3. Demonstrates the framework&#x27;s effectiveness across diverse scientific domains (antibiotic, materials, DNA, chemical process design), showing improved discovery outcomes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9035dcf8fd16c34c235e39c8960c63fa826c45df283663a01722181f7ab419d8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9035dcf8fd16c34c235e39c8960c63fa826c45df283663a01722181f7ab419d8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces SAGA, a framework for scientific discovery where LLM agents autonomously evolve and refine the objective functions used to guide optimization, rather than relying on fixed human-specified goals. This bi-level architecture enables systematic exploration of objective spaces and their trade-offs. The method is shown to substantially improve the effectiveness of discovery agents across multiple application domains.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Synthetic Financial Data Generation for Enhanced Financial Modelling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [synthetic data generation], [synthetic financial data, TimeGAN, ARIMA-GARCH, VAE, Maximum Mean Discrepancy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Christophe D. Hounwanou, Yae Ulrich Gaba, Pierre Ntakirutimana</p>
</li>
<li class="">
<p><strong>institution:</strong> AIMS Rwanda, Carnegie Mellon University, Sefako Makgatho Health Sciences University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21791" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21791</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified multi-criteria evaluation framework for synthetic financial data. 2. Empirically compares three generative paradigms (ARIMA-GARCH, VAE, TimeGAN) on fidelity, temporal structure, and downstream utility. 3. Provides practical guidelines for model selection and releases a reproducible codebase to standardize benchmarking.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3f9e4bbcf426a2c32c05089e0964e9937c0becca521cac8a83b48c652d0d86c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3f9e4bbcf426a2c32c05089e0964e9937c0becca521cac8a83b48c652d0d86c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses data scarcity in finance by proposing a unified evaluation framework for synthetic financial data generation. It compares ARIMA-GARCH, VAE, and TimeGAN models using S&amp;P 500 data, finding that TimeGAN offers the best trade-off between realism and temporal coherence. The work concludes with practical guidelines and aims to standardize benchmarking in the field.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Multi-agent Adaptive Mechanism Design</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [mechanism design], [distributionally robust optimization, online learning, incentive compatibility, adaptive mechanism, regret analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qiushi Han, David Simchi-Levi, Renfei Tan, Zishuo Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Massachusetts Institute of Technology, University of Illinois Urbana-Champaign</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21794" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21794</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces DRAM, a novel framework combining mechanism design and online learning to handle unknown agent beliefs. 2. Provides theoretical guarantees of high-probability truthfulness and achieves optimal <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mo stretchy="false">{</mo><mo>~</mo></mover><mi>O</mi><mo stretchy="false">}</mo><mo stretchy="false">(</mo><msqrt><mo stretchy="false">{</mo></msqrt><mi>T</mi><mo stretchy="false">}</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tilde\{O\}(\sqrt\{T\})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2919em;vertical-align:-0.305em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9869em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mopen">{</span></span><span style="top:-3.669em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">~</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.25em"><span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mclose">}</span><span class="mopen">(</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.935em"><span class="svg-align" style="top:-3.2em"><span class="pstrut" style="height:3.2em"></span><span class="mopen" style="padding-left:1em">{</span></span><span style="top:-2.895em"><span class="pstrut" style="height:3.2em"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.28em" viewBox="0 0 400000 1296" preserveAspectRatio="xMinYMin slice"><path d="M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.305em"><span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mclose">})</span></span></span></span> cumulative regret with a matching lower bound. 3. Generalizes the framework (DRAM+) to support plug-in estimators, structured priors, and delayed feedback.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ece7d8b60855735e1eee48c51256eacf5f3997d56ced3396434f12a30ad44_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ece7d8b60855735e1eee48c51256eacf5f3997d56ced3396434f12a30ad44_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of designing a truthful mechanism when the principal has no prior knowledge of agents&#x27; beliefs. It proposes the Distributionally Robust Adaptive Mechanism (DRAM), which iteratively learns beliefs and updates a robust optimization problem to minimize cost while ensuring truthfulness. The mechanism is proven to achieve optimal regret, and the framework is the first to maintain truthfulness under these general learning conditions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] VAMP-Net: An Interpretable Multi-Path Framework of Genomic Permutation-Invariant Set Attention and Quality-Aware 1D-CNN for MTB Drug Resistance</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [bioinformatics], [Set Attention Transformer, 1D-CNN, Multi-Path Network, Interpretable Machine Learning, Genomic Variant Analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aicha Boutorh, Kamar Hibatallah Baghdadi, Anais Daoud</p>
</li>
<li class="">
<p><strong>institution:</strong> National School of Artificial Intelligence (ENSIA)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21786" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21786</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel multi-path deep learning architecture (VAMP-Net) that combines a permutation-invariant Set Attention Transformer for capturing epistatic interactions and a 1D-CNN for analyzing sequencing quality metrics. 2. Introduces a comparative evaluation of unmasked vs. padding-masked Set Attention Blocks within the architecture. 3. Provides dual-layer interpretability through attention weight analysis for epistatic networks and gradient-based methods for feature importance on both genetic variants and quality metrics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf587b5249dfee9f3a9866a887e3d791367588c15a447d0c86619ea9c8df8a45_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf587b5249dfee9f3a9866a887e3d791367588c15a447d0c86619ea9c8df8a45_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces VAMP-Net, a multi-path deep learning framework designed to predict drug resistance in Mycobacterium tuberculosis. It combines a Set Attention Transformer to model genetic interactions and a 1D-CNN to assess data quality, achieving high accuracy and AUC. The work concludes that this approach offers superior predictive performance and dual-layer interpretability for clinical genomics.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [cluster infrastructure], [LSTM, Random Forest, MQTT, InfluxDB, Streamlit]</p>
</li>
<li class="">
<p><strong>authors:</strong> Krishna Chaitanya Sunkara, Rambabu Konakanchi</p>
</li>
<li class="">
<p><strong>institution:</strong> Oracle, Charles Schwab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21801" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21801</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Probabilistic LSTM forecasting validated within ±30-minute windows for coolant leaks, 2. 96.5% F1-score Random Forest detection for immediate leak identification, 3. Integrated smart IoT architecture design with MQTT streaming, InfluxDB storage, and Streamlit dashboards for energy-efficient data center operations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa113d59b2dc6ec7a3bf00f9eebc8541689a6235867cf59ce376cdcfa30a2a5c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa113d59b2dc6ec7a3bf00f9eebc8541689a6235867cf59ce376cdcfa30a2a5c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes a smart IoT monitoring system combining LSTM neural networks for probabilistic leak forecasting and Random Forest classifiers for instant detection in liquid-cooled AI data centers. The system, tested on synthetic data, achieves 96.5% detection accuracy and 87% forecasting accuracy, potentially preventing significant energy waste through proactive maintenance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [adversarial attacks], [entropy-guided attacks, vision-language models, adversarial robustness, harmful content generation, transferability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mengqi He, Xinyu Tian, Xin Shen, Jinhong Ni, Shu Zou, Zhaoyuan Yang, Jing Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Australian National University, The University of Queensland, GE Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21815" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21815</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies that only a small fraction (~20%) of high-entropy tokens (critical decision points) disproportionately govern output trajectories in autoregressive VLMs, challenging the prior assumption of equal token importance. 2. Proposes a selective attack strategy that concentrates adversarial perturbations on these high-entropy positions, achieving strong semantic degradation with a smaller perturbation budget and inducing 35-49% of benign outputs to become harmful. 3. Demonstrates that vulnerable high-entropy forks recur across diverse VLMs, enabling feasible transferable attacks (17-26% harmful rates), and proposes the Entropy-bank Guided Adversarial attack (EGA) method which achieves high attack success rates (93-95%) while exposing new safety weaknesses.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper reveals that adversarial attacks on Vision-Language Models (VLMs) can be made more efficient and dangerous by targeting only the critical high-entropy tokens during autoregressive generation, rather than all tokens. The proposed Entropy-bank Guided Adversarial attack (EGA) concentrates perturbations on these positions, achieving high attack success and converting many benign outputs into harmful ones, while also showing significant transferability across models. This exposes a critical and previously overlooked vulnerability in current VLM safety mechanisms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Scalable Class-Incremental Learning Based on Parametric Neural Collapse</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [class-incremental learning], [parametric neural collapse, equiangular tight frame, knowledge distillation, adaptive expansion, feature alignment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chuangxin Zhang, Guangfeng Lin, Enhui Zhao, Kaiyang Liao, Yajun Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in the provided content. Affiliation information is not included.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21845" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21845</a></p>
</li>
<li class="">
<p><strong>code:</strong> this https URLETF2</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a scalable class-incremental learning method (SCL-PNC) based on parametric neural collapse, enabling demand-driven, minimal-cost backbone expansion via an adapt-layer. 2. Introduces a dynamic parametric Equiangular Tight Frame (ETF) classifier to address class misalignment caused by evolving class distributions. 3. Presents a parallel expansion framework with a knowledge distillation algorithm to counteract feature drift and ensure feature consistency across expansion modules.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses challenges in class-incremental learning, such as overfitting and catastrophic forgetting, by proposing SCL-PNC. This method uses a parametric neural collapse framework with an adaptive backbone expansion and a dynamic ETF classifier to efficiently handle growing categories while maintaining feature consistency via distillation. Experiments show the method is effective and efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Comedy of Estimators: On KL Regularization in RL Training of LLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [KL divergence, policy gradient, on-policy sampling, off-policy training, gradient bias]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro, Yoshua Bengio, Nikolay Malkin, Moksh Jain, Siddarth Venkatraman, Aaron Courville</p>
</li>
<li class="">
<p><strong>institution:</strong> Mila – Quebec AI Institute, Université de Montréal, McGill University, LLNL, University of Edinburgh, CIFAR</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21852" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21852</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Systematic analysis of KL divergence estimator configurations in RL for LLMs, revealing how design choices introduce gradient bias. 2. Empirical demonstration that estimator configurations with unbiased gradients lead to better and more stable performance on both in-domain and out-of-domain tasks. 3. Investigation showing KL regularization can stabilize off-policy RL training in asynchronous setups.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96999f081bc421143202d98f560b5d13a6fb0c09613b8c160b121158bce3811a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96999f081bc421143202d98f560b5d13a6fb0c09613b8c160b121158bce3811a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes the use of various estimators for the KL divergence regularization term in RL fine-tuning of LLMs, finding that common practices introduce biased gradients. Through experiments on models like Qwen2.5-7B, the study shows that using estimator configurations with unbiased gradients improves training stability and downstream task performance. The work also finds that KL regularization helps stabilize off-policy RL training.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [feature-level fusion, convolutional neural networks, diabetic retinopathy screening, EfficientNet, DenseNet]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan</p>
</li>
<li class="">
<p><strong>institution:</strong> North South University, Korea Institute of Oriental Medicine, American International University–Bangladesh</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21861" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21861</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [Privacy-preserving machine learning], [dataset distillation, random forest, synthetic data generation, explainable AI, membership-inference attack]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yiming Qian, Thorsten Neumann, Xueyining Huang, David Hardoon, Fei Gao, Yong Liu, Siow Mong Rick Goh</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of High Performance Computing (A*STAR), Standard Chartered Bank, Xi’an Jiaotong–Liverpool University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21866" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21866</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a privacy-preserving dataset distillation framework that converts a trained random forest into transparent rule regions and generates synthetic data by uniform sampling within these regions, creating a compact, auditable surrogate dataset. 2. Enables explainable AI by providing both global pattern summaries (e.g., support, lift) from aggregated rules and local, human-readable rationales with calibrated uncertainty for individual cases based on tree-vote disagreement. 3. Demonstrates strong utility-privacy trade-offs, showing the distilled data maintains competitive model performance (e.g., precision, F1) with significant data reduction (85-93%), improves cross-institution learning, and resists membership-inference attacks (AUC ~0.5), indicating low memorization risk.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6994d363f1e76d7cc78ab02d48685c11199b353aab61b3eb5297064b1df9b722_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6994d363f1e76d7cc78ab02d48685c11199b353aab61b3eb5297064b1df9b722_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a dataset distillation method for financial fraud detection that converts a random forest model into interpretable rule regions to generate synthetic data, preserving privacy and model utility. The approach produces a compact, explainable dataset that supports collaborative learning across institutions while resisting privacy attacks. Experiments show it reduces data volume by over 85% with minimal performance loss and enhances cross-cluster fraud detection.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] MMCTOP: A Multimodal Textualization and Mixture-of-Experts Framework for Clinical Trial Outcome Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [multimodal fusion, sparse mixture-of-experts, schema-guided textualization, clinical trial prediction, temperature scaling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Carolina Aparício, Qi Shi, Bo Wen, Tesfaye Yadete, Qiwei Han</p>
</li>
<li class="">
<p><strong>institution:</strong> Nova School of Business and Economics, Hogarthian Technologies, IBM Research, Cleveland Clinic, Oregon Health &amp; Science University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21897" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21897</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A multimodal framework (MMCTOP) that integrates molecular structures, protocol metadata, eligibility narratives, and disease ontologies for clinical trial outcome prediction. 2. A novel architecture combining schema-guided textualization for data normalization and a drug-disease-conditioned sparse Mixture-of-Experts (SMoE) for context-aware, specialized multimodal fusion. 3. Demonstrates improved prediction performance (precision, F1, AUC) over baselines and incorporates operational safeguards like temperature scaling for calibrated probabilities to enhance auditability and reproducibility.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb552211ea905d5cbf8e190ead9078caf65c5d200327a02c7a6dab156a030645_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb552211ea905d5cbf8e190ead9078caf65c5d200327a02c7a6dab156a030645_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes MMCTOP, a multimodal framework for predicting clinical trial outcomes. It uses schema-guided textualization to normalize heterogeneous data and a sparse Mixture-of-Experts model for specialized fusion, achieving better performance than existing methods and providing calibrated risk estimates.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] GQ-VAE: A gated quantized VAE for learning variable length tokens</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [tokenization], [GQ-VAE, variable-length tokens, VQ-VAE, neural tokenizer, byte-pair encoding]</p>
</li>
<li class="">
<p><strong>authors:</strong> Theo Datta, Kayla Huang, Sham Kakade, David Brandfonbrener</p>
</li>
<li class="">
<p><strong>institution:</strong> Kempner Institute, Harvard University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21913" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21913</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Theo-Datta-115/gq-vae" target="_blank" rel="noopener noreferrer" class="">https://github.com/Theo-Datta-115/gq-vae</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GQ-VAE, a novel gated quantized variational autoencoder architecture for learning discrete, variable-length tokens. 2. Demonstrates the model can serve as a standalone, pre-trained drop-in replacement for existing tokenizers, improving over fixed-chunk baselines. 3. Shows that with equivalent compression, GQ-VAE improves downstream language model learning compared to BPE.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49412452efda1a18023f64f968f3406b217d747381b47a09694b7d37c61c918a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49412452efda1a18023f64f968f3406b217d747381b47a09694b7d37c61c918a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the limitations of traditional tokenizers like BPE and complex neural tokenizers by proposing GQ-VAE, a novel architecture that learns variable-length discrete tokens. GQ-VAE can be independently pre-trained as a drop-in replacement, approaching BPE&#x27;s performance and, under equivalent compression, improving downstream language model learning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [Tabular Data Generation, Large Language Models, Multi-Arm Bandit, Data Diversity, In-context Learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yafeng Tang, Xiaoou Ding, Jianzhuo Du, Zishuo Yan, Zhuang Ma, Zheng Liang, Zekai Qian, Hongzhi Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Harbin Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21915" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21915</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/windblow32/DATE" target="_blank" rel="noopener noreferrer" class="">https://github.com/windblow32/DATE</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces DATE, a framework that partitions heterogeneous tabular data into diverse subsets to prepare high-quality examples for in-context learning. 2. Proves the selection problem in heterogeneous data generation lacks the greedy-choice property and designs a Multi-Arm Bandit-based sampling algorithm to balance diversity and quality. 3. Demonstrates that DATE-generated data improves downstream tasks like Direct Preference Optimization (DPO) and enhances LLM reasoning on target data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d8fa8039f8a5fa0717fc5e4a9c7ba2cf1fd158e44821059f4a767bc88790eaf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d8fa8039f8a5fa0717fc5e4a9c7ba2cf1fd158e44821059f4a767bc88790eaf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of generating high-quality synthetic tabular data from heterogeneous distributions. It proposes DATE, a framework that uses LLMs with decision tree feedback for subset-specific generation and a Multi-Arm Bandit algorithm for data selection. Experiments show DATE outperforms existing methods, reducing error rates and improving downstream model performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning from human feedback (rlhf)], [preference optimization, single-index model, semiparametric, link function, policy learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nathan Kallus</p>
</li>
<li class="">
<p><strong>institution:</strong> Netflix, Cornell University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21917" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21917</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulates policy alignment as a semiparametric single-index model problem, relaxing the need for a known link function between preferences and rewards. 2. Develops novel policy learners based on profiling, orthogonalizing, and link-agnostic ranking objectives, providing theoretical error bounds. 3. Proposes practical first-order optimization implementations that are robust to unknown preference noise and scale, enabling direct policy optimization without explicit reward fitting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/378875cc0ef0eb142c184d960e479724ea3df83c84cf81e185efea5469a4387e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/378875cc0ef0eb142c184d960e479724ea3df83c84cf81e185efea5469a4387e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of bias in aligning language models when the assumed link function between human preferences and latent rewards is misspecified. It proposes a semiparametric framework that treats the link function as unknown, develops several robust policy learning algorithms, and provides theoretical guarantees. The main conclusion is that this approach enables more reliable policy alignment without needing to correctly specify the preference noise distribution.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] AutoPP: Towards Automated Product Poster Generation and Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image generation], [product poster generation, click-through rate optimization, isolated direct preference optimization, AutoPP1M dataset, unified design module]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiahao Fan, Yuxin Qin, Wei Feng, Yanyin Chen, Yaoyu Li, Ao Ma, Yixiu Li, Li Zhuang, Haoyi Bian, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law</p>
</li>
<li class="">
<p><strong>institution:</strong> JD.COM</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21921" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21921</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/JD-GenX/AutoPP" target="_blank" rel="noopener noreferrer" class="">https://github.com/JD-GenX/AutoPP</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An automated pipeline (AutoPP) for end-to-end product poster generation and optimization, requiring only basic product information as input. 2. A novel optimization method that uses systematic element replacement and Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to specific poster elements. 3. The creation and release of AutoPP1M, the largest dataset for product poster generation and optimization, containing one million posters and user feedback.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces AutoPP, an automated pipeline that generates product posters from basic product information and then optimizes them for higher Click-Through Rate (CTR) using online feedback and a novel Isolated Direct Preference Optimization technique. It is supported by a large-scale dataset, AutoPP1M. Experiments show that AutoPP achieves state-of-the-art performance in both offline and online evaluations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Data relativistic uncertainty framework for low-illumination anime scenery image enhancement</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [low-light image enhancement], [Data Relativistic Uncertainty, Unsupervised Learning, EnlightenGAN, Anime Scenery, Domain Gap]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yiquan Gao, John See</p>
</li>
<li class="">
<p><strong>institution:</strong> Heriot-Watt University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21944" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21944</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Hybrid Combinatorial Multi-armed Bandits with Probabilistically Triggered Arms</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-armed bandits], [combinatorial multi-armed bandits, probabilistically triggered arms, hybrid learning, offline data, online interaction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kongchang Zhou, Tingyu Zhang, Wei Chen, Fang Kong</p>
</li>
<li class="">
<p><strong>institution:</strong> Southern University of Science and Technology, Microsoft Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21925" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21925</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new hybrid CMAB-T framework that integrates offline data with online interaction to address the complementary weaknesses of purely online or offline methods. 2. Introduces the hybrid CUCB algorithm, which leverages offline data to guide exploration and strategically uses online interactions to correct dataset bias. 3. Provides theoretical regret guarantees and empirical results demonstrating the algorithm&#x27;s consistent advantage over purely online or offline baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e97b8cf09a0691d48238a8272fecd32791ddc20b9ba00115a757d778b1e2017f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e97b8cf09a0691d48238a8272fecd32791ddc20b9ba00115a757d778b1e2017f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a hybrid framework for combinatorial multi-armed bandits with probabilistically triggered arms (CMAB-T) that combines offline data with online interaction. The core method is the hybrid CUCB algorithm, which uses offline data to accelerate learning and online interaction to correct for dataset limitations. Theoretical and empirical results show this hybrid approach outperforms purely online or offline methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [hallucination mitigation, adversarial parametric editing, parameter clustering, visual-language models, activation dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji, Zhongshi He</p>
</li>
<li class="">
<p><strong>institution:</strong> Chongqing University, Xinjiang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21999" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21999</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/hujiayu1223/ALEAHallu" target="_blank" rel="noopener noreferrer" class="">https://github.com/hujiayu1223/ALEAHallu</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel adversarial parametric editing framework (ALEAHallu) following an Activate-Locate-Edit Adversarially paradigm for mitigating hallucinations in VLMs. 2. Introduces a method to construct an activation dataset with grounded and hallucinatory response pairs and identifies critical hallucination-prone parameter clusters via differential hidden state analysis. 3. Demonstrates the framework&#x27;s effectiveness by fine-tuning identified parameter clusters with adversarially tuned prefixes to force the model to prioritize visual evidence over linguistic priors.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the hallucination problem in Vision-Language Models (VLMs), where models generate content misaligned with visual inputs due to over-reliance on linguistic priors. The authors propose ALEAHallu, an adversarial parametric editing framework that identifies and fine-tunes hallucination-prone parameter clusters using adversarially optimized prompts to enhance visual grounding. Evaluations show the method significantly reduces hallucinations in both generative and discriminative VLM tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [computational biology], [protein language model, ESM-2, dual-stream architecture, 1D CNN, transformer encoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aicha Boutorh, Soumia Bouyahiaoui, Sara Belhadj, Nour El Yakine Guendouz, Manel Kara Laouar</p>
</li>
<li class="">
<p><strong>institution:</strong> National School of Artificial Intelligence (ENSIA)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22007" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22007</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DuaDeep-SeqAffinity, a novel sequence-only deep learning framework for antigen-antibody affinity prediction using a dual-stream hybrid architecture. 2. Integrates pre-trained ESM-2 embeddings with 1D CNNs for local motifs and Transformer encoders for global context, followed by a fusion module. 3. Demonstrates superior performance over single-branch models and existing SOTA methods, even surpassing some structure-sequence hybrid models, proving the efficacy of sequence-only high-fidelity embeddings.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1985304be62407b10b5e5be53aea80fe4ff1468be03e261847b49774ddd937a8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1985304be62407b10b5e5be53aea80fe4ff1468be03e261847b49774ddd937a8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces DuaDeep-SeqAffinity, a deep learning framework that predicts antigen-antibody binding affinity using only amino acid sequences. It combines ESM-2 embeddings with a dual-stream architecture of 1D CNNs and Transformers to capture local and global features. The model outperforms existing methods, showing that sequence-only models can effectively capture binding patterns and accelerate therapeutic discovery.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] HWL-HIN: A Hypergraph-Level Hypergraph Isomorphism Network as Powerful as the Hypergraph Weisfeiler-Lehman Test with Application to Higher-Order Network Robustness</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [hypergraph isomorphism network, hypergraph weisfeiler-lehman test, higher-order network robustness, hypergraph neural networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chengyu Tian, Wenbin Pei</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in the provided content.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22014" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22014</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hypergraph-level Hypergraph Isomorphism Network (HWL-HIN) framework for hypergraph learning. 2. Theoretically proves the model&#x27;s expressive power is strictly equivalent to the Hypergraph Weisfeiler-Lehman test. 3. Successfully applies the model to predict hypergraph robustness, demonstrating superior performance and efficiency over existing graph-based and conventional HGNN models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3b228ae57b3d7937d8586b0c60419df83b49466ba7ca3b946109dd8186f827c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3b228ae57b3d7937d8586b0c60419df83b49466ba7ca3b946109dd8186f827c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the computational overhead of traditional robustness assessment and the limited expressive power of existing hypergraph neural networks by proposing a hypergraph-level Hypergraph Isomorphism Network (HWL-HIN). The method is proven theoretically to be as powerful as the Hypergraph Weisfeiler-Lehman test and is applied to predict hypergraph robustness. Experiments show it outperforms existing graph-based models and conventional HGNNs in tasks prioritizing topological structure while maintaining high efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models for drug discovery], [hit-like molecule generation, autoregressive models, diffusion models, docking scores, multi-stage filtering]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nagham Osman, Vittorio Lembo, Giovanni Bottegoni, Laura Toni</p>
</li>
<li class="">
<p><strong>institution:</strong> University College London, University of Urbino Carlo Bo</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22031" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22031</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Framing hit-like molecule generation as a standalone task for generative models. 2. Proposing a tailored evaluation framework integrating physicochemical, structural, and bioactivity criteria. 3. Benchmarking autoregressive and diffusion models, with synthesized GSK-3β hits confirmed active in vitro.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0d0d188f2e13e0f8561de5950d5637586c871a0ee36935ebfbd5bb2bad540_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0d0d188f2e13e0f8561de5950d5637586c871a0ee36935ebfbd5bb2bad540_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether generative models can replace the hit identification step in drug discovery. It proposes a multi-stage evaluation framework and benchmarks autoregressive and diffusion models, showing they can generate valid, diverse, and biologically relevant compounds, with some synthesized hits confirmed active in vitro.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Direction Finding with Sparse Arrays Based on Variable Window Size Spatial Smoothing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [signal processing], [DOA estimation, sparse arrays, coarrays, spatial smoothing, MUSIC]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wesley S. Leite, Rodrigo C. de Lamare, Yuriy Zakharov, Wei Liu, Martin Haardt</p>
</li>
<li class="">
<p><strong>institution:</strong> University of York, Pontifical Catholic University of Rio de Janeiro, University of York, University of York, Ilmenau University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22024" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22024</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a variable window size (VWS) spatial smoothing framework for coarray-based DOA estimation with sparse linear arrays. 2. Proposes the VWS-CA-MUSIC and VWS-CA-rMUSIC algorithms that replace perturbed data terms with unperturbed ones to improve signal-noise subspace separation. 3. Derives theoretical bounds for the compression parameter to guarantee identifiability and demonstrates performance improvements and complexity savings over fixed-window methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/20bb39a95f66255cc62bbe7ee6e002de101d9a346c703b72e27fc3d3b08e1d30_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/20bb39a95f66255cc62bbe7ee6e002de101d9a346c703b72e27fc3d3b08e1d30_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of direction-of-arrival (DOA) estimation for correlated sources using sparse linear arrays. It proposes a variable window size spatial smoothing framework and new VWS-CA-MUSIC algorithms that enhance estimation accuracy by compressing the smoothing aperture. Simulations show the method provides significant performance gains and reduced computational complexity compared to standard fixed-window coarray MUSIC.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] LibContinual: A Comprehensive Library towards Realistic Continual Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [catastrophic forgetting, stability-plasticity dilemma, modular architecture, memory budget, online continual learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenbin Li, Shangge Liu, Borui Kang, Yiyang Chen, KaXuan Lew, Yang Chen, Yinghuan Shi, Lei Wang, Yang Gao, Jiebo Luo</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanjing University, University of Wollongong, University of Rochester</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22029" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22029</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/RL-VIG/LibContinual" target="_blank" rel="noopener noreferrer" class="">https://github.com/RL-VIG/LibContinual</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed LibContinual, a unified, modular, and reproducible library for Continual Learning (CL) that integrates 19 representative algorithms across five methodological categories. 2. Systematically identified and investigated three unrealistic implicit assumptions (offline data accessibility, unregulated memory, intra-task semantic homogeneity) prevalent in mainstream CL evaluation. 3. Conducted a comprehensive analysis under stricter, more realistic settings (strict online CL, unified memory budget, category-randomized tasks), revealing significant performance drops in many existing methods and highlighting the need for resource-aware and semantically robust CL strategies.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a733a967f663a216bbd5fb0cee657ed8bb70a4e6f0205d669f983cbf9bb6fd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a733a967f663a216bbd5fb0cee657ed8bb70a4e6f0205d669f983cbf9bb6fd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces LibContinual, a comprehensive library designed to unify and standardize research in Continual Learning (CL). By using this framework to evaluate existing methods under more realistic constraints, the study shows that many current CL algorithms suffer significant performance drops, underscoring the gap between common evaluation practices and real-world applicability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Why Smooth Stability Assumptions Fail for ReLU Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [optimization theory], [ReLU networks, nonsmooth optimization, stability analysis, generalized derivatives, learning dynamics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ronald Katende</p>
</li>
<li class="">
<p><strong>institution:</strong> Kabale University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22055" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22055</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that no uniform smoothness-based stability proxy (e.g., gradient Lipschitzness) can hold globally for ReLU networks, even in simple, empirically stable settings. 2. Provides a concrete counterexample showing the failure of classical stability bounds for ReLU learning dynamics. 3. Identifies a minimal generalized derivative condition under which stability statements can be meaningfully restored for nonsmooth learning systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3150186f6ab289f5e6db87d8ca89851af409290b0b4c37667d493ee4b7e894b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3150186f6ab289f5e6db87d8ca89851af409290b0b4c37667d493ee4b7e894b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper shows that smoothness assumptions like gradient Lipschitzness, commonly used in stability analyses, fail globally for ReLU networks due to their inherent nonsmoothness. It provides a counterexample to classical bounds and proposes a minimal condition based on generalized derivatives to restore meaningful stability analysis. This clarifies the limitations of smooth approximations and motivates nonsmooth-aware frameworks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Scaling Adversarial Training via Data Selection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [adversarial robustness], [adversarial training, PGD, sample selection, gradient matching, margin-based sampling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Youran Ye, Dejin Wang, Ajinkya Bhandare</p>
</li>
<li class="">
<p><strong>institution:</strong> Northeastern University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22069" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22069</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/youranye/Selective-Adversarial-Training" target="_blank" rel="noopener noreferrer" class="">https://github.com/youranye/Selective-Adversarial-Training</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Selective Adversarial Training, which perturbs only a critical subset of samples per minibatch to reduce computational cost., 2. Introduces two novel sample selection criteria: margin-based sampling and gradient-matching sampling., 3. Demonstrates that the method achieves comparable or superior robustness to full PGD adversarial training while reducing adversarial computation by up to 50%.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad822d1db3ef050d2caa84f51828c7b48f93eee442f9a9f954f4f36b07929f44_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad822d1db3ef050d2caa84f51828c7b48f93eee442f9a9f954f4f36b07929f44_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high computational cost of Projected Gradient Descent (PGD) in adversarial training. It proposes Selective Adversarial Training, which uses principled criteria to select and perturb only critical samples in each batch. Experiments show the method maintains robustness while cutting adversarial computation by up to 50%.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [SRAM, frequency scaling, energy-delay product, systolic array, memory bandwidth]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hannah Atmer, Yuan Yao, Thiemo Voigt, Stefanos Kaxiras</p>
</li>
<li class="">
<p><strong>institution:</strong> Uppsala University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22066" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22066</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Quantified the distinct energy and performance impacts of SRAM size and operating frequency on the compute-bound prefill and memory-bound decode phases of LLM inference. 2. Demonstrated a counter-intuitive result: high compute frequency can reduce total energy by shortening execution time and reducing static energy more than the dynamic power increase. 3. Identified an optimal hardware configuration (high frequency, small SRAM buffer) that minimizes the energy-delay product, providing concrete design insights for energy-efficient accelerators.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52c2db009cb44a92a388e1684ea0d1bdb9ae27c0c4a4e683651eb7bd091bbe93_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52c2db009cb44a92a388e1684ea0d1bdb9ae27c0c4a4e683651eb7bd091bbe93_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates how on-chip SRAM size and operating frequency affect the energy efficiency of LLM inference. Using a simulation methodology combining OpenRAM, LLMCompass, and ScaleSIM, it finds that a high-frequency, small-SRAM configuration optimizes the energy-delay product, as memory bandwidth caps decode phase improvements. The analysis provides architectural guidance for designing efficient LLM accelerators.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Unifying Learning Dynamics and Generalization in Transformers Scaling Law</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [learning theory], [scaling law, learning dynamics, generalization error, transformer, stochastic gradient descent]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chiwun Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> Sun Yat-sen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22088" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22088</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formalizes the learning dynamics of transformers as an ODE system and approximates it to kernel behaviors, moving beyond toy models to analyze SGD on multi-layer transformers with arbitrary data distributions. 2. Establishes a theoretical upper bound on excess risk with a distinct phase transition: exponential decay in the optimization phase and a power-law decay of Θ(C^{-1/6}) in the statistical phase. 3. Derives isolated scaling laws for model size, training time, and dataset size, explaining how each variable independently governs generalization bounds.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9b067b56202cd4607e684058e78ac331373bf12bf6848ca444276e2dcafe9f9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9b067b56202cd4607e684058e78ac331373bf12bf6848ca444276e2dcafe9f9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper provides a theoretical foundation for the empirical scaling laws of large language models. It models transformer learning dynamics as an ODE system and analyzes SGD training on realistic data. The main result is a unified theory showing a phase transition in generalization error, from exponential to power-law decay, as computational resources scale.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [multi-agent pipeline, automated data analysis, insight generation, report synthesis, visual analytics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuyu Gan, Renxiang Wang, James Mooney, Dongyeop Kang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Minnesota</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22101" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22101</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A Data Analyzer agent that orchestrates data profiling, generates diverse visualizations, filters low-quality charts, and automatically scores candidate insights for depth, correctness, and actionability. 2. A Presenter agent that sequences topics, composes chart-grounded narratives from top insights, writes transitions, and revises the document to produce a coherent, publication-ready report. 3. An end-to-end Analyzer-to-Presenter (A2P) pipeline that operationalizes co-analysis by coupling quality-assured analysis with narrative synthesis, improving the real-world usefulness of automated data analysis.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92651ded84480402816f8db1df902e28dd62cce1b0958cece60e0f518bdd7e1c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92651ded84480402816f8db1df902e28dd62cce1b0958cece60e0f518bdd7e1c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents A2P-Vis, a two-part multi-agent pipeline designed to automate the generation of data visualization reports. The system uses a Data Analyzer to create and vet visual insights and a Presenter to assemble them into a coherent narrative. The authors claim this end-to-end approach improves the practical utility of automated data analysis for practitioners.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Explainable Multimodal Regression via Information Decomposition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal machine learning], [Partial Information Decomposition (PID), multimodal regression, interpretability, Gaussianity assumption, conditional independence regularizer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhaozhao Ma, Shujian Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Georgia Institute of Technology, Vrije Universiteit Amsterdam, UiT - The Arctic University of Norway</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22102" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22102</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/xxx/PIDReg" target="_blank" rel="noopener noreferrer" class="">https://github.com/xxx/PIDReg</a> (URL placeholder from abstract)</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel multimodal regression framework based on Partial Information Decomposition (PID) to quantify unique, redundant, and synergistic information from modalities. 2. Introduction of a Gaussianity assumption to resolve the underdetermined nature of PID, enabling analytical computation of its terms. 3. Derivation of a closed-form conditional independence regularizer to isolate unique information within each modality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a07cab8b6bb765060f8a015e46e79e686a9acb69bac3aa8045cf96acec9f61e6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a07cab8b6bb765060f8a015e46e79e686a9acb69bac3aa8045cf96acec9f61e6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of interpretability in multimodal regression by proposing a new framework based on Partial Information Decomposition (PID). The method resolves PID&#x27;s underdetermination by enforcing Gaussianity and uses a conditional independence regularizer to isolate unique modality information. Experiments on six datasets, including brain age prediction, show the framework outperforms state-of-the-art methods in both accuracy and interpretability, while aiding modality selection.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Sensitivity Analysis of the Consistency Assumption</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [causal inference], [consistency assumption, sensitivity analysis, hidden versions of treatment, partial identification, stable unit treatment value assumption (SUTVA)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Brian Knaeble, Qinyun Lin, Erich Kummerfeld, Kenneth A. Frank</p>
</li>
<li class="">
<p><strong>institution:</strong> Utah Valley University, University of Gothenburg, University of Minnesota, Michigan State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21379" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21379</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A formal mathematical framework for analyzing violations of the consistency assumption. 2. A novel sensitivity parameter to quantify bias induced by hidden versions of treatment. 3. Methods for specifying bounds on this parameter to enable partial identification of causal effects.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0816c25e8631081977ffb91ecadbb3f527cfd99c7daf15f81c3f18a332125fcc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0816c25e8631081977ffb91ecadbb3f527cfd99c7daf15f81c3f18a332125fcc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses violations of the consistency assumption in causal inference, which posits no hidden versions of treatment. It proposes a new sensitivity analysis method focused on confounding by hidden treatment versions, introducing a formal framework and a novel sensitivity parameter. The work enables researchers to assess and bound the bias from consistency violations when interpreting causal estimates.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Dynamic Attention (DynAttn): Interpretable High-Dimensional Spatio-Temporal Forecasting (with Application to Conflict Fatalities)</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [spatio-temporal forecasting], [dynamic attention, zero-inflated negative binomial, elastic-net gating]</p>
</li>
<li class="">
<p><strong>authors:</strong> Stefano M. Iacus, Haodong Qi, Marcello Carammia, Thomas Juneau</p>
</li>
<li class="">
<p><strong>institution:</strong> Harvard University, Stockholm University, Malmö University, University of Catania, University of Toronto</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21435" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21435</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces DynAttn, an interpretable dynamic-attention framework for high-dimensional spatio-temporal count processes. 2. Combines rolling-window estimation, shared elastic-net feature gating, a compact self-attention encoder, and a ZINB likelihood for calibrated forecasts. 3. Demonstrates superior predictive accuracy and enables structured interpretation of regional conflict dynamics, showing the roles of persistence, diffusion, and climate stress.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad96b82177be36f56ddfe0b4e4d6be51396aa49723cdabef2c85f538301f2e0e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad96b82177be36f56ddfe0b4e4d6be51396aa49723cdabef2c85f538301f2e0e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces DynAttn, an interpretable forecasting framework for sparse, high-dimensional spatio-temporal data like conflict fatalities. It combines dynamic attention, feature gating, and a specialized likelihood to produce accurate, multi-horizon forecasts and probabilities. The method outperforms benchmarks, particularly in sparse settings, and provides interpretable insights into conflict drivers such as persistence, spatial diffusion, and conditional climate effects.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Atomistic Simulation Guided Convolutional Neural Networks for Thermal Modeling of Friction Stir Welding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [physics-informed machine learning], [molecular dynamics, convolutional neural network, friction stir welding, explainable AI, LAMMPS]</p>
</li>
<li class="">
<p><strong>authors:</strong> Akshansh Mishra</p>
</li>
<li class="">
<p><strong>institution:</strong> Politecnico di Milano, AI Fab Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21344" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21344</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a novel method to transform atomistic simulation data (atomic positions and velocities) into physics-based 2D spatial grids for deep learning input. 2. Created and optimized a 2D CNN model to directly predict temperature evolution from spatially resolved atomistic data, achieving high accuracy (R²=0.94). 3. Used Class Activation Map analysis to provide explainability, showing the model&#x27;s focus aligns with physical mechanisms (e.g., tool-material interface).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34d6f71500319f52aecac1e95e1951a537fdc504134a8ba43851f31acc2e41c6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34d6f71500319f52aecac1e95e1951a537fdc504134a8ba43851f31acc2e41c6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a method that combines molecular dynamics simulations with convolutional neural networks for thermal modeling in friction stir welding. The method transforms atomic-scale simulation data into spatial grids and uses a CNN to accurately predict temperature, with results validated against physical mechanisms. The approach demonstrates that deep learning can effectively learn from atomistic data to model complex thermomechanical processes.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] An approach to Fisher-Rao metric for infinite dimensional non-parametric information geometry</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [information geometry], [Fisher-Rao metric, non-parametric, G-entropy, Covariate Fisher Information Matrix (cFIM), intrinsic dimensionality]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bing Cheng, Howell Tong</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in provided content. Affiliation likely inferred from author names or arXiv metadata, but not present in the given text.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21451" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21451</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces an orthogonal decomposition of the tangent space to derive a finite-dimensional, computable Covariate Fisher Information Matrix (cFIM) from the infinite-dimensional Fisher-Rao metric. 2. Establishes the geometric foundation of G-entropy via a Trace Theorem, linking it to total explainable statistical information. 3. Connects the cFIM to the Cramér-Rao Lower Bound and the Manifold Hypothesis, providing a method for estimating intrinsic dimensionality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a8f179358a2d0d118621b506d45d051ad949b1cc0acfe723d3c3268c4390e5a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a8f179358a2d0d118621b506d45d051ad949b1cc0acfe723d3c3268c4390e5a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the intractability of the infinite-dimensional Fisher-Rao metric in non-parametric information geometry by proposing an orthogonal decomposition of the tangent space. This decomposition yields a finite-dimensional Covariate Fisher Information Matrix (cFIM), which serves as a computable geometric object for analyzing statistical information and model efficiency. The framework rigorously grounds G-entropy, links to fundamental statistical bounds, and provides a testable condition for the Manifold Hypothesis, bridging abstract geometry with explainable AI.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Deep learning-enhanced dual-mode multiplexed optical sensor for point-of-care diagnostics of cardiovascular diseases</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [biomedical sensing and diagnostics], [vertical flow assay, dual-mode detection, neural network-based quantification, multiplexed optical sensor, point-of-care diagnostics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gyeo-Re Han, Merve Eryilmaz, Artem Goncharov, Yuzhu Li, Shun Ye, Aoi Tomoeda, Emily Ngo, Margherita Scussat, Xiao Wang, Zixiang Ji, Max Zhang, Jeffrey J. Hsu, Omai B. Garner, Dino Di Carlo, Aydogan Ozcan</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Los Angeles (UCLA)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21389" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21389</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a dual-mode (colorimetric and chemiluminescent) multiplexed vertical flow assay (xVFA) for simultaneous detection of three cardiac biomarkers, achieving a wide dynamic range of ~6 orders of magnitude. 2. Integrated a deep learning-based quantification pipeline with a portable optical reader to automate analysis and enhance accuracy, achieving high correlation (Pearson&#x27;s r &gt; 0.96) with reference assays. 3. Created a compact, cost-effective, and rapid (23 min) point-of-care diagnostic system for cardiovascular diseases using only 50 µL of serum.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6eb23e46dc346744879ce56ec8c667b82f8cab434c54e4a4922a2eb842bf77d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6eb23e46dc346744879ce56ec8c667b82f8cab434c54e4a4922a2eb842bf77d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a deep learning-enhanced, dual-mode optical sensor for point-of-care cardiovascular diagnostics. The system uses a multiplexed vertical flow assay with colorimetric and chemiluminescent detection to simultaneously measure three key biomarkers with high sensitivity across a wide dynamic range, and a neural network pipeline for accurate quantification. The result is a rapid, portable, and automated diagnostic tool validated on patient samples.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Quantum Nondecimated Wavelet Transform: Theory, Circuits, and Applications</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [quantum signal processing], [quantum nondecimated wavelet transform, epsilon decimation, Hadamard test, quantum wavelet shrinkage, shift invariance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Brani Vidakovic</p>
</li>
<li class="">
<p><strong>institution:</strong> Texas A&amp;M University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21478" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21478</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a quantum formulation of the Nondecimated Wavelet Transform (NDWT) based on epsilon-decimation, using a quantum register for shift indices and controlled circular shifts to realize all shifted transforms simultaneously. 2. Proposed an alternative quantum NDWT formulation using the Hadamard test and diagonal phase operators to directly access shift-invariant energy scalograms without full coefficient reconstruction. 3. Demonstrated that quantum redundancy and translation invariance can be exploited for applications like denoising and feature extraction, providing a foundation for multiscale quantum signal processing.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01489d687d6b4e38b42c82e3054e553beab66883e073aec7483d40c87388a47a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01489d687d6b4e38b42c82e3054e553beab66883e073aec7483d40c87388a47a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces two quantum formulations of the classical Nondecimated Wavelet Transform (NDWT) to achieve shift invariance and redundancy in quantum computation. The first uses controlled shifts and a wavelet analysis unitary, while the second employs the Hadamard test with phase operators for direct energy measurement. The work shows these quantum NDWTs enable coherent multiscale signal processing tasks like denoising and feature extraction.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [diffusion models, generative modeling, evidence lower bound, residual learning, two-stage framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Takuro Kutsuna</p>
</li>
<li class="">
<p><strong>institution:</strong> Toyota Central R&amp;D Labs., Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21593" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21593</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Incorporating rank-free coupling and external field via an amplitude-only modulated spatial photonic Ising machine</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [photonic computing], [spatial photonic Ising machine, Hadamard product, amplitude-only modulation, rank-free coupling, incoherent light field]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ze Zheng, Yuegang Li, Hang Xu, Jingzheng Huang, Tailong Xiao, Guihua Zeng</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, Hefei National Laboratory, Shanghai Research Center for Quantum Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21587" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21587</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an amplitude-only modulated, rank-free spatial photonic Ising machine (AR-SPIM) that eliminates the need for repeated/auxiliary operations by reformulating the Ising Hamiltonian as a sum of Hadamard products. 2. Demonstrates direct mapping of a 797-spin Ising model with external fields into an incoherent light field using aligned amplitude modulators, achieving high encoding accuracy (correlation &gt;0.98). 3. Shows the system&#x27;s capability for ground-state search with &lt;0.3% error, complex phase transition observation, and scalable spin counts for sparse problems by removing zero-valued terms.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fe8a02df2acd50923ce3480e4b3fb28b68540ae3331e12dd83853a5b046a5c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fe8a02df2acd50923ce3480e4b3fb28b68540ae3331e12dd83853a5b046a5c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a new spatial photonic Ising machine (AR-SPIM) that uses an amplitude-only modulation scheme to encode arbitrary-rank spin-spin couplings and external fields directly into an incoherent light field. The method reformulates the Ising Hamiltonian as a sum of Hadamard products, enabling efficient and accurate computation. The demonstrated system achieves high-speed iterations, low error rates for optimization problems, and offers scalability for practical applications in optimization and quantum simulations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Tilt Matching for Scalable Sampling and Fine-Tuning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [Tilt Matching, stochastic interpolants, flow matching, unnormalized densities, fine-tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Peter Potaptchik, Cheuk-Kit Lee, Michael S. Albergo</p>
</li>
<li class="">
<p><strong>institution:</strong> Harvard University, University of Oxford, Kempner Institute, IAIFI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21829" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21829</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Tilt Matching, a simple, scalable algorithm for sampling and fine-tuning based on stochastic interpolants and an implicit stochastic optimal control solution., 2. Demonstrates that the method has strictly lower variance than standard flow matching and does not require gradients of the reward or backpropagation through trajectories., 3. Empirically shows state-of-the-art results on sampling under Lennard-Jones potentials and competitive performance on fine-tuning Stable Diffusion without reward multipliers.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2cc2b096f1ad1f69d17a5fbdbac71d5ccac470d3cc86a47d74fed9dba5202c88_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2cc2b096f1ad1f69d17a5fbdbac71d5ccac470d3cc86a47d74fed9dba5202c88_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces Tilt Matching, a new algorithm for sampling from complex distributions and fine-tuning generative models. It works by deriving a velocity field from stochastic interpolants to target a &quot;tilted&quot; distribution, offering lower variance than flow matching and avoiding gradient computations. The method is shown to be scalable and effective, achieving strong results on molecular sampling and image generation tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Modeling high dimensional point clouds with the spherical cluster model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [clustering], [spherical cluster model, high-dimensional median, non-smooth optimization, Clarke gradient, stratified cell complex]</p>
</li>
<li class="">
<p><strong>authors:</strong> Frédéric Cazals, Antoine Commaret, Louis Goldenberg</p>
</li>
<li class="">
<p><strong>institution:</strong> Université Côte d&#x27;Azur, Inria, Ecole Polytechnique</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21960" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21960</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Showed that fitting a spherical cluster model is a strictly convex but non-smooth combinatorial optimization problem. 2. Presented an exact solver using the Clarke gradient on a stratified cell complex derived from an arrangement of hyperspheres. 3. Demonstrated through experiments that the exact solver is significantly faster than BFGS heuristics for many datasets and that the model&#x27;s center behaves as a parameterized high-dimensional median.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c9618adf1a45723b5b136e8c21eaa91dc645be7664e2fae293ae569f2adde20_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c9618adf1a45723b5b136e8c21eaa91dc645be7664e2fae293ae569f2adde20_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces the Spherical Cluster (SC) model for approximating high-dimensional point clouds with a sphere. It proposes an exact solver for this non-smooth optimization problem, which is shown to be much faster than heuristic methods for many datasets, especially in high dimensions. The model&#x27;s center is found to act as a robust, parameterized median.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Frobenius-Optimal Projection for Enforcing Linear Conservation in Learned Dynamical Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [dynamical systems, numerical linear algebra], [linear conservation laws, Frobenius norm, orthogonal projection, matrix correction, data-driven models]</p>
</li>
<li class="">
<p><strong>authors:</strong> John M. Mango, Ronald Katende</p>
</li>
<li class="">
<p><strong>institution:</strong> Makerere University, Kabale University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22084" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22084</a></p>
</li>
<li class="">
<p><strong>contributions:</strong>  1. Provides a closed-form, Frobenius-norm optimal projection to enforce linear conservation laws on any learned linear dynamical operator. 2. Proves that the correction is uniquely defined, low-rank, and fully determined by the constraint violation, reducing to a rank-one update for a single invariant. 3. Demonstrates that the method enforces exact conservation while minimally perturbing the learned dynamics, verified with a numerical example.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff37179b43efd7a7b64ff062f4e49c8aeaa1dc0b65febe24067b7ddd46dba12c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff37179b43efd7a7b64ff062f4e49c8aeaa1dc0b65febe24067b7ddd46dba12c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of learned linear dynamical models violating known linear conservation laws. It proposes a closed-form orthogonal projection that minimally perturbs a learned operator in the Frobenius norm to exactly satisfy the constraints. The method provides a general, optimal post-processing step for embedding invariants into data-driven linear models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-30">2025-12-30<a href="#2025-12-30" class="hash-link" aria-label="Direct link to 2025-12-30" title="Direct link to 2025-12-30" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251230] Pruning Graphs by Adversarial Robustness Evaluation to Strengthen GNN Defenses</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [adversarial robustness, graph pruning, message passing, spurious connections, graph defense]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yongyu Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Michigan Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22128" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22128</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a novel graph pruning framework that uses adversarial robustness evaluation to identify fragile graph components. 2. Proposes a method that selectively prunes edges based on robustness scores to improve model reliability and resilience. 3. Validates the framework by instantiating it on three representative GNN architectures and demonstrating significant defense enhancement in high-perturbation regimes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d658f9bc62a6d2a57e4602f69b9d0c69056551601abd2ba0336e97d592bae1dc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d658f9bc62a6d2a57e4602f69b9d0c69056551601abd2ba0336e97d592bae1dc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the vulnerability of Graph Neural Networks (GNNs) to adversarial attacks and noise in graph structure. It proposes a pruning framework that uses adversarial robustness scores to identify and remove detrimental edges, resulting in cleaner and more resilient graph representations. Experiments show this method significantly strengthens GNN defenses against high levels of perturbation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent reinforcement learning], [ad-hoc teamwork, retrieval-augmented generation, teammate modeling, Overcooked]</p>
</li>
<li class="">
<p><strong>authors:</strong> Conor Wallace, Umer Siddique, Yongcan Cao</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Texas at San Antonio</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22129" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22129</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces COLLAB, a novel language-based framework that uses LLMs as behavioral world models to classify unseen teammate types in ad-hoc teamwork. 2. Extends COLLAB to RECOLLAB by incorporating retrieval-augmented generation (RAG) with exemplar trajectories to stabilize inference and improve adaptation. 3. Demonstrates empirically in the Overcooked environment that RECOLLAB achieves Pareto-optimal trade-offs between classification accuracy and episodic return, highlighting the value of retrieval grounding.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/078fe396118022eaa0d391e86072d08c5b6617a143aba1478029ca3185af6472_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/078fe396118022eaa0d391e86072d08c5b6617a143aba1478029ca3185af6472_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of ad-hoc teamwork, where an agent must collaborate with unseen teammates. It proposes RECOLLAB, a framework that uses retrieval-augmented LLMs to model and classify teammate behavior from short interaction traces. The method is shown to effectively improve adaptation and coordination in the cooperative Overcooked environment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On Harnessing Idle Compute at the Edge for Foundation Model Training</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [edge computing, tensor parallelism, parameter server, device heterogeneity, fault-tolerance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Leyang Xue, Meghana Madhyastha, Myungjin Lee, Amos Storkey, Randal Burns, Mahesh K. Marina</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Edinburgh, Johns Hopkins University, Cisco Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22142" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22142</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel selective hybrid tensor parallelism method to finely partition training operations for edge devices. 2. A parameter server-centric training framework to cope with device memory limits and avoid communication bottlenecks. 3. A cost optimization model to guide device selection and workload distribution, effectively handling device heterogeneity and churn.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fefe0f72fa70ae7be2cad54f15e74f96fd08cc506630060214e298521278148_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fefe0f72fa70ae7be2cad54f15e74f96fd08cc506630060214e298521278148_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of decentralized foundation model training on edge devices, which is hindered by memory limits, communication overhead, and device heterogeneity. It proposes Cleave, a new paradigm that uses selective hybrid tensor parallelism and a parameter server framework to partition training efficiently. The evaluation shows Cleave matches cloud-based training performance, scales to thousands of devices, and handles failures with much faster recovery than prior methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [hpc], [gpu kernels], [Minimal Executable Program (MEP), Automatic Error Repair, Performance Pattern Inheritance, iterative optimization, cross-platform]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ruifan Chu, Anbang Wang, Xiuxiu Bai, Shuai Liu, Xiaoshe Dong</p>
</li>
<li class="">
<p><strong>institution:</strong> School of Software Engineering, Xi’an Jiaotong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22147" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22147</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an end-to-end LLM framework that optimizes GPU kernels by constructing Minimal Executable Programs (MEPs) to avoid expensive full application builds and executions. 2. Introduces Automatic Error Repair and Performance Pattern Inheritance to automatically fix faults and reuse effective optimization strategies, reducing search cost. 3. Demonstrates cross-platform portability and effectiveness on NVIDIA GPUs and the Haiguang DCU platform, achieving significant speedups over direct LLM optimization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd593bd3569f30bdbf11d361054f51142863fb91e592b76bc4eb2f600850c5e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd593bd3569f30bdbf11d361054f51142863fb91e592b76bc4eb2f600850c5e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high cost of full builds for GPU kernel optimization in large HPC applications by proposing an LLM framework that uses Minimal Executable Programs (MEPs) for iterative optimization. The method integrates automatic error repair and performance pattern inheritance to maintain correctness and reuse strategies. It achieves substantial speedups across different hardware platforms without requiring full-source dependencies.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Towards Unsupervised Causal Representation Learning via Latent Additive Noise Model Causal Autoencoders</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [causal representation learning], [additive noise model, Wasserstein auto-encoder, identifiability, unsupervised learning, causal discovery]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hans Jarett J. Ong, Brian Godwin S. Lim, Dominic Dayta, Renzo Roel P. Tan, Kazushi Ikeda</p>
</li>
<li class="">
<p><strong>institution:</strong> Nara Institute of Science and Technology, Kyoto University, Ateneo de Manila University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22150" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22150</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed LANCA, a novel autoencoder framework that operationalizes the Additive Noise Model (ANM) as an inductive bias for unsupervised causal representation learning., 2. Provided a theoretical analysis showing that the ANM constraint restricts admissible transformations to the affine class, resolving component-wise indeterminacy., 3. Introduced a deterministic WAE architecture with a differentiable ANM layer to explicitly optimize for residual independence, overcoming limitations of stochastic VAEs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/383ef9534443c860dced6678ba459335afb022e411bf183cc7f2e3dbad2bb385_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/383ef9534443c860dced6678ba459335afb022e411bf183cc7f2e3dbad2bb385_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of unsupervised causal representation learning by proposing LANCA, a method that uses the Additive Noise Model as an inductive bias within a deterministic autoencoder framework. Theoretically, it shows this constraint narrows down the solution space, and empirically, LANCA outperforms existing methods on synthetic and photorealistic benchmarks by being more robust to spurious correlations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning from human feedback (rlhf)], [reward model, video generation, reward hacking, bradley-terry loss, hierarchical attention]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiesong Lian, Ruizhe Zhong, Zixiang Zhou, Xiaoyue Mi, Yixue Hao, Yuan Zhou, Qinglin Lu, Long Hu, Junchi Yan</p>
</li>
<li class="">
<p><strong>institution:</strong> Huazhong University of Science and Technology, Shanghai Jiao Tong University, Tencent Hunyuan, University of Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22170" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22170</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A data collection and pairing strategy using single-item binary annotations and cross-prompt pairing to reduce labeling noise. 2. A Hierarchical Progressive Query Attention mechanism for better feature aggregation in the reward model architecture. 3. A modified Bradley-Terry loss function that accommodates win-tie scenarios to regularize the score distribution and mitigate reward hacking.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04ea37599d144feb85d3d1e8303d5d59adfabb579e172f06aef976d3783ee4ef_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04ea37599d144feb85d3d1e8303d5d59adfabb579e172f06aef976d3783ee4ef_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes SoliReward, a systematic framework to improve reward models for video generation alignment. It addresses data noise and reward hacking through a new data annotation strategy, a hierarchical attention architecture, and a modified loss function. The approach shows improved performance on benchmarks for video quality and enhances the effectiveness of post-training video models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Wireless Traffic Prediction with Large Language Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [spatio-temporal forecasting], [large language model, wireless traffic prediction, spatial-temporal correlation, prompt engineering, fine-tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chuanting Zhang, Haixia Zhang, Jingping Qiao, Zongzhang Li, Mohamed-Slim Alouini</p>
</li>
<li class="">
<p><strong>institution:</strong> Shandong University, Shandong Normal University, China Mobile Communications Group Shandong Co., Ltd, King Abdullah University of Science and Technology (KAUST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22178" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22178</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes TIDES, an LLM-based framework that captures spatial-temporal correlations for urban wireless traffic prediction. 2. Introduces a prompt engineering scheme to bridge the domain gap between numerical traffic data and language models by embedding statistical features as structured inputs. 3. Designs a DeepSeek module enabling spatial alignment via cross-domain attention, allowing the LLM to leverage information from related regions, and employs efficient fine-tuning of lightweight components.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/402a3d6681d9c203daf7e8ef09e0d0af8998f3eba780abc404c945025563d6a8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/402a3d6681d9c203daf7e8ef09e0d0af8998f3eba780abc404c945025563d6a8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes TIDES, a novel framework that uses a large language model (LLM) enhanced with spatial awareness for urban wireless traffic prediction. It addresses the lack of spatial modeling in existing LLM-based predictors through region clustering, prompt engineering, and a spatial alignment module, achieving superior accuracy and robustness on real-world datasets, which is key for intelligent 6G network management.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Latent Sculpting for Zero-Shot Generalization: A Manifold Learning Approach to Out-of-Distribution Anomaly Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [anomaly detection], [manifold learning, normalizing flows, dual-centroid compactness loss, out-of-distribution detection, zero-shot generalization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rajeeb Thapa Chhetri, Zhixiong Chen, Saurab Thapa</p>
</li>
<li class="">
<p><strong>institution:</strong> Mercy University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22179" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22179</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Latent Sculpting, a novel two-stage framework that decouples manifold structure learning from density estimation for OOD anomaly detection. 2. Introduces the Dual-Centroid Compactness Loss (DCCL) to actively sculpt a compact, low-entropy latent manifold for benign data. 3. Demonstrates superior zero-shot generalization on the CIC-IDS-2017 benchmark, significantly outperforming supervised and unsupervised baselines on complex distribution shifts like &quot;Infiltration&quot;.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfcac5687161ad2a09f74eab3c1b7f91a350a2435fb71a0c791f2e305dff108c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfcac5687161ad2a09f74eab3c1b7f91a350a2435fb71a0c791f2e305dff108c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of &quot;Generalization Collapse&quot; in supervised models when detecting Out-of-Distribution (OOD) anomalies. It proposes Latent Sculpting, a two-stage method that first uses a novel loss to sculpt a compact latent manifold for benign data and then applies a normalizing flow for density estimation. The results show this approach enables robust zero-shot anomaly detection, significantly outperforming existing methods on unseen attack scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [dimensionality reduction], [Locally Linear Embedding (LLE), AI-enhanced LLE, medical data analysis, medical billing, transcription]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hassan Khalid, Muhammad Mahad Khaliq, Muhammad Jawad Bashir</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Science and Technology (NUST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22182" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22182</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an innovative integration of AI with Locally Linear Embedding (LLE) to handle high-dimensional medical data. 2. Develops a comprehensive mathematical model for the AI-enhanced LLE technique. 3. Demonstrates the model&#x27;s application in real-world healthcare scenarios, showing significant improvements in data processing accuracy and operational efficiency for medical billing and transcription.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces an AI-enhanced Locally Linear Embedding (LLE) model to improve the analysis of high-dimensional medical data. The method is applied to automate and enhance medical billing and transcription services. The results show significant improvements in processing accuracy and operational efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Dueling Double Deep Q-Network, curriculum learning, tennis simulation, sequential decision-making, sports analytics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vishnu Mohan</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22186" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22186</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a custom tennis simulation environment that models hierarchical scoring, tactical decisions, fatigue, and opponent skill. 2. Integrated a Dueling Double Deep Q-Network (DDQN) with curriculum learning to enable stable and effective strategy learning in a long-horizon, stochastic domain. 3. Identified a key limitation of win-rate optimization, revealing a learned defensive bias and highlighting challenges in reward design for sports RL.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/543e2f9f07244abac63adfdbdefd7fccfefed9147b55aed87016c10657f91bae_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/543e2f9f07244abac63adfdbdefd7fccfefed9147b55aed87016c10657f91bae_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a reinforcement learning framework using a Dueling Double Deep Q-Network trained with curriculum learning to optimize tennis strategy in a custom simulation. The method achieves high win rates and demonstrates stable convergence, but analysis reveals the learned policy is overly defensive, pointing to a fundamental issue with reward design in sports simulations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part I: Basic Concepts, Neural Networks, and Variants</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [prognostics &amp; health management (phm)], [Neural Networks, Convolutional Neural Networks, Reinforcement Learning, Uncertainty Quantification, Physics-Informed Machine Learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jose I. Aizpurua</p>
</li>
<li class="">
<p><strong>institution:</strong> University of the Basque Country (UPV/EHU)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22190" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22190</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the application of Neural Networks (NNs) and their variants, specifically Convolutional Neural Networks (CNNs), for transformer condition monitoring using diverse data modalities. 2. Discusses the integration of NN concepts within the Reinforcement Learning (RL) paradigm for decision-making and control in transformer health management. 3. Provides perspectives on emerging research directions at the intersection of physics-informed machine learning and transformer Prognostics &amp; Health Management (PHM).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73f2611de029a059f651f47ffd6b707f684cdbc0c0f865e4c8568c2765f5fede_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73f2611de029a059f651f47ffd6b707f684cdbc0c0f865e4c8568c2765f5fede_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of traditional, rule-based transformer condition monitoring by proposing the use of machine learning, particularly Neural Networks and their variants. It explores Convolutional Neural Networks for processing diverse sensor data and discusses Reinforcement Learning for control, concluding that physics-informed ML provides a powerful framework for more accurate diagnostics, prognostics, and decision-making in power transformer health management.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part II: Physics-Informed Neural Networks and Uncertainty Quantification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [physics-informed machine learning], [Physics-Informed Neural Networks (PINNs), Bayesian PINNs, uncertainty quantification, Prognostics &amp; Health Management (PHM), transformer condition monitoring]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jose I. Aizpurua</p>
</li>
<li class="">
<p><strong>institution:</strong> University of the Basque Country (UPV/EHU)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22189" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22189</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Physics-Informed Neural Networks (PINNs) for integrating physics into neural network training for transformer applications like thermal modeling and insulation ageing. 2. Presents Bayesian PINNs as a framework to quantify epistemic uncertainty, enabling robust predictions under sparse data conditions. 3. Outlines emerging research directions for physics-aware and trustworthy machine learning in the management of critical power assets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d981ba5912467b9788cda82e843171a8af166b1ec965beb37ff22f102f1e02c8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d981ba5912467b9788cda82e843171a8af166b1ec965beb37ff22f102f1e02c8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes using Physics-Informed Neural Networks (PINNs) and their Bayesian extension to improve transformer condition monitoring. The method integrates physical laws directly into the learning process and quantifies model uncertainty, aiming to deliver more reliable predictions with limited data. The work highlights the potential of physics-aware machine learning for robust prognostics and health management of critical power infrastructure.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Frequency Regularization: Unveiling the Spectral Inductive Bias of Deep Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [regularization theory], [Spectral Bias, L2 Regularization, Frequency Principle, Spectral Suppression Ratio, Inductive Bias]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiahao Lu</p>
</li>
<li class="">
<p><strong>institution:</strong> The Chinese University of Hong Kong, Shenzhen</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22192" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22192</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/lujiahao760/FrequencyRegularization" target="_blank" rel="noopener noreferrer" class="">https://github.com/lujiahao760/FrequencyRegularization</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a Visual Diagnostic Framework to track the dynamic evolution of weight frequencies during CNN training. 2. Proposed a novel metric, the Spectral Suppression Ratio (SSR), to quantify the &quot;low-pass filtering&quot; intensity of different regularizers. 3. Empirically revealed a critical Accuracy-Robustness Trade-off in L2-regularized models, showing their sensitivity to broadband noise but superior robustness to high-frequency information loss.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7443604e19a6c972d6ac6ad332d345bd15a272d6f58a489ef07c494950a3d274_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7443604e19a6c972d6ac6ad332d345bd15a272d6f58a489ef07c494950a3d274_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the spectral inductive bias of deep neural networks, focusing on how regularization techniques like L2 affect feature frequency selection. The authors propose a visual diagnostic framework and a new metric to quantify spectral suppression, demonstrating that L2 regularization strongly suppresses high-frequency energy and leads to a trade-off between accuracy and robustness. The work confirms that regularization enforces a strong spectral bias towards low-frequency structures, providing a signal-processing perspective on generalization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MatKV: Trading Compute for Flash Storage in LLM Inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [retrieval-augmented generation, key-value cache, flash storage, prefill optimization, power efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kun-Woo Shin, Jay H. Park, Moonwook Oh, Yohan Jo, Jaeyoung Do, Sang-Won Lee</p>
</li>
<li class="">
<p><strong>institution:</strong> Seoul National University, Samsung Electronics</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22195" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22195</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/kunwooshin/MatKV" target="_blank" rel="noopener noreferrer" class="">https://github.com/kunwooshin/MatKV</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes MatKV, a scheme to precompute and materialize KV vectors of RAG documents in flash storage to avoid recomputation during inference. 2. Demonstrates that MatKV reduces inference time and power consumption by half for RAG workloads with minimal accuracy impact. 3. Shows MatKV enables additional optimizations like overlapping KV loading with decoding and enabling the use of low-end GPUs for decoding.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d47111ab2d615579a09c00ff5a99f391f035b974cc23e324cddfbabf4d23cec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d47111ab2d615579a09c00ff5a99f391f035b974cc23e324cddfbabf4d23cec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high compute and energy cost of the prefill phase in RAG-based LLM inference. It proposes MatKV, which precomputes and stores key-value vectors of documents in flash storage for reuse, trading compute for storage. Experiments show this approach halves inference time and power consumption while maintaining accuracy and enabling further hardware optimizations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AETAS: Analysis of Evolving Temporal Affect and Semantics for Legal History</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [semantic change detection], [diachronic embeddings, orthogonal Procrustes, lexical drift]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qizhi Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> PingCAP, Data &amp; AI-Innovation Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22196" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22196</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A reproducible, expert-system style pipeline for quantifying and visualizing lexical drift in historical corpora. 2. A method coupling interpretable semantic trajectories with legally meaningful axes (e.g., mercy-versus-retribution). 3. The application of the pipeline to the Old Bailey Corpus, exposing the evolution of legal concepts like justice and crime alongside historical events.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df36f3ea25509e1c01d661a7893c2b940f15cfeb346560d105c4488a5fba4140_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df36f3ea25509e1c01d661a7893c2b940f15cfeb346560d105c4488a5fba4140_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a reproducible pipeline for analyzing semantic drift in historical legal texts. The method involves training and aligning diachronic word embeddings to quantify and visualize lexical change. The analysis of the Old Bailey Corpus reveals how concepts of justice and crime evolved with penal reforms and societal debates.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [dynamic routing, residual networks, cosine incompatibility, Gumbel-Softmax, FLOPs regularization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yogeswar Reddy Thota</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Texas at Dallas</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22206" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22206</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces CosineGate, an end-to-end differentiable architecture for dynamic routing in residual networks using cosine incompatibility as a self-supervised skip signal. 2. Proposes the Cosine Incompatibility Ratio (CIR) to measure semantic redundancy and employs Gumbel-Softmax relaxation for per-sample, per-block gating during training. 3. Incorporates a progressive FLOPs regularization term to control average computational usage without destabilizing the optimization process.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed7e3fa1c114a73795152210d2b55ffe2541fede331ce04d228e11ca599688fa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed7e3fa1c114a73795152210d2b55ffe2541fede331ce04d228e11ca599688fa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the computational inefficiency in residual networks, where all blocks are evaluated for every input. It proposes CosineGate, a method that uses the cosine incompatibility between identity and residual features to dynamically skip redundant blocks, achieving significant FLOPs savings on CIFAR-10 while maintaining or improving accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Emotion-Inspired Learning Signals (EILS): A Homeostatic Framework for Adaptive Autonomous Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [intrinsic motivation, homeostatic control, adaptive optimization, non-stationary learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dhruv Tiwari</p>
</li>
<li class="">
<p><strong>institution:</strong> Lovely Professional University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22200" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22200</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel framework, Emotion-Inspired Learning Signals (EILS), that models emotions as continuous, homeostatic appraisal signals (e.g., Curiosity, Stress, Confidence) for adaptive control. 2. Formalizes these signals as vector-valued internal states derived from interaction history to dynamically modulate the agent&#x27;s optimization landscape in real-time. 3. Hypothesizes that this closed-loop homeostatic regulation enables superior sample efficiency and adaptation to non-stationary environments compared to standard baselines like PPO.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a45d2a56af1becf3eaddb05dbaeff3cf5453d19d41771b6d8ce1c1a70d3825c2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a45d2a56af1becf3eaddb05dbaeff3cf5453d19d41771b6d8ce1c1a70d3825c2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies the fragility of standard AI agents that rely on static, external rewards in open-ended environments. It proposes the Emotion-Inspired Learning Signals (EILS) framework, which uses bio-inspired internal signals like curiosity and stress to dynamically control learning. The authors hypothesize this approach will lead to more robust, adaptive, and sample-efficient autonomous agents.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [multimodal large language models], [Moxin, open-source LLM, vision-language-action, Model Openness Framework, multimodal models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Arash Akbari, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Northeastern University, Harvard University, Cornell University, Tulane University, University of Washington, Roboraction.ai, Futurewei, AIBAO LLC</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22208" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22208</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/moxin-org/Moxin-LLM" target="_blank" rel="noopener noreferrer" class="">https://github.com/moxin-org/Moxin-LLM</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Moxin 7B, a fully open-source LLM developed under the Model Openness Framework, promoting transparency in training, datasets, and implementation. 2. Develops three specialized variants of Moxin: Moxin-VLM for vision-language tasks, Moxin-VLA for vision-language-action tasks, and Moxin-Chinese for Chinese language capabilities. 3. Demonstrates superior performance of the proposed models in various evaluations using open-source frameworks and data, with all models, code, and data publicly released.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79873d0c3143fc2b06f1be1be9b8bc80555fa0aefd4a6f2b8d6bda39bce5f227_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79873d0c3143fc2b06f1be1be9b8bc80555fa0aefd4a6f2b8d6bda39bce5f227_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Moxin 7B, a fully transparent open-source large language model, and extends it into three multimodal variants for vision-language, vision-language-action, and Chinese tasks. The models are trained using open-source frameworks and data. The authors release the models, code, and data, reporting superior performance in evaluations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Transformer Reconstructed with Dynamic Value Attention</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [transformer architecture], [dynamic value attention, single-head attention, feed forward network removal]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiaowei Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> College of Artificial Intelligence, China University of Petroleum (Beijing)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22212" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22212</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Dynamic Value Attention (DVA), a method to dynamically decide a value for each query, addressing the limitation of static values in standard attention heads. 2. Enabled the removal of redundant multi-head attention, reducing the architecture to a single-head attention mechanism. 3. Demonstrated that the subsequent feed-forward network can be entirely removed, as the revised embeddings already capture sufficient contextual information.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79cd017ace993f77d704944ed5eeaa739c2f83168d5d9f241b0355966c04ea5f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79cd017ace993f77d704944ed5eeaa739c2f83168d5d9f241b0355966c04ea5f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a limitation in the standard Transformer where static values are used for all queries within an attention head. It proposes Dynamic Value Attention (DVA), which computes a dynamic value per query, allowing the model to use only a single attention head and remove the feed-forward network entirely. Experiments show DVA reduces training time by 37.6% while improving learning capability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On the Existence and Behaviour of Secondary Attention Sinks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [attention sinks, transformer, mlp, attention mechanism, large language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jeffrey T.H. Wong, Cheng Zhang, Louis Mahon, Wayne Luk, Anton Isopoussu, Yiren Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Imperial College London, UnlikelyAI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22213" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22213</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and characterizes a new class of &quot;secondary attention sinks&quot; that arise in middle layers, have variable lifetimes, and draw moderate attention, differing from persistent primary sinks like BOS. 2. Shows that secondary sinks are formed by specific middle-layer MLP modules that map token representations to align with the primary sink&#x27;s direction, with their L2-norm determining sink strength and lifetime. 3. Observes that in larger models, these sink patterns (sink levels) become more deterministic and frequent, with distinct levels identified in models like QwQ-32B and Qwen3-14B.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec1462ca646134f445eac98192ff5189abb63f37682802d695aadace9f83b0d3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec1462ca646134f445eac98192ff5189abb63f37682802d695aadace9f83b0d3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies a new phenomenon called &quot;secondary attention sinks&quot; in transformer LLMs, which are distinct from the known primary sinks (like BOS). The authors show these secondary sinks are created by middle-layer MLPs aligning tokens with the primary sink direction, and their properties (strength, lifetime) become more structured in larger models. This provides new insights into the internal mechanics of attention in large language models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Interpretable and Adaptive Node Classification on Heterophilic Graphs via Combinatorial Scoring and Hybrid Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [heterophily, interpretability, combinatorial inference, hybrid learning, node classification]</p>
</li>
<li class="">
<p><strong>authors:</strong> Soroush Vahidi</p>
</li>
<li class="">
<p><strong>institution:</strong> New Jersey Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22221" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22221</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an interpretable, adaptive framework for node classification using explicit combinatorial inference instead of deep message passing, with a tunable scoring function. 2. Introduces a validation-gated hybrid strategy that optionally refines combinatorial predictions with a lightweight neural model only when beneficial. 3. Ensures a leakage-free evaluation protocol by computing all adaptation signals strictly from training data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31111cc9c8d13c529d7f271adf29c47c440b43adfaa3481dde88da7f87b76463_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31111cc9c8d13c529d7f271adf29c47c440b43adfaa3481dde88da7f87b76463_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of node classification on heterophilic graphs where standard GNNs struggle. It proposes an interpretable framework based on combinatorial scoring and a conditional hybrid learning strategy, achieving competitive performance with modern GNNs while offering better interpretability, tunability, and efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [megakernel, kernel fusion, SM-level graph, software pipelining, CUDA]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xinhao Cheng, Zhihao Zhang, Yu Zhou, Jianan Ji, Jinchen Jiang, Zepeng Zhao, Ziruo Xiao, Zihao Ye, Yingyi Huang, Ruihang Lai, Hongyi Jin, Bohan Hou, Mengdi Wu, Yixin Dong, Anthony Yip, Zihao Ye, Songting Wang, Wenqin Yang, Xupeng Miao, Tianqi Chen, Zhihao Jia</p>
</li>
<li class="">
<p><strong>institution:</strong> Carnegie Mellon University, Tsinghua University, NVIDIA, University of Michigan, Purdue University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22219" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22219</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/mirage-project/mirage" target="_blank" rel="noopener noreferrer" class="">https://github.com/mirage-project/mirage</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces an SM-level graph representation for capturing fine-grained data dependencies across GPU streaming multiprocessors. 2. Develops a compiler and an in-kernel parallel runtime that automatically transforms multi-operator inference into a single, high-performance mega-kernel. 3. Enables previously infeasible GPU optimizations like cross-operator software pipelining and fine-grained kernel overlap, significantly reducing inference latency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f886cd06ce6c0f773c062fa38aae0fa982d862cc66ef54da9fbbfd6cf62dd86a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f886cd06ce6c0f773c062fa38aae0fa982d862cc66ef54da9fbbfd6cf62dd86a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces Mirage Persistent Kernel (MPK), a compiler and runtime system that automatically fuses multiple GPU kernels for model inference into a single, optimized mega-kernel. It achieves this by using a novel SM-level graph representation and decentralized scheduling to enable fine-grained optimizations like software pipelining. Evaluation shows MPK reduces LLM inference latency by up to 1.7x, pushing performance close to hardware limits.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [neural network architecture], [Müntz-Szász Networks, fractional power bases, physics-informed neural networks, universal approximation, singular function approximation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gnankan Landry Regis N&#x27;guessan</p>
</li>
<li class="">
<p><strong>institution:</strong> Axiom Research Group, The Nelson Mandela African Institution of Science and Technology (NM-AIST), African Institute for Mathematical Sciences (AIMS) Research and Innovation Centre</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22222" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22222</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Müntz-Szász Networks (MSN), a novel neural architecture with learnable fractional power bases to approximate functions with singular or fractional power behavior. 2. Provides theoretical analysis proving MSN inherits universal approximation from the Müntz-Szász theorem and establishes superior approximation rates compared to standard MLPs for singular functions. 3. Demonstrates empirical superiority, achieving significantly lower error with fewer parameters in supervised regression and 3-6x improvement in physics-informed neural network benchmarks, while learning interpretable exponents.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66c3ba4917449496481593b1432346dc5ef9301c3c66f4713e327bbb234969d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66c3ba4917449496481593b1432346dc5ef9301c3c66f4713e327bbb234969d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces Müntz-Szász Networks (MSN), a neural architecture that replaces fixed activation functions with learnable fractional power bases to better approximate singular functions common in physics. It proves MSN&#x27;s universal approximation capability and shows it achieves much lower error with fewer parameters than standard MLPs on regression and physics-informed tasks, demonstrating the value of theory-guided design.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video understanding], [streaming video, multimodal large language models, event segmentation, hierarchical representation, elastic-scale]</p>
</li>
<li class="">
<p><strong>authors:</strong> Naishan Zheng, Jie Huang, Qingpei Guo, Feng Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Science and Technology of China, Ant Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22226" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22226</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/zheng980629/VideoScaffold" target="_blank" rel="noopener noreferrer" class="">https://github.com/zheng980629/VideoScaffold</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes VideoScaffold, a dynamic representation framework for streaming video understanding in MLLMs that adaptively adjusts event granularity. 2. Introduces Elastic-Scale Event Segmentation (EES) for prediction-guided, dynamic boundary refinement. 3. Introduces Hierarchical Event Consolidation (HEC) for progressively aggregating segments into multi-level abstractions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of understanding long, streaming videos with MLLMs by proposing VideoScaffold, a framework that dynamically segments and hierarchically consolidates video events to adapt granularity and preserve semantics. It achieves state-of-the-art performance on benchmarks and can extend image-based MLLMs to video comprehension.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [retrieval-augmented generation (RAG), network traffic analysis, large language models (LLMs), hierarchical retrieval, explainable AI]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shaghayegh Shajarian, Kennedy Marsh, James Benson, Sajad Khorsandroo, Mahmoud Abdelsalam</p>
</li>
<li class="">
<p><strong>institution:</strong> North Carolina A&amp;T State University, University of Texas at San Antonio</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22223" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22223</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/270771/llm-traffictraffic" target="_blank" rel="noopener noreferrer" class="">https://github.com/270771/llm-traffictraffic</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ReGAIN, a multi-stage framework combining traffic summarization, RAG, and LLM reasoning for transparent network traffic analysis. 2. Introduces a hierarchical retrieval pipeline with metadata filtering, MMR sampling, cross-encoder reranking, and an abstention mechanism to ground responses and reduce hallucinations. 3. Demonstrates high accuracy (95.95%-98.82%) on real-world attack traces and outperforms traditional baselines while providing explainable, evidence-cited outputs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56c5ba03e3d4a510212509143bfecf0fa8b76f9171aa38dd765dadecc7b1ab32_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56c5ba03e3d4a510212509143bfecf0fa8b76f9171aa38dd765dadecc7b1ab32_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents ReGAIN, a framework that uses retrieval-augmented generation (RAG) and LLMs to analyze network traffic. It converts traffic into summaries, retrieves relevant evidence from a vector database, and generates interpretable, grounded analyses. The method achieves high accuracy on attack detection and provides explainable results, outperforming traditional rule-based and ML approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [representation analysis], [sentence embeddings, probing, hierarchical geometry, transformer models, cognitive states]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sophie Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Georgia Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22227" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22227</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Constructed a novel dataset of 480 sentences annotated with continuous energy scores and discrete tier labels for seven ordered cognitive categories. 2. Demonstrated that both continuous scores and discrete tier labels are reliably decodable from fixed transformer sentence embeddings using linear and nonlinear probes, with nonlinear probes providing consistent gains. 3. Provided statistical and qualitative evidence (via permutation tests, UMAP visualizations, and confusion matrices) that the embedding space exhibits a hierarchical geometric organization aligned with human-defined cognitive attributes, beyond surface word statistics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fec0b31a0f9f75593cbc3cdadecae63f4a1a7b7b6d910165a358bc72dde0f1d7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fec0b31a0f9f75593cbc3cdadecae63f4a1a7b7b6d910165a358bc72dde0f1d7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether transformer-based sentence embeddings encode a hierarchical structure aligned with cognitive states. The authors construct an annotated dataset and use linear and nonlinear probes to decode continuous scores and discrete labels from embeddings, finding reliable recoverability and a structured geometric gradient. The results show that transformer embedding spaces exhibit a systematic organization corresponding to interpretable cognitive attributes.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] We are not able to identify AI-generated images</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image forensics], [AI-generated images, human evaluation, MidJourney, CC12M, synthetic media detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Adrien Pavão</p>
</li>
<li class="">
<p><strong>institution:</strong> (Institution not explicitly stated in provided content. Author name is Adrien Pavão; no affiliation or email domain is given. Therefore, institution cannot be reliably inferred.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22236" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22236</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a controlled web-based experiment to empirically test human ability to distinguish real photographs from AI-generated portraits, finding performance near random chance (54% accuracy). 2. Created and released a curated, challenging dataset of 120 images (real from CC12M and AI-generated counterparts from MidJourney) designed to be difficult for humans. 3. Demonstrated that human judgment is insufficient for reliable detection of synthetic media, highlighting the need for greater public awareness and ethical guidelines as AI-generated content becomes more realistic.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b100d4e4882d297b51639fb736da62f90b11f1d810da4b6665f9da69ef3f31_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b100d4e4882d297b51639fb736da62f90b11f1d810da4b6665f9da69ef3f31_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper tests the assumption that humans can easily identify AI-generated images through an interactive web experiment where participants classified 20 images as real or AI-generated. Using a carefully curated dataset of 120 difficult portrait images (real from CC12M and AI-generated from MidJourney), the study found an average human accuracy of only 54%, barely above random guessing. The results show that humans struggle to reliably detect AI-generated content, indicating that human judgment alone is becoming insufficient and underscoring the need for awareness and ethical guidelines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] DiRL: An Efficient Post-Training Framework for Diffusion Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [Diffusion Language Models, FlexAttention, Group Relative Policy Optimization, LMDeploy, blockwise training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ying Zhu, Jiaxin Wan, Xiaoran Liu, Siyanag He, Qiqi Wang, Xu Guo, Tianyi Liang, Zengfeng Huang, Ziwei He, Xipeng Qiu</p>
</li>
<li class="">
<p><strong>institution:</strong> Fudan University, Shanghai Innovation Institute, OpenMoss Team</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22234" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22234</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/OpenMOSS/DiRL" target="_blank" rel="noopener noreferrer" class="">https://github.com/OpenMOSS/DiRL</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DiRL, an efficient post-training framework for Diffusion Language Models (dLLMs) that integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. 2. Introduces DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation specifically designed for dLLMs. 3. Demonstrates state-of-the-art math reasoning performance for dLLMs by training DiRL-8B-Instruct, surpassing comparable models like Qwen2.5 series on benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0ae2d009d9099214203b1dcca9a8b460cf0609d952e240f981b2689f247e17d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0ae2d009d9099214203b1dcca9a8b460cf0609d952e240f981b2689f247e17d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the underdeveloped and inefficient post-training landscape for Diffusion Language Models (dLLMs). It proposes DiRL, an efficient framework combining accelerated training and optimized inference, and introduces DiPO, a tailored reinforcement learning method. The resulting model, DiRL-8B-Instruct, achieves state-of-the-art math performance among dLLMs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Enhanced geometry prediction in laser directed energy deposition using meta-learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [meta-learning], [meta-learning, model-agnostic meta-learning, reptile, laser-directed energy deposition, bead geometry prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abdul Malik Al Mardhouf Al Saadi, Amrita Basak</p>
</li>
<li class="">
<p><strong>institution:</strong> The Pennsylvania State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22241" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22241</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a cross-dataset knowledge transfer model for L-DED bead geometry prediction using meta-learning to address data scarcity and heterogeneity. 2. Investigated and applied two gradient-based meta-learning algorithms (MAML and Reptile) for rapid adaptation to new deposition conditions with limited data. 3. Demonstrated strong generalization performance of the meta-learning models across diverse L-DED processes (powder-fed, wire-fed, hybrid) using minimal training examples, outperforming conventional neural networks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4239294fb44dd0fe97ebc3a123162ba7aaa82e51c2805d4460072daeffe6de9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4239294fb44dd0fe97ebc3a123162ba7aaa82e51c2805d4460072daeffe6de9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of predicting bead geometry in laser-directed energy deposition (L-DED) where experimental data is scarce and heterogeneous. It proposes using meta-learning algorithms, specifically MAML and Reptile, to enable rapid model adaptation to new printing conditions with very few training examples. The results show that this approach achieves accurate predictions and outperforms traditional neural networks under similar data constraints, demonstrating effective knowledge transfer across different L-DED settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical imaging], [algorithmic fairness, subgroup performance analysis, JustEFAB framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shaurya Gaur, Michel Vitale, Alessa Hering, Johan Kwisthout, Colin Jacobs, Lena Philipp, Fennie van der Graaf</p>
</li>
<li class="">
<p><strong>institution:</strong> Radboud University Medical Center, Radboud University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22242" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22242</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a fairness evaluation of three lung cancer risk estimation models (Sybil, Venkadesh21, PanCan2b) using the JustEFAB framework to assess ethically significant biases. 2. Identified and quantified statistically significant performance disparities across demographic subgroups (e.g., gender, race) that were not explained by available clinical confounders. 3. Highlighted the critical need for monitoring and improving model fairness in lung cancer screening AI to ensure equitable clinical application.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study evaluates the fairness of AI models for lung cancer risk estimation from CT scans. Using the JustEFAB framework, it assessed performance disparities across demographic groups and found significant, unexplained biases in two deep learning models. The findings underscore the importance of algorithmic fairness in medical AI to ensure equitable screening outcomes.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Predicting Mycotoxin Contamination in Irish Oats Using Deep and Transfer Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [transfer learning], [TabPFN, FT-Transformer, permutation-based variable importance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alan Inglis, Fiona Doohan, Subramani Natarajan, Breige McNulty, Chris Elliott, Anne Nugent, Julie Meneely, Brett Greer, Stephen Kildea, Diana Bucur, Martin Danaher, Melissa Di Rocco, Lisa Black, Adam Gauley, Naoise McKenna, Andrew Parnell</p>
</li>
<li class="">
<p><strong>institution:</strong> Maynooth University, University College Dublin, Queen&#x27;s University Belfast, Teagasc, Agri-Food and Biosciences Institute (AFBI)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22243" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22243</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Evaluated and compared the performance of multiple deep learning and transfer learning models (MLP, TabPFN, TabNet, FT-Transformer) for predicting mycotoxin contamination in oats. 2. Applied these models to a multi-response prediction task using a dataset of environmental, agronomic, and geographical predictors from Irish oat samples. 3. Conducted a permutation-based variable importance analysis, identifying weather history in the 90-day pre-harvest period and seed moisture content as the most influential predictors.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64407e4c1b4c79c19f30f80502744b2e2eadb71d078c9e48b84cad3e1286130e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64407e4c1b4c79c19f30f80502744b2e2eadb71d078c9e48b84cad3e1286130e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study uses neural networks and transfer learning to predict mycotoxin contamination in Irish oat crops. The models, including TabPFN, TabNet, and FT-Transformer, were evaluated on a multi-response task, with TabPFN achieving the best overall performance. The analysis found that weather patterns before harvest and seed moisture are the most critical factors for prediction.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Masking Teacher and Reinforcing Student for Distilling Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [knowledge distillation, reinforcement learning, vision-language models, progressive masking, offline RL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Byung-Kwan Lee, Yu-Chiang Frank Wang, Ryo Hachiuma</p>
</li>
<li class="">
<p><strong>institution:</strong> NVIDIA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22238" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22238</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Masters, a mask-progressive RL distillation framework that first masks non-dominant teacher weights to reduce complexity and then progressively restores them for stable student learning. 2. Introduces an offline RL stage with complementary accuracy and distillation rewards, leveraging pre-generated responses from masked teachers for efficient guidance. 3. Demonstrates that progressive teacher scaling (e.g., from 14B to 38B) yields smoother convergence and stronger generalization than one-shot distillation, providing a scalable path to efficient VLMs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of distilling large vision-language models (VLMs) into compact ones by proposing Masters, a framework that uses progressive masking of the teacher model and offline reinforcement learning. This method enables stable knowledge transfer and efficient training, resulting in small VLMs that achieve strong performance, sometimes surpassing larger models, while being far more efficient for deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [interpretability], [mechanistic multiplicity, explanatory stability, stochastic optimization, model explanations, diagnostic framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chama Bensmail</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Hertfordshire, Omics Data Solutions LTD</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22240" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22240</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/bensmailchama-boop/EvoXplain" target="_blank" rel="noopener noreferrer" class="">https://github.com/bensmailchama-boop/EvoXplain</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces EvoXplain, a diagnostic framework for measuring the stability of model explanations across repeated training runs, treating explanations as samples from the optimization process. 2. Demonstrates that high-accuracy models (e.g., Logistic Regression, Random Forests) can rely on multiple distinct internal mechanisms, revealing explanatory multimodality not captured by single-model or averaged explanations. 3. Reframes interpretability as a property of a model class under repeated instantiation, challenging the assumption that a single high-accuracy model yields a unique or trustworthy explanation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a39d3b0c4608e98a5ffde3a753078019f45b0c48774cb02ee158633c35e823d2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a39d3b0c4608e98a5ffde3a753078019f45b0c48774cb02ee158633c35e823d2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces EvoXplain, a framework to diagnose if models achieving similar high accuracy do so via the same or different internal mechanisms by analyzing explanation stability across training runs. It finds that even simple, stable models like Logistic Regression can exhibit multiple distinct explanatory modes on datasets like Breast Cancer and COMPAS, showing that single-model explanations can be misleading. This work highlights explanatory instability as a measurable property and reframes interpretability as a characteristic of a model class rather than a single trained instance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [uncertainty estimation, calibration, linear probe, brier score, llm-as-judge]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bhaktipriya Radharapu, Eshika Saxena, Kenneth Li, Chenxi Whitehouse, Adina Williams, Nicola Cancedda</p>
</li>
<li class="">
<p><strong>institution:</strong> Meta (FAIR at Meta, Meta Superintelligence Labs)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22245" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22245</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a method using linear probes on LLM hidden states to provide calibrated uncertainty estimates for LLM judges, requiring no additional model training. 2. Demonstrates superior calibration and ≈10x computational savings compared to baseline methods like verbalized confidence. 3. Shows the method generalizes robustly across different model architectures, training paradigms, and unseen evaluation domains.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33ea018b43295c51d076b3840537c861c2e2c7f22ca2bdd183164d1f1feed91d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33ea018b43295c51d076b3840537c861c2e2c7f22ca2bdd183164d1f1feed91d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of obtaining efficient and well-calibrated uncertainty estimates for LLM-based judges. It proposes using linear probes trained with a Brier score loss on the model&#x27;s hidden states. The method achieves better calibration with significant computational savings and provides a practical plug-and-play solution for production deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Amortized Inference for Model Rocket Aerodynamics: Learning to Estimate Physical Parameters from Simulation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [physics-informed machine learning], [amortized inference, sim-to-real transfer, model rocketry, neural network, parameter estimation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rohit Pandey, Rohan Pandey</p>
</li>
<li class="">
<p><strong>institution:</strong> Bellevue High School, University of Washington</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22248" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22248</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulates model rocket parameter estimation as an amortized inference problem and demonstrates neural networks can invert physics simulations from sparse observations. 2. Proposes a simulation-based amortized inference approach that enables zero-shot sim-to-real transfer for aerodynamic parameter estimation. 3. Provides quantitative analysis of the sim-to-real gap and shows the learned model reduces apogee prediction error compared to a traditional baseline (OpenRocket).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59b12172a46941853e291b33e21d912e272992d414baf27e4495c7137fe4266c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59b12172a46941853e291b33e21d912e272992d414baf27e4495c7137fe4266c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a simulation-based amortized inference method that trains a neural network on synthetic flight data to predict aerodynamic parameters like drag coefficient from a single apogee measurement. The trained model is applied directly to real flights without fine-tuning, achieving promising zero-shot sim-to-real transfer and reducing apogee prediction error compared to traditional tools.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Affine Divergence: Aligning Activation Updates Beyond Normalisation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [optimization theory], [activation updates, gradient descent, normalisation, PatchNorm, affine divergence]</p>
</li>
<li class="">
<p><strong>authors:</strong> George Bird</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Manchester</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22247" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22247</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies a systematic mismatch between mathematically ideal and effective activation updates during gradient descent, providing a new theoretical framework for understanding optimization. 2. Derives normalisation from first principles as a solution to this mismatch and proposes a functionally distinct, non-scale-invariant alternative that outperforms conventional normalisers. 3. Introduces &quot;PatchNorm&quot;, a new compositionally inseparable normaliser for convolutional layers, and argues for reframing normalisers as activation-function-like maps with parameterised scaling.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48ed42bcb2d9358f1931e9e0ea31246380c7ef78409f6004933a0fb2461eb9d1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48ed42bcb2d9358f1931e9e0ea31246380c7ef78409f6004933a0fb2461eb9d1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a mismatch between the ideal steepest-descent direction for activations and their effective updates during gradient descent. It proposes a theoretical framework that derives normalisation as a solution and introduces a new alternative, including &quot;PatchNorm&quot; for convolutions, which empirically outperforms standard normalisers. This reframes normalisation&#x27;s role and questions the standard affine+nonlinear model-building approach.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Analyzing Skill Element in Online Fantasy Cricket</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [game theory and decision making], [statistical framework, team selection strategies, dynamic tournament model, softmax reweighting, IPL dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sarthak Sarkar, Supratim Das, Purushottam Saha, Diganta Mukherjee, Tridib Mukherjee</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Statistical Institute, Kolkata</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22254" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22254</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Development of a statistical framework to assess the role of skill in online fantasy cricket. 2. Construction and analysis of a range of deterministic and stochastic team selection strategies. 3. Introduction of a dynamic tournament model with agent populations evolving via a softmax reweighting mechanism.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e92b7b67260659549974352e85a2e375a90fd015a5561a1d909fb586dcfd635_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e92b7b67260659549974352e85a2e375a90fd015a5561a1d909fb586dcfd635_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper develops a statistical framework to analyze whether success in online fantasy cricket is driven by skill or chance. It constructs various team selection strategies and a dynamic tournament model, testing them on IPL 2024 data. The results provide quantitative evidence supporting the presence of a skill element in these platforms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Graph Attention-based Adaptive Transfer Learning for Link Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [graph attention network, link prediction, transfer learning, graph transformer, contrastive loss]</p>
</li>
<li class="">
<p><strong>authors:</strong> Huashen Lu, Wensheng Gan, Guoting Chen, Zhichao Huang, Philip S. Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> Jinan University, Great Bay University, JD Technology, University of Illinois Chicago</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22252" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22252</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/DSI-Lab1/GAATNet" target="_blank" rel="noopener noreferrer" class="">https://github.com/DSI-Lab1/GAATNet</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GAATNet, a novel graph attention adaptive transfer network combining pre-training and fine-tuning for cross-dataset knowledge transfer in link prediction. 2. Incorporates distant neighbor embeddings as biases in self-attention to capture global node features. 3. Introduces a lightweight self-adapter module during fine-tuning to improve training efficiency and generalization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e9ea1091686509af1da226ce7553577b4005c5646524a5c6718473e451d3c39_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e9ea1091686509af1da226ce7553577b4005c5646524a5c6718473e451d3c39_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses challenges in link prediction on large-scale sparse graphs and cross-dataset transfer learning by proposing GAATNet, which integrates graph attention with adaptive transfer strategies. The method uses distant neighbor embeddings and a self-adapter module to enhance global feature capture and training efficiency. Experiments on seven datasets show state-of-the-art performance, offering a scalable solution for integrating GNNs with transfer learning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human motion segmentation], [temporal vision semantics, subspace clustering, large language model, temporal regularizer, feedback framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zheng Xing, Weibing Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Shenzhen University, Shenzhen MSU-BIT University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22249" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22249</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel method to learn Temporal Vision Semantics (TVS) from human motion sequences by querying a Large Language Model (LLM) to determine motion consistency between consecutive frames. 2. Develops a TVS-integrated subspace clustering framework that incorporates a temporal regularizer and constraint to enforce similarity among temporal neighbors in the subspace embedding and segmentation. 3. Introduces a feedback-enabled optimization framework that iteratively refines the subspace embedding based on the segmentation output to improve performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fce87220cc17109bbd9138b27d5aa353003bb134fb0924f68b7fc59fdfd3ee0d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fce87220cc17109bbd9138b27d5aa353003bb134fb0924f68b7fc59fdfd3ee0d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of unsupervised human motion segmentation by integrating temporal semantic information into subspace clustering. The proposed method uses a Large Language Model to extract textual motion descriptions from consecutive frames and incorporates this learned temporal neighbor information as constraints within the clustering framework. Experimental results show the method outperforms state-of-the-art approaches on four benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [biomedical knowledge graph, graph attention network, gene perturbation, multimodal embeddings, PrimeKG++]</p>
</li>
<li class="">
<p><strong>authors:</strong> Pascal Passigan, Kevin zhu, Angelina Ning</p>
</li>
<li class="">
<p><strong>institution:</strong> Massachusetts Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22251" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22251</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a novel framework for gene perturbation prediction by merging PrimeKG++ with LINCS L1000 data into a heterogeneous biomedical knowledge graph, moving beyond binary drug-disease association tasks. 2. Demonstrates the application of a Graph Attention Network (GAT) to predict delta gene expression profiles for drug-cell pairs, outperforming MLP baselines. 3. Provides interpretability through ablation studies (edge shuffling, node feature randomization) showing the critical role of biomedical KG edges in enhancing perturbation-level prediction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a8b52522e1bdfc53ae9978a144c2011810eca2532cb9819ad999bf9b2b6cbb6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a8b52522e1bdfc53ae9978a144c2011810eca2532cb9819ad999bf9b2b6cbb6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the gap in predicting detailed gene expression changes (perturbations) caused by drugs by constructing a merged biomedical knowledge graph from PrimeKG++ and LINCS L1000 data. The proposed method uses a Graph Attention Network to predict delta expression profiles for drug-cell pairs, which outperforms baseline models and demonstrates the value of graph structure for mechanistic understanding.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [prompt engineering], [Logic Sketch Prompting, deterministic prompting, interpretability, rule adherence, clinical decision support]</p>
</li>
<li class="">
<p><strong>authors:</strong> Satvik Tripathi</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Pennsylvania</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22258" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22258</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/satviktri/LSP" target="_blank" rel="noopener noreferrer" class="">https://github.com/satviktri/LSP</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Logic Sketch Prompting (LSP), a lightweight prompting framework that introduces typed variables and deterministic condition evaluators for structured reasoning., 2. Incorporates a rule-based validator to produce traceable and repeatable outputs, enhancing auditability., 3. Demonstrates significant performance gains over standard prompting methods (zero-shot, chain-of-thought, concise) on pharmacologic logic-compliance tasks across multiple open-weight LLMs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b5d49d54027e4f7e6b110a48568a0255a45010197e26e8bc344e0cd3e1785a9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b5d49d54027e4f7e6b110a48568a0255a45010197e26e8bc344e0cd3e1785a9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the unreliability of LLMs on tasks requiring strict rule adherence and determinism. It proposes Logic Sketch Prompting (LSP), a framework using typed variables and rule-based validation to produce traceable outputs. Evaluations on clinical tasks show LSP significantly outperforms standard prompting methods in accuracy and F1 score, making it suitable for safety-critical systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [reasoning], [chain-of-thought, synthetic data, distribution shift, fine-tuning, reasoning robustness]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abhranil Chandra, Ayush Agrawal, Arian Hosseini, Sebastian Fischmeister, Rishabh Agarwal, Navin Goyal, Aaron Courville</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Waterloo, University of Massachusetts Amherst, MILA - Quebec AI Institute, Université de Montréal, Microsoft Research India, Google DeepMind, Periodic Labs</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22255" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22255</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that training on synthetic chain-of-thought traces from more capable models, even when they lead to incorrect final answers, can improve a language model&#x27;s reasoning performance more than training on human-annotated datasets. 2. Proposes and validates two hypotheses for this phenomenon: the distributional alignment of synthetic data with the model, and the partial validity of reasoning steps within flawed traces. 3. Shows that paraphrasing human traces to align with the model&#x27;s distribution improves performance, and investigates model tolerance to increasingly flawed reasoning steps.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf038d101bb93ad9f8c253c481419dec618655c74a6b867d0128b6358a3fa331_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf038d101bb93ad9f8c253c481419dec618655c74a6b867d0128b6358a3fa331_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper challenges the assumption that correctness is the primary determinant of data quality for training language models on reasoning tasks. It shows that fine-tuning on synthetic, incorrect chain-of-thought traces from stronger models can outperform training on correct human-annotated data, primarily because the synthetic data&#x27;s distribution is closer to the model&#x27;s own. The key conclusion is that aligning the training data distribution with the model&#x27;s is more critical for performance than the correctness of the final answers.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Cardiac mortality prediction in patients undergoing PCI based on real and synthetic data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [clinical prediction], [synthetic data, class imbalance, permutation feature importance, tabular data, mortality prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Daniil Burakov, Ivan Petrov, Dmitrii Khelimskii, Ivan Bessonov, Mikhail Lazarev</p>
</li>
<li class="">
<p><strong>institution:</strong> HSE University, Meshalkin National Medical Research Center, Tyumen Cardiology Research Center</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22259" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22259</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed and evaluated machine learning models for predicting 3-year cardiac mortality after PCI using a dataset of patients with bifurcation lesions. 2. Demonstrated that augmenting the training set with synthetic samples effectively addresses class imbalance, improving minority-class recall and probability quality with minimal impact on AUROC. 3. Identified key clinical predictors (Age, Ejection Fraction, Peripheral Artery Disease, Cerebrovascular Disease) through feature importance analysis and highlighted the brittleness of models on external validation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8124ba060c0a4a10502266c7255b089103b4cb860b15006e80971c9e28cfdc68_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8124ba060c0a4a10502266c7255b089103b4cb860b15006e80971c9e28cfdc68_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study developed machine learning models to predict cardiac death within three years for patients undergoing percutaneous coronary intervention (PCI). To handle class imbalance, the authors augmented the real patient data with synthetic samples, which improved the models&#x27; ability to identify high-risk patients. The analysis identified key risk factors and demonstrated that data augmentation can reduce model brittleness in imbalanced clinical prediction tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Physics Constraint Paradox: When Removing Explicit Constraints Improves Physics-Informed Data for Machine Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [physics-informed machine learning], [physics-constrained data generation, ablation study, grating coupler, Fabry-Perot oscillations, energy conservation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rahul D Ray</p>
</li>
<li class="">
<p><strong>institution:</strong> BITS Pilani, Hyderabad Campus</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22261" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22261</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies the &quot;physics constraint paradox,&quot; demonstrating that explicit energy conservation enforcement can be mathematically redundant in physically consistent models. 2. Shows that removing Fabry-Perot oscillations significantly reduces bandwidth variability and improves downstream ML model accuracy for bandwidth prediction. 3. Reveals a subtle pitfall where standard noise-addition-and-renormalization pipelines can introduce unphysical negative absorption values.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c510a23d89c2ae438eca78e6b993f8ce6b30f94405a1290b3509d74226bafc1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c510a23d89c2ae438eca78e6b993f8ce6b30f94405a1290b3509d74226bafc1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates physics-constrained data generation for machine learning through an ablation study of a grating coupler spectrum generator. It finds that explicitly enforcing certain physical constraints, like energy conservation, can be redundant, while others, like Fabry-Perot oscillations, can hinder machine learning performance for specific prediction tasks. The main conclusion is that increased physical realism in data generation does not always improve ML learnability, and ML performance can be used to diagnose the relevance of physical constraints.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph representation learning], [temporal motifs, dynamic graphs, llm agent, structure-aware dispatcher, prompting techniques]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bing Hao, Minglai Shao, Zengyi Wo, Yunlong Chu, Yuhang Liu, Ruijie Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tianjin University, Beihang University, Guangxi Normal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22266" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22266</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Wjerry5/LLMTM" target="_blank" rel="noopener noreferrer" class="">https://github.com/Wjerry5/LLMTM</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LLMTM, a comprehensive benchmark for evaluating LLMs on six tasks across nine types of temporal motifs in dynamic graphs. 2. Develops a tool-augmented LLM agent that uses engineered prompts to achieve high accuracy on temporal motif analysis tasks. 3. Introduces a structure-aware dispatcher that intelligently routes queries between standard LLM prompting and the more powerful agent to balance accuracy and cost.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ebbb05fef920a296d5863029d0c69e932a75230132aa0658a09e6bd8d04010c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ebbb05fef920a296d5863029d0c69e932a75230132aa0658a09e6bd8d04010c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the use of Large Language Models (LLMs) for analyzing temporal motifs in dynamic graphs, an area that is relatively unexplored. The authors propose a new benchmark (LLMTM), develop a high-accuracy but costly tool-augmented LLM agent, and then introduce a structure-aware dispatcher to reduce cost while maintaining performance. Their experiments show the dispatcher effectively maintains high accuracy while reducing cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [RGB-LWIR fusion, multispectral imagery, YOLO, adaptive framework, illumination conditions]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aahan Sachdeva, Dhanvinkumar Ganeshkumar, James E. Gallagher, Tyler Treat, Edward J. Oughton</p>
</li>
<li class="">
<p><strong>institution:</strong> George Mason University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22263" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22263</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An adaptive framework that dynamically selects the optimal RGB-LWIR fusion ratio and detection model based on real-time illumination conditions. 2. Creation of a comprehensive dataset and model set, training 33 YOLO models on over 22,000 annotated images across three light levels with eleven fusion ratios. 3. Demonstrated significant performance improvements over RGB-only and thermal-only baselines, particularly in full-light and dim-light conditions, enhancing detection reliability for autonomous systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of RGB and thermal-only object detection in variable lighting by proposing an adaptive framework that fuses RGB and LWIR video streams at multiple ratios and selects the best model for the current illumination. The method, evaluated on a large dataset, showed that optimized fusion models significantly outperformed baseline models in full and dim light, improving detection confidence and reliability for autonomous robotic vision.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LuxIA: A Lightweight Unitary matriX-based Framework Built on an Iterative Algorithm for Photonic Neural Network Training</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [photonic neural networks, transfer matrix, Slicing method, back-propagation, simulation framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tzamn Melendez Carmona, Federico Marchesin, Marco P. Abrate, Peter Bienstman, Stefano Di Carlo, Alessandro Savino Senior</p>
</li>
<li class="">
<p><strong>institution:</strong> Politecnico di Torino, Ghent University - imec, University College London</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22264" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22264</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the Slicing method, an efficient transfer matrix computation approach compatible with back-propagation for training Photonic Neural Networks (PNNs). 2. Introduces LuxIA, a unified simulation and training framework that integrates the Slicing method to enable scalable PNN training. 3. Demonstrates through experiments that LuxIA surpasses existing tools in speed and scalability for training large-scale PNNs on standard datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/362a4190a58349fa96aae555a8a4643a7fdd50b962ff88817d9c12e4678fbe98_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/362a4190a58349fa96aae555a8a4643a7fdd50b962ff88817d9c12e4678fbe98_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the scalability challenges in simulating and training large-scale Photonic Neural Networks (PNNs) by introducing the Slicing method for efficient transfer matrix computation. The method is integrated into the LuxIA framework, which significantly reduces memory usage and training time. Experimental results show LuxIA outperforms existing tools, enabling the exploration of larger and more complex photonic architectures.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Hierarchical Stacking Optimization Using Dirichlet&#x27;s Process (SoDip): Towards Accelerated Design for Graft Polymerization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [bayesian optimization], [Dirichlet Process Mixture Model, Gaussian Process Regression, Transformer, TabNet, XGBoost]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amgad Ahmed Ali Ibrahim, Hein Htet, Ryoji Asahi</p>
</li>
<li class="">
<p><strong>institution:</strong> Nagoya University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22279" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22279</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a hierarchical stacking optimization framework (SoDip) integrating a Transformer for text, TabNet/XGBoost for multimodal features, and GPR with DPMM for uncertainty. 2. Curated a diverse dataset for radiation-induced grafting using automated tools to handle numerical and textual variables. 3. Demonstrated ~33% performance improvement over standard GPR with calibrated confidence intervals for identifying low-reproducibility regimes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e1ff3feaf3b0e588ef295bb1595e516fbcdb32563ea40c93095217ac66e1fc5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e1ff3feaf3b0e588ef295bb1595e516fbcdb32563ea40c93095217ac66e1fc5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses reproducibility issues in radiation-induced graft polymerization by proposing SoDip, a hierarchical data-driven framework that combines Transformer encoders, multimodal feature models, and Bayesian optimization with uncertainty quantification. The method integrates sparse textual and numerical data, showing a 33% improvement over Gaussian Process Regression and providing reliable confidence estimates. This establishes a foundation for morphology-aware, reproducible design in polymer research.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Valori: A Deterministic Memory Substrate for AI Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [memory &amp; caching], [deterministic memory, fixed-point arithmetic, vector embeddings, approximate nearest neighbor search, state machine]</p>
</li>
<li class="">
<p><strong>authors:</strong> Varshith Gudur</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher (Valori Kernel Project)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22280" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22280</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/varshith-Git/Valori-Kernel" target="_blank" rel="noopener noreferrer" class="">https://github.com/varshith-Git/Valori-Kernel</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and characterizes the fundamental non-determinism in AI memory systems caused by hardware-dependent floating-point arithmetic, which leads to divergent memory states and retrieval results. 2. Proposes Valori, a deterministic AI memory substrate that replaces floating-point operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. 3. Demonstrates that Valori guarantees bit-identical memory states, snapshots, and search results across different hardware platforms (e.g., x86 vs. ARM), establishing deterministic memory as a necessary primitive for trustworthy AI.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb7ed17a8a06e6a2b8760f08fa9345391995aee74a3542cacf55dd051b383f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb7ed17a8a06e6a2b8760f08fa9345391995aee74a3542cacf55dd051b383f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies non-determinism in AI memory systems due to hardware-dependent floating-point arithmetic, which compromises replayability and auditability. It proposes Valori, a memory substrate using fixed-point arithmetic and a state machine model to guarantee bit-identical behavior across platforms. The work concludes that deterministic memory is essential for building trustworthy AI systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Cluster Aggregated GAN (CAG): A Cluster-Based Hybrid Model for Appliance Pattern Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models], [Generative Adversarial Networks, Non-Intrusive Load Monitoring, Clustering, LSTM, Pattern Generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zikun Guoa, Adeyinka.P. Adedigbaa, Rammohan Mallipeddi</p>
</li>
<li class="">
<p><strong>institution:</strong> Kyungpook National University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22287" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22287</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hybrid GAN framework that routes appliances to specialized branches based on behavioral characteristics (intermittent vs. continuous). 2. Introduces a clustering module for intermittent appliances to group similar activation patterns and allocate dedicated generators, improving modeling of both common and rare modes. 3. Employs a separate LSTM-based generator branch for continuous appliances to capture gradual temporal evolution while maintaining training stability through sequence compression.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37f2c4c207022049ee869567d36df9e77e5a04d1beb0cc584dc57ee6ad1145b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37f2c4c207022049ee869567d36df9e77e5a04d1beb0cc584dc57ee6ad1145b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes the Cluster Aggregated GAN (CAG), a hybrid generative model that synthesizes appliance load patterns by separating intermittent and continuous devices into specialized branches, using clustering for the former and an LSTM for the latter. Experiments on the UVIC dataset show it outperforms baselines in realism, diversity, and training stability. The integration of clustering as an active component also enhances the model&#x27;s interpretability and scalability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [causal inference], [Double Machine Learning, Moderated Mediation, Algorithmic Control, Nonmonotonic Effects, Gig Economy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Arunkumar V, Nivethitha S, Sharan Srinivas, Gangadharan G.R</p>
</li>
<li class="">
<p><strong>institution:</strong> Anna University, National Institute of Technology Tiruchirappalli, University of Missouri</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22290" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22290</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Applied a Double Machine Learning framework to estimate a moderated mediation model without restrictive linear assumptions in organizational research. 2. Uncovered a nonmonotonic relationship between algorithmic oversight, worker wellbeing, and performance, highlighting a &quot;murky middle&quot; of confusing oversight. 3. Demonstrated that simple linear models can be misleading and provided practical insights for designing transparent and explainable algorithmic management systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ffeb8d364908888226033cff39cbb354772ae1044ba1a51632db140d81dca17_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ffeb8d364908888226033cff39cbb354772ae1044ba1a51632db140d81dca17_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the nonlinear effects of algorithmic control on gig workers. Using a Double Machine Learning approach on survey data, it finds that the link between supportive HR practices and worker performance weakens under opaque algorithmic oversight but strengthens again when the oversight is transparent and explainable.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [Masked Diffusion Models, Markov Decision Process, Group Relative Policy Optimization, inference schedule optimization, trajectory-level training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Renping Zhou, Zanlin Ni, Tianyi Chen, Zeyu Liu, Yang Yue, Yulin Wang, Yuxuan Wang, Jingshu Liu, Gao Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University (Leap Lab), Anyverse Dynamics</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22288" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22288</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://co-grpo.github.io" target="_blank" rel="noopener noreferrer" class="">https://co-grpo.github.io</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and addresses the discrepancy between the single-step training and multi-step inference of Masked Diffusion Models (MDMs). 2. Proposes Co-GRPO, a method that reformulates MDM generation as a unified Markov Decision Process to jointly optimize model parameters and inference schedule parameters. 3. Introduces a trajectory-level optimization using Group Relative Policy Optimization that avoids costly backpropagation through the multi-step generation process.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the misalignment between the training and inference procedures of Masked Diffusion Models (MDMs). It proposes Co-GRPO, a method that jointly optimizes the MDM and its inference schedule as a unified Markov Decision Process using Group Relative Policy Optimization. The approach improves generation quality across multiple benchmarks without requiring expensive backpropagation through the full generation trajectory.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scientific machine learning], [Physics-informed neural networks, Kolmogorov-Arnold networks, Adaptive weighting, B-splines, Partial differential equations]</p>
</li>
<li class="">
<p><strong>authors:</strong> Guokan Chen, Yao Xiao</p>
</li>
<li class="">
<p><strong>institution:</strong> Fujian University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22283" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22283</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DBAW-PIKAN, a novel architecture combining a Kolmogorov-Arnold Network (KAN) with learnable B-splines for enhanced function representation in solving PDEs., 2. Introduces an adaptive weighting strategy with a dynamic decay upper bound to mitigate gradient flow stiffness and spectral bias, addressing key failure modes of PINNs., 3. Demonstrates significant improvements in convergence speed and solution accuracy (at least an order of magnitude) on benchmarks like Klein-Gordon, Burgers, and Helmholtz equations without added computational cost.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb9e65232c0c340c6072237e2ad1388255462aafca80cf91d4b7f3054eb9fe8c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb9e65232c0c340c6072237e2ad1388255462aafca80cf91d4b7f3054eb9fe8c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes DBAW-PIKAN, a novel neural network that integrates a Kolmogorov-Arnold architecture with an adaptive weighting strategy to overcome the stiffness and spectral bias challenges faced by Physics-Informed Neural Networks (PINNs) when solving multi-scale PDEs. The method accelerates convergence and improves solution accuracy by at least an order of magnitude on standard benchmarks without increasing computational complexity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [language model safety], [sparse autoencoder, feature orthogonalization, stealth slip, pragmatic interpretation, statistical co-occurrence]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tsogt-Ochir Enkhbayar</p>
</li>
<li class="">
<p><strong>institution:</strong> Mongol-AI (inferred from email domain)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22293" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22293</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Empirically demonstrates that warning-framed training data fails to teach language models to avoid warned-against behaviors, showing generation rates similar to direct exposure. 2. Provides a mechanistic interpretation using sparse autoencoders, identifying a failure of feature orthogonalization where &quot;describing&quot; and &quot;performing&quot; an action activate overlapping latent features. 3. Identifies and names the &quot;stealth slip&quot; phenomenon, where conversational preambles can rotate activations into subspaces undetectable by linear probes, and shows that training-time feature ablation, not prompting, is required to address the issue.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fddb803a434c3d49e04dd722184b33f238c4b81254540d6bb0d1961a3d09e1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fddb803a434c3d49e04dd722184b33f238c4b81254540d6bb0d1961a3d09e1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates why language models trained on warning-framed examples (e.g., &quot;DO NOT USE&quot;) still learn to generate the warned-against content. Through behavioral experiments and sparse autoencoder analysis, it finds that models learn statistical co-occurrences rather than pragmatic intent, due to overlapping latent features for description and action. The core conclusion is that current architectures prioritize pattern completion over understanding speaker intent, requiring training-time interventions like feature ablation for correction.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multi-Head Spectral-Adaptive Graph Anomaly Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph anomaly detection], [spectral graph neural network, hypernetwork, Chebyshev filter, teacher-student contrastive learning, Barlow Twins loss]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qingyue Cao, Bo Jin, Changwei Gong, Xin Tong, Wenzheng Li, Xiaodong Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> People&#x27;s Public Security University of China, Third Research Institute of the Ministry of Public Security, Shanghai Police College</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22291" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22291</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Multi-Head Spectral-Adaptive Graph Neural Network (MHSA-GNN) that uses a lightweight hypernetwork to dynamically generate instance-specific Chebyshev filter parameters based on a &#x27;spectral fingerprint&#x27;. 2. Introduces a novel dual regularization strategy combining teacher-student contrastive learning (TSC) and Barlow Twins diversity loss (BTD) to prevent mode collapse and ensure representation accuracy and head orthogonality in the multi-head mechanism. 3. Demonstrates through extensive experiments that the method effectively preserves high-frequency anomaly signals and outperforms state-of-the-art methods, especially on highly heterogeneous datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ad6dee88128393dc0036e3430c2763e19882412f9750f09622c52d9ae28dc6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ad6dee88128393dc0036e3430c2763e19882412f9750f09622c52d9ae28dc6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of graph anomaly detection where fixed filters in spectral GNNs cause over-smoothing and fail to adapt to varying graph structures. It proposes MHSA-GNN, which uses a hypernetwork to generate adaptive filters per instance and a dual regularization strategy to stabilize multi-head learning. Experiments show the method preserves critical high-frequency signals and achieves superior performance, particularly on heterogeneous graphs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Hybrid Quantum-Classical Mixture of Experts: Unlocking Topological Advantage via Interference-Based Routing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [Quantum Machine Learning, Mixture-of-Experts, Parameterized Quantum Circuits, Quantum Router, Interference Hypothesis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Reda Heddad, Lamiae Bouanane</p>
</li>
<li class="">
<p><strong>institution:</strong> Al Akhawayn University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22296" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22296</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Design of a Hybrid Quantum-Classical Mixture of Experts (QMoE) architecture with a Quantum Router for an ablation study to isolate the source of quantum advantage. 2. Validation of the Interference Hypothesis, demonstrating the Quantum Router&#x27;s topological advantage and superior parameter efficiency on non-linearly separable data. 3. Empirical analysis of the architecture&#x27;s robustness against quantum noise, confirming its feasibility for near-term (NISQ) hardware.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407a17903b2360f889e7afbc3b5f776273cc33f5cf3daf0fa26caf69bc1ed179_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407a17903b2360f889e7afbc3b5f776273cc33f5cf3daf0fa26caf69bc1ed179_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a Hybrid Quantum-Classical Mixture of Experts (QMoE) architecture that uses a Quantum Router to address limitations like expert imbalance in classical MoE systems. The core finding is that the Quantum Router, leveraging quantum interference, provides a topological advantage for routing complex data more efficiently than classical routers. The method is shown to be robust to noise and feasible for near-term quantum hardware.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human activity recognition], [driver monitoring systems, edge AI, quantization, temporal decision head, confounder-aware labeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vesal Ahsani, Babak Hossein Khalaj</p>
</li>
<li class="">
<p><strong>institution:</strong> Sharif University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22298" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22298</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A deployable single-camera driver behavior recognition pipeline optimized for low-cost edge hardware (Raspberry Pi 5 and Google Coral Edge TPU). 2. A confounder-aware label design to reduce false positives from visually similar actions. 3. A temporal decision head that generates stable alerts based on sustained, confident predictions rather than noisy per-frame outputs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bd52e06dc77080ab346fc3d6fe46b900bf06b5c1eaeb6ae89801ac125ee7c51_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bd52e06dc77080ab346fc3d6fe46b900bf06b5c1eaeb6ae89801ac125ee7c51_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a real-time driver behavior recognition system designed for low-cost edge hardware to address the challenges of compute, power, and cost constraints in vehicles. The method combines a compact vision model, confounder-aware labeling, and a temporal decision head to recognize 17 distraction and drowsiness-related behaviors. The optimized system achieves 16 FPS on a Raspberry Pi 5 and 25 FPS on a Coral Edge TPU, enabling practical deployment for in-cabin monitoring.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Statistical and Machine Learning Analysis of Traffic Accidents on US 158 in Currituck County: A Comparison with HSM Predictions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [transportation safety analytics], [Random Forest, Kernel Density Estimation (KDE), Moran&#x27;s I, Highway Safety Manual (HSM), Negative Binomial Regression]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jennifer Sawyer, Julian Allagan</p>
</li>
<li class="">
<p><strong>institution:</strong> Elizabeth City State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22302" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22302</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Applied and compared advanced statistical and machine learning methods (Random Forest, KDE, Negative Binomial Regression) to rural highway crash data, demonstrating a methodological advancement beyond basic techniques. 2. Validated spatial clustering of accidents using Moran&#x27;s I test and identified specific crash hotspots via KDE, extending previous hotspot analysis. 3. Showed that a Random Forest classifier for injury severity prediction outperformed the standard Highway Safety Manual (HSM) Safety Performance Function (SPF) model.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87320e45eec3961a5ef58f2548cb06120fd2e4baee855440998207874d568dbe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87320e45eec3961a5ef58f2548cb06120fd2e4baee855440998207874d568dbe_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study analyzes traffic accident data from a rural highway using advanced statistical and machine learning techniques, including Random Forest and spatial analysis. The proposed Random Forest model for predicting injury severity achieved 67% accuracy, outperforming the standard HSM model, and spatial analysis confirmed crash clustering near intersections. The results provide actionable insights for targeted safety interventions on US 158.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PDx -- Adaptive Credit Risk Forecasting Model in Digital Lending using Machine Learning Operations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [MLOps, champion-challenger framework, out-of-time validation, probability of default, data drift]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sultan Amed, Chan Yu Hang, Sayantan Banerjee</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Management Indore, National University of Singapore</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22305" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22305</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes PDx, an adaptive, MLOps-driven decision system for end-to-end lifecycle management of credit risk models. 2. Introduces a dynamic champion-challenger framework with regular model updates and out-of-time validation to combat data drift. 3. Empirically demonstrates that decision tree-based ensemble models perform best for default classification but require frequent retraining, and validates PDx&#x27;s effectiveness across multiple lending datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c779eabc4c2fa3f75801b8b61c3eed3c105bd748ce6aa84ff37317acd929db3a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c779eabc4c2fa3f75801b8b61c3eed3c105bd748ce6aa84ff37317acd929db3a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes PDx, an adaptive credit risk forecasting system that uses an MLOps pipeline and a champion-challenger framework to continuously monitor, retrain, and validate models against data drift. The study finds decision tree ensembles are most effective but degrade without updates, and shows PDx mitigates value erosion in digital lending, especially for short-term loans.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LLMBoost: Make Large Language Models Stronger with Boosting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [ensemble learning, boosting, cross-model attention, chain training, near-parallel inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zehao Chen, Tianxiang Ai, Yifei Li, Gongxun Li, Yuyang Wei, Wang Zhou, Guanghui Li, Bin Yu, Zhijun Chen, Hailong Sun, Fuzhen Zhuang, Jianxin Li, Deqing Wang, Yikun Ban</p>
</li>
<li class="">
<p><strong>institution:</strong> Beihang University, China Telecom eSurfing Cloud</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22309" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22309</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A cross-model attention mechanism that allows successor models to access and fuse hidden states from predecessors for hierarchical error correction and knowledge transfer. 2. A chain training paradigm that progressively fine-tunes connected models with an error-suppression objective to rectify predecessor mispredictions efficiently. 3. A near-parallel inference paradigm that pipelines hidden states across models layer by layer, achieving inference efficiency close to single-model decoding.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb6a601c8d3bba7ec992d00772421c31e3e03d00d8a89a06f09ad3fe3c1b6ce1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb6a601c8d3bba7ec992d00772421c31e3e03d00d8a89a06f09ad3fe3c1b6ce1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes LLMBoost, a novel ensemble fine-tuning framework for LLMs that leverages intermediate hidden states across models. Inspired by boosting, it introduces cross-model attention, chain training, and a near-parallel inference pipeline to improve accuracy and reduce latency. Experiments on reasoning tasks show it consistently boosts performance while maintaining efficient inference.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [hardware security, model protection], [logic locking, intellectual property protection, hardware accelerator, model theft, supply chain security]</p>
</li>
<li class="">
<p><strong>authors:</strong> You Li, Guannan Zhao, Yuhao Ju, Yunqi He, Jie Gu, Hai Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> Northwestern University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22307" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22307</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LLA, a hardware-software co-design scheme for protecting generative AI models by embedding key bits into neurons and using invariance transformations to obscure them. 2. Integrates a lightweight, dataflow-compatible locking module into the AI accelerator, using the accelerator with a secret key as a license for model access. 3. Demonstrates that the approach is resilient against oracle-guided key optimization attacks while adding minimal computational overhead (&lt;0.1% for 7,168 key bits).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df323cc56f8d048243dce9e6af97041fca6264165872c422e5f81437cb03ef0d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df323cc56f8d048243dce9e6af97041fca6264165872c422e5f81437cb03ef0d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces LLA, a method to protect generative AI models from supply chain threats like theft and corruption by combining software-based key embedding in neurons with a hardware locking module in the accelerator. This approach uses the accelerator as a license key, ensuring only authorized hardware can run the model correctly. Evaluation shows it effectively resists attacks with negligible performance overhead.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Optimistic Feasible Search for Closed-Loop Fair Threshold Decision-Making</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [optimistic feasible search, closed-loop decision-making, demographic parity, bandit feedback, threshold policy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenzhang Du</p>
</li>
<li class="">
<p><strong>institution:</strong> Mahanakorn University of Technology, International College (MUTIC)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22313" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22313</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Optimistic Feasible Search (OFS), a simple grid-based method for constrained closed-loop threshold learning using optimism under uncertainty. 2. Introduced synthetic and semi-synthetic closed-loop benchmarks with stable contraction dynamics and real-world datasets (German Credit, COMPAS) to evaluate feedback effects. 3. Demonstrated that OFS achieves near-oracle performance with higher reward and lower cumulative constraint violation compared to unconstrained and primal-dual bandit baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/baad942bfd56e4e5f21dcd74fbfaad4a484372898d0862f9bc3c15ebb2b11958_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/baad942bfd56e4e5f21dcd74fbfaad4a484372898d0862f9bc3c15ebb2b11958_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses online learning of a threshold policy under fairness and service constraints in closed-loop decision systems with bandit feedback. It proposes Optimistic Feasible Search (OFS), which selects thresholds based on optimistic confidence bounds to maximize reward while minimizing constraint violations. Experiments show OFS outperforms baselines and achieves near-oracle performance across synthetic and real-world benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] LangPrecip: Language-Aware Multimodal Precipitation Nowcasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [weather forecasting], [multimodal nowcasting, rectified flow, semantic motion constraint, latent space integration, large-scale dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xudong Ling, Tianxi Huang, Qian Dong, Tao He, Chaorong Li, Guiduo Duan</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China (UESTC), Chengdu Textile College, Yibin University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22317" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22317</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed LangPrecip, a language-aware multimodal nowcasting framework that uses meteorological text as a semantic motion constraint to guide precipitation evolution. 2. Introduced LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. 3. Formulated nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm for efficient and physically consistent multimodal integration.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes LangPrecip, a novel framework that integrates meteorological text descriptions with radar data to constrain precipitation nowcasting. By formulating the problem as semantically constrained trajectory generation using Rectified Flow, it achieves significant performance gains, especially for heavy rainfall at long lead times, as demonstrated on Swedish and MRMS datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Decomposing Uncertainty in Probabilistic Knowledge Graph Embeddings: Why Entity Variance Is Not Enough</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [knowledge graph embeddings], [probabilistic embeddings, uncertainty quantification, out-of-distribution detection, semantic uncertainty, structural uncertainty]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chorok Lee</p>
</li>
<li class="">
<p><strong>institution:</strong> Korea Advanced Institute of Science and Technology (KAIST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22318" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22318</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and formalizes the fundamental limitation of relation-agnostic uncertainty in probabilistic KG embeddings, proving an impossibility result for detecting novel relational contexts using only entity-level statistics. 2. Proposes a novel decomposition of uncertainty into complementary semantic (entity variance) and structural (entity-relation co-occurrence) components, proving their non-redundancy and the superiority of their combination. 3. Introduces the CAGP method that combines semantic and structural uncertainty with learned weights, achieving significant improvements (60-80% relative gain) in temporal OOD detection and selective prediction performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5efb28f1949226a86993ed6d92d592762282d32746964ad0c85ab5a6de90ee36_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5efb28f1949226a86993ed6d92d592762282d32746964ad0c85ab5a6de90ee36_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that probabilistic knowledge graph embeddings use relation-agnostic entity variances, which conflates two distinct types of out-of-distribution data: emerging entities and novel relational contexts. To address this, the authors propose decomposing uncertainty into semantic and structural components and introduce the CAGP method to combine them. This approach achieves a 60-80% relative improvement in temporal OOD detection performance over existing baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [self-verifying agent, proactive evidence seeking, LLM-as-a-Judge, 3C Principles, agentic reinforcement learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University, Tencent</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22322" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22322</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://huggingface.co/collections/yolay/smartsnap" target="_blank" rel="noopener noreferrer" class="">https://huggingface.co/collections/yolay/smartsnap</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed SmartSnap, a paradigm shift from passive, post-hoc task verification to proactive, in-situ self-verification by the agent itself. 2. Introduced the Self-Verifying Agent, a new agent type with dual missions to complete tasks and prove accomplishment via curated snapshot evidences guided by 3C Principles (Completeness, Conciseness, Creativity). 3. Demonstrated that the SmartSnap paradigm enables scalable training of LLM-driven agents, achieving significant performance gains (up to 26.08% and 16.66%) on mobile tasks and competitive results against larger models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the scalability bottleneck in agentic RL caused by costly and unreliable post-hoc task verification. It proposes SmartSnap, a paradigm where agents proactively seek minimal, decisive snapshot evidence to prove task completion during execution, guided by 3C Principles. Experiments show this approach significantly improves agent performance and enables scalable training, achieving competitive results with much larger models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [TimeXer, Global M2 Liquidity, exogenous variable, long-horizon forecasting]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sravan Karthick T</p>
</li>
<li class="">
<p><strong>institution:</strong> RV College of Engineering (RVCE), Bengaluru, India</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22326" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22326</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the integration of Global M2 Liquidity as a leading exogenous variable with a 12-week lag for Bitcoin price forecasting. 2. Proposes a liquidity-conditioned forecasting model (TimeXer-Exog) based on the TimeXer architecture. 3. Demonstrates that explicit macroeconomic conditioning significantly stabilizes and improves long-horizon forecasts, outperforming a univariate baseline by over 89% at a 70-day horizon.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38de8480bbb274bd2bcfff3317dfee4cd7813e42e3af4cc61efcd6d928a0ae36_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38de8480bbb274bd2bcfff3317dfee4cd7813e42e3af4cc61efcd6d928a0ae36_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of long-horizon Bitcoin price forecasting by proposing a model that integrates Global M2 Liquidity as an exogenous variable into the TimeXer transformer architecture. The proposed TimeXer-Exog model significantly outperforms univariate benchmarks, showing that conditioning on global macroeconomic factors substantially improves forecast stability and accuracy over long horizons.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Emotion classification using EEG headset signals and Random Forest</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [affective computing], [EEG, Random Forest, emotion classification, brain-computer interface, real-time prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ricardo Vasquez, Diego Riofrío-Luzcando, Joe Carrion-Jumbo, Cesar Guevara</p>
</li>
<li class="">
<p><strong>institution:</strong> Universidad Internacional SEK, Universidad Indoamérica, The Institute of Mathematical Sciences (ICMAT-CSIC)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22333" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22333</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a model for classifying human emotions (happiness, sadness, relaxation) using EEG signals from a consumer-grade headset (EMOTIV EPOC). 2. Applied the Random Forest algorithm to achieve high accuracy, particularly for happiness (97.21%). 3. Implemented a real-time emotion prediction system that captures EEG signals, processes them, and visually displays the predicted emotion.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e19385b22eca0965c66f7006e387468776c757a86a9b784693bc7b77b3c7533_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e19385b22eca0965c66f7006e387468776c757a86a9b784693bc7b77b3c7533_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a system to classify human emotions (happiness, sadness, relaxation) from EEG signals using a Random Forest model. The model was trained on data from 50 participants and achieved high accuracy, especially for happiness. The work was extended to create a real-time prediction algorithm that outputs the result with representative images.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [LoRA, catastrophic forgetting, KL divergence, instruction-tuning, parameter-efficient fine-tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Matthew Riemer, Erik Miehling, Miao Liu, Djallel Bouneffouf, Murray Campbell</p>
</li>
<li class="">
<p><strong>institution:</strong> IBM Research, Mila, Université de Montréal</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22337" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22337</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that catastrophic forgetting is a severe problem even during parameter-efficient fine-tuning (LoRA) of LLMs on small datasets. 2. Proposes a simple, low-overhead regularized approximate replay method that penalizes KL divergence from the initial model and interleaves next-token prediction data. 3. Shows that this method effectively preserves the model&#x27;s general knowledge while maintaining plasticity for new tasks, applied to Qwen models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2e24fadff03a1d6696f3147893642a90d9f500d5c68233669842e5792db7332_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2e24fadff03a1d6696f3147893642a90d9f500d5c68233669842e5792db7332_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies that catastrophic forgetting is a major issue during LoRA-based supervised fine-tuning of large language models, even with small datasets. To solve this, the authors propose a regularized approximate replay method that uses KL divergence regularization and interleaves general pre-training-like data. Their approach successfully preserves the model&#x27;s original capabilities while allowing adaptation to new instructions, with minimal computational overhead.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [pseudo-colouring, few-shot learning, prototypical networks, ResNet-18, explainability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alaa Alahmadi, Mohamed Hasan</p>
</li>
<li class="">
<p><strong>institution:</strong> Newcastle University, University of Leeds</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22349" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22349</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a perception-informed pseudo-colouring technique to encode clinically salient temporal ECG features (like QT-interval) into structured colour representations. 2. Demonstrates that this technique enables effective few-shot and one-shot learning for a complex physiological data task (drug-induced LQTS) using prototypical networks and ResNet-18. 3. Shows that the method improves model explainability by guiding attention to clinically meaningful features and that aggregating multiple cardiac cycles (mirroring human perceptual averaging) further boosts performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problems of data inefficiency and poor interpretability in deep learning models for physiological signal analysis. It proposes a human-inspired pseudo-colouring technique to encode ECG features, enabling effective few-shot learning and improving model explainability by focusing on clinically relevant signal components. The results demonstrate that incorporating human-like perceptual encoding can bridge data efficiency and interpretability in medical AI.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PHANTOM: Physics-Aware Adversarial Attacks against Federated Learning-Coordinated EV Charging Management System</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [Cyber-Physical Systems Security], [False Data Injection (FDI), Physics-Informed Neural Network (PINN), Multi-Agent Reinforcement Learning (MARL)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mohammad Zakaria Haider, Amit Kumar Podder, Prabin Mali, Aranya Chakrabortty, Sumit Paudyal, Mohammad Ashiqur Rahman</p>
</li>
<li class="">
<p><strong>institution:</strong> Florida International University, North Carolina State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22381" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22381</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes PHANTOM, a physics-aware adversarial attack framework that integrates a federated learning-enabled PINN as a digital twin for accurate modeling of EV charging systems. 2. Develops a multi-agent RL environment using DQN and SAC to generate stealthy FDI attack strategies that bypass conventional detection. 3. Constructs a T&amp;D co-simulation platform to demonstrate the cascading, cross-boundary grid impacts (e.g., load imbalance, voltage instability) of the learned attacks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc4a49b77c0e517eadb20d321d77564888677d1f33b27adf452e13f7c0ffcb8c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc4a49b77c0e517eadb20d321d77564888677d1f33b27adf452e13f7c0ffcb8c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes PHANTOM, a physics-aware adversarial attack framework against federated learning-coordinated EV charging management. It uses a PINN-based digital twin and multi-agent RL to generate stealthy false data injection attacks, which are shown through co-simulation to cause significant grid instability, highlighting the need for physics-aware cybersecurity in vehicle-grid integration.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Self-Evaluation Unlocks Any-Step Text-to-Image Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [text-to-image generation, flow matching, self-evaluation, any-step inference, from-scratch training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xin Yu, Xiaojuan Qi, Zhengqi Li, Kai Zhang, Richard Zhang, Zhe Lin, Eli Shechtman, Tianyu Wang, Yotam Nitzan</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Hong Kong, Adobe Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22374" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22374</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Self-Evaluating Model (Self-E), a novel from-scratch training framework that combines local flow matching with a self-evaluation mechanism, eliminating the need for a pretrained teacher model. 2. Enables &quot;any-step&quot; inference, allowing the same model to perform both high-quality few-step and many-step generation, bridging the gap between local supervision and global matching paradigms. 3. Demonstrates competitive performance with state-of-the-art flow matching models at high step counts while excelling at very low step counts, offering a unified and scalable solution.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that traditional diffusion/flow models require many inference steps, and distillation methods need a pretrained teacher. It proposes Self-E, a model trained from scratch that uses self-evaluation as a dynamic teacher to enable high-quality generation at any number of steps. The results show Self-E excels at few-step generation and is competitive at many steps, providing a unified framework for efficient and scalable text-to-image synthesis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [hyperparameter transfer, Complete(d)P parameterisation, per-module hyperparameter optimisation, scaling laws, evolutionary strategy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bruno Mlodozeniec, Pierre Ablin, Louis Béthune, Dan Busbridge, Michal Klein, Jason Ramapuram, Marco Cuturi</p>
</li>
<li class="">
<p><strong>institution:</strong> Apple, University of Cambridge</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22382" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22382</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the Complete(d)P parameterisation, a unified framework for scaling hyperparameters across model width, depth, batch size, and training duration. 2. Investigates and enables the transfer of per-module hyperparameters (e.g., learning rates, weight decay) across model scales, moving beyond global hyperparameter transfer. 3. Provides practical guidelines for navigating the high-dimensional per-module hyperparameter optimization landscape and demonstrates significant training speed improvements in Large Language Models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06e0593eb453191fce60eeebdd4eb6147ab462084db9eb8d81ea8e2486d468be_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06e0593eb453191fce60eeebdd4eb6147ab462084db9eb8d81ea8e2486d468be_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of hyperparameter transfer across different model scales and configurations. It introduces the Complete(d)P parameterisation to unify scaling across width, depth, batch size, and duration, and demonstrates that with this method, even granular per-module hyperparameters can be optimized on a small model and successfully transferred to much larger models, leading to faster training and improved performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] BLISS: Bandit Layer Importance Sampling Strategy for Efficient Training of Graph Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [Graph Neural Networks, Multi-armed Bandits, Layer-wise Sampling, Node Importance, Efficient Training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Omar Alsaqa, Linh Thi Hoang, Muhammed Fatih Balin</p>
</li>
<li class="">
<p><strong>institution:</strong> Wilfrid Laurier University, Singapore Management University, Georgia Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22388" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22388</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces BLISS, a novel adaptive sampling strategy using multi-armed bandits to dynamically select informative nodes at each GNN layer. 2. Balances exploration and exploitation to ensure comprehensive graph coverage and adapts to evolving node importance, unlike static methods. 3. Demonstrates versatility by integrating with different GNN architectures (GCNs and GATs) and maintaining or exceeding full-batch training accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1be93ab34a4af58594bc48c8d52cc9f7177c298fc314982c8ac7ae27b2086b70_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1be93ab34a4af58594bc48c8d52cc9f7177c298fc314982c8ac7ae27b2086b70_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the computational bottleneck in training Graph Neural Networks (GNNs) on large graphs by proposing BLISS, a Bandit Layer Importance Sampling Strategy. BLISS uses multi-armed bandits to dynamically and adaptively sample the most informative nodes at each layer. Experiments show that this method maintains or even surpasses the accuracy of full-batch training while being more efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Differentiable Inverse Modeling with Physics-Constrained Latent Diffusion for Heterogeneous Subsurface Parameter Fields</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [latent diffusion model, differentiable physics, inverse problems, parameter estimation, flow in porous media]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zihan Lin, QiZhi He</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Minnesota</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22421" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22421</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LD-DIM, a novel framework that integrates a pretrained latent diffusion prior with a differentiable PDE solver for high-dimensional inverse problems. 2. Enables stable gradient-based optimization in a low-dimensional latent space, improving numerical conditioning and preserving sharp discontinuities. 3. Demonstrates superior numerical stability and reconstruction accuracy compared to PINNs and physics-embedded VAE baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb75073de82d3659f4e40522fca5a1fe8743332c5df8e21ca285525b2a200bca_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb75073de82d3659f4e40522fca5a1fe8743332c5df8e21ca285525b2a200bca_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces LD-DIM, a method that combines a latent diffusion model with a differentiable numerical solver to reconstruct heterogeneous parameter fields from sparse observations in PDE-constrained inverse problems. It shows improved stability and accuracy over existing baselines while maintaining sharp material interfaces.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [hyperspherical learning, native sparse attention, anisotropic patch embed]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amil Khan, Matheus Palhares Viana, Suraj Mishra, B.S. Manjunath</p>
</li>
<li class="">
<p><strong>institution:</strong> UC Santa Barbara, Allen Institute for Cell Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22423" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22423</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A 4B-parameter foundation model (Bright-4B) that learns on the unit hypersphere for segmenting subcellular structures directly from 3D brightfield volumes. 2. Novel architectural components: a hardware-aligned Native Sparse Attention mechanism, depth-width residual HyperConnections, and a soft Mixture-of-Experts for adaptive capacity. 3. A plug-and-play anisotropic patch embed that respects confocal point-spread and axial thinning for geometry-faithful 3D tokenization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces Bright-4B, a 4-billion parameter foundation model designed for volumetric segmentation of subcellular structures directly from label-free 3D brightfield microscopy images. It employs novel architectural components like hyperspherical learning, native sparse attention, and an anisotropic patch embed to handle 3D context and anisotropic sampling. The model outperforms contemporary baselines in preserving fine structural detail across depth and cell types without requiring fluorescence or heavy post-processing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Causality-Inspired Safe Residual Correction for Multivariate Time Series</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [residual correction, causality-inspired encoder, non-degradation guarantee, safety mechanism, multivariate time series]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jianxiang Xie, Yuncheng Hua</p>
</li>
<li class="">
<p><strong>institution:</strong> University of New South Wales, University of Science and Technology of China, The Hong Kong University of Science and Technology (Guangzhou)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22428" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22428</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CRC, a plug-and-play residual correction framework explicitly designed to guarantee non-degradation of model performance. 2. Introduces a causality-inspired encoder that decouples self- and cross-variable dynamics to expose direction-aware structure for safer correction. 3. Designs a strict four-fold safety mechanism to govern the correction process and prevent harmful updates, ensuring high non-degradation rates.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7659f53cdce63801de4caead558593caeed0c4662c0c76c85dae564f12a6d78_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7659f53cdce63801de4caead558593caeed0c4662c0c76c85dae564f12a6d78_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that existing post-hoc residual correction methods for multivariate time series forecasting are greedy and can degrade performance. To solve this, it proposes CRC, a causality-inspired safe residual correction framework with a four-fold safety mechanism. Experiments show CRC consistently improves accuracy while ensuring exceptionally high non-degradation rates.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [analog circuit design, multi-agent framework, stratified memory, simulation-grounded feedback, self-evolving]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zining Wang, Jian Gao, Weimin Fu, Xiaolong Guo, Xuan Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Northeastern University, Kansas State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22435" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22435</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes AnalogSAGE, an open-source self-evolving multi-agent framework for analog circuit design that coordinates three-stage agent explorations through four stratified memory layers, enabling iterative refinement with simulation-grounded feedback. 2. Introduces a stratified context mechanism to selectively preserve stage-relevant information, enhancing long-horizon reasoning and reliability under stringent specifications. 3. Demonstrates significant improvements in pass rates and search space reduction through a benchmark of ten operational amplifier design problems using the open-source SKY130 PDK and ngspice.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf0a7cad048acb4f93f29281281d9e76543f81e5aa1dd6e183e005cda30a7c56_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf0a7cad048acb4f93f29281281d9e76543f81e5aa1dd6e183e005cda30a7c56_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of automating analog circuit design, which traditionally relies heavily on human intuition, by introducing AnalogSAGE, a self-evolving multi-agent framework with stratified memory and simulation-grounded feedback. This approach enables iterative refinement across topology selection, refinement, and parameter optimization stages. Evaluations show it achieves a 10x overall pass rate and 4x reduction in parameter search space compared to existing methods, enhancing reliability and autonomy in analog design automation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [hierarchical filtering, two-pass generation, citation verification, query formulation, model cascade]</p>
</li>
<li class="">
<p><strong>authors:</strong> Cattalyya Nuengsigkapian</p>
</li>
<li class="">
<p><strong>institution:</strong> Google</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22442" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22442</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hierarchical content filtering pipeline to replace standard vector similarity search, improving context precision. 2. Introduces a model cascade strategy using a cost-efficient model (Gemini 2.5 Flash) for filtering and a powerful model (Gemini 2.5 Pro) for final generation. 3. Demonstrates significant performance gains on the MMU-RAGent benchmark and a custom dataset for post-cutoff knowledge.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9840d615edee0e72c18b93838479ebb342195ea1805803858fb9effaf9ba2e95_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9840d615edee0e72c18b93838479ebb342195ea1805803858fb9effaf9ba2e95_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents HiFi-RAG, a system designed to improve open-domain RAG by addressing irrelevant retrieved information. The method uses a multi-stage pipeline with hierarchical filtering and a two-pass generation strategy employing different LLMs for efficiency and quality. The system won a NeurIPS 2025 competition and showed substantial improvements over baselines in evaluation metrics.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [LoRA, Parameter-Efficient Fine-Tuning, Activation Function Annealing, Non-linear Adaptation, Model Merging]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiacheng Li, Jianchao Tan, Zhidong Yang, Feiye Huo, Yerui Sun, Yuchen Xie, Xunliang Cai</p>
</li>
<li class="">
<p><strong>institution:</strong> Meituan, Hong Kong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22455" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22455</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes AFA-LoRA, a novel training strategy that introduces non-linear expressivity into LoRA while preserving its seamless mergeability., 2. Introduces an annealed activation function that transitions from non-linear to linear during training, enabling strong initial learning and final linear integration., 3. Demonstrates the method&#x27;s effectiveness across multiple tasks, including supervised fine-tuning, reinforcement learning, and speculative decoding, reducing the performance gap with full-parameter training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a5573f75296283a39c5bdbbb0c94652e0aeb935ae378c12528544ca5e188deb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a5573f75296283a39c5bdbbb0c94652e0aeb935ae378c12528544ca5e188deb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the limited expressive power of linear Low-Rank Adaptation (LoRA) by proposing AFA-LoRA, a method that uses an annealed activation function to enable non-linear training while ensuring the final adapter remains mergeable. This approach narrows the performance gap between LoRA and full-parameter fine-tuning across various tasks, offering a more powerful and practical parameter-efficient adaptation paradigm.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AMBIT: Augmenting Mobility Baselines with Interpretable Trees</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [urban computing, spatial data science], [origin-destination flow prediction, spatial interaction models, gradient-boosted trees, SHAP analysis, gray-box model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qizhi Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> PingCAP, Data &amp; AI-Innovation Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22466" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22466</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducts a comprehensive audit of classical spatial interaction models on high-resolution mobility data, identifying PPML gravity as the strongest physical baseline. 2. Proposes AMBIT, a gray-box framework that augments interpretable physical baselines with gradient-boosted trees to learn residuals, balancing accuracy and interpretability. 3. Demonstrates that physics-grounded and POI-anchored residual learners achieve competitive accuracy and robust spatial generalization, providing a reproducible pipeline with diagnostics for urban decision-making.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d0d9ab13b8b2f60e318c90f38821b29e87e0bdd9bf0d9bc963b48c5ac7c2759_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d0d9ab13b8b2f60e318c90f38821b29e87e0bdd9bf0d9bc963b48c5ac7c2759_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes AMBIT, a gray-box framework that combines interpretable physical mobility baselines with gradient-boosted trees to predict origin-destination flows. It shows that learning residuals on top of physics-based models can achieve accuracy close to strong black-box predictors while maintaining interpretable structure, with POI-anchored residuals being particularly robust for spatial generalization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GLUE: Gradient-free Learning to Unify Experts</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [expert mixing, model initialization, gradient-free optimization, SPSA, transfer learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jong-Ik Park, Shreyas Chaudhari, Srinivasa Pranav, Carlee Joe-Wong, José M. F. Moura</p>
</li>
<li class="">
<p><strong>institution:</strong> Carnegie Mellon University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22467" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22467</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GLUE, a gradient-free method to learn mixture coefficients for blending expert models into a single initialization prior for a target domain. 2. Introduces a two-point (SPSA) update rule that requires only two forward passes per step, avoiding expensive backpropagation through the full network. 3. Demonstrates that GLUE outperforms heuristic blending baselines and matches or outperforms gradient-based learning of mixture coefficients across multiple datasets and architectures.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58e1db10a62e7b3683bf994e8914b7bcb88a1d8d308b99746a86c5ebb6d9b5c6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58e1db10a62e7b3683bf994e8914b7bcb88a1d8d308b99746a86c5ebb6d9b5c6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of initializing a model for a new target domain by blending multiple pretrained expert models. It proposes GLUE, a gradient-free method that learns the blending coefficients efficiently using only forward passes. Experiments show GLUE creates a better initialization prior, leading to higher fine-tuned accuracy than heuristic methods and comparable or better performance than gradient-based approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Bayesian Geometry of Transformer Attention</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [interpretability], [Bayesian inference, transformer attention, mechanistic interpretability, Bayesian wind tunnels, geometric analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra</p>
</li>
<li class="">
<p><strong>institution:</strong> Columbia University, Dream Sports, Google DeepMind</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22471" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22471</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces &quot;Bayesian wind tunnels&quot; as controlled environments to rigorously test if transformers perform Bayesian inference, where true posteriors are known and memorization is impossible. 2. Demonstrates that small transformers implement Bayesian inference via a consistent geometric mechanism (residual streams as belief substrate, FFNs for updates, attention for routing), while MLPs fail. 3. Identifies specific geometric diagnostics (orthogonal key bases, query-key alignment, low-dimensional value manifold) and a frame-precision dissociation during training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5333e070f21cfd2abea4d13cddb84bf6d9f3c3124c93bf9142879f24f1e739b1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5333e070f21cfd2abea4d13cddb84bf6d9f3c3124c93bf9142879f24f1e739b1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether transformers perform genuine Bayesian inference. To test this, the authors create controlled &quot;Bayesian wind tunnel&quot; tasks with known posteriors and show that small transformers accurately reproduce Bayesian posteriors via a specific geometric mechanism, while MLPs fail, establishing attention as crucial for this capability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Collaborative Optimization of Multiclass Imbalanced Learning: Density-Aware and Region-Guided Boosting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [imbalanced learning], [collaborative optimization, density-aware, region-guided boosting, sample weight update, dynamic sampling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chuantao Li, Zhi Li, Jiahao Xu, Jie Li, Sheng Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Guangdong Ocean University, University of Electronic Science and Technology of China</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22478" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22478</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/ChuantaoLi/DARG" target="_blank" rel="noopener noreferrer" class="">https://github.com/ChuantaoLi/DARG</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a collaborative optimization Boosting model for multiclass imbalanced learning that integrates imbalanced learning and model training. 2. Designs a noise-resistant weight update mechanism and a dynamic sampling strategy using density and confidence factors. 3. Achieves tight integration of modules for weight updates, sample region partitioning, and region-guided sampling to enhance performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d829fa49f320d741a98494ca09a1f6019a6cccef04a93af08effdf4810adc20_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d829fa49f320d741a98494ca09a1f6019a6cccef04a93af08effdf4810adc20_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of classification bias in multiclass imbalanced data by proposing a collaborative optimization Boosting model. The method integrates density-aware and region-guided techniques to update sample weights and perform dynamic sampling, achieving improved performance over existing baselines on 20 public datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding for Fine-Grained sEMG-Based Movement Decoding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [biomedical signal processing], [self-supervised learning, surface electromyography, rotary position encoding, spectral pre-training, movement decoding]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zihan Weng, Chanlin Yi, Pouya Bashivan, Jing Lu, Fali Li, Dezhong Yao, Jingming Hou, Yangsong Zhang, Peng Xu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22481" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22481</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel self-supervised pre-training task that uses masked prediction of clustered STFT pseudo-labels to learn robust, physiologically relevant frequency patterns from sEMG signals. 2. A novel Cylindrical Rotary Position Embedding (CyRoPE) that factorizes embeddings along temporal and annular spatial dimensions to explicitly model the cylindrical topology of forearm electrode arrays. 3. The SPECTRE framework, which integrates these contributions to establish a new state-of-the-art for fine-grained movement decoding, validated on multiple datasets including data from individuals with amputation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5801cbdcbac93bf7df15e18531c5ff2653259e25003568da5b53b240d06d32c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5801cbdcbac93bf7df15e18531c5ff2653259e25003568da5b53b240d06d32c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces SPECTRE, a domain-specific self-supervised learning framework for decoding fine-grained movements from surface electromyography (sEMG) signals. It proposes a spectral pre-training task using masked pseudo-label prediction and a novel cylindrical rotary position encoding to model sensor topology. Evaluations show SPECTRE significantly outperforms existing supervised and generic self-supervised baselines, providing a robust foundation for practical myoelectric interfaces.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection via Latent Space Representation Learning and Alignment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [network intrusion detection], [concept drift, latent space alignment, graph neural network (GNN), IoT botnet detection, variational autoencoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hassan Wasswa, Timothy Lynar</p>
</li>
<li class="">
<p><strong>institution:</strong> University of New South Wales</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22488" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22488</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a scalable framework for adaptive IoT threat detection that eliminates the need for continuous classifier retraining, reducing computational overhead and catastrophic forgetting. 2. Introduces a method using latent space representation learning and an alignment model to map new traffic to a historical latent space, preserving knowledge of past attacks. 3. Transforms latent representations into a graph structure and uses a Graph Attention Network (GAT) to capture inter-instance relationships among attack samples for improved detection.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4c1a80d42c79bd3adbbc2c072359068d65253efabadc2bd6d3617f632439f72_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4c1a80d42c79bd3adbbc2c072359068d65253efabadc2bd6d3617f632439f72_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of concept drift in IoT botnet detection by proposing a framework that trains a classifier once on historical traffic representations. It uses an alignment model to map new traffic to this learned latent space and a graph neural network to classify the data, maintaining robust performance without retraining. Experimental results on real-world datasets show the framework&#x27;s effectiveness in dynamic IoT environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Quest for Winning Tickets in Low-Rank Adapters</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [parameter-efficient fine-tuning], [Lottery Ticket Hypothesis, Low-Rank Adaptation, Parameter-Efficient Fine-Tuning, Sparse Subnetworks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hamed Damirchi, Cristian Rodriguez-Opazo, Ehsan Abbasnejad, Zhen Zhang, Javen Shi</p>
</li>
<li class="">
<p><strong>institution:</strong> Australian Institute for Machine Learning (Adelaide University), Monash University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22495" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22495</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Empirically validates that the Lottery Ticket Hypothesis (LTH) holds within Low-Rank Adaptation (LoRA) methods, revealing the existence of sparse, high-performing subnetworks (&quot;winning tickets&quot;) in adapters. 2. Discovers that the effectiveness of these sparse subnetworks depends more on the sparsity distribution across layers than on the specific weights selected. 3. Proposes Partial-LoRA, a novel method to systematically identify and train these sparse low-rank adapters, achieving significant parameter reduction (up to 87%) while maintaining or improving performance across vision and language tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19921047ca06f45b5a51cefcf5b1cd9502b9ad5650b66c746d8b15f622036ef0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19921047ca06f45b5a51cefcf5b1cd9502b9ad5650b66c746d8b15f622036ef0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether the Lottery Ticket Hypothesis extends to parameter-efficient fine-tuning, specifically Low-Rank Adaptation (LoRA). The authors propose Partial-LoRA, a method to identify and train sparse, task-aligned subnetworks within LoRA adapters. Experiments show Partial-LoRA can reduce trainable parameters by up to 87% while matching or surpassing the performance of dense adapters.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Role-Based Fault Tolerance System for LLM RL Post-Training</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [fault-tolerance], [role-based fault tolerance, RL post-training, UCX communication, warm standby, Effective Training Time Ratio (ETTR)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhenqian Chen, Baoquan Zhong, Xiang Li, Qing Dai, Xinkui Zhao, Miao Ye, Ren Cheng, Lufei Zhang, Jianwei Yin</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, State Key Laboratory of Mathematical Engineering and Advanced Computing</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22492" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22492</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a role-based fault isolation and recovery system (RobustRL) for RL post-training, enabling recovery of only the failed component (trainer, rollout) instead of restarting the entire task. 2. Introduces a role-aware monitoring mechanism to accurately detect failures and avoid false positives/delays specific to different RL roles. 3. Implements dynamic, UCX-based point-to-point communication to reconnect recovered roles and synchronize weights immediately, replacing static collective communication.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceff77cfb9d0c7d7e763916b77716ee6534c3aa3d54d41253fef6ea4d9a86ec0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceff77cfb9d0c7d7e763916b77716ee6534c3aa3d54d41253fef6ea4d9a86ec0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the lack of fault tolerance for RL post-training of LLMs, which interleaves training and inference workloads. It proposes RobustRL, a system that isolates and recovers failed roles (e.g., trainer, rollout) individually using a Detect-Restart-Reconnect paradigm, instead of restarting the entire job. This approach significantly improves the Effective Training Time Ratio and reduces end-to-end training time compared to baseline methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Decomposing Task Vectors for Refined Model Editing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [model editing], [task vector, parameter decomposition, invariant subspace, concept interference, LoRA]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hamed Damirchi, Ehsan Abbasnejad, Zhen Zhang, Javen Shi</p>
</li>
<li class="">
<p><strong>institution:</strong> Australian Institute for Machine Learning (University of Adelaide), Monash University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22511" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22511</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A principled decomposition method to separate a task vector into shared and unique components., 2. Application of the decomposition to improve multi-task merging in image classification, enable clean style mixing in diffusion models, and reduce toxicity in language models., 3. A new framework for understanding and controlling task vector arithmetic to address interference during concept composition.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e938f563a374559600295d0e58643d1a4f7e55e5b56e3e6aca0b3daec488d0b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e938f563a374559600295d0e58643d1a4f7e55e5b56e3e6aca0b3daec488d0b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of unpredictable outcomes when performing arithmetic operations on task vectors due to overlapping concepts. It proposes a method to decompose each task vector into shared and unique components using invariant subspaces. This enables more precise model editing, demonstrated by improved multi-task merging, clean style mixing, and significant toxicity reduction while preserving general knowledge.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [hallucination detection], [correctness prediction, metadata signals, prompting strategies, log probability, response consistency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lucky Susanto, Anasta Pranawijayana, Cortino Sukotjo, Soni Prasad, Derry Wijaya</p>
</li>
<li class="">
<p><strong>institution:</strong> Monash University Indonesia, University of Pittsburgh, University of North Carolina Adams School of Dentistry, Boston University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22508" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22508</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel method to predict LLM correctness (not just hallucination) using metadata and hallucination signals in a high-stakes medical domain (prosthodontics). 2. Demonstrates that metadata-based predictors can improve accuracy over a naive baseline and achieve high precision, but are not reliable for directly predicting hallucination. 3. Shows that prompting strategies significantly alter model internal behavior and the predictive utility of metadata, despite not changing overall task accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbe159260b97cf5e7a59edbee0a23b697a76a89a0fdbf6e0c9797739cc322b42_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbe159260b97cf5e7a59edbee0a23b697a76a89a0fdbf6e0c9797739cc322b42_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study investigates predicting the correctness of LLM answers on a prosthodontics exam using metadata (like log probability and consistency) and hallucination signals. The method, applied to GPT-4o and OSS-120B across different prompts, shows improved accuracy over a baseline but is not yet robust for high-stakes deployment. The research highlights that prompting strategies change model behavior and metadata utility, offering a direction for reliability signals.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [adversarial robustness], [Spiking Neural Networks, surrogate gradient, adversarial attack, gradient vanishing, adaptive optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jihang Wang, Dongcheng Zhao, Ruolin Chen, Qian Zhang, Yi Zeng</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Beijing Institute of AI Safety and Governance</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22522" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22522</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Theoretical analysis of gradient vanishing in surrogate gradient methods for SNNs. 2. Proposal of Adaptive Sharpness Surrogate Gradient (ASSG) to adaptively adjust the surrogate function for more accurate gradients. 3. Design of Stable Adaptive Projected Gradient Descent (SA-PGD), an adversarial attack with adaptive step size for stable convergence under imprecise gradients.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7b10ed2a96f6e46c5c2afb21a8e1184dd9c5e1f342efdb93f3cd317a08c20ea_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7b10ed2a96f6e46c5c2afb21a8e1184dd9c5e1f342efdb93f3cd317a08c20ea_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the unreliable evaluation of adversarial robustness in Spiking Neural Networks (SNNs) caused by gradient vanishing from surrogate gradients. The authors propose a framework combining an adaptive surrogate gradient method (ASSG) and an adaptive-step attack (SA-PGD) to generate stronger attacks. Experiments show this framework significantly increases attack success rates, revealing that current SNN robustness is overestimated and highlighting the need for more reliable adversarial training.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time-series forecasting], [encoder-decoder, latent bottleneck representations, learnable queries, generalized forecasting]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jaebin Lee, Hankook Lee</p>
</li>
<li class="">
<p><strong>institution:</strong> Sungkyunkwan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22550" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22550</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/efficient-learning-lab/TimePerceiver" target="_blank" rel="noopener noreferrer" class="">https://github.com/efficient-learning-lab/TimePerceiver</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Generalizes the time-series forecasting task to include diverse objectives like extrapolation, interpolation, and imputation. 2. Proposes a novel encoder-decoder architecture with latent bottleneck representations to capture temporal and cross-channel dependencies. 3. Introduces learnable queries for decoding to effectively retrieve information for arbitrarily positioned target timestamps.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/091fdcd1058e0acfad18820d025c1fe50ff4315cbd5ec8a06f5d86eb9767513c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/091fdcd1058e0acfad18820d025c1fe50ff4315cbd5ec8a06f5d86eb9767513c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes TimePerceiver, a unified encoder-decoder framework for generalized time-series forecasting. It handles diverse prediction objectives by using latent bottleneck encodings and learnable query-based decoding. Extensive experiments show it consistently outperforms prior state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Computing Pure-Strategy Nash Equilibria in a Two-Party Policy Competition: Existence and Algorithmic Approaches</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [game theory], [pure-strategy Nash equilibrium, continuous games, policy competition, gradient-based algorithm, grid-based search]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chuang-Chieh Lin, Chi-Jen Lu, Po-An Chen, Chih-Chieh Hung</p>
</li>
<li class="">
<p><strong>institution:</strong> National Taiwan Ocean University, Academia Sinica, National Yang Ming Chiao Tung University, National Chung Hsing University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22552" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22552</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulated a two-party policy competition game and validated the isotonicity hypothesis of winning probability through simulations. 2. Proved the existence of a pure-strategy Nash equilibrium (PSNE) in both one- and multi-dimensional policy spaces. 3. Proposed and experimentally validated a decentralized gradient-based algorithm and a polynomial-time grid-based search algorithm for finding approximate PSNEs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ef0be95e6f7da168b174f0a6d600f4ee8f0f3d2bcaba49c996d43be6d1be4b7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ef0be95e6f7da168b174f0a6d600f4ee8f0f3d2bcaba49c996d43be6d1be4b7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper models two-party policy competition as a continuous non-cooperative game where parties choose policy vectors and a party&#x27;s payoff is the expected utility for its supporters. The authors prove the existence of a pure-strategy Nash equilibrium and propose two algorithms—a gradient-based method and a grid-based search—that efficiently find approximate equilibria.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [disaggregated infrastructure, hardware-affinity mapping, fine-grained asynchrony]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wei Gao, Yuheng Zhao, Tianyuan Wu, Shaopan Xiong, Weixun Wang, Dakai An, Lunxi Cao, Dilxat Muhtar, Zichen Liu, Haizhou Zhao, Ju Huang, Siran Yang, Yongbin Li, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> HKUST, Alibaba Group</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22560" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22560</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/alibaba/ROLL" target="_blank" rel="noopener noreferrer" class="">https://github.com/alibaba/ROLL</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A hardware-affinity workload mapping strategy that routes compute-bound and bandwidth-bound tasks to best-fit GPU devices. 2. A fine-grained asynchrony mechanism that manages execution at the trajectory level to mitigate resource bubbles and improve utilization. 3. A statefulness-aware computation design that offloads stateless components to serverless infrastructure for elastic scaling.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc8e408c613097b7b60bc32e41ec3137faa8dd94184658c8a802b65cbf57c593_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc8e408c613097b7b60bc32e41ec3137faa8dd94184658c8a802b65cbf57c593_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents RollArt, a distributed system designed to scale agentic reinforcement learning training on disaggregated infrastructure. It addresses the heterogeneity of agentic RL workloads by proposing three core techniques: hardware-affinity workload mapping, fine-grained asynchrony, and statefulness-aware computation. The system demonstrates significant improvements in training throughput, achieving 1.35-2.05x speedup over baselines, and scales to thousands of GPUs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On Admissible Rank-based Input Normalization Operators</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [machine learning theory], [rank-based normalization, differentiable sorting, monotone invariance, batch independence, Lipschitz continuity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Taeyun Kim</p>
</li>
<li class="">
<p><strong>institution:</strong> Seoul National University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22587" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22587</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identified and formalized three axioms (invariance and stability properties) that a valid rank-based input normalization operator must satisfy. 2. Proved a structural characterization theorem showing any admissible operator must factor into a feature-wise rank representation and a monotone-Lipschitz scalarization map. 3. Constructed a minimal operator meeting the proposed axioms, empirically demonstrating the non-triviality of the constraints and delineating the design space from existing differentiable sorting methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49ce297e3baab406a1befffbcf116551c1492e7a6451fb1af02514d5f1ddb86b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49ce297e3baab406a1befffbcf116551c1492e7a6451fb1af02514d5f1ddb86b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that widely used differentiable sorting/ranking operators are structurally unstable under monotone transformations and batch variations. To address this, it proposes formal axioms for stable rank-based normalization and proves that any admissible operator must have a specific factored structure. The authors construct a minimal operator satisfying these axioms, formally separating valid normalization from existing differentiable sorting approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Data-Driven Analysis of Crash Patterns in SAE Level 2 and Level 4 Automated Vehicles Using K-means Clustering and Association Rule Mining</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [data mining], [K-means clustering, Association Rule Mining, crash pattern analysis, automated vehicles, NHTSA data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jewel Rana Palit, Vijayalakshmi K Kumarasamy, Osama A. Osman</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Tennessee at Chattanooga, Collier County Government</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22589" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22589</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a two-stage data mining framework combining K-means clustering and Association Rule Mining to analyze AV crash data. 2. Applied the framework to a large-scale dataset of over 2,500 AV crash records from NHTSA, covering SAE Levels 2 and 4. 3. Uncovered interpretable multivariate relationships between crash patterns and environmental/operational factors, providing actionable insights for AV safety.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c3b11717362343c168a38a05a8cdd65b40a65cd3df1b048cfb816c2493f25b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c3b11717362343c168a38a05a8cdd65b40a65cd3df1b048cfb816c2493f25b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study analyzes crash patterns in SAE Level 2 and 4 Automated Vehicles using a large-scale NHTSA dataset. It proposes a two-stage data mining framework: first using K-means clustering to segment crashes into behavioral clusters, then applying Association Rule Mining to find relationships between crash factors within each cluster. The results provide actionable guidance for improving AV safety and deployment strategies.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Cryptocurrency Price Prediction Using Parallel Gated Recurrent Units</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [Gated Recurrent Units, parallel neural networks, cryptocurrency price prediction, mean absolute percentage error, recurrent neural networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Milad Asadpour, Alireza Rezaee, Farshid Hajati</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Tehran, University of New England</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22599" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22599</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Parallel Gated Recurrent Units (PGRU) model for cryptocurrency price forecasting. 2. Employs parallel and independent recurrent neural networks with distinct price-related feature inputs. 3. Demonstrates higher accuracy and efficiency with lower computational cost and less input data compared to existing methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1b0d946f543a3b63440ffb89cc57536eb655abef18c7f847881484fd9e06122_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1b0d946f543a3b63440ffb89cc57536eb655abef18c7f847881484fd9e06122_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a new deep learning model called Parallel Gated Recurrent Units (PGRU) for predicting cryptocurrency prices. The model uses parallel recurrent neural networks that process different price features independently, and their outputs are combined by another neural network for the final forecast. Experimental results show the model achieves low prediction errors (e.g., 2.641% MAPE) with higher efficiency and lower computational cost than previous methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Energy-Guided Flow Matching Enables Few-Step Conformer Generation and Ground-State Identification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models], [flow matching, conformer generation, energy guidance, ground-state identification, molecular geometry]</p>
</li>
<li class="">
<p><strong>authors:</strong> Guikun Xu, Xiaohan Yi, Peilin Zhao, Yatao Bian</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, National University of Singapore, Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22597" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22597</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Rich-XGK/EnFlow.git" target="_blank" rel="noopener noreferrer" class="">https://github.com/Rich-XGK/EnFlow.git</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes EnFlow, a unified framework that couples flow matching with an explicitly learned energy model for conformer generation. 2. Introduces an energy-guided sampling scheme along a non-Gaussian flow matching path to steer trajectories toward lower-energy regions, improving fidelity in few-step regimes. 3. Enables accurate ground-state identification by using the learned energy function to rank generated ensembles, reducing prediction errors.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8be1fad6f55102be97d0cd80a5b8da136a8f70c287b7f11b3a11156b5bc80a04_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8be1fad6f55102be97d0cd80a5b8da136a8f70c287b7f11b3a11156b5bc80a04_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents EnFlow, a method that integrates flow matching with an energy model to guide the generation of molecular conformers. By using energy-gradient guidance during sampling, it improves conformational accuracy with few steps and enables better identification of the ground-state structure. Experiments show it outperforms state-of-the-art methods on standard benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Gold Price Prediction Using Long Short-Term Memory and Multi-Layer Perceptron with Gray Wolf Optimizer</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [LSTM, Multilayer Perceptron (MLP), Gray Wolf Optimizer (GWO), gold price prediction, trading strategy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hesam Taghipour, Alireza Rezaee, Farshid Hajati</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Tehran, University of New England</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22606" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22606</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a hybrid LSTM-MLP model for multi-timeframe (daily and monthly) gold price forecasting. 2. Utilized the Gray Wolf Optimizer (GWO) to optimize the number of neurons in the neural networks for improved accuracy. 3. Developed and backtested a trading strategy based on the model&#x27;s predictions, reporting a high simulated return of 171% over three months.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46725f71da8c268509e63f86ad823980c405c360b229a1a903d358d4d2dd61d5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46725f71da8c268509e63f86ad823980c405c360b229a1a903d358d4d2dd61d5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents an AI-based model for predicting gold prices. It uses two LSTM networks for daily and monthly forecasts, integrates their outputs with an MLP, and optimizes the network structure using the Gray Wolf Optimizer. The model achieved low prediction errors and a high simulated trading return, demonstrating its potential for financial forecasting.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Communication Compression for Distributed Learning with Aggregate and Server-Guided Feedback</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [communication compression, error feedback, biased compression, control variates, distributed gradient descent]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tomas Ortega, Chun-Yin Huang, Xiaoxiao Li, Hamid Jafarkhani</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Irvine; University of British Columbia; Vector Institute</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22623" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22623</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Compressed Aggregate Feedback (CAFe), a novel framework for biased compression in distributed learning that uses the previous round&#x27;s aggregated update as a shared control variate, eliminating the need for client-side state. 2. Proposed Server-Guided Compressed Aggregate Feedback (CAFe-S), an extension that leverages a small server-side dataset to generate a more accurate predictor update, improving convergence when server data is representative. 3. Provided theoretical convergence guarantees for both CAFe and CAFe-S in non-convex settings, proving CAFe&#x27;s superiority over standard distributed compressed gradient descent and showing CAFe-S&#x27;s improved rate with more representative server data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/988420401d89faee52e66a7e209751875e33b7f833b8dc7f136f1f1d2650454d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/988420401d89faee52e66a7e209751875e33b7f833b8dc7f136f1f1d2650454d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the communication bottleneck and privacy issues in Federated Learning by proposing two novel compression frameworks, CAFe and CAFe-S, which enable biased compression without requiring client-side state. CAFe uses the previous aggregated update as a shared control variate, while CAFe-S leverages a small server dataset for a better predictor. Theoretical and experimental results demonstrate their superiority over existing compression schemes.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Tree Meets Transformer: A Hybrid Architecture for Scalable Power Allocation in Cell-Free Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [binary tree compression, Transformer, scalable inference, power allocation, cell-free]</p>
</li>
<li class="">
<p><strong>authors:</strong> Irched Chafaa, Giacomo Bacci, Luca Sanguinetti</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Pisa</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22639" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22639</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel hybrid Tree-Transformer architecture for scalable per-user power allocation in wireless networks. 2. Introduces a method that compresses user features via a binary tree to a global root representation, applies a Transformer encoder only to the root, and decodes powers with a shared decoder, achieving logarithmic depth and linear total complexity. 3. Demonstrates that the model achieves near-optimal performance for the max-min fairness problem in cell-free massive MIMO systems while significantly reducing inference time compared to full-attention baselines, without needing retraining for different network sizes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/967237dcd593c2fe2d6f3b16cb72307d4ae0dbaa94a8031ad460a273da33c12a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/967237dcd593c2fe2d6f3b16cb72307d4ae0dbaa94a8031ad460a273da33c12a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high computational cost of Transformer models for power allocation in large-scale wireless networks. It proposes a hybrid Tree-Transformer architecture that compresses user features into a root node for processing, achieving linear complexity and scalable inference. The model demonstrates near-optimal performance with significantly reduced inference time for cell-free massive MIMO systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Scaling Unverifiable Rewards: A Case Study on Visual Insights</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [Test-Time Scaling, multi-agent pipeline, process-based refinement, LLM-as-Judge, unverifiable rewards]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuyu Gan, James Mooney, Pan Hao, Renxiang Wang, Mingyi Hong, Qianwen Wang, Dongyeop Kang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Minnesota</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22650" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22650</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://minnesotanlp.github.io/insight-scaling-webpage" target="_blank" rel="noopener noreferrer" class="">https://minnesotanlp.github.io/insight-scaling-webpage</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Selective Test-Time Scaling, a process-based refinement framework that scales inference across stages in multi-agent pipelines instead of repeated refinement over time. 2. Designed a reliable LLM-based judge model aligned with human experts for evaluating visual insights. 3. Demonstrated improved insight quality under fixed compute budget in a data science pipeline application.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60e14b2c6ac8597444bea3610d692bad067ed798ec62c0920b0fe7c54b7fcd14_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60e14b2c6ac8597444bea3610d692bad067ed798ec62c0920b0fe7c54b7fcd14_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of scaling LLM agents for tasks with unverifiable rewards by introducing Selective Test-Time Scaling, which distributes compute across pipeline stages and prunes low-quality branches early using process-specific judges. Applied to generating visual insights from datasets, the method increases mean quality scores and reduces variance compared to traditional time-based refinement. The work provides a foundation for scaling complex, open-ended tasks like scientific discovery and story generation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Clinically Calibrated Machine Learning Benchmarks for Large-Scale Multi-Disorder EEG Classification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [medical signal processing], [electroencephalography, multi-disorder classification, sensitivity-oriented modeling, clinical calibration, feature importance analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Argha Kamal Samanta, Deepak Mewada, Monalisa Sarma, Debasis Samanta</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology, Kharagpur</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22656" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22656</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a clinically calibrated, sensitivity-prioritized machine learning framework for classifying eleven diverse neurological disorders from EEG data, addressing severe class imbalance. 2. Establishes realistic performance baselines for multi-disorder EEG classification, demonstrating recall exceeding 80% for most disorders with significant gains for low-prevalence conditions after threshold calibration. 3. Provides physiologically plausible feature importance analysis that aligns with established clinical EEG markers, validating the model&#x27;s clinical relevance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/636894ed25045336665fcbac593a58130411dff99c5d2a3e5a0cd50067ee44ec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/636894ed25045336665fcbac593a58130411dff99c5d2a3e5a0cd50067ee44ec_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study addresses the challenge of automated, multi-disorder screening from clinical EEG data by developing disorder-aware machine learning models with decision thresholds explicitly calibrated to prioritize diagnostic sensitivity. The method uses a multi-domain feature set and is evaluated on a large, heterogeneous dataset, achieving high recall for most neurological disorder categories. The results establish performance baselines and demonstrate that sensitivity-prioritized automated analysis can support scalable EEG screening and triage in clinical practice.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival Melanocytic Intraepithelial Lesion Grading</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [multi-task learning, inter-task consistency, digital pathology, foundation models, combinatorial partial supervision]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mert Ikinci, Luna Toma, Karin U. Loeffler, Leticia Ussem, Daniela Süsskind, Julia M. Weller, Yousef Yeganeh, Martina C. Herwig-Carl, Shadi Albarqouni</p>
</li>
<li class="">
<p><strong>institution:</strong> University Hospital Bonn, Technical University of Munich</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22666" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22666</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces INTERACT-CMIL, a multi-head deep learning framework for jointly predicting five histopathological axes for CMIL grading. 2. Proposes a Shared Feature Learning with Combinatorial Partial Supervision and an Inter-Dependence Loss to enforce cross-task consistency. 3. Curates and evaluates on a new multi-center dataset, showing significant performance improvements over baselines and providing a reproducible benchmark.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ed4ee67160f114b6db4089d38a1fbeae9ea01e9231d6d4df14bea6d6144469c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ed4ee67160f114b6db4089d38a1fbeae9ea01e9231d6d4df14bea6d6144469c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the difficult problem of grading Conjunctival Melanocytic Intraepithelial Lesions (CMIL) by introducing INTERACT-CMIL, a multi-task deep learning framework that uses shared feature learning and an inter-dependence loss to jointly and consistently predict five diagnostic criteria. Evaluated on a new multi-center dataset, the method outperforms CNN and foundation model baselines, offering a coherent and interpretable computational tool for standardized diagnosis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [3D Inception, two-stream networks, CNN-RNN, EchoNet-Dynamic, video analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shravan Saranyan, Pramit Saha</p>
</li>
<li class="">
<p><strong>institution:</strong> Branham High School, University of Oxford</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22657" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22657</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A systematic investigation and comparison of multiple deep learning architectures (3D Inception, two-stream, CNN-RNN) for automated LVEF estimation from echocardiography videos. 2. Identification that modified 3D Inception architectures achieve the best performance (RMSE of 6.79%) on the EchoNet-Dynamic dataset. 3. Key insights on model design, including the tendency for smaller, simpler models to generalize better and the high sensitivity of performance to hyperparameters like kernel size and normalization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates deep learning models for automating the estimation of left ventricular ejection fraction (LVEF) from echocardiography videos to address the time-consuming and variable nature of manual assessment. The authors systematically evaluate and modify several architectures, including 3D Inception, two-stream, and CNN-RNN models, finding that a modified 3D Inception model performs best. The study concludes that while these models show promise, they are prone to overfitting and their performance is highly sensitive to specific hyperparameter choices.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Quantum Generative Models for Computational Fluid Dynamics: A First Exploration of Latent Space Learning in Lattice Boltzmann Simulations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [Quantum Generative Models, Computational Fluid Dynamics, Lattice Boltzmann Method, Vector Quantized Variational Autoencoder, Quantum Circuit Born Machine]</p>
</li>
<li class="">
<p><strong>authors:</strong> Achraf Hsain, Fouad Mohammed Abbou</p>
</li>
<li class="">
<p><strong>institution:</strong> Al Akhawayn University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22672" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22672</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A complete open-source pipeline bridging CFD simulation and quantum machine learning. 2. The first empirical study of quantum generative modeling on compressed latent representations of physics simulations. 3. A comparative analysis of quantum (QCBM, QGAN) and classical (LSTM) generative models for a physics-derived latent distribution.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6094a8215d7304400e5580e08cc0e81135106aaf9b51ef11b7f4c5d62734237e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6094a8215d7304400e5580e08cc0e81135106aaf9b51ef11b7f4c5d62734237e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper explores the application of quantum generative models to Computational Fluid Dynamics (CFD) data. The authors compress fluid simulation data into a discrete latent space using a VQ-VAE and then compare quantum (QCBM, QGAN) and classical (LSTM) models for generating samples from this distribution. Under their experimental conditions, the quantum models, particularly the QCBM, outperformed the classical baseline in generating samples closer to the true distribution.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [width pruning, expansion ratio, Maximum Absolute Weight (MAW), GLU-MLP, instruction-following]</p>
</li>
<li class="">
<p><strong>authors:</strong> Pere Martra</p>
</li>
<li class="">
<p><strong>institution:</strong> Universidad Internacional Menéndez Pelayo (UIMP)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22671" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22671</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Systematically characterizes a capability dichotomy where structured width pruning degrades parametric knowledge (e.g., MMLU) but significantly improves instruction-following (e.g., IFEval). 2. Discovers and quantifies a robust inverse correlation between factual knowledge and truthfulness, linking knowledge degradation under pruning to improved misconception discrimination. 3. Identifies the expansion ratio as a critical architectural parameter that selectively modulates cognitive capabilities, rather than just a compression metric, and quantifies its context-dependent efficiency trade-offs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4df6003ce0dd5672f67ef0c36b943a93c7cbb475cc96f2c7592e1f230af17d53_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4df6003ce0dd5672f67ef0c36b943a93c7cbb475cc96f2c7592e1f230af17d53_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates structured width pruning of GLU-MLP layers in Llama-3.2 models using the Maximum Absolute Weight (MAW) criterion. It finds that pruning creates a dichotomy: while parametric knowledge degrades, instruction-following improves and multi-step reasoning remains robust, challenging the assumption of uniform degradation. The main conclusion is that width pruning acts as a selective filter, reducing knowledge but preserving or enhancing behavioral alignment, with identified trade-offs in efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Beyond Centralization: Provable Communication Efficient Decentralized Multi-Task Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [decentralized learning, multi-task representation learning, low-rank structure, communication complexity, projected gradient descent]</p>
</li>
<li class="">
<p><strong>authors:</strong> Donghwa Kang, Shana Moothedath</p>
</li>
<li class="">
<p><strong>institution:</strong> Iowa State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22675</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new alternating projected gradient descent and minimization algorithm for decentralized multi-task representation learning with provable accuracy guarantees. 2. Provides comprehensive theoretical analysis of time, communication, and sample complexities, showing communication complexity is independent of target accuracy. 3. Identifies regimes (e.g., large number of nodes, low bandwidth) where the decentralized algorithm can outperform centralized federated learning approaches.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96a6cd15411863b0b8dc43a27acc5efd5e57130db256aae85b8195706d6a64ca_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96a6cd15411863b0b8dc43a27acc5efd5e57130db256aae85b8195706d6a64ca_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies decentralized multi-task representation learning where tasks share a low-rank structure. The authors propose a new alternating projected gradient descent algorithm with provable guarantees, showing its communication cost is independent of the target accuracy. Numerical simulations validate the theory and demonstrate scenarios where decentralized learning outperforms centralized federated methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image registration], [Neural ODEs, Structural Descriptors, Diffeomorphic Registration, Multimodal, Local Mutual Information]</p>
</li>
<li class="">
<p><strong>authors:</strong> Salvador Rodriguez-Sanz, Monica Hernandez</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Zaragoza, Aragon Institute for Engineering Research (I3A)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22689" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22689</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an instance-specific multimodal diffeomorphic registration framework using Neural ODEs, eliminating the need for large training datasets and avoiding performance drop on unseen modalities. 2. Introduces three method variants that integrate modality-agnostic structural descriptors (image-based or feature-based) with local mutual information for similarity measurement. 3. Demonstrates superior performance, robustness to varying regularization, suitability for different deformation scales, and computational efficiency compared to state-of-the-art baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/064c09ec95bfe97d740e2afb1b657324c426af97aabecb55dcfa4db080514f55_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/064c09ec95bfe97d740e2afb1b657324c426af97aabecb55dcfa4db080514f55_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of existing nonrigid registration methods, which often require intensity correlation and are limited to monomodal settings. It proposes a multimodal diffeomorphic registration method that combines Neural ODEs with structural descriptors and local mutual information. The method shows superior accuracy, robustness, and efficiency in aligning medical images from different modalities without requiring extensive training data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Learning with the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span></span></span></span>-adics</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [representation learning], [p-adic numbers, ultrametric space, hierarchical representation, non-archimedean geometry, semantic networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> André F. T. Martins</p>
</li>
<li class="">
<p><strong>institution:</strong> Instituto Superior Técnico, Universidade de Lisboa; Instituto de Telecomunicações</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22692" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22692</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes using the p-adic number field (Q_p) as an alternative to real numbers for machine learning, leveraging its ultrametric and hierarchical structure. 2. Develops foundational building blocks for classification, regression, and representation learning models and algorithms within the p-adic framework. 3. Demonstrates a novel application by representing simple Quillian semantic networks as compact p-adic linear networks, which is not achievable with real numbers.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec090da18463864436ff27e6cb4651eb7e01719b66dcdae5d6644770e6a7b488_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec090da18463864436ff27e6cb4651eb7e01719b66dcdae5d6644770e6a7b488_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper explores using p-adic numbers, an ultrametric and non-archimedean field, as an alternative to real numbers for machine learning. It introduces theoretical models and algorithms for classification, regression, and representation learning, showing that p-adics enable compact representations of hierarchical structures like semantic networks. The work opens new research directions by leveraging the unique geometric properties of p-adic spaces.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Predictive Modeling of Power Outages during Extreme Events: Integrating Weather and Socio-Economic Factors</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [predictive modeling], [power outage prediction, LSTM, socio-economic factors, machine learning, resilience]</p>
</li>
<li class="">
<p><strong>authors:</strong> Antar Kumar Biswas, Masoud H. Nazari</p>
</li>
<li class="">
<p><strong>institution:</strong> Wayne State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22699" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22699</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel learning-based framework specifically for predicting low-probability, high-consequence (LPHC) power outages during extreme events. 2. Integrates a comprehensive set of features from public data, including weather, socio-economic, infrastructure, and seasonal event data, to reveal community vulnerability patterns. 3. Empirically validates the framework on a large-scale Michigan dataset, demonstrating that the LSTM model achieves the lowest prediction error and identifying correlations between economic/infrastructure factors and outage occurrence.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3d66053b661eb55e66139efa4ce581315354dbf33b1490862dcf700576f9ef5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3d66053b661eb55e66139efa4ce581315354dbf33b1490862dcf700576f9ef5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a machine learning framework to predict power outages caused by extreme events by integrating weather, socio-economic, and infrastructure data. The authors evaluate four models (RF, SVM, AdaBoost, LSTM) on a dataset from Michigan and find that the LSTM performs best, with results showing that better economic conditions and infrastructure are linked to fewer outages.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] What Matters in Deep Learning for Time Series Forecasting?</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [channel-independence, locality, globality, forecasting model card]</p>
</li>
<li class="">
<p><strong>authors:</strong> Valentina Moretti, Andrea Cini, Ivan Marisca, Cesare Alippi</p>
</li>
<li class="">
<p><strong>institution:</strong> IDSIA (Università della Svizzera italiana), Politecnico di Milano</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22702" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22702</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Analyzes the design space of deep learning for time series forecasting, emphasizing principles like locality and globality over specific sequence modeling layers. 2. Highlights how overlooked implementation details (e.g., parameter sharing) fundamentally alter model classes and empirical results. 3. Proposes an auxiliary forecasting model card to systematically characterize architectures based on key design choices, advocating for improved benchmarking practices.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97a34d1ef2fb71ea41bf423f0bb9c22a61efb8b9f97da7ec25666712a5fa044b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97a34d1ef2fb71ea41bf423f0bb9c22a61efb8b9f97da7ec25666712a5fa044b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper critically examines deep learning architectures for time series forecasting, arguing that foundational design principles (e.g., locality vs. globality) are more crucial for accuracy than complex sequence modeling layers. It shows that simple, well-designed models can match state-of-the-art performance and reveals how implementation details significantly impact results. The authors propose a forecasting model card to standardize architecture characterization and call for a rethink of benchmarking practices.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [hope speech detection], [transformer models, multilingual classification, low-resource languages, XLM-RoBERTa, UrduBERT]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ahmed Abdullah, Sana Fatima, Haroon Mahmood</p>
</li>
<li class="">
<p><strong>institution:</strong> FAST-National University, Al Ain University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22705" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22705</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a multilingual framework for hope speech detection, specifically addressing the underrepresentation of low-resource languages like Urdu. 2. Applies and evaluates multiple pretrained transformer models (XLM-RoBERTa, mBERT, EuroBERT, UrduBERT) on the PolyHope-M 2025 benchmark for this task. 3. Demonstrates strong performance, achieving high F1-scores for Urdu classification, validating the use of existing multilingual models in low-resource settings.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c79c484e6d35762080aa8d6e1dbf075222d30335d656555a46ddf73380d7fe88_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c79c484e6d35762080aa8d6e1dbf075222d30335d656555a46ddf73380d7fe88_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of resources for hope speech detection in low-resource languages by proposing a multilingual framework using pretrained transformer models like XLM-RoBERTa and UrduBERT. The method involves simple preprocessing and training classifiers, which achieve high F1-scores on the PolyHope-M 2025 benchmark, particularly for Urdu. The results show that existing multilingual models can be effectively implemented to identify hope speech and foster positive digital discourse in low-resource environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Memento-II: Learning by Stateful Reflective Memory</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [stateful reflective decision process, episodic memory, policy iteration, continual learning, retrieval-augmented generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jun Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> University College London (UCL)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22716" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22716</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Stateful Reflective Decision Process (SRDP), a formal theoretical framework that models continual learning in LLM agents as a two-stage read-write interaction with episodic memory, linking it to policy evaluation and improvement. 2. Provides a theoretical analysis showing that the reflective learning process induces an equivalent Markov Decision Process, enabling the use of classical dynamic programming and RL tools, and establishes convergence guarantees when instantiated with entropy-regularised policy iteration. 3. Unifies heuristic approaches like case-based reasoning and retrieval-augmented generation with principled reinforcement learning, offering a rigorous mathematical foundation for building memory-augmented agents capable of online adaptation without parameter updates.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a theoretical framework for continual learning in LLM agents that uses episodic memory and reflection instead of back-propagation. The core method formalizes learning as a Stateful Reflective Decision Process, where writing to memory is policy evaluation and reading from it is policy improvement. The main conclusion is that this framework provides a principled, convergent foundation for agents to self-improve through interaction without fine-tuning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Data Augmentation for Classification of Negative Pregnancy Outcomes in Imbalanced Data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [text classification], [data augmentation, imbalanced dataset, social media analysis, natural language processing, pregnancy outcome]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Badsha Biswas</p>
</li>
<li class="">
<p><strong>institution:</strong> George Mason University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22732" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22732</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel approach to use public social media data (e.g., Twitter) as an adjunctive resource for studying negative pregnancy outcomes, addressing data scarcity in traditional epidemiological research. 2. Constructs an NLP pipeline to automatically identify and classify pregnancy experiences from unstructured, noisy social media text, distinguishing between positive and negative outcomes. 3. Investigates and evaluates various data augmentation techniques specifically to address the severe class imbalance inherent in social media data for this sensitive health domain.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb9c28a95fb880100ca20beffad94909ef9e73c38a75789c38961258454c014a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb9c28a95fb880100ca20beffad94909ef9e73c38a75789c38961258454c014a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of classifying negative pregnancy outcomes from imbalanced social media data. It proposes an NLP pipeline to extract and categorize pregnancy experiences from Twitter and investigates data augmentation techniques to balance the dataset. The research demonstrates the viability of social media data as a supplementary resource for epidemiological studies on pregnancy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [context folding, long-horizon RL, non-stationary observation, gradient dilution, selective segment training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiaqi Shao, Yufeng Miao, Wei Zhang, Bing Luo</p>
</li>
<li class="">
<p><strong>institution:</strong> Hong Kong University of Science and Technology, Duke Kunshan University, Microsoft AI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22733" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22733</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/SHAO-Jiaqi757/FoldAct" target="_blank" rel="noopener noreferrer" class="">https://github.com/SHAO-Jiaqi757/FoldAct</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Separated loss computation for independent gradient signals on summary and action tokens to address gradient dilution. 2. Full context consistency loss to reduce distribution shift caused by policy-dependent observation changes. 3. Selective segment training to reduce computational cost by processing unique contexts efficiently.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebdb4b7ca8ea3a44c0e368eed5fbbebfc656b663cf280d1891abfbf823c742fa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebdb4b7ca8ea3a44c0e368eed5fbbebfc656b663cf280d1891abfbf823c742fa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that treating context folding (history summarization) as a standard action in long-horizon RL for LLMs creates a non-stationary observation distribution, leading to training instability and inefficiency. It proposes FoldAct, a framework with three innovations—separated loss, consistency loss, and selective training—to stabilize training and improve efficiency. The method achieves stable training and a 5.19× speedup.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] When Does Multi-Task Learning Fail? Quantifying Data Imbalance and Task Independence in Metal Alloy Property Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-task learning], [negative transfer, data imbalance, task independence, metal alloys, property prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sungwoo Kang</p>
</li>
<li class="">
<p><strong>institution:</strong> Korea University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22740" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22740</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Empirical demonstration of a dichotomy where MTL degrades regression but improves classification for metal alloy property prediction. 2. Quantitative analysis linking MTL failure to severe data imbalance and near-zero inter-task dependencies. 3. Practical guidelines for materials informatics: use independent models for precise regression and MTL for classification tasks requiring high recall.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c5a4e5491f4e12b8112ec2efc4d5432ecc40f379be7394990c01d0cb507caab_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c5a4e5491f4e12b8112ec2efc4d5432ecc40f379be7394990c01d0cb507caab_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tests the assumption of Multi-Task Learning (MTL) in materials informatics by predicting three metal alloy properties. The results show MTL harms regression performance due to negative transfer from data imbalance but improves classification recall, as the properties are found to be independent. The work concludes with recommendations for when to use or avoid MTL in material discovery.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Bridging Global Intent with Local Details: A Hierarchical Representation Approach for Semantic Validation in Text-to-SQL</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [text-to-sql], [semantic validation, hierarchical representation, logical plan, abstract syntax tree, nested message passing neural network]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rihong Qiu, Zhibang Yang, Xinke Jiang, Weibin Liao, Xin Gao, Xu Chu, Junfeng Zhao, Yasha Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22744" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22744</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed HEROSQL, a hierarchical SQL representation approach that integrates global intent (via Logical Plans) and local details (via Abstract Syntax Trees) for semantic validation. 2. Employed a Nested Message Passing Neural Network (NMPNN) to capture relational information in SQL and aggregate schema-guided semantics across LPs and ASTs. 3. Introduced an AST-driven sub-SQL augmentation strategy to generate high-quality negative samples for robust optimization of fine-grained semantic inconsistencies.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2268a4534cffc0a143786b6b8d80dbddb03e7bf7ffd9234c8d468b2a06e151f0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2268a4534cffc0a143786b6b8d80dbddb03e7bf7ffd9234c8d468b2a06e151f0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of semantic validation in Text-to-SQL systems by proposing HEROSQL, a hierarchical representation method that combines global intent and local SQL details using Logical Plans and Abstract Syntax Trees, enhanced by a Nested Message Passing Neural Network. It also introduces an AST-driven augmentation strategy for generating negative samples. Experiments show that HEROSQL outperforms state-of-the-art methods in detecting semantic inconsistencies, improving AUPRC by 9.40% and AUROC by 12.35% on average.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] From Confounding to Learning: Dynamic Service Fee Pricing on Third-Party Platforms</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [demand learning, instrumental variables, deep neural networks, regret bound, confounding]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rui Ai, David Simchi-Levi, Feng Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> Massachusetts Institute of Technology (MIT)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22749" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22749</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed an algorithm for dynamic service fee pricing with optimal regret, showing a phase transition based on supply-side noise. 2. Demonstrated that non-i.i.d. actions can serve as instrumental variables to address confounding in demand learning. 3. Proposed a novel homeomorphic construction to establish estimation bounds for learning demand with deep neural networks without requiring star-shapedness assumptions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a53b1242a3048a4f1795be36eec0fe0875f0b532bc8001a000dcba26693c66da_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a53b1242a3048a4f1795be36eec0fe0875f0b532bc8001a000dcba26693c66da_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the problem of dynamic service fee pricing on third-party platforms, where only equilibrium price and quantity are observable, creating a confounding demand learning problem. The authors develop an algorithm with optimal regret, using non-i.i.d. actions as instrumental variables and a novel homeomorphic construction for deep neural network-based demand estimation. The results show that supply-side noise fundamentally impacts learnability and the method is validated with simulations and real-world data from Zomato and Lyft.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Micro-Macro Machine Learning Framework for Predicting Childhood Obesity Risk Using NHANES and Environmental Determinants</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [public health informatics], [multi-level modeling, XGBoost, environmental vulnerability index, NHANES, machine learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Eswarasanthosh Kumar Mamillapalli, Nishtha Sharma</p>
</li>
<li class="">
<p><strong>institution:</strong> n/a (Inferred from email: nishtha <a href="mailto:sharma@nh.gov" target="_blank" rel="noopener noreferrer" class="">sharma@nh.gov</a> suggests potential affiliation with a state health department, but no clear academic/research institution is specified. The other author&#x27;s email is a personal Gmail address.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22758" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22758</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a novel micro-macro machine learning framework that integrates individual-level health data with macro-level environmental data to predict childhood obesity risk. 2. Constructed a composite environmental vulnerability index (EnvScore) from USDA and EPA data to quantify state-level structural risk factors. 3. Demonstrated a scalable, data-driven modeling pipeline that reveals geographic alignment between high environmental burden and predicted individual obesity risk, enabling identification of environment-driven health disparities.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7578fbec69afd6c4e44bd1d6715e6f98a34a451a47c81c03388bed0ae331bd4c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7578fbec69afd6c4e44bd1d6715e6f98a34a451a47c81c03388bed0ae331bd4c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a machine learning framework that combines individual health data from NHANES with environmental data (food access, air quality) from USDA/EPA to predict childhood obesity. The best-performing model was XGBoost, and a state-level environmental risk score was created. The study found a strong geographic correlation between areas of high environmental burden and high predicted obesity risk, showing the value of multi-scale data integration for public health.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Active Constraint Learning in High Dimensions from Demonstrations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robot learning], [active learning, constraint inference, Gaussian processes, learning from demonstration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zheng Qiu, Chih-Yuan Chiu, Glen Chou</p>
</li>
<li class="">
<p><strong>institution:</strong> Georgia Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22757" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22757</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an iterative active constraint learning (ACL) algorithm that intelligently queries for new demonstrations to reduce constraint uncertainty. 2. Integrates a Gaussian process (GP) model within the learning from demonstrations (LfD) paradigm to represent and infer unknown constraints. 3. Demonstrates superior performance over a random-sampling baseline in recovering nonlinear constraints from sparse, informative demonstrations in high-dimensional settings with nonlinear dynamics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5acbf135d5aa431114b6b72db2fb101427cb6bd1e55fb1a8afd3028c0874cb4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5acbf135d5aa431114b6b72db2fb101427cb6bd1e55fb1a8afd3028c0874cb4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the data inefficiency of learning unknown constraints from demonstrations by proposing an active learning algorithm. The method iteratively trains a Gaussian process on demonstration data to model constraints and uses the model&#x27;s uncertainty to query for new, informative start/goal states to generate more demonstrations. Experiments show the approach outperforms a random-sampling baseline in accurately inferring constraints from fewer demonstrations in high-dimensional, nonlinear environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Understanding the Mechanisms of Fast Hyperparameter Transfer</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [hyperparameter optimization], [hyperparameter transfer, scale-aware hyperparameters, Maximal Update Parameterization (μP), compute-optimal grid search]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nikhil Ghosh, Denny Wu, Alberto Bietti</p>
</li>
<li class="">
<p><strong>institution:</strong> Flatiron Institute, New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22768" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22768</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Develops a formal conceptual framework defining &quot;fast&quot; hyperparameter transfer and proves its equivalence to &quot;useful&quot; transfer for compute-optimal grid search. 2. Demonstrates that the fast transfer property is not universal and depends critically on problem structure, showing synthetic cases where it succeeds or fails. 3. Proposes and provides empirical evidence for a mechanistic hypothesis explaining fast transfer, decomposing the loss reduction into width-stable and width-sensitive components.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b19c9825d64e8e70117e18cd478466aaf2627a7a5167d401bcac21353870d508_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b19c9825d64e8e70117e18cd478466aaf2627a7a5167d401bcac21353870d508_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the mechanisms behind fast hyperparameter transfer, a strategy to reduce tuning costs by transferring optimal hyperparameters from small to large models. It formally defines fast transfer and shows it is computationally advantageous, then explains the phenomenon by hypothesizing a decomposition of the optimization trajectory into stable and sensitive components, supported by empirical evidence.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [causal inference], [causal transportability, domain adaptation, few-shot learning, circuit composition, distribution shift]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kasra Jalaldoust, Elias Bareinboim</p>
</li>
<li class="">
<p><strong>institution:</strong> Columbia University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22777" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22777</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Circuit-TR, an algorithm for zero-shot compositional generalization based on causal transportability theory, using modules learned from source data. 2. Introduced a supervised domain adaptation scheme that leverages circuit transportability without requiring an explicit causal graph, using only limited target data. 3. Provided theoretical characterization of few-shot learnable tasks using graphical circuit transportability criteria, linking generalizability to circuit size complexity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b025a886304d6097d2893ec4af3bb34a59528db65e22ddf5b4dfff4439a096_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b025a886304d6097d2893ec4af3bb34a59528db65e22ddf5b4dfff4439a096_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of generalization under distribution shift by proposing a method based on causal transportability theory. The method, Circuit-TR, learns local predictors (modules) from source data and composes them into a circuit for prediction in a target domain, enabling both zero-shot and few-shot adaptation. The theoretical results connect few-shot learnability to circuit transportability criteria and complexity, which are supported by simulations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [temporal graph neural networks, explainable ai, graph explanation, recurrent neural networks, breadth-first search]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xuyan Li, Jie Wang, Zheng Yan</p>
</li>
<li class="">
<p><strong>institution:</strong> Xidian University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22772" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22772</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GRExplainer, a universal explanation method applicable to both snapshot-based and event-based Temporal Graph Neural Networks (TGNNs). 2. Introduces an efficient approach using breadth-first search and temporal information to construct node sequences, reducing computational cost. 3. Designs a user-friendly generative model based on Recurrent Neural Networks (RNNs) for automated and continuous explanation generation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0722c64cfeebe092d7b253f417faaa51990e69198068745c39fe241716082408_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0722c64cfeebe092d7b253f417faaa51990e69198068745c39fe241716082408_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of explainability in Temporal Graph Neural Networks (TGNNs) by proposing GRExplainer, a universal and efficient method that uses node sequences and an RNN-based generative model to provide explanations. Experiments on six datasets with three TGNNs demonstrate that GRExplainer outperforms existing methods in generality, efficiency, and user-friendliness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Schrodinger AI: A Unified Spectral-Dynamical Framework for Classification, Reasoning, and Operator-Based Generalization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [quantum-inspired machine learning], [spectral decomposition, Hamiltonian learning, semantic wavefunctions, operator calculus, emergent manifolds]</p>
</li>
<li class="">
<p><strong>authors:</strong> Truong Son Nguyen</p>
</li>
<li class="">
<p><strong>institution:</strong> Arizona State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22774" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22774</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A unified spectral-dynamical framework for machine learning inspired by quantum mechanics, comprising a time-independent wave-energy solver, a time-dependent dynamical solver, and a low-rank operator calculus. 2. Demonstration of emergent semantic manifolds that reflect class relations without explicit supervision, dynamic reasoning that adapts to changing environments, and exact operator generalization on symbolic tasks. 3. Proposing a new foundational direction for ML where learning is cast as discovering and navigating an underlying semantic energy landscape, offering an alternative to cross-entropy training and transformer attention.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58825c1201e17641b4e19a581a742615913539c66fbf08e5c113620bf7eec6de_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58825c1201e17641b4e19a581a742615913539c66fbf08e5c113620bf7eec6de_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Schrödinger AI, a novel machine learning framework inspired by quantum mechanics that treats perception as spectral decomposition and reasoning as wavefunction dynamics. The method demonstrates robust generalization, interpretable semantics, and emergent topology on tasks like classification, maze navigation, and modular arithmetic. The results suggest a promising new paradigm for AI that learns by discovering an underlying semantic energy landscape.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Discovering Transmission Dynamics of COVID-19 in China</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [information extraction], [natural language processing, transmission chain, epidemiological analysis, data mining, statistical analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhou Yang, Edward Dougherty, Chen Zhang, Zhenhe Pan, Fang Jin</p>
</li>
<li class="">
<p><strong>institution:</strong> George Washington University, Roger Williams University, Texas Tech University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22787" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22787</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Constructed transmission/tracking chains for COVID-19 in China by applying NLP and manual curation to data mined from diverse public sources. 2. Conducted a comprehensive spatiotemporal analysis by integrating case tracking data with population mobility data from Wuhan. 3. Quantified key transmission dynamics, revealing regional differences, hospitalization timelines, and the evolution of infection sources over the course of the pandemic.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eca2aa3a96ff37b70cb26e6ad2cfa0f84cc9a5cf8c76b6e1404ce5fa2474828e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eca2aa3a96ff37b70cb26e6ad2cfa0f84cc9a5cf8c76b6e1404ce5fa2474828e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the transmission dynamics of COVID-19 in China by mining and processing public case reports using NLP to construct transmission chains. The analysis integrates these chains with mobility data to quantify spatiotemporal spread. Key findings include significant regional differences in infection rates, rapid hospitalization of symptomatic cases, and a shift in infection sources from travel to social activities over time.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] CNSight: Evaluation of Clinical Note Segmentation Tools</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [text segmentation], [clinical note segmentation, transformer models, large language models, MIMIC-IV, rule-based baselines]</p>
</li>
<li class="">
<p><strong>authors:</strong> Risha Surana, Adrian Law, Sunwoo Kim, Rishab Sridhar, Angxiao Han, Peiyu Hong</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Southern California</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22795" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22795</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A comprehensive evaluation of diverse methods (rule-based, domain-specific transformers, and large language models) for the task of clinical note segmentation. 2. The curation and use of a dataset of 1,000 notes from MIMIC-IV for benchmarking segmentation performance. 3. Empirical findings that large API-based models (e.g., GPT-5-mini) achieve the best overall performance, while lightweight baselines remain competitive only on structured tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be9738f44f0f558dc344bd32b267879341b566035c5dbe67f97ac2e4529b479_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be9738f44f0f558dc344bd32b267879341b566035c5dbe67f97ac2e4529b479_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper evaluates various methods for segmenting unstructured clinical notes into distinct sections. It compares rule-based baselines, domain-specific transformers, and large language models on a curated dataset from MIMIC-IV. The main conclusion is that large API-based models like GPT-5-mini achieve the best overall segmentation performance, providing guidance for method selection in downstream clinical applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition via Spherical Normalization and Mahalanobis Distance</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [open-set recognition], [spherical normalization, Mahalanobis distance, electronic nose, open-set recognition, feature drift]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuai Chen, Chen Wang, Ziran Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> School of Mechanical Engineering, Shandong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22792" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22792</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A geometric decoupling mechanism using cascaded batch normalization and L2 normalization to project features onto a unit hypersphere, eliminating signal intensity fluctuations. 2. The introduction of Mahalanobis distance as a scoring mechanism to construct adaptive ellipsoidal decision boundaries that account for anisotropic feature distributions. 3. A universal, architecture-agnostic framework (SNM-Net) that can be seamlessly integrated with various backbone networks (CNN, RNN, Transformer) for robust open-set gas recognition.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d69d593ebbfea881a49530f570b5c2a934cb8b5cd782c1d7e7c9fba99a92906_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d69d593ebbfea881a49530f570b5c2a934cb8b5cd782c1d7e7c9fba99a92906_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes SNM-Net, a universal framework for robust open-set gas recognition in electronic nose systems. It addresses signal drift and unknown interference by projecting features onto a hypersphere for intensity normalization and using Mahalanobis distance for scoring. The method achieves state-of-the-art performance with high accuracy and exceptional robustness across different sensor conditions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ReDiF: Reinforced Distillation for Few Step Diffusion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [reinforcement learning, knowledge distillation, policy optimization, denoising paths, model agnostic]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amirhossein Tighkhorshid, Zahra Dehghanian, Gholamali Aminian, Chengchun Shi, Hamid R. Rabiee</p>
</li>
<li class="">
<p><strong>institution:</strong> Sharif University of Technology, Alan Turing Institute, London School of Economics</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22802" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22802</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel reinforcement learning framework for distilling diffusion models, treating distillation as a policy optimization problem. 2. Introduces a reward signal based on alignment with teacher outputs, allowing the student model to explore multiple denoising paths and take longer, optimized steps. 3. Demonstrates a model-agnostic framework that achieves superior performance with fewer inference steps and computational resources compared to existing distillation techniques.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73d47eb5443f9f8848454e65825da3569c70d954239d9794e6cff086fa5bc17a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73d47eb5443f9f8848454e65825da3569c70d954239d9794e6cff086fa5bc17a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the slow sampling problem in diffusion models by proposing ReDiF, a reinforcement learning-based distillation framework. Instead of using fixed losses, it treats distillation as policy optimization, using a reward signal to guide the student to take longer, optimized steps. The method achieves better performance with fewer steps and is applicable to various diffusion models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MoR: Mixture Of Representations For Mixed-Precision Training</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [mixed-precision training, FP8, dynamic quantization, tensor representation, low-precision training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bor-Yiing Su, Peter Dykas, Mike Chrzanowski, Jatin Chhugani</p>
</li>
<li class="">
<p><strong>institution:</strong> Nvidia, Meta</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22804" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22804</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Mixture-of-Representations (MoR), a novel per-tensor and sub-tensor level quantization framework that dynamically selects numerical representations based on tensor properties. 2. Introduces and experiments with concrete algorithms that dynamically choose between FP8 and BF16 representations at different granularities. 3. Demonstrates a universal approach that preserves model quality across datasets and achieves state-of-the-art results with 98.38% of tensors quantized to FP8, showing potential for even lower precision formats like NVFP4.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14c70a65c4f996e1c88d9996c77e4d780e13466bf11a0601ad82d27376753968_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14c70a65c4f996e1c88d9996c77e4d780e13466bf11a0601ad82d27376753968_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces MoR, a dynamic quantization framework for mixed-precision training that analyzes tensor properties to select between representations like FP8 and BF16. It achieves high FP8 quantization rates (98.38%) while maintaining model quality, offering a robust approach for low-precision training that can be combined with other methods for even lower precision formats.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Long-Range Distillation: Distilling 10,000 Years of Simulated Climate into Long Timestep AI Weather Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [climate informatics], [long-range distillation, synthetic training data, subseasonal-to-seasonal forecasting, probabilistic forecasting, autoregressive models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Scott A. Martin, Noah Brenowitz, Dale Durran, Michael Pritchard</p>
</li>
<li class="">
<p><strong>institution:</strong> NVIDIA Research, University of Washington</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22814" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22814</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes long-range distillation, a method to train a long-timestep probabilistic model using massive synthetic data from an autoregressive teacher model. 2. Demonstrates the generation and use of over 10,000 years of simulated climate data from the DLESyM model for training. 3. Shows that distilled models achieve S2S forecast skill comparable to ECMWF ensembles after fine-tuning, with skill scaling with synthetic data volume.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4b3dd444e162236a7d1cc0aaa69d999407d7ebf78184fada39f979dccbd0692f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4b3dd444e162236a7d1cc0aaa69d999407d7ebf78184fada39f979dccbd0692f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of long-range weather forecasting by introducing long-range distillation, a method that trains a single-step probabilistic model using a massive synthetic dataset generated by an autoregressive AI model. The distilled model, trained on over 10,000 years of simulated climate, achieves subseasonal-to-seasonal forecast skill comparable to state-of-the-art ensemble methods, demonstrating that AI-generated synthetic data can effectively scale long-range forecast skill.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [curriculum learning, goal-conditioned reinforcement learning, temporal variance, student-teacher paradigm, Q-function]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gaurav Chaudhary, Laxmidhar Behera</p>
</li>
<li class="">
<p><strong>institution:</strong> IIT Kanpur</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22824" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22824</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Student-Teacher learning paradigm with a Temporal Variance-Driven Curriculum for accelerating Goal-Conditioned RL. 2. Establishes a theoretical connection between the temporal variance of Q-values and policy evolution. 3. Demonstrates the algorithm-agnostic nature of the approach, showing consistent improvements across 11 robotic manipulation and maze navigation tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c410b802a9fdb44624e8bf6d607803fec918def24d7a3c81bf3fb12d7fb2e63_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c410b802a9fdb44624e8bf6d607803fec918def24d7a3c81bf3fb12d7fb2e63_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the sample inefficiency of uniform goal selection in multi-goal reinforcement learning. It proposes a TEACH framework where a teacher module dynamically selects goals with the highest temporal variance in Q-values to create an adaptive curriculum. The method is shown to improve learning efficiency over state-of-the-art curriculum learning methods across diverse robotic tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [video generation], [Human-Object Interaction, Diffusion Transformer, Relative Coordinate Maps, Progressive Curriculum Learning, Geometry Consistency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bangya Liu, Xinyu Gong, Zelin Zhao, Ziyang Song, Yulei Lu, Suhui Wu, Jun Zhang, Suman Banerjee, Hao Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Wisconsin-Madison, ByteDance, Georgia Institute of Technology, The Hong Kong Polytechnic University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22854" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22854</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://neutrinoliu.github.io/byteloom/" target="_blank" rel="noopener noreferrer" class="">https://neutrinoliu.github.io/byteloom/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ByteLoom, a Diffusion Transformer-based framework for generating realistic HOI videos with geometrically consistent objects. 2. Introduces the RCM-cache mechanism using Relative Coordinate Maps to maintain object geometry consistency and control 6-DoF transformations. 3. Designs a progressive training curriculum to compensate for HOI dataset scarcity and relax the need for fine-grained hand mesh annotations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9f5ff16308904b889be6695aafd24bfc28b798f080cc6c723a25066bcdc2bdf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9f5ff16308904b889be6695aafd24bfc28b798f080cc6c723a25066bcdc2bdf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of poor cross-view consistency and reliance on hand mesh annotations in Human-Object Interaction (HOI) video generation. It proposes ByteLoom, a framework that uses a novel RCM-cache mechanism for geometry consistency and a progressive curriculum learning strategy for training. The method effectively preserves human identity and object geometry while generating smooth motion.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [blockchain security, IoT security, adversarial machine learning], [Fully Homomorphic Encryption, Attribute-Based Access Control, Multi-Agent Reinforcement Learning, Byzantine Fault Tolerance, Trust-Based Consensus]</p>
</li>
<li class="">
<p><strong>authors:</strong> Soham Padia, Dhananjay Vaidya, Ramchandra Mangrulkar</p>
</li>
<li class="">
<p><strong>institution:</strong> Northeastern University, Dwarkadas J. Sanghvi College of Engineering</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22860" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22860</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel trust-based delegated consensus framework for blockchain IoT that integrates Fully Homomorphic Encryption (FHE) with Attribute-Based Access Control (ABAC) for privacy-preserving policy evaluation. 2. Systematically compares the performance of three reinforcement learning approaches (RL, DRL, MARL) against five distinct and sophisticated adversarial attack families. 3. Empirically demonstrates that Multi-Agent RL (MARL) provides superior defense against collusive attacks and identifies the catastrophic vulnerability of all learning agents to Time-Delayed Poisoning (sleeper) attacks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373f990d6c218afb4f9e660bb84fd86dc8e9d7006d2b7bdef3d956a991960bff_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373f990d6c218afb4f9e660bb84fd86dc8e9d7006d2b7bdef3d956a991960bff_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses securing blockchain-enabled IoT networks by proposing a trust-based consensus framework that combines privacy-preserving techniques (FHE and ABAC) with learning-based defenses. It compares RL, DRL, and MARL against five attack types, finding MARL most effective against collusive attacks but revealing that all methods are highly vulnerable to time-delayed poisoning attacks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent reinforcement learning], [Reinforcement Networks, directed acyclic graph (DAG), credit assignment, LevelEnv, hierarchical RL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Maksim Kryzhanovskiy, Svetlana Glazyrina, Roman Ischenko, Konstantin Vorontsov</p>
</li>
<li class="">
<p><strong>institution:</strong> Lomonosov Moscow State University, Institute for Artificial Intelligence, Lomonosov Moscow State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22876" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22876</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Reinforcement Networks framework, a general approach for collaborative MARL that organizes agents as vertices in a directed acyclic graph (DAG)., 2. Formalizes training and inference methods for the framework and connects it to the LevelEnv concept for reproducible construction and evaluation., 3. Demonstrates improved performance over standard MARL baselines and unifies hierarchical, modular, and graph-structured views of MARL.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b613175c92e9bdddfbd57ed84d044a5846b7b8148cca8ab0405e69847f66c33a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b613175c92e9bdddfbd57ed84d044a5846b7b8148cca8ab0405e69847f66c33a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of end-to-end training for AI systems with multiple learnable components. It proposes Reinforcement Networks, a framework that organizes agents in a directed acyclic graph for flexible credit assignment and coordination in multi-agent reinforcement learning. The method shows improved performance over baselines and provides a principled foundation for designing complex multi-agent systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Fundamental Novel Consistency Theory: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span></span></span></span>-Consistency Bounds</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [statistical learning theory], [H-consistency bounds, surrogate loss, minimizability gaps, adversarial robustness, comp-sum losses]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yutao Zhong</p>
</li>
<li class="">
<p><strong>institution:</strong> New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22880" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22880</a></p>
</li>
<li class="">
<p><strong>contributions:</strong>  1. Introduces a novel theoretical framework for deriving H-consistency bounds, which provide stronger and more informative guarantees than Bayes-consistency or H-calibration by accounting for the hypothesis set. 2. Establishes the first H-consistency bounds for a wide range of losses in binary and multi-class classification, including convex surrogates, max/sum/constrained losses, and comp-sum losses (e.g., cross-entropy), and extends the analysis to adversarial scenarios. 3. Analyzes the growth rates of H-consistency bounds, proving a universal square-root growth rate for smooth surrogates, and introduces the analysis of minimizability gaps to guide the selection of surrogate loss functions for learning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16541a1efc1f9f95010f468aecfaeb95d92c6aea20df9566f6ef60456e371fde_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16541a1efc1f9f95010f468aecfaeb95d92c6aea20df9566f6ef60456e371fde_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a new theoretical framework for analyzing the estimation error of target losses using surrogate losses in machine learning. It introduces H-consistency bounds, which offer stronger guarantees by incorporating the hypothesis set, and derives these bounds for various loss functions in both standard and adversarial settings. The main conclusion is that these bounds provide a more precise tool for understanding surrogate loss performance and can guide the design of robust learning algorithms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Theory and Algorithms for Learning with Multi-Class Abstention and Multi-Expert Deferral</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [machine learning theory], [multi-expert deferral, abstention, H-consistency, surrogate losses, two-stage learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Anqi Mao</p>
</li>
<li class="">
<p><strong>institution:</strong> New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22886" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22886</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced new surrogate loss families and proved strong consistency guarantees for multi-class learning with abstention, resolving open questions. 2. Designed new surrogate losses with H-consistency bounds for general multi-expert deferral in classification, leading to effective algorithms. 3. Proposed a novel framework and surrogate losses for regression with deferral, accommodating multiple experts and various cost structures.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed6952a3ce4104bfb4e2da48692fa87841baf950369a1d08640407f2002ece4c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed6952a3ce4104bfb4e2da48692fa87841baf950369a1d08640407f2002ece4c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This thesis addresses the problems of learning with abstention and multi-expert deferral to improve the reliability and efficiency of models like LLMs. It proposes new surrogate loss formulations for classification and regression, proves strong theoretical consistency guarantees, and demonstrates the empirical effectiveness of the resulting algorithms on datasets like CIFAR-10 and CIFAR-100.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Debugging Tabular Log as Dynamic Graphs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [dynamic graph, graph neural network, tabular log, log debugging, heterogeneous nodes]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chumeng Liang, Zhanyang Jin, Zahaib Akhtar, Mona Pereira, Haofei Yu, Jiaxuan You</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Illinois Urbana-Champaign, Amazon</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22903" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22903</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GraphLogDebugger, a novel framework that models tabular log data as dynamic graphs with heterogeneous nodes for objects and events, 2. Demonstrates that a simple dynamic GNN can outperform large language models (LLMs) in debugging tasks using this graph representation, 3. Validates the approach on real-world datasets from computer systems and academic papers, showing improved flexibility and scalability over LLM-based methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfc0dd96717274f366abd002f1a9147f7afbdaba795d251628919a0d25289925_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfc0dd96717274f366abd002f1a9147f7afbdaba795d251628919a0d25289925_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces GraphLogDebugger, a framework that converts tabular log data into dynamic graphs to detect inconsistencies in real-world systems. By representing logs as evolving graphs with object and event nodes, a lightweight dynamic Graph Neural Network effectively debugs logs, outperforming larger LLM-based models in experiments on system and academic log datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Neural Network-Based Real-time Casing Collar Recognition System for Downhole Instruments</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [Casing Collar Locator (CCL), ARM Cortex-M7, Depthwise Separable Convolutions, MACs, Inference Latency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Si-Yu Xiao, Xin-Di Zhao, Xiang-Zhan Wang, Tian-Hao Mao, Ying-Kai Liao, Xing-Yu Liao, Yu-Qiao Chen, Jun-Jie Wang, Shuang Liu, Tu-Pei Chen, Yang Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China, China National Petroleum Corporation Logging Co., Ltd., Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22901" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22901</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an in-situ, real-time collar recognition system using embedded neural networks to overcome signal degradation in traditional surface-based monitoring. 2. Introduces lightweight &quot;Collar Recognition Nets&quot; (CRNs) optimized for resource-constrained ARM Cortex-M7 microprocessors, using temporal and depthwise separable convolutions. 3. Demonstrates a highly efficient model achieving 8,208 MACs, an F1 score of 0.972, and an average inference latency of 343.2 µs, proving feasibility for downhole power/space constraints.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/235a8bd15d5c0da93f1ea31dd4b6da44b238cc7be57fd42a7bef3e629f9c6495_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/235a8bd15d5c0da93f1ea31dd4b6da44b238cc7be57fd42a7bef3e629f9c6495_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of accurate downhole positioning in oil/gas operations by developing a real-time, embedded neural network system for casing collar recognition. The method introduces lightweight &quot;Collar Recognition Nets&quot; optimized for ARM Cortex-M7 processors, achieving high accuracy with minimal computational cost. The results demonstrate that robust, autonomous signal processing is feasible within the severe power and space limitations of downhole instrumentation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Federated Multi-Task Clustering</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [federated clustering, spectral clustering, multi-task learning, tensor methods, ADMM]</p>
</li>
<li class="">
<p><strong>authors:</strong> S. Dai, G. Sun, F. Li, X. Tang, Q. Wang, Y. Cong</p>
</li>
<li class="">
<p><strong>institution:</strong> South China University of Technology, Dalian University of Technology, Xidian University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22897" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22897</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Federated Multi-Task Clustering (FMTC) framework that learns personalized models for heterogeneous clients while capturing shared knowledge in a privacy-preserving manner. 2. Introduces a client-side parameterized mapping model for robust out-of-sample inference, eliminating the need for unreliable pseudo-labels. 3. Develops a server-side tensorial correlation module using low-rank regularization on a unified model tensor to explicitly discover common subspace across clients, solved via a privacy-preserving ADMM-based distributed algorithm.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e64c06637c228c53ac30a2069610927ac906eea1fc53a050d87f6f985e001ab_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e64c06637c228c53ac30a2069610927ac906eea1fc53a050d87f6f985e001ab_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes FMTC, a federated multi-task clustering framework that addresses the limitations of centralized spectral clustering and unreliable pseudo-labels in federated settings. It combines client-side personalized clustering models with a server-side tensorial module to capture shared knowledge, using an ADMM-based algorithm for efficient, privacy-preserving optimization. Experiments show FMTC outperforms existing federated clustering methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MetaCD: A Meta Learning Framework for Cognitive Diagnosis based on Continual Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [educational data mining], [cognitive diagnosis, meta-learning, continual learning, long-tailed distribution, parameter protection mechanism]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jin Wu, Chanjin Zheng</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Institute of Artificial Intelligence for Education, East China Normal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22904" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22904</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes MetaCD, a novel framework that integrates meta-learning and continual learning for cognitive diagnosis. 2. Uses meta-learning to learn an optimal initialization to alleviate the long-tailed data problem, enabling good performance with few samples. 3. Incorporates a continual learning parameter protection mechanism to adapt to dynamic data changes and new tasks while preventing catastrophic forgetting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d62de4bde455a8e131e7247a8b8ce8fa94feb82289f4b3dc1660955e7645dfa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d62de4bde455a8e131e7247a8b8ce8fa94feb82289f4b3dc1660955e7645dfa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes MetaCD, a meta-learning framework based on continual learning, to address the challenges of long-tailed data distribution and dynamic changes in cognitive diagnosis for intelligent education. It uses meta-learning for optimal initialization and a parameter protection mechanism for continual adaptation, achieving superior accuracy and generalization on five real-world datasets compared to baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Q-learning, ensemble learning, satisficing, distillation, bounded rationality]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ünver Çiftçi</p>
</li>
<li class="">
<p><strong>institution:</strong> Tekirdağ Namık Kemal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22910" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22910</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a two-phase framework (Sat-EnQ) that first trains an ensemble of lightweight Q-networks using a satisficing objective to limit early value growth and reduce variance. 2. Provides theoretical proof that the satisficing objective induces bounded updates and cannot increase target variance, with a corollary for substantial reduction. 3. Demonstrates empirical results including significant variance reduction, elimination of catastrophic failures, robustness to noise, and improved compute efficiency compared to baseline methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d12c2382ed5ee9e4da47d1775097d950b626f676e5d3552cd0ff19b6c385b2a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d12c2382ed5ee9e4da47d1775097d950b626f676e5d3552cd0ff19b6c385b2a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the instability of deep Q-learning, especially early in training, by introducing Sat-EnQ. This framework first trains a satisficing ensemble of weak Q-learners to produce stable, low-variance estimates, then distills and fine-tunes the ensemble. The method significantly improves training reliability, robustness, and computational efficiency compared to standard approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Geometric Structural Knowledge Graph Foundation Model</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [knowledge graph reasoning], [structural foundation model, geometric attention, inductive link prediction, multi-head transformation, relational fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ling Xin, Mojtaba Nayyeri, Zahra Makki Nayeri, Steffen Staab</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Stuttgart, University of Southampton, Shahrood University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22931" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22931</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Gamma, a novel structural KG foundation model that replaces the single relational transformation with multiple parallel geometric transformations (real, complex, split-complex, dual). 2. Introduces a relational conditioned attention fusion mechanism with entropy regularization to adaptively fuse these geometric representations at the link level. 3. Provides a full formalization of the algebraic message functions and demonstrates through extensive experiments on 56 KGs that Gamma consistently outperforms the prior state-of-the-art (Ultra) in zero-shot inductive link prediction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1926af9f65861ace968c86c6c18e3eaa892e2114d0d505e3fce8a7ae39975f1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1926af9f65861ace968c86c6c18e3eaa892e2114d0d505e3fce8a7ae39975f1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies a key limitation in existing structural knowledge graph foundation models: their reliance on a single relational transformation limits their ability to capture diverse relational patterns. To address this, the authors propose Gamma, a new model that employs multi-head geometric attention, using parallel transformations from different algebraic spaces and a fusion mechanism to adaptively combine them. Comprehensive experiments show that Gamma outperforms the previous best model, Ultra, in zero-shot inductive link prediction across diverse benchmarks, demonstrating the benefit of complementary geometric representations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multiple Token Divergence: Measuring and Steering In-Context Computation Density</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [language model interpretability], [in-context computation, KL divergence, decoding method, computational effort, prediction head]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vincent Herrmann, Eric Alcaide, Michael Wand, Jürgen Schmidhuber</p>
</li>
<li class="">
<p><strong>institution:</strong> The Swiss AI Lab IDSIA/USI/SUPSI, King Abdullah University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22944" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22944</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Multiple Token Divergence (MTD), a simple and non-invasive metric to measure a language model&#x27;s in-context computational effort by comparing the output distributions of the full model and a shallow auxiliary head. 2. Introduces Divergence Steering, a novel decoding method that uses MTD to control the computational character of generated text. 3. Empirically demonstrates that MTD effectively distinguishes task complexity, correlates with problem difficulty, and that lower MTD is associated with more accurate reasoning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef98fdc1c198bde00876e87ff3cca172c3925998a10f9847042108bebc8c5e99_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef98fdc1c198bde00876e87ff3cca172c3925998a10f9847042108bebc8c5e99_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of measuring the in-context computational effort of language models. It proposes Multiple Token Divergence (MTD), a lightweight metric based on KL divergence between output distributions, and a corresponding decoding method called Divergence Steering. The authors show that MTD effectively correlates with task difficulty and can be used to analyze and steer model computation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] APO: Alpha-Divergence Preference Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning from human feedback (rlhf)], [alpha-divergence, preference optimization, mode collapse, anchored coordinates, gradient variance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wang Zixian</p>
</li>
<li class="">
<p><strong>institution:</strong> China Mobile Communications Group Shandong Co., Ltd. Tai’an Branch</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22953" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22953</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces APO, an anchored framework using Csiszár alpha-divergence to continuously interpolate between forward and reverse KL behavior for RLHF. 2. Derives unified gradient dynamics parameterized by alpha and analyzes gradient variance properties. 3. Proposes a practical reward-and-confidence-guarded alpha schedule to transition from mode-covering to mode-seeking behavior safely.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a407212dc95985ef8918d58e7c65f70fd3f6adf8764c95f85871cd1924b3528_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a407212dc95985ef8918d58e7c65f70fd3f6adf8764c95f85871cd1924b3528_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the trade-off between stable but under-exploitative mode-covering updates and high-reward but unstable mode-seeking updates in LLM alignment. It proposes APO, an anchored preference optimization framework that uses alpha-divergence to smoothly interpolate between these regimes via a guarded schedule. Experiments show APO achieves competitive performance while maintaining training stability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FLOW: A Feedback-Driven Synthetic Longitudinal Dataset of Work and Wellbeing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [synthetic data generation], [synthetic dataset, longitudinal data, feedback-driven simulation, behavioral modeling, benchmarking]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wafaa El Husseini</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated. Affiliation inferred from email domain (gmail) is insufficient. Likely independent or institutional affiliation not provided in the excerpt.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22956" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22956</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces FLOW, a novel synthetic longitudinal dataset modeling daily interactions between workload, lifestyle, and wellbeing. 2. Provides a configurable data generation tool for reproducible experimentation under adjustable assumptions. 3. Creates a publicly available, controlled experimental environment for methodological development and benchmarking where real-world data is inaccessible.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abeb0ef64c6a9a1178335cb2abf4717155e2b0e41b97fbf7173bb6448bdb8de9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abeb0ef64c6a9a1178335cb2abf4717155e2b0e41b97fbf7173bb6448bdb8de9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces FLOW, a synthetic longitudinal dataset generated via a rule-based, feedback-driven simulation to model daily interactions between work and wellbeing variables. It addresses the lack of accessible real-world data due to privacy and logistical constraints. The dataset and its configurable generation tool are released as a public resource to support reproducible research, methodological benchmarking, and education.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Context-Aware Temporal Modeling through Unified Multi-Scale Temporal Encoding and Hierarchical Sequence Learning for Single-Channel EEG Sleep Staging</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [biomedical signal processing], [multi-scale feature extraction, hierarchical BiLSTM, class-weighted loss, temporal modeling, sleep staging]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amirali Vakili, Salar Jahanshiri, Armin Salimi-Badr</p>
</li>
<li class="">
<p><strong>institution:</strong> Shahid Beheshti University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22976" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22976</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a context-aware and interpretable framework combining compact multi-scale feature extraction with hierarchical temporal modeling (BiLSTM) for single-channel EEG sleep staging. 2. Addresses class imbalance, especially for the N1 stage, using class-weighted loss functions and data augmentation techniques. 3. Introduces a sub-epoch chunking and probability averaging strategy to enhance contextual representation and robustness in predictions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c34e7add689f932eed2a6d2c02f530d1c09fc8e52104033716567656d05392d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c34e7add689f932eed2a6d2c02f530d1c09fc8e52104033716567656d05392d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a novel deep learning framework for automatic sleep staging using single-channel EEG. The method integrates multi-scale feature extraction with hierarchical sequence learning (BiLSTM) and employs strategies like chunk-based probability averaging to handle class imbalance and improve context modeling. The approach achieves state-of-the-art performance on the SleepEDF datasets, with a significant improvement in detecting the challenging N1 sleep stage, while maintaining model interpretability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Fusion or Confusion? Multimodal Complexity Is Not All You Need</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal training], [multimodal learning, late-fusion, hyperparameter tuning, empirical study, reliability checklist]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tillmann Rheude, Roland Eils, Benjamin Wild</p>
</li>
<li class="">
<p><strong>institution:</strong> Berlin Institute of Health at Charité - Universitätsmedizin Berlin, Intelligent Medicine Institute at Fudan University, Freie Universität Berlin</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22991" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22991</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A large-scale benchmark of 19 multimodal architectures under a unified experimental protocol across nine diverse datasets. 2. The proposal of SimBaMM, a simple late-fusion Transformer baseline, which performs comparably to more complex methods under standardized conditions. 3. The provision of a pragmatic reliability checklist to promote robust and trustworthy future evaluations in multimodal learning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d808ed59e8dc58198573816741433c5c0873c34c1d5fe67d63a8ffb9d40342a6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d808ed59e8dc58198573816741433c5c0873c34c1d5fe67d63a8ffb9d40342a6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper challenges the assumption that architectural complexity is necessary for performance in multimodal learning. Through a large-scale empirical study, it shows that a simple late-fusion Transformer baseline (SimBaMM) performs comparably to 19 more complex methods when all are rigorously tuned and evaluated under standardized conditions. The authors argue for a shift in research focus from architectural novelty to methodological rigor.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Merge before Forget: A Single LoRA Continual Learning via Continual Merging</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [continual learning, LoRA, catastrophic forgetting, orthogonal basis, parameter-efficient]</p>
</li>
<li class="">
<p><strong>authors:</strong> Fuli Qiao, Mehrdad Mahdavi</p>
</li>
<li class="">
<p><strong>institution:</strong> The Pennsylvania State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23017" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23017</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel continual learning method that sequentially merges LoRA updates into a single unified LoRA, maintaining constant memory complexity with respect to the number of tasks. 2. Introduces orthogonal basis extraction from previous LoRA to initialize new task learning, minimizing task interference. 3. Employs a time-aware scaling mechanism to balance new and old knowledge during merging, improving performance over asymmetric LoRA merging.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68eb3af6647a5e47f9615997c14929688be663c5428838d0271ab503cc1b8c78_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68eb3af6647a5e47f9615997c14929688be663c5428838d0271ab503cc1b8c78_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the issues of memory growth and task interference in LoRA-based continual learning for LLMs. It proposes a method that orthogonally initializes and sequentially merges LoRAs into a single LoRA, using a time-aware scaling mechanism. The approach demonstrates effectiveness and efficiency in mitigating catastrophic forgetting while maintaining constant memory usage.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [interpretability], [chain-of-thought, faithfulness, causal mediation analysis, biasing features, explainability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kerem Zaman, Shashank Srivastava</p>
</li>
<li class="">
<p><strong>institution:</strong> UNC Chapel Hill</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23032" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23032</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Argues that the Biasing Features metric conflates unfaithfulness with incompleteness in Chain-of-Thought explanations. 2. Introduces a new faithful@k metric showing increased token budgets improve hint verbalization. 3. Uses Causal Mediation Analysis to show non-verbalized hints can still causally mediate predictions through the CoT.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/527440e442abe55ce371c5ad3ce8f49609f0398a6001b33523b6a3aa4bbc6e44_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/527440e442abe55ce371c5ad3ce8f49609f0398a6001b33523b6a3aa4bbc6e44_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper challenges the use of hint-verbalization metrics like Biasing Features for evaluating the faithfulness of Chain-of-Thought reasoning. It proposes that apparent unfaithfulness is often due to incompleteness from lossy compression and tight token limits, not a lack of alignment, and demonstrates this using new metrics and causal mediation analysis. The conclusion advocates for a broader interpretability toolkit beyond hint-based evaluations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Mechanistic Analysis of Circuit Preservation in Federated Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [Mechanistic Interpretability, circuit collapse, weight sparsity, Intersection-over-Union, Non-IID data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Muhammad Haseeb, Salaar Masood, Muhammad Abdullah Sohail</p>
</li>
<li class="">
<p><strong>institution:</strong> Lahore University of Management Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23043" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23043</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/ha405/FedMI" target="_blank" rel="noopener noreferrer" class="">https://github.com/ha405/FedMI</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel mechanistic interpretability (MI) framework to analyze the internal failure mode of FedAvg under Non-IID data, introducing the concept of &quot;circuit collapse&quot;. 2. Demonstrates the use of inherently interpretable, weight-sparse neural networks to identify and track functional circuits across clients and communication rounds in FL. 3. Provides the first mechanistic evidence, quantified via Intersection-over-Union (IoU), that Non-IID data causes structural circuit divergence and degradation, reframing statistical drift as a failure of mechanistic preservation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34be0814c16f6dee307b048c588d94abf81104abdb1f1491d1c23901df46fc7d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34be0814c16f6dee307b048c588d94abf81104abdb1f1491d1c23901df46fc7d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates why Federated Learning (FedAvg) performance degrades under Non-IID data by applying Mechanistic Interpretability. The method uses weight-sparse networks to identify and track functional &quot;circuits&quot; across clients, measuring their preservation with IoU. The main conclusion is that Non-IID data causes &quot;circuit collapse&quot; due to conflicting updates, providing a mechanistic explanation for the accuracy drop.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PI-MFM: Physics-informed multimodal foundation model for solving partial differential equations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scientific machine learning], [physics-informed neural networks, multimodal foundation model, partial differential equations, multi-operator learning, zero-shot fine-tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Min Zhu, Jingmin Sun, Zecheng Zhang, Hayden Schaeffer, Lu Lu</p>
</li>
<li class="">
<p><strong>institution:</strong> Yale University, Johns Hopkins University, University of Notre Dame, University of California Los Angeles</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23056" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23056</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a physics-informed multimodal foundation model (PI-MFM) framework that directly enforces governing PDEs during both pretraining and adaptation, moving beyond purely data-driven approaches. 2. Introduces a method that takes symbolic PDE representations as input and automatically assembles PDE residual losses via vectorized derivative computation, enabling unified physics-informed training across diverse equation families. 3. Demonstrates effective zero-shot physics-informed fine-tuning to unseen PDE families, achieving low error using only PDE residuals and initial/boundary conditions without labeled solution data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db03e9853bb30e32977fcd1b0e24e9037354712e7841661bdbb52b955e3b2849_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db03e9853bb30e32977fcd1b0e24e9037354712e7841661bdbb52b955e3b2849_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes PI-MFM, a physics-informed multimodal foundation model framework for solving partial differential equations. It integrates governing equations directly into the training and adaptation process, enabling data-efficient and transferable learning of PDE solution operators. The method outperforms data-driven models, especially with sparse data, and shows strong zero-shot adaptation capabilities to new PDE families.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Reward Model Selection Crisis in Personalized Alignment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [alignment &amp; personalization], [reward-guided decoding, policy accuracy, Pref-LaMP benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Fady Rezk, Yuangang Pan, Chuan-Sheng Foo, Xun Xu, Nancy Chen, Henry Gouk, Timothy Hospedales</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Edinburgh, Agency for Science, Technology and Research (A*STAR)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23067" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23067</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and demonstrates the failure of standard reward model (RM) accuracy as a selection criterion for deployment-ready personalized alignment. 2. Introduces a new metric, policy accuracy, to evaluate the token-level discrimination ability of reward models under inference-time adaptation (reward-guided decoding). 3. Introduces Pref-LaMP, the first personalized alignment benchmark with ground-truth user completions, enabling direct behavioral evaluation and revealing a decoupling between reward discrimination and actual generation quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a0a1bd7d940db8c394f189ea84b7dbcfae7cd34b8e3662ea7c7a8babfdfefe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a0a1bd7d940db8c394f189ea84b7dbcfae7cd34b8e3662ea7c7a8babfdfefe_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies a crisis in personalized alignment, showing that optimizing reward models for preference ranking accuracy does not translate to effective behavioral adaptation under realistic deployment constraints like reward-guided decoding. The authors propose a new metric (policy accuracy) and a new benchmark (Pref-LaMP) to evaluate this gap, finding that reward model accuracy poorly predicts generation quality and that simple in-context learning often outperforms reward-guided methods for larger models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Breaking the Memory Wall: Exact Analytical Differentiation via Tiled Operator-Space Evolution</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [memory &amp; caching], [Selective State Space Models, analytical differentiation, memory complexity, Tiled Operator-Space Evolution, Phase Gradient Flow]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuhuan Wang, Yuzhen Xie, Jiayi Li, Yinliang Diao</p>
</li>
<li class="">
<p><strong>institution:</strong> South China Agricultural University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23068" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23068</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Phase Gradient Flow (PGF), a framework for computing exact analytical derivatives for SSMs without materializing the intermediate computational graph. 2. Proposes Tiled Operator-Space Evolution (TOSE) to reframe SSM dynamics, achieving O(1) memory complexity relative to sequence length. 3. Demonstrates significant practical improvements, including a 94% reduction in peak VRAM and a 23x throughput increase, enabling chromosome-scale sensitivity analysis on a single GPU.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/821c684e39ce34e6f8c4157ec1cc31189105864e7ee30f0d13f0b32203a73fc5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/821c684e39ce34e6f8c4157ec1cc31189105864e7ee30f0d13f0b32203a73fc5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the O(L) memory bottleneck in gradient-based sensitivity analysis for Selective State Space Models (SSMs). It proposes Phase Gradient Flow (PGF), which uses Tiled Operator-Space Evolution (TOSE) to compute exact analytical derivatives with O(1) memory complexity. This enables the handling of extreme-length sequences (e.g., 128,000 steps) on consumer hardware, significantly reducing memory usage and increasing throughput compared to standard backpropagation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [vision-language models], [Mask Fine-Tuning (MFT), Parameter Efficient Fine-Tuning (PEFT), Low-Rank Adaptation (LoRA), structural reparameterization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mingyuan Zhang, Yue Bai, Yifan Wang, Yiyang Huang, Yun Fu</p>
</li>
<li class="">
<p><strong>institution:</strong> Northeastern University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23073" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23073</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Ming-K9/MFT-VLM" target="_blank" rel="noopener noreferrer" class="">https://github.com/Ming-K9/MFT-VLM</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes applying Mask Fine-Tuning (MFT), a structural reparameterization method, to Vision-Language Models (VLMs) for adaptation., 2. Demonstrates that MFT, which learns gating scores for weights instead of updating them, consistently outperforms LoRA variants and full fine-tuning., 3. Reveals that effective adaptation can emerge from reestablishing connections within a model&#x27;s existing knowledge, not just from updating weights.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92b8916bd08492d9f9fbfd3b3a163d12c7de7a6a1fe0822bc6d711ca118dfb28_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92b8916bd08492d9f9fbfd3b3a163d12c7de7a6a1fe0822bc6d711ca118dfb28_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper rethinks fine-tuning for Vision-Language Models (VLMs) by applying Mask Fine-Tuning (MFT), a method that learns to gate existing weights instead of updating them. Experiments show MFT surpasses both Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA and full fine-tuning, demonstrating that reorganizing internal subnetworks is a powerful alternative to weight updates.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FLEX-MoE: Federated Mixture-of-Experts with Load-balanced Expert Assignment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [Mixture-of-Experts, Federated Learning, Load Balancing, Expert Assignment, Non-IID Data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Boyang Zhang, Xiaobing Chen, Songyang Zhang, Shuai Zhang, Xiangwei Zhou, Mingxuan Sun</p>
</li>
<li class="">
<p><strong>institution:</strong> The affiliations are not explicitly listed in the provided content. Based on the author names and common patterns, it is likely from a Chinese university or research institute (e.g., Tsinghua University, Peking University, Chinese Academy of Sciences). A specific institution cannot be reliably inferred.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23070" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23070</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FLEX-MoE, a novel federated MoE framework that jointly optimizes expert assignment and load balancing under limited client capacity. 2. Introduces client-expert fitness scores to quantify expert suitability for local datasets using training feedback. 3. Employs an optimization-based algorithm to maximize client-expert specialization while enforcing balanced expert utilization system-wide.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4afb2d6d1d993854d25c34b1c5c48d2b0479953fa05feac83a247dd11404202_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4afb2d6d1d993854d25c34b1c5c48d2b0479953fa05feac83a247dd11404202_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenges of deploying Mixture-of-Experts models in Federated Learning, specifically resource constraints on edge devices and expert load imbalance caused by non-IID data. It proposes FLEX-MoE, a framework that uses client-expert fitness scores and an optimization algorithm to assign experts to clients for specialization while balancing system-wide expert utilization. Experiments on three datasets show that FLEX-MoE achieves superior performance and maintains balanced expert utilization in resource-constrained scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Trust Region Masking for Long-Horizon LLM Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [trust region, policy gradient, off-policy mismatch, KL divergence, sequence-level masking]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yingru Li, Jiacai Liu, Jiawei Xu, Yuxuan Tong, Ziniu Li, Baoxiang Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> (Institutions not explicitly listed in provided content; inferred from author names and common affiliations in the field, but not specified. Therefore, output is left blank.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23075" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23075</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Deriving two novel, tighter theoretical bounds (Pinsker-Marginal and Mixed) on the approximation error in off-policy LLM-RL, scaling better with sequence length than classical O(T^2) bounds. 2. Identifying that these bounds depend on a sequence-level quantity (maximum token-level KL divergence) that cannot be controlled by token-independent methods like PPO clipping. 3. Proposing the Trust Region Masking (TRM) algorithm, which masks entire sequences from gradient updates to enforce the trust region, providing non-vacuous monotonic improvement guarantees for long-horizon tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74d8872516a568f2933a83e36b0cf25d414f3a25ffa113cd0cee809d2b39c6ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74d8872516a568f2933a83e36b0cf25d414f3a25ffa113cd0cee809d2b39c6ac_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that classical trust region bounds become vacuous for long-horizon LLM reinforcement learning due to unavoidable off-policy mismatch. It proposes Trust Region Masking (TRM), a method that excludes entire sequences from gradient computation if any token violates a trust region constraint. This approach, supported by new tighter theoretical bounds, provides the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multimodal Functional Maximum Correlation for Emotion Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal learning], [self-supervised learning, dual total correlation, functional maximum correlation analysis, affective computing, physiological signals]</p>
</li>
<li class="">
<p><strong>authors:</strong> Deyang Zheng, Tianyi Zhang, Wenming Zheng, Shujian Yu</p>
</li>
<li class="">
<p><strong>institution:</strong> Southeast University, Westlake University, Vrije Universiteit Amsterdam</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23076" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23076</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/DY9910/MFMC" target="_blank" rel="noopener noreferrer" class="">https://github.com/DY9910/MFMC</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel self-supervised learning framework (MFMC) that maximizes higher-order multimodal dependence using a Dual Total Correlation objective. 2. Derives a tight sandwich bound and optimizes it using a functional maximum correlation analysis-based trace surrogate to capture joint interactions. 3. Demonstrates state-of-the-art or competitive performance on affective computing benchmarks, showing robustness to inter-subject variability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369b23e1c7a17085940402e88d2347afb3386819a237c48ac347be73127aea2a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369b23e1c7a17085940402e88d2347afb3386819a237c48ac347be73127aea2a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of learning joint dynamics from scarce and subjective emotion labels by proposing a self-supervised learning framework called MFMC. It captures higher-order multimodal dependencies beyond pairwise alignment, leading to improved emotion recognition performance on physiological signal benchmarks. The results show significant accuracy gains, particularly in subject-independent settings, highlighting the method&#x27;s effectiveness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [reinforcement learning, training-inference mismatch, vocabulary pruning, gradient estimation, numerical stability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yingru Li, Jiawei Xu, Jiacai Liu, Yuxuan Tong, Ziniu Li, Tianle Cai, Ge Zhang, Qian Liu, Baoxiang Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> (Institutions not explicitly listed in provided content. Affiliation inference requires author list with affiliations or email domains, which are not present in the given text. Therefore, cannot be determined from the provided snippet.)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23087" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23087</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proves that the training-inference mismatch in LLM RL has an asymmetric effect, where the bound on log-probability mismatch scales with (1-p), making low-probability &quot;tail&quot; tokens the primary source of instability. 2. Proposes a novel method to stabilize RL training by dynamically pruning the vocabulary to exclude the extreme tail tokens, trading large, biased mismatches for a small, bounded optimization bias. 3. Provides both empirical demonstration of stable training and a theoretical bound on the optimization bias introduced by the proposed vocabulary pruning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83b750895381feb238b14991a4015088fa8d05eb24ab0374082f6c25fb3ddd7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83b750895381feb238b14991a4015088fa8d05eb24ab0374082f6c25fb3ddd7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a fundamental training-inference mismatch in LLM reinforcement learning caused by differing numerical precision between high-throughput inference and stable training systems. To address this, the authors propose dynamically pruning low-probability &quot;tail&quot; tokens from the vocabulary during RL optimization, which stabilizes training by replacing large, biased errors with a small, bounded bias. Both theoretical analysis and empirical results support the effectiveness of this method.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [Imitation Learning, Reinforcement Learning, KL divergence, Dense Gradient, Sparse Gradient]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yingru Li, Ziniu Li, Jiacai Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated in provided content.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23097" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23097</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Derives the exact gradient decomposition of a unified KL+reward objective into analytic Dense and sampled Sparse terms. 2. Provides an efficient logit-level gradient formula for GPU implementation. 3. Establishes mathematical equivalence to KL-regularized RLHF and discusses training curriculum implications.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c49fbea41eedc7a9f58604cc114a9246db61882fc20a851c8ec68a24ff6b343_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c49fbea41eedc7a9f58604cc114a9246db61882fc20a851c8ec68a24ff6b343_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a unified framework for fine-tuning LLMs that integrates Imitation Learning and Reinforcement Learning. It analyzes the gradient of a combined objective to decompose it into a token-level Dense Gradient and a long-horizon Sparse Gradient, enabling efficient implementation. The work clarifies its relationship to existing methods like RLHF and discusses practical training considerations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [reinforcement learning, vision-language model, supervised fine-tuning, generalization paradox, cross-dataset transferability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Armin Berger, Manuela Bergau, Helen Schneider, Saad Ahmad, Tom Anglim Lagones, Gianluca Brugnara, Martha Foltyn-Dumitru, Kai Schlamp, Philipp Vollmuth, Rafet Sifa</p>
</li>
<li class="">
<p><strong>institution:</strong> Fraunhofer IAIS, University of Bonn, Lamarr Institute, Department of Health Queensland, Griffith University, University Hospital Bonn</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23090" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23090</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced ChexReason, a resource-efficient vision-language model for medical imaging trained with an R1-style (SFT+GRPO) method using minimal data and compute. 2. Identified a fundamental tension where RL optimization (GRPO) improves in-distribution benchmark performance but significantly degrades cross-dataset generalization, a pattern also observed in high-resource models. 3. Discovered a generalization paradox where the SFT checkpoint uniquely improves cross-dataset performance, suggesting teacher-guided reasoning captures more institution-agnostic features than RL optimization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c87b0b1a571aa28f5dd9685e96b13ff3420ed36b0e1d569fff8b1394d564751f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c87b0b1a571aa28f5dd9685e96b13ff3420ed36b0e1d569fff8b1394d564751f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper investigates applying reinforcement learning (RL) to vision-language models for medical imaging, finding that while RL improves performance on the training benchmark, it harms the model&#x27;s ability to generalize to new datasets. The authors conclude that for clinical robustness, curated supervised fine-tuning may be more effective than aggressive RL optimization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Osmotic Learning: A Self-Supervised Paradigm for Decentralized Contextual Data Representation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [self-supervised learning, representation learning, distributed learning, decentralized clustering, contextual data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mario Colosi, Reza Farahani, Maria Fazio, Radu Prodan, Massimo Villari</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Messina, University of Klagenfurt, University of Innsbruck</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23096" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23096</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Osmotic Learning (OSM-L), a novel self-supervised paradigm for learning from distributed data without raw data exchange. 2. Proposes an &quot;osmosis&quot; process that aligns local representations to converge to a dynamic equilibrium, capturing contextual patterns. 3. Demonstrates that OSM-L functions as a decentralized clustering mechanism, identifying correlated data groups during training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2b452fb94443ab9846af33524787f0bc6c709b6e90ae3be4e653738c6fe592b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2b452fb94443ab9846af33524787f0bc6c709b6e90ae3be4e653738c6fe592b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Osmotic Learning (OSM-L), a self-supervised distributed learning paradigm that extracts higher-level latent knowledge from decentralized data sources without sharing raw data. It achieves this through an iterative &quot;osmosis&quot; process that aligns local representations to converge to a contextual equilibrium, also enabling decentralized clustering. Experimental results show OSM-L achieves high accuracy in local information alignment while preserving contextual integrity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] How Much Data Is Enough? Uniform Convergence Bounds for Generative &amp; Vision-Language Models under Low-Dimensional Structure</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [statistical learning theory], [uniform convergence, calibration, low-dimensional structure, vision-language models, sample complexity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Paul M. Thompson</p>
</li>
<li class="">
<p><strong>institution:</strong> Stevens Institute of Neuroimaging and Informatics, University of Southern California</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23109" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23109</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides finite-sample uniform convergence bounds for accuracy and calibration of VLM-induced classifiers under Lipschitz stability assumptions. 2. Derives sample complexity bounds that depend on the intrinsic/effective dimension of the embedding space, not the ambient dimension. 3. Offers spectrum-dependent bounds that explicitly link eigenvalue decay in embedding covariance to data requirements, explaining reliable generalization with fewer samples.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dca56fb1537f7d92cc10d66ba9adb9e3272fcc872fe5fdbf475ccf92cc17e24_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dca56fb1537f7d92cc10d66ba9adb9e3272fcc872fe5fdbf475ccf92cc17e24_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies when generative and vision-language models can achieve uniformly accurate and calibrated predictions with practical sample sizes. By assuming model outputs depend smoothly on a low-dimensional semantic representation, it derives finite-sample uniform convergence bounds for VLM-induced classifiers. The main conclusion is that sample complexity depends on intrinsic dimension and eigenvalue decay, providing a framework to assess data sufficiency for reliable biomedical predictions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [regression], [squeeze and excitation, channel attention, residual connections, multi-layer perceptron, penetration acceleration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yankang Li, Changsheng Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanjing University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23131" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23131</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed SE-MLP, a novel MLP architecture integrating a channel attention mechanism for feature prediction. 2. Incorporated residual connections into the MLP framework to enhance model stability and performance. 3. Demonstrated the model&#x27;s superior accuracy, generalization, and engineering applicability for rapidly predicting penetration acceleration features.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0dab35d2c00d22564e8f3e36c067728bb8b4d1bb60f2ed675bfe34288c07503a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0dab35d2c00d22564e8f3e36c067728bb8b4d1bb60f2ed675bfe34288c07503a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes SE-MLP, a multi-layer perceptron model enhanced with squeeze-and-excitation channel attention and residual connections, to rapidly predict prior acceleration features for penetration signals. The model establishes a nonlinear mapping from physical parameters to acceleration features, outperforming baseline models like MLP, XGBoost, and Transformer in accuracy and stability. The results validate its feasibility and provide a practical basis for engineering applications in penetration fuse design.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning from human feedback (rlhf)], [preference optimization, direct preference optimization, self-reflection, invariance, bradley-terry model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yu Li, Tian Lan, Zhengling Qi</p>
</li>
<li class="">
<p><strong>institution:</strong> George Washington University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23126" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23126</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies two fundamental limitations of DPO: lack of invariance to modeling choices and theoretical suboptimality due to ignoring comparative information in pairwise data. 2. Proposes Intrinsic Self-reflective Preference Optimization (InSPO), a novel family of methods that derives a globally optimal policy conditioned on both context and alternative responses, formalizing self-reflection. 3. Theoretically demonstrates InSPO&#x27;s superiority over DPO/RLHF and its invariance properties, and practically shows it as a plug-and-play enhancement that improves win rates and length-controlled metrics without inference overhead.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4933654befdce9d244a4f36811432e84021ec775f760f24a3cb71dec1951db76_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4933654befdce9d244a4f36811432e84021ec775f760f24a3cb71dec1951db76_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies limitations in Direct Preference Optimization (DPO), such as its sensitivity to modeling choices and failure to use comparative data fully. It proposes InSPO, a method that conditions the policy on both the context and the alternative response to enable intrinsic self-reflection. Experiments show InSPO consistently improves model alignment and robustness as a plug-and-play enhancement to DPO-family algorithms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [machine learning security], [TTPs, threat graph, multi-agent RAG, model stealing, jailbreaking]</p>
</li>
<li class="">
<p><strong>authors:</strong> Armstrong Foundjem, Lionel Nganyewou Tidjon, Leuson Da Silva, Foutse Khomh</p>
</li>
<li class="">
<p><strong>institution:</strong> Polytechnique Montréal (based on author affiliations and sMIEEE notation)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23132" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23132</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a large-scale empirical analysis of ML security, extracting 93 distinct threats from multiple sources including real-world incidents and code repositories. 2. Developed a multi-agent RAG system to automatically build an ontology-driven threat graph linking TTPs, vulnerabilities, and lifecycle stages from over 300 articles. 3. Identified unreported threats and dominant attack patterns (e.g., commercial LLM API model stealing, preference-guided jailbreaks) and highlighted vulnerability clusters in ML libraries with poor patch propagation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3da226bc9e7639392b95f6f00808f162580d1a90050c1261b3a0703259e42cf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3da226bc9e7639392b95f6f00808f162580d1a90050c1261b3a0703259e42cf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper characterizes modern security risks in AI systems by analyzing threats from multiple sources and using a multi-agent RAG system to construct a threat graph. The analysis uncovers unreported attack vectors and dominant TTPs, concluding that adaptive, ML-specific security frameworks are urgently needed to mitigate supply-chain and inference-time risks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Principled Algorithms for Optimizing Generalized Metrics in Binary Classification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [binary classification], [metric optimization, H-consistency, surrogate loss, cost-sensitive learning, METRO]</p>
</li>
<li class="">
<p><strong>authors:</strong> Anqi Mao, Mehryar Mohri, Yutao Zhong</p>
</li>
<li class="">
<p><strong>institution:</strong> Courant Institute of Mathematical Sciences (NYU), Google Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23133" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23133</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Reformulated the optimization of generalized classification metrics (e.g., Fβ, Jaccard) as a generalized cost-sensitive learning problem. 2. Designed novel surrogate loss functions with provable H-consistency guarantees for this problem. 3. Developed the METRO algorithm with strong finite-sample generalization bounds, offering a principled alternative to existing threshold-based methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57d73e082d311622a2b2e8ab3bd4da18cd359831ac4e0139fd4555ea2ba93861_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57d73e082d311622a2b2e8ab3bd4da18cd359831ac4e0139fd4555ea2ba93861_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of directly optimizing non-decomposable binary classification metrics like the Fβ-measure. The authors propose a principled framework that reformulates metric optimization as a cost-sensitive learning problem, leading to new surrogate losses and the METRO algorithm. Experiments show the proposed method is effective and outperforms prior baseline approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Graph Neural Networks with Transformer Fusion of Brain Connectivity Dynamics and Tabular Data for Forecasting Future Tobacco Use</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [graph neural networks, transformer, dynamic functional connectivity, longitudinal fMRI, multimodal fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Runzhi Zhou, Xi Luo</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Texas Health Science Center at Houston</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23137" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23137</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel time-aware Graph Neural Network model with Transformer Fusion (GNN-TF) for integrating non-Euclidean brain connectivity and Euclidean tabular data. 2. Introduces an end-to-end framework that leverages the temporal order of longitudinal data for forecasting future clinical outcomes. 3. Demonstrates superior predictive performance for forecasting future tobacco use compared to established machine learning and deep learning models on a longitudinal fMRI dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6106dd6744068e14ed98be012be1d450afa8c685980d7b00ae696592ebd7665_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6106dd6744068e14ed98be012be1d450afa8c685980d7b00ae696592ebd7665_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces GNN-TF, a time-aware model that integrates dynamic brain connectivity graphs and tabular data using a transformer for fusion, to forecast future tobacco use. It is evaluated on longitudinal fMRI data from the NCANDA study and is shown to outperform other state-of-the-art methods in predictive accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Weak Signal Learning Dataset and Its Baseline Method</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [weak signal learning], [weak signal learning, dual-view representation, class imbalance, low SNR, multi-source complementarity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xianqi Liu, Xiangru Li, Lefeng He, Ziyu Fang</p>
</li>
<li class="">
<p><strong>institution:</strong> South China Normal University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23160" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23160</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Constructed the first specialized dataset for weak signal feature learning, featuring low SNR dominance and extreme class imbalance., 2. Proposed a dual-view representation (vector + time-frequency map) and the PDVFN model tailored for low SNR, distribution skew, and dual imbalance., 3. Established a foundational benchmark for future weak signal learning (WSL) research, demonstrating improved accuracy and robustness in challenging scenarios.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/426f082649cd556014218d670accdb2545dfd625d214b420b39fcce46739f01d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/426f082649cd556014218d670accdb2545dfd625d214b420b39fcce46739f01d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of weak signal learning (WSL) by constructing the first dedicated dataset with low SNR and class imbalance, and proposes a dual-view PDVFN model to extract complementary features. The method shows higher accuracy and robustness in handling weak signals, noise, and imbalance. This work provides a dataset, baseline model, and foundation for future WSL research.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Diffusion-based Decentralized Federated Multi-Task Representation Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [decentralized learning, multi-task representation learning, projected gradient descent, diffusion-based consensus, sample complexity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Donghwa Kang, Shana Moothedath</p>
</li>
<li class="">
<p><strong>institution:</strong> Iowa State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23161" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23161</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel decentralized projected gradient descent-based algorithm for multi-task representation learning using a diffusion-based communication strategy. 2. Provides theoretical guarantees, including a lower bound on sample complexity and an upper bound on iteration complexity for the proposed algorithm. 3. Demonstrates through analysis and simulations that the algorithm is fast, communication-efficient, and outperforms benchmark methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb19ab164bf383bc246da4c65f212d69031d00619ab4c384a67af532a30b00e2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb19ab164bf383bc246da4c65f212d69031d00619ab4c384a67af532a30b00e2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of decentralized multi-task representation learning, where multiple linear regression models share a common low-dimensional representation across a network of nodes. The authors propose a diffusion-based decentralized algorithm using alternating projected gradient descent and minimization to recover the shared feature matrix. Theoretical analysis proves the algorithm&#x27;s efficiency in terms of sample and iteration complexity, and numerical simulations validate its superior performance compared to benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Evaluating Parameter Efficient Methods for RLVR</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Parameter-Efficient Fine-Tuning, Reinforcement Learning with Verifiable Rewards, LoRA, Spectral Collapse, Mathematical Reasoning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qingyu Yin, Yulun Wu, Zhennan Shen, Sunbowen Li, Zhilin Wang, Yanshu Li, Chak Tou Leong, Jiale Kang, Jinjin Gu</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, HKUST, WUST, USTC, Brown University, Hong Kong Polytechnic University, INSAIT</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23165" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23165</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted the first comprehensive evaluation of over 12 PEFT methods for RLVR, challenging the default use of standard LoRA. 2. Identified that structural PEFT variants (DoRA, AdaLoRA, MiSS) consistently outperform LoRA in this setting. 3. Discovered and explained the failure of SVD-informed initialization methods (e.g., PiSSA) due to a &quot;spectral collapse&quot; phenomenon and misalignment with RL optimization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87ad6f372a6a1a3b13e37f8468a6816e52a585402f7e4505a01391ffaed0621c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87ad6f372a6a1a3b13e37f8468a6816e52a585402f7e4505a01391ffaed0621c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper systematically evaluates Parameter-Efficient Fine-Tuning (PEFT) methods for Reinforcement Learning with Verifiable Rewards (RLVR) on mathematical reasoning tasks. It finds that structural variants like DoRA outperform standard LoRA, while SVD-based methods fail due to spectral collapse, and extreme parameter reduction bottlenecks performance. The work provides a guide for selecting PEFT methods in RLVR.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [LLM planning, Monte Carlo Tree Search (MCTS), multi-agent architecture, symbolic reasoning, self-correction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yifan Zhang, Giridhar Ganapavarapu, Srideepika Jayaraman, Bhavna Agrawal, Dhaval Patel, Achille Fokoue</p>
</li>
<li class="">
<p><strong>institution:</strong> IBM T.J. Watson Research Center, Vanderbilt University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23167" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23167</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/IBM/SPIRAL" target="_blank" rel="noopener noreferrer" class="">https://github.com/IBM/SPIRAL</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces SPIRAL, a novel framework that embeds a cognitive architecture of three specialized LLM agents (Planner, Simulator, Critic) into an MCTS loop for planning. 2. Transforms MCTS from a brute-force search into a guided, self-correcting reasoning process by leveraging dense, semantic-aware feedback from the agents. 3. Demonstrates superior performance and token efficiency on benchmark datasets (e.g., DailyLifeAPIs) compared to Chain-of-Thought and other state-of-the-art planning agents.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c24b184565ce8e4c4a35d80f46a857779e111625dc4ea57e56333bef27bea7e6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c24b184565ce8e4c4a35d80f46a857779e111625dc4ea57e56333bef27bea7e6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of LLMs struggling with complex planning tasks due to linear reasoning and lack of self-correction. It proposes SPIRAL, a framework that integrates three specialized LLM agents into a Monte Carlo Tree Search loop to create a guided, reflective, and grounded planning process. The method significantly outperforms existing planning approaches in accuracy and efficiency on benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [vertical federated learning, machine unlearning, primal-dual optimization, sample unlearning, label unlearning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yu Jiang, Xindi Tong, Ziyao Liu, Xiaoxi Zhang, Kwok-Yan Lam, Chee Wei Tan</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanyang Technological University, Sun Yat-sen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23171" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23171</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FedORA, a primal-dual optimization framework for sample and label unlearning in Vertical Federated Learning (VFL). 2. Introduces a new unlearning loss function that promotes classification uncertainty instead of misclassification. 3. Employs an adaptive step size and an asymmetric batch design to enhance stability and reduce computational costs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05dcd254c3941fc5cbf6bc3c063f2a726b8607659a3f7b4a526ad900e9e2b5de_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05dcd254c3941fc5cbf6bc3c063f2a726b8607659a3f7b4a526ad900e9e2b5de_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of data removal (unlearning) in Vertical Federated Learning (VFL), where different parties hold different features of the same data samples. The authors propose FedORA, a method that formulates unlearning as a constrained optimization problem solved via a primal-dual algorithm. Experiments show FedORA achieves unlearning effectiveness and model utility comparable to retraining from scratch, but with lower computational and communication costs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [molecular language modeling], [HELM notation, DeBERTa, cyclic peptide, membrane permeability, peptide-protein interaction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Seungeon Lee, Takuto Koyama, Itsuki Maeda, Shigeyuki Matsumoto, Yasushi Okuno</p>
</li>
<li class="">
<p><strong>institution:</strong> Kyoto University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23175" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23175</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes HELM-BERT, the first encoder-based peptide language model trained on HELM notation, designed to capture hierarchical dependencies. 2. Pre-trains the model on a curated corpus of 39,079 chemically diverse linear and cyclic peptides. 3. Demonstrates superior performance over SMILES-based models in downstream tasks like cyclic peptide membrane permeability and peptide-protein interaction prediction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83ec939e21fe471473f433077555782bca673e92ad1bfd3578b0fe729e20446_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83ec939e21fe471473f433077555782bca673e92ad1bfd3578b0fe729e20446_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces HELM-BERT, a transformer model based on DeBERTa and trained on HELM notation to better represent therapeutic peptides. It shows that this approach significantly outperforms existing SMILES-based models in predicting key peptide properties, demonstrating the data-efficiency advantages of topology-aware representations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Machine Learning-Assisted Vocal Cord Ultrasound Examination: Project VIPR</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [vocal cord ultrasound, image segmentation, VIPRnet, vocal cord paralysis, classification model]</p>
</li>
<li class="">
<p><strong>authors:</strong> Will Sebelik-Lassiter, Evan Schubert, Muhammad Alliyu, Quentin Robbins, Excel Olatunji, Mustafa Barry</p>
</li>
<li class="">
<p><strong>institution:</strong> Milwaukee School of Engineering, Emory University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23177" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23177</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a machine learning pipeline for automated analysis of vocal cord ultrasound (VCUS) videos. 2. Created a segmentation model to automatically identify vocal cords in ultrasound images with 96% validation accuracy. 3. Proposed VIPRnet, a classification model to distinguish normal vocal cords from vocal cord paralysis (VCP) with 99% validation accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5abcb457a7fbefffa7d7836af553a8db8fa248fdd34a0459a027299b3ce2ce44_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5abcb457a7fbefffa7d7836af553a8db8fa248fdd34a0459a027299b3ce2ce44_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a machine learning-assisted system to automate the analysis of vocal cord ultrasound (VCUS) to reduce operator dependency. The method involves segmenting the vocal cords and classifying them as normal or paralyzed using models trained on frames from volunteer videos. The results show high validation accuracy (96% for segmentation, 99% for classification), indicating promise for improving diagnostic accuracy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PGOT: A Physics-Geometry Operator Transformer for Complex PDEs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [neural operator learning], [Physics-Geometry Operator Transformer, Spectrum-Preserving Geometric Attention, geometric aliasing, linear complexity, spatially adaptive routing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhuo Zhang, Xi Yang, Yuan Zhao, Canqun Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Defense Technology, National SuperComputer Center in Tianjin</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23192" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23192</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes PGOT, a novel transformer architecture designed to reconstruct physical feature learning through explicit geometry awareness for solving PDEs on complex geometries. 2. Introduces Spectrum-Preserving Geometric Attention (SpecGeo-Attention), which uses a physics slicing-geometry injection mechanism to incorporate multi-scale geometric encodings, preserving critical boundary information while maintaining linear computational complexity. 3. Implements a dynamic routing mechanism that adaptively selects low-order linear paths for smooth regions and high-order non-linear paths for discontinuities, enabling high-precision, spatially adaptive modeling.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1247f066008cc6ba78670544cbf446b4e7c3e18fb9b55a53416b7a831c93e80f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1247f066008cc6ba78670544cbf446b4e7c3e18fb9b55a53416b7a831c93e80f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of modeling PDEs on large-scale unstructured meshes with complex geometries using transformers, where efficient architectures often lose critical boundary information due to geometric aliasing. It proposes the Physics-Geometry Operator Transformer (PGOT), which introduces a geometry-aware attention mechanism and adaptive computational routing to preserve multi-scale features and model shocks precisely. PGOT achieves state-of-the-art performance on standard benchmarks and excels in large-scale industrial design tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Simple, Optimal and Efficient Algorithm for Online Exp-Concave Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [online learning], [Online Newton Step, Mahalanobis projection, regret minimization, exp-concave optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yi-Han Wang, Peng Zhao, Zhi-Hua Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> National Key Laboratory for Novel Software Technology, Nanjing University; School of Artificial Intelligence, Nanjing University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23190" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23190</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes LightONS, a simple variant of ONS that reduces computational cost by delaying expensive Mahalanobis projections via a hysteresis mechanism. 2. Achieves optimal O(d log T) regret with a total runtime of O(d²T + d^ω √(T log T)), improving over ONS&#x27;s O(d^ω T) runtime. 3. Provides an SXO algorithm with runtime ~O(d³/ε), solving the COLT&#x27;13 open problem posed by Koren.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2632ee5796bfd9a4795aa8e1212d8f65e7f8732a96264f4ae4629a2216e3e5ba_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2632ee5796bfd9a4795aa8e1212d8f65e7f8732a96264f4ae4629a2216e3e5ba_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the computational bottleneck of the Online Newton Step (ONS) algorithm for online exp-concave optimization, where the Mahalanobis projection step is costly. The authors propose LightONS, a simple variant that introduces a hysteresis mechanism to delay expensive projections, preserving optimal regret while significantly reducing runtime. This leads to an efficient stochastic optimization method that resolves a long-standing open problem.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Energy and Memory-Efficient Federated Learning With Ordered Layer Freezing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [Ordered Layer Freezing, Tensor Operation Approximation, Non-IID Data, Edge Computing, Model Compression]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ziru Niu, Hai Dong, A.K. Qin, Tao Gu, Pengcheng Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> RMIT University, Swinburne University of Technology, Macquarie University, Hohai University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23200" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23200</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed Federated Learning with Ordered Layer Freezing (FedOLF), a method that freezes model layers in a predefined order before training to reduce computation and memory requirements. 2. Introduced Tensor Operation Approximation (TOA) as a lightweight alternative to conventional quantization to further reduce communication and energy costs while preserving model accuracy. 3. Demonstrated superior performance of FedOLF over non-IID data across multiple datasets and model architectures, achieving higher accuracy, energy efficiency, and lower memory footprint compared to existing works.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cf67406a118e6d8eff8ded5d05e3b12c6eeded41300a0b8517fec19cbc8d930_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cf67406a118e6d8eff8ded5d05e3b12c6eeded41300a0b8517fec19cbc8d930_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces FedOLF, a federated learning method that freezes model layers in a predefined order to reduce resource demands on edge devices, combined with a Tensor Operation Approximation technique for efficient communication. The proposed approach is shown to achieve higher accuracy and better energy/memory efficiency than existing methods when training on non-IID data across several benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Anka: A Domain-Specific Language for Reliable LLM Code Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [Domain-Specific Language, Constrained Syntax, Code Generation, Data Transformation Pipeline, In-Context Learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Saif Khalfan Saif Al Mazrouei</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Wisconsin-Madison</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23214" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23214</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced Anka, a domain-specific language (DSL) with explicit, constrained syntax designed to reduce ambiguity in LLM code generation. 2. Demonstrated that LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy without prior training. 3. Showed that purposefully designed DSLs can outperform general-purpose languages (e.g., Python) on complex multi-step tasks, significantly reducing errors in operation sequencing and state management.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6e450f3b6354a05c4c0dfa0c22c4f8b8dfc33c08282380080deb2d2f3a335d4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6e450f3b6354a05c4c0dfa0c22c4f8b8dfc33c08282380080deb2d2f3a335d4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper hypothesizes that the flexibility of general-purpose languages leads to systematic errors in LLM code generation for complex tasks. To test this, it introduces Anka, a constrained DSL for data transformation pipelines. The results show that LLMs can learn Anka from prompts and achieve significantly higher accuracy on multi-step tasks compared to Python, demonstrating the advantage of constrained syntax for reliable code generation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [graph federated learning, fairness, overlapping subgraphs, privacy-preserving, weighted aggregation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zihao Zhou, Shusen Yang, Fangyuan Zhao, Xuebin Ren</p>
</li>
<li class="">
<p><strong>institution:</strong> Xi&#x27;an Jiaotong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23235" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23235</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Uncover and theoretically analyze the unfairness issue in graph federated learning caused by imbalanced overlapping subgraphs across clients. 2. Propose FairGFL, a novel algorithm that uses a privacy-preserving estimation of overlapping ratios and an interpretable weighted aggregation approach to enhance cross-client fairness. 3. Improve the tradeoff between model utility and fairness by integrating a carefully crafted regularizer into the federated composite loss function.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c67614889dfdf1e0e6de4fd0bd950e8649eb4d988fb1661c29ef6c14b73bba25_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c67614889dfdf1e0e6de4fd0bd950e8649eb4d988fb1661c29ef6c14b73bba25_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a fairness problem in graph federated learning when client subgraphs overlap in an imbalanced way. To solve this, it proposes FairGFL, a method that uses privacy-preserving overlap estimation and a fairness-aware regularizer to balance utility and fairness. Experiments show FairGFL outperforms baselines in both utility and fairness on benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [gpu kernels], [agentic kernel coding, heterogeneous accelerators, retrieval-augmented prompt synthesis, graph-based search, Triton/CuTe DSL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gang Liao, Hongsen Qin, Ying Wang, Alicia Golden, Michael Kuchnik, Yavuz Yetim, Jia Jiunn Ang, Chunli Fu, Yihan He, Samuel Hsia, Zewei Jiang, Dianshi Li, Uladzimir Pashkevich, Varna Puvvada, Feng Shi, Matt Steiner, Ruichao Xiao, Nathan Yan, Xiayu Yu, Zhou Fang, Abdul Zainul-Abedin, Ketan Singh, Hongtao Yu, Wenyuan Chi, Barney Huang, Sean Zhang, Noah Weller, Zach Marine, Wyatt Cook, Carole-Jean Wu, Gaoxiang Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Meta Platforms</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23236" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23236</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes KernelEvolve, an agentic framework that automates kernel generation and optimization for DLRMs across heterogeneous hardware (NVIDIA/AMD GPUs, Meta accelerators) by operating at multiple programming abstractions. 2. Introduces a kernel optimization process modeled as a graph-based search with dynamic adaptation to runtime context via retrieval-augmented prompt synthesis and a persistent hardware knowledge base. 3. Demonstrates the system&#x27;s effectiveness by achieving 100% correctness on benchmark suites and substantial performance speedups (up to 17x) in production, reducing development time from weeks to hours and lowering the programmability barrier for new AI hardware.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/641ba327ba0d01461cd8fabad9a237e7b6667ce170be08aa3e89e6624ada0d38_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/641ba327ba0d01461cd8fabad9a237e7b6667ce170be08aa3e89e6624ada0d38_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents KernelEvolve, an agentic framework that automates the generation and optimization of compute kernels for deep learning recommendation models to address challenges posed by model, kernel, and hardware heterogeneity. The method uses a graph-based search process enhanced with retrieval-augmented prompts and operates across multiple programming abstractions. The system was validated on production models and benchmarks, showing significant performance improvements and reduced development time, effectively mitigating the programmability barrier for new AI accelerators.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On the Inverse Flow Matching Problem in the One-Dimensional and Gaussian Cases</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models], [flow matching, inverse problem, uniqueness, generative AI, continuity equation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alexander Korotin, Gudmund Pammer</p>
</li>
<li class="">
<p><strong>institution:</strong> Applied AI Institute, Graz University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23265" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23265</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formally defines the inverse problem of flow matching (FM) for distributions with finite exponential moment, 2. Establishes the uniqueness of the solution to the inverse FM problem in the one-dimensional (D=1) setting, 3. Establishes the uniqueness of the solution to the inverse FM problem in the Gaussian case.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3894b8881e76b35830e6504158fd61b8ec4e18f8adfb99a0b03911f3928bd297_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3894b8881e76b35830e6504158fd61b8ec4e18f8adfb99a0b03911f3928bd297_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the inverse problem of flow matching, aiming to recover the original transport plan given the initial distribution and the learned velocity field. It proves that the solution to this inverse problem is unique in two specific cases: one-dimensional distributions and Gaussian distributions. The general multidimensional case remains an open problem for future research.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PFed-Signal: An ADR Prediction Model based on Federated Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [federated learning, transformer, euclidean distance, adverse drug reaction, data cleaning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tao Li, Peilin Li, Kui Lu, Yilei Wang, Junliang Shang, Guangshun Li, Huiyu Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> Qufu Normal University, University of Leicester</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23262" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23262</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed PFed-Split, a method to split the original FAERS dataset based on Adverse Drug Reactions (ADRs). 2. Introduced a federated learning-based biased data identification method that uses Euclidean distance to filter out noisy records and generate a clean dataset. 3. Developed an ADR prediction model based on the Transformer architecture, trained on the cleaned dataset, which achieves superior performance in accuracy and signal detection metrics (ROR, PRR) compared to traditional statistical methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c6bff5c67d3939353727926690ff57b0800331885e90983cf3850de46090975_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c6bff5c67d3939353727926690ff57b0800331885e90983cf3850de46090975_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes PFed-Signal, a federated learning-based model for predicting Adverse Drug Reactions (ADRs). The method first cleans biased data from the FAERS database using Euclidean distance within a federated framework and then trains a Transformer model on the cleaned data for prediction. The results show that this approach outperforms traditional statistical methods in key metrics like accuracy, F1 score, and AUC.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [Sparse Autoencoders (SAEs), Low-Rank Adaptation (LoRA), Safety Alignment, Interpretability, Parameter-efficient Fine-tuning (PEFT)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dianyun Wang, Qingsen Ma, Yuhu Shang, Zhifeng Lu, Lechen Ning, Zhenbo Xu, Huijia Wu, Zhaofeng He</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing University of Posts and Telecommunications</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23260" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23260</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel method that uses pre-trained Sparse Autoencoders (SAEs) to construct an explicit, interpretable low-rank subspace for adapter initialization, addressing the black-box nature of traditional LoRA. 2. Provides theoretical analysis proving that SAE-based subspace identification achieves arbitrarily small recovery error under monosemanticity, while direct identification suffers an irreducible error floor due to polysemanticity. 3. Demonstrates state-of-the-art performance on safety alignment, achieving up to 99.6% safety rate while updating only 0.19-0.24% of parameters, and provides interpretable insights into the learned alignment subspace.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e615e6d561cef5b79dc991ed964fd9b6fb069427af26a4b7b42cd33cea4315a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e615e6d561cef5b79dc991ed964fd9b6fb069427af26a4b7b42cd33cea4315a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of interpretability in standard Low-Rank Adaptation (LoRA) methods for fine-tuning large language models. The proposed method leverages Sparse Autoencoders (SAEs) to identify task-relevant features in a disentangled space and uses them to construct an explicit, interpretable low-rank subspace for adapter initialization. The approach achieves superior safety alignment performance and provides transparency into the learned adaptation process.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Revealing design archetypes and flexibility in e-molecule import pathways using Modeling to Generate Alternatives and interpretable machine learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [Energy Systems Optimization], [Modeling to Generate Alternatives, Interpretable Machine Learning, Decision Trees, Energy System Optimization Model, E-molecules]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mahdi Kchaou, Francesco Contino, Diederik Coppitters</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Mechanics, Materials and Civil Engineering (iMMC), Université catholique de Louvain (UCLouvain)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23284" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23284</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Applied Modeling to Generate Alternatives (MGA) to produce a diverse set of near-cost-optimal e-molecule import pathway designs, moving beyond a single optimal solution. 2. Used interpretable machine learning (specifically decision trees) to extract high-level insights and design archetypes from the complex, multi-dimensional solution space generated by MGA. 3. Demonstrated the flexibility of hydrogen import pathways, showing that specific technologies (solar, wind, storage) are not strictly required to stay within a 10% cost margin, and revealed how constraints shift the preferred design archetypes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b329c74249d56df55cd55db503dedb7b277979173c0419ff415764d3a34be11_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b329c74249d56df55cd55db503dedb7b277979173c0419ff415764d3a34be11_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of single cost-optimal designs for green e-molecule import pathways by using Modeling to Generate Alternatives to create diverse near-optimal solutions and then applying interpretable machine learning to analyze them. The method is applied to hydrogen import pathways considering different carriers. The main finding is a broad near-optimal space with significant flexibility, where specific renewable sources are not strictly necessary, and constraints shift preferences toward different carrier and technology combinations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Spectral Analysis of Hard-Constraint PINNs: The Spatial Modulation Mechanism of Boundary Functions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scientific machine learning], [Physics-Informed Neural Networks, Neural Tangent Kernel, spectral analysis, hard constraints, boundary functions]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuchen Xie, Honghang Chi, Haopeng Quan, Yahui Wang, Wei Wang, Yu Ma</p>
</li>
<li class="">
<p><strong>institution:</strong> Sun Yat-sen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23295" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23295</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Established a rigorous Neural Tangent Kernel (NTK) framework for Hard-Constraint PINNs (HC-PINNs), deriving the explicit kernel composition law. 2. Revealed that the boundary function acts as a multiplicative spatial modulator and spectral filter, fundamentally altering the learning landscape and potentially causing spectral collapse. 3. Identified the effective rank of the residual kernel as a superior, deterministic predictor of training convergence compared to classical condition numbers.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb45ca3a4beb6281022392033c68aa10cb18c53e62610b8818daca2495a7eee_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb45ca3a4beb6281022392033c68aa10cb18c53e62610b8818daca2495a7eee_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the training dynamics of Physics-Informed Neural Networks with hard constraints (HC-PINNs). It establishes an NTK framework to show that the boundary function acts as a spectral filter, and identifies the kernel&#x27;s effective rank as a key predictor of convergence. The work provides a theoretical foundation for designing boundary functions to avoid optimization stagnation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [domain-specific foundation model, agentic physical ai, variance collapse, physics-based validation, policy distillation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yoonpyo Lee, Kazuma Kobayashi, Sai Puppala, Sajedul Talukder, Seid Koric, Souvik Chakraborty, Syed Bahauddin Alam</p>
</li>
<li class="">
<p><strong>institution:</strong> Hanyang University, University of Illinois Urbana-Champaign, Southern Illinois University, University of Texas at El Paso, National Center for Supercomputing Applications, Indian Institute of Technology Delhi</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23292" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23292</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new paradigm of Agentic Physical AI, where policy optimization is driven by physics-based outcome validation instead of perceptual inference, addressing the structural limitation of general-purpose models in control tasks. 2. Demonstrates that scaling data for a compact (360M parameter) model induces a sharp phase transition and variance collapse (&gt;500x reduction), leading to stable, execution-level behavior for safety-critical control. 3. Shows the model autonomously distills a robust policy (concentrating on a single strategy) and its learned representations transfer across different physics and input modalities without architectural changes, exhibiting early foundation-model properties.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb921102dfde629395ab8293510cb369a00c1199cadd4c88269dc076f8774a1a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb921102dfde629395ab8293510cb369a00c1199cadd4c88269dc076f8774a1a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a fundamental limitation of general-purpose AI models in safety-critical physical control tasks, where they prioritize semantic plausibility over physical correctness. To address this, it introduces Agentic Physical AI, a paradigm using compact language models trained with physics-based validation on synthetic nuclear reactor control data. The key finding is that sufficient data scaling induces a sharp variance collapse, stabilizing the model&#x27;s behavior and enabling it to autonomously distill a reliable control policy that generalizes across tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [Lyapunov Optimization, Deep Reinforcement Learning, Edge-Cloud Partitioning, Transformer Decomposition, Queue Stability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abolfazl Younesi, Abbas Shabrang Maryan, Elyas Oustad, Zahra Najafabadi Samani, Mohsen Ansari, Thomas Fahringer</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Innsbruck, Sharif University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23310" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23310</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a fine-grained, adaptive partitioning framework (Splitwise) that decomposes transformer layers into attention heads and feed-forward sub-blocks, enabling exponentially more partition choices than layer-wise schemes. 2. Introduces a hierarchical DRL policy guided by Lyapunov optimization to jointly optimize latency, energy, and accuracy while guaranteeing queue stability under stochastic workloads and variable bandwidth. 3. Ensures robustness through partition checkpoints with exponential backoff recovery for communication failures, validated on real edge devices with large models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/121b71aa214f4a7c1671c9df76bf67a9cd64f3cb9e74186606a380ce86f7634f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/121b71aa214f4a7c1671c9df76bf67a9cd64f3cb9e74186606a380ce86f7634f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes Splitwise, a Lyapunov-assisted DRL framework for dynamically partitioning LLM inference between edge and cloud at a fine-grained sub-layer level. It aims to minimize latency and energy while maintaining accuracy under fluctuating network conditions. Experiments show Splitwise significantly reduces latency and energy consumption compared to existing methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Deep learning for pedestrians: backpropagation in Transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [backpropagation], [backpropagation, transformers, gradient derivation, LoRA, PyTorch implementation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Laurent Boué</p>
</li>
<li class="">
<p><strong>institution:</strong> Oracle, Microsoft</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23329" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23329</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a vectorized, index-free derivation of backpropagation for transformer architectures, extending previous work on CNNs. 2. Derives gradient expressions for key transformer components like embedding, multi-headed self-attention, layer normalization, and LoRA layers. 3. Includes a complete PyTorch implementation of a minimal GPT-like network alongside analytical gradient expressions for pedagogical clarity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d5ded3610ac31d4f19d95237254231c2e03d3870df7749cabfc471eeb5008ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d5ded3610ac31d4f19d95237254231c2e03d3870df7749cabfc471eeb5008ac_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper manually derives the backpropagation algorithm for transformer-based next-token-prediction models using a vectorized, index-free methodology. It provides gradient expressions for core layers (embedding, self-attention, layer norm) and LoRA, aiming to build deeper intuition for how operations influence the final output. A complete PyTorch implementation is also provided to illustrate the theoretical derivations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Visual Language Hypothesis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [representation learning], [visual language hypothesis, fiber bundle, semantic quotient, expand-and-snap, topology change]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiu Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Bytedance</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23335" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23335</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the &quot;Visual Language Hypothesis,&quot; framing visual understanding as requiring a discrete semantic language, leading to a fiber-bundle-like structure for the observation space. 2. Derives a theoretical requirement for semantic invariance, arguing it necessitates a non-homeomorphic, discriminative target (e.g., supervised labels, multimodal alignment) rather than smooth deformation alone. 3. Identifies an architectural requirement for models, proposing an &quot;expand-and-snap&quot; process to achieve the necessary topology change for semantic abstraction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d70511c4d2f57ecb4e49170ed73c0a81de0fcfcdbdb84cc7bcbdca254140c3f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d70511c4d2f57ecb4e49170ed73c0a81de0fcfcdbdb84cc7bcbdca254140c3f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes the &quot;Visual Language Hypothesis,&quot; which posits that visual understanding requires a discrete semantic structure, implying the visual observation space is organized like a fiber bundle. From this, the authors theoretically argue that achieving semantic invariance demands discriminative supervision and that model architectures must support a specific &quot;expand-and-snap&quot; process to change topology. The framework provides a topological interpretation for empirical patterns in large-scale discriminative and multimodal models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scaling laws], [scaling laws, model ensembling, multi-model collaboration, cross-entropy loss, parameter budget]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dakuan Lu, Jiaqi Zhang, Cheng Yuan, Jiawei Shao, Chi Zhang, Xuelong Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Artificial Intelligence (TeleAI), China Telecom</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23340" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23340</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the &quot;Law of Multi-model Collaboration,&quot; a novel scaling law for predicting the performance limits of LLM ensembles based on aggregated parameters. 2. Establishes a method-agnostic theoretical framework using an idealized integration oracle to quantify the intrinsic upper bound of multi-model collaboration. 3. Empirically demonstrates that multi-model systems follow a power-law scaling with better trends and lower loss floors than single models, and that heterogeneous ensembles outperform homogeneous ones.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68212ad5f9cd50ef959cdd80f4b7274178d9a6b124904010fc5b0cf0834b21a1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68212ad5f9cd50ef959cdd80f4b7274178d9a6b124904010fc5b0cf0834b21a1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of a theoretical framework for scaling in multi-model LLM systems. It proposes the &quot;Law of Multi-model Collaboration,&quot; a scaling law based on aggregated parameters, and finds that ensembles scale better and achieve lower loss than single models, with diversity being a key driver of gains.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ECG-RAMBA: Zero-Shot ECG Generalization by Morphology-Rhythm Disentanglement and Long-Range Modeling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [medical signal processing], [ECG classification, morphology-rhythm disentanglement, Mamba, zero-shot generalization, Power Mean pooling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hai Duong Nguyen, Xuan-The Tran</p>
</li>
<li class="">
<p><strong>institution:</strong> HAI-Smartlink Research Lab (Anchi STE Company), Vietnam Maritime University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23347" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23347</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ECG-RAMBA, a framework that explicitly disentangles ECG morphology (via MiniRocket) and rhythm (via HRV descriptors) before fusing them for robust classification. 2. Introduces a numerically stable Power Mean pooling operator (Q=3) for windowed inference to emphasize high-evidence segments. 3. Demonstrates strong zero-shot cross-dataset generalization for ECG classification using a bi-directional Mamba backbone for long-range contextual modeling.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f9b49242e586cadf1889fa40730ea1652c1b4ef5b15cf15697b58832c4dbf6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f9b49242e586cadf1889fa40730ea1652c1b4ef5b15cf15697b58832c4dbf6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of poor generalization of deep learning models for ECG classification across different datasets. It proposes ECG-RAMBA, a method that separates and then fuses morphological and rhythm features, using a Mamba backbone and a novel pooling operator. The results show that this approach achieves robust zero-shot performance on external datasets, outperforming a baseline model.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ISOPO: Proximal policy gradients without pi-old</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [natural policy gradient, proximal policy optimization, reinforcement learning fine-tuning, Fisher metric, neural tangent kernel]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nilin Abrahamsen</p>
</li>
<li class="">
<p><strong>institution:</strong> None (No affiliation or email domain provided in the given content)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23353" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23353</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces ISOPO, a method to approximate the natural policy gradient in a single gradient step, contrasting with existing methods like GRPO/PPO that require multiple steps. 2. Proposes a simple form of ISOPO that normalizes log-probability gradients in the Fisher metric before contracting with advantages. 3. Presents a variant that transforms microbatch advantages based on the neural tangent kernel layer-wise, enabling efficient implementation with negligible overhead compared to REINFORCE.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa80f3dc48bfe2cf2fc158ecc6c0bb7aefffb89047088c0b23d1d764889fea16_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa80f3dc48bfe2cf2fc158ecc6c0bb7aefffb89047088c0b23d1d764889fea16_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Isometric Policy Optimization (ISOPO), a new method for approximating the natural policy gradient in reinforcement learning fine-tuning. Unlike existing proximal policy methods like GRPO/PPO which require multiple gradient steps with a reference policy, ISOPO performs the approximation in a single step by normalizing gradients or transforming advantages, achieving this with minimal computational overhead.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [post-training quantization, W8A8, W4A8, Ascend NPU, Chain-of-Thought (CoT)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yilun Luo, HuaQing Zheng, Haoqian Meng, Wenyuan Liu, Peng Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tianjin University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23367" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23367</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a unified low-bit inference framework for openPangu-Embedded models, supporting INT8 (W8A8) and W4A8 quantization optimized for the Atlas A2 Ascend NPU. 2. Provides a comprehensive evaluation of quantization across three distinct CoT reasoning modes (slow_think, auto_think, no_think) on code generation benchmarks (HumanEval, MBPP). 3. Demonstrates that INT8 quantization preserves over 90% of FP16 accuracy with a 1.5x prefill speedup, while W4A8 significantly reduces memory consumption, enabling efficient CoT reasoning on edge NPUs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07e9cf3e7e8252aa7eae3fdf2e7647007d4786dd6ade9f5c4e940d1c74c4e2cd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07e9cf3e7e8252aa7eae3fdf2e7647007d4786dd6ade9f5c4e940d1c74c4e2cd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high memory and latency overhead of deploying Chain-of-Thought (CoT) enabled openPangu models on Ascend NPUs by applying post-training quantization (INT8 and W4A8). The proposed framework, optimized for the Atlas A2 hardware, maintains high accuracy for INT8 and reduces memory for W4A8, enabling efficient on-device CoT reasoning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Diffusion priors enhanced velocity model building from time-lag images using a neural operator</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [neural operator, velocity model building, reverse time migration, diffusion model, automatic differentiation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiao Ma, Mohammad Hasyim Taufik, Tariq Alkhalifah</p>
</li>
<li class="">
<p><strong>institution:</strong> King Abdullah University of Science and Technology (KAUST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23375" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23375</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel framework that combines generative models (diffusion priors) with neural operators for velocity model building. 2. Uses a neural operator as a fast surrogate for the forward modeling and migration process to generate time-lag images from velocity models. 3. Employs the trained neural operator and automatic differentiation to update the migration velocity, enhanced by a generative model as a regularizer to produce high-resolution, cleaner predictions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0c5cd7e3ef87aa48d65e94484e956e08ebef522c14cc7eee60600b85109e02a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0c5cd7e3ef87aa48d65e94484e956e08ebef522c14cc7eee60600b85109e02a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a new deep learning framework for efficient, high-resolution velocity model building in seismic imaging. The method combines a neural operator, which acts as a fast surrogate for seismic modeling and migration, with a diffusion generative model that serves as a prior to regularize the solution. Experiments on synthetic and field data show the approach effectively builds cleaner, higher-resolution velocity models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [log anomaly detection], [collaborative transformers, multi-head impressed attention, modality adaptation layer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mohammad Nasirzadeh, Jafar Tahmoresnezhad, Parviz Rashidi-Khazaee</p>
</li>
<li class="">
<p><strong>institution:</strong> Urmia University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23380" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23380</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/your-repo/CoLog" target="_blank" rel="noopener noreferrer" class="">https://github.com/your-repo/CoLog</a> (Note: The provided text states &quot;We also provide the implementation of CoLog atthis https URL.&quot; but the specific URL is cut off in the input. Based on the placeholder, the typical format is used. If the exact URL is required, it would be the one following &quot;atthis&quot; in the original text.)</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes CoLog, a unified framework for detecting both point and collective anomalies in OS logs by applying multimodal sentiment analysis concepts. 2. Introduces collaborative transformers and multi-head impressed attention to learn interactions between different log data modalities. 3. Incorporates a modality adaptation layer to handle heterogeneity and adapt representations from different log modalities.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of log anomaly detection, where existing methods struggle with the multimodal nature of log data and the interactions between these modalities. It proposes CoLog, a framework that uses collaborative transformers and a modality adaptation layer to learn nuanced patterns across log modalities for comprehensive anomaly detection. Extensive experiments show CoLog achieves state-of-the-art performance, with mean precision, recall, and F1 scores over 99.5% across seven benchmark datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Beyond-Diagonal Reconfigurable Intelligent Surfaces for 6G Networks: Principles, Challenges, and Quantum Horizons</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [communication &amp; networking], [beyond-diagonal RIS, passive beamforming, hybrid quantum-classical ML, 6G networks, reconfigurable intelligent surfaces]</p>
</li>
<li class="">
<p><strong>authors:</strong> Abd Ullah Khan, Uman Khalid, Muhammad Tanveer, Trung Q. Duong, Hyundong Shin</p>
</li>
<li class="">
<p><strong>institution:</strong> Kyung Hee University, University of Management and Technology, Memorial University of Newfoundland, Queen&#x27;s University Belfast</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23400" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23400</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a systematic introduction to Beyond-Diagonal Reconfigurable Intelligent Surfaces (BD-RIS), detailing its principles, architecture, advantages, and classification. 2. Presents a case study comparing four beamforming algorithms for BD-RIS, analyzing their performance in terms of sum rate and computation cost. 3. Proposes and analyzes hybrid quantum-classical machine learning models to enhance beam prediction for 6G BD-RIS, validated using the real-world DeepSense 6G dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4756534e0e202824dc7facc5a963e3a3205fc85e2596f60033bcdb6ae4cab497_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4756534e0e202824dc7facc5a963e3a3205fc85e2596f60033bcdb6ae4cab497_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Beyond-Diagonal Reconfigurable Intelligent Surfaces (BD-RIS) as a key technology for 6G networks to overcome high-frequency propagation challenges. It systematically reviews BD-RIS principles, analyzes beamforming algorithms, and explores quantum-enhanced machine learning for beam prediction. The work concludes with insights into the practical implications and future potential of BD-RIS for advanced wireless communication.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Task-driven Heterophilic Graph Structure Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [graph structure learning, heterophilic graphs, spectral filtering, topology inference, graph rewiring]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ayushman Raghuvanshi, Gonzalo Mateos, Sundeep Prabhakar Chepuri</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Science, University of Rochester</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23406" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23406</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FgGSL, an end-to-end framework that jointly learns complementary homophilic and heterophilic graph structures using a learnable masking function and processes them with low- and high-pass graph filter banks. 2. Introduces a label-based structural loss to explicitly promote the recovery of homophilic and heterophilic edges, enabling task-driven graph structure learning, and provides theoretical stability bounds for this loss and robustness guarantees for the filters. 3. Demonstrates through experiments on six heterophilic benchmarks that FgGSL consistently outperforms state-of-the-art GNNs and graph rewiring methods, validating the benefit of combining frequency information with supervised topology inference.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3fec9358f601e6c323ecab2e7bcfe1880fd9b9d4351503536e33c58d8cedb6e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3fec9358f601e6c323ecab2e7bcfe1880fd9b9d4351503536e33c58d8cedb6e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of learning discriminative node representations on heterophilic graphs, where connected nodes often have dissimilar labels. The authors propose FgGSL, a framework that jointly learns homophilic and heterophilic graph structures using spectral filters and a task-driven structural loss. Experiments show FgGSL outperforms existing methods, highlighting the advantage of combining frequency guidance with supervised graph inference.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On the Sample Complexity of Learning for Blind Inverse Problems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [inverse problems], [blind inverse problems, Linear Minimum Mean Square Estimators (LMMSEs), Tikhonov regularization, random forward operators, error bounds]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nathan Buskulic, Luca Calatroni, Lorenzo Rosasco, Silvia Villa</p>
</li>
<li class="">
<p><strong>institution:</strong> Università degli studi di Genova, Italian Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23405" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23405</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Deriving closed-form expressions for optimal Linear Minimum Mean Square Estimators (LMMSEs) for blind inverse problems and establishing their equivalence with distribution-dependent Tikhonov regularization. 2. Proving convergence results for these estimators under source condition assumptions. 3. Deriving rigorous finite-sample error bounds that quantify the impact of operator randomness, noise level, and sample count on estimator performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/daa1ffc1715fdd9a990c7a6af28c2a415193fdef2bd99ec47e6ece431b0f3406_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/daa1ffc1715fdd9a990c7a6af28c2a415193fdef2bd99ec47e6ece431b0f3406_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper provides a theoretical analysis of learning for blind inverse problems, where the forward operator is unknown. It focuses on Linear Minimum Mean Square Estimators (LMMSEs), deriving their optimal forms and connecting them to Tikhonov regularization. The main conclusion is the establishment of rigorous error bounds and convergence rates that characterize how estimator performance depends on noise, problem conditioning, and the randomness of the forward operator.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Theoretical Foundations of Scaling Law in Familial Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [familial models, scaling law, early exiting, IsoFLOP design, compute-optimal training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Huan Song, Qingfei Zhao, Ting Long, Shuyu Tian, Hongjun An, Jiawei Shao, Chi Zhang, Xuelong Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Institute of Artificial Intelligence (TeleAI), China Telecom</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23407" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23407</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Theoretically and empirically extends the neural scaling law to the &quot;familial models&quot; paradigm by introducing granularity (G) as a new fundamental scaling variable alongside model size (N) and tokens (D). 2. Proposes a rigorous IsoFLOP experimental design to decouple architectural impact from computational scale, enabling high-fidelity parameterization of the unified scaling law L(N, D, G). 3. Quantifies that the granularity penalty follows a multiplicative power law with an extremely small exponent (γ≈0.041), validating the &quot;train once, deploy many&quot; paradigm without compromising compute-optimality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc66a2a88c82327d2e67ccabca47fcc7a15e81e139a0ed0135b0f3ea93534985_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc66a2a88c82327d2e67ccabca47fcc7a15e81e139a0ed0135b0f3ea93534985_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of traditional neural scaling laws, which assume a single model, by extending them to familial models that generate multiple sub-models from one backbone. The authors propose a unified scaling law incorporating granularity (G) and validate it using a rigorous IsoFLOP experimental design. The key finding is that the performance penalty for increased granularity is very small, proving that deployment flexibility can be achieved efficiently.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Directly Constructing Low-Dimensional Solution Subspaces in Deep Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [intrinsic dimension, low-rank approximation, subspace-native distillation, weight matrices, empirical spectral density]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yusuf Kalyoncuoglu</p>
</li>
<li class="">
<p><strong>institution:</strong> RWTH Aachen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23410" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23410</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a constructive method to decouple solution geometry from the ambient search space, bypassing the non-convex optimization bottleneck. 2. Empirically demonstrates significant compression (e.g., factor of 16) of classification heads in models like ResNet-50, ViT, and BERT with minimal performance loss. 3. Introduces &quot;Subspace-Native Distillation&quot; as a novel paradigm to provide a stable geometric coordinate system for student models, enabling &quot;Train Big, Deploy Small&quot;.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d34f960cfbb8a9db281aca58a1b30934c42fc68964647e8066a3754aeb92d38_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d34f960cfbb8a9db281aca58a1b30934c42fc68964647e8066a3754aeb92d38_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the redundancy in large neural networks by proposing a method to directly construct low-dimensional solution subspaces, decoupling the solution geometry from the high-dimensional optimization search space. It shows that classification heads can be heavily compressed without significant performance drops. This leads to a new distillation paradigm that allows student models to learn in a stable, low-dimensional subspace, potentially realizing efficient deployment of compact models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Towards Integrating Uncertainty for Domain-Agnostic Segmentation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [semantic segmentation], [uncertainty quantification, domain-agnostic, Segment Anything Model (SAM), Laplace approximation, benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jesse Brouwers, Xiaoyan Xing, Alexander Timans</p>
</li>
<li class="">
<p><strong>institution:</strong> UvA-Bosch Delta Lab, University of Amsterdam</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23427" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23427</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/JesseBrouw/UncertSAM" target="_blank" rel="noopener noreferrer" class="">https://github.com/JesseBrouw/UncertSAM</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Curated UncertSAM, a benchmark of eight datasets to stress-test segmentation models under challenging conditions like shadows and camouflage. 2. Evaluated a suite of lightweight, post-hoc uncertainty estimation methods for segmentation foundation models. 3. Assessed a preliminary uncertainty-guided prediction refinement step, finding that last-layer Laplace approximation yields uncertainty estimates well-correlated with segmentation errors.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15865ed74ed61443aa69769e51585f4a7341748fc10c0250eef9243be675f215_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15865ed74ed61443aa69769e51585f4a7341748fc10c0250eef9243be675f215_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether uncertainty quantification can improve the robustness of foundation segmentation models like SAM in domain-agnostic settings. The authors propose a benchmark (UncertSAM) and evaluate several post-hoc uncertainty estimation methods, finding that a last-layer Laplace approximation provides meaningful uncertainty signals. The results indicate the potential of integrating uncertainty to enhance model generalizability, though refinement benefits are preliminary.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [gpu kernels], [kernel generation, multi-agent system, domain-specific languages (DSLs), performance tuning, Triton]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jinye Du, Quan Yuan, Zuyao Zhang, Yanzhi Yi, Jiahui Hu, Wangyi Chen, Yiyang Zhu, Qishui Zheng, Wenxiang Zou, Xiangyu Chang, Zuohe Zheng, Zichun Ye, Chao Liu, Shanni Li, Renwei Zhang, Yiping Deng, Xinwei Hu, Xuefeng Jin, Jie Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Huawei Technologies Co., Ltd., Hunan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23424" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23424</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed AKG kernel agent, a multi-agent framework that automates the generation, migration, and performance tuning of computational kernels for diverse hardware platforms. 2. Designed the system to support multiple Domain-Specific Languages (DSLs) like Triton, TileLang, CPP, and CUDA-C, enabling cross-platform portability and correctness. 3. Demonstrated the system&#x27;s effectiveness through evaluation on KernelBench, achieving an average 1.46x speedup over PyTorch Eager baselines on GPU and NPU backends.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40d5942348375e7b86203ab7a7420bba7105494296f349fdd48174b020a1527e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40d5942348375e7b86203ab7a7420bba7105494296f349fdd48174b020a1527e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes AKG kernel agent, a multi-agent framework that automates the development and optimization of high-performance computational kernels for modern AI workloads across diverse hardware. The system supports multiple DSLs for portability and uses LLMs for code generation and tuning. Evaluation shows it achieves a 1.46x average speedup over baseline implementations, effectively accelerating kernel development.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Stochastic Siamese MAE Pretraining for Longitudinal Medical Images</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [masked autoencoder, siamese network, stochastic process, longitudinal data, variational inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Taha Emre, Arunava Chakravarty, Thomas Pinetz, Dmitrii Lachinov, Martin J. Menten, Hendrik Scholl, Sobha Sivaprasad, Daniel Rueckert, Andrew Lotery, Stefan Sacu, Ursula Schmidt-Erfurth, Hrvoje Bogunović</p>
</li>
<li class="">
<p><strong>institution:</strong> Medical University of Vienna, Imperial College London, Technical University of Munich, University College London, University of Southampton</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23441" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23441</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed STAMP, a novel Siamese MAE framework that incorporates temporal information by conditioning on the time difference between input volumes. 2. Introduced a stochastic learning approach by reframing the MAE reconstruction loss as a conditional variational inference objective to model the uncertainty in disease progression. 3. Demonstrated superior performance of STAMP-pretrained models over existing temporal MAE methods and foundation models on predicting progression of Age-Related Macular Degeneration and Alzheimer&#x27;s Disease.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c93396b0ded8fc65364d5714bd12e652a23ffc22eb276cbbca01426ecd0db791_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c93396b0ded8fc65364d5714bd12e652a23ffc22eb276cbbca01426ecd0db791_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the lack of temporal awareness in self-supervised learning methods like MAE for longitudinal medical images. It proposes STAMP, a stochastic Siamese MAE framework that learns temporal dynamics by conditioning on time differences and using a variational inference objective. The method outperformed existing approaches on disease progression prediction tasks for OCT and MRI datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Assessing behaviour coverage in a multi-agent system simulation for autonomous vehicle testing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent systems], [Model Predictive Control, Coverage-Based Testing, Edge-Case Exploration, Multi-Agent Simulation, Behaviour Coverage]</p>
</li>
<li class="">
<p><strong>authors:</strong> Manuel Franco-Vivo</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Bristol</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23445" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23445</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A systematic approach to measure and assess behaviour coverage within a multi-agent simulation for autonomous vehicle testing. 2. The proposal of a Model Predictive Control (MPC) pedestrian agent designed to generate interesting tests and realistic behaviour. 3. Insights and analysis for improving and optimizing simulation frameworks through behaviour coverage metrics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385ed91e6ca24b7cc0e8d369cc587a3dd677cf4643f89f27d85aaadf4cd4ea70_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385ed91e6ca24b7cc0e8d369cc587a3dd677cf4643f89f27d85aaadf4cd4ea70_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the need for comprehensive testing of autonomous vehicles by analyzing behaviour coverage in multi-agent simulations. It proposes a systematic method to measure coverage and introduces an MPC-based pedestrian agent to generate more realistic and challenging test scenarios. The research concludes that assessing behaviour coverage is crucial for validating the robustness of autonomous systems and improving simulation frameworks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [Mixture-of-Experts, Router-Expert Coupling, Auxiliary Loss, Expert Specialization, Efficient Training]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ang Lv, Jin Ma, Yiyuan Ma, Siyuan Qiao</p>
</li>
<li class="">
<p><strong>institution:</strong> ByteDance, Renmin University of China</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23447" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23447</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel lightweight auxiliary loss (ERC loss) to explicitly couple router decisions with expert capabilities in MoE models. 2. Introduces a computationally efficient method that scales with the square of the number of experts (n^2), independent of batch size, unlike prior token-dependent methods. 3. Enables flexible control and quantitative tracking of expert specialization levels during training, providing new insights into MoE model dynamics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6534d4f2f88c89a450614cf67b57f76f33fc90a18f24833876fd8e55d3e326b9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6534d4f2f88c89a450614cf67b57f76f33fc90a18f24833876fd8e55d3e326b9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the misalignment between router decisions and expert capabilities in Mixture-of-Experts (MoE) models. It proposes an Expert-Router Coupling (ERC) loss, a lightweight auxiliary loss that enforces constraints via perturbed router embeddings to ensure each expert specializes in its routed tokens and each router embedding faithfully represents its expert. The method is shown to be effective and computationally efficient, enabling better control and analysis of expert specialization during large-scale pre-training.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Dynamic Subspace Composition: Efficient Adaptation via Contractive Basis Expansion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [Mixture of Experts, Parameter-Efficient Fine-Tuning, Low-Rank Adaptation, Memory-Bandwidth Bottleneck, Dynamic Sparse Dictionary Learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vladimer Khasia</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23448" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23448</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/VladimerKhasia/DSC" target="_blank" rel="noopener noreferrer" class="">https://github.com/VladimerKhasia/DSC</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Dynamic Subspace Composition (DSC), a framework that models weight updates as a residual trajectory within a Star-Shaped Domain using Magnitude-Gated Simplex Interpolation for continuity. 2. Decouples storage and adaptation rank, constructing compositional approximations from a shared basis bank to reduce parameter complexity from O(Mrd) to O(Md) and memory traffic to O(Kd). 3. Introduces Frame-Theoretic regularization and spectral constraints to provide rigorous worst-case bounds on the dynamic update, addressing representation collapse and gradient instability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/713ad8af3e30c492117d0d3f26babbf5fe909df2e68e0ab479fc866276a3d80c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/713ad8af3e30c492117d0d3f26babbf5fe909df2e68e0ab479fc866276a3d80c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the memory-bandwidth bottleneck and optimization instability in Mixture of Experts (MoE) models. It proposes Dynamic Subspace Composition (DSC), a method that approximates context-dependent weights via a sparse expansion of a shared basis, reducing parameter complexity and memory traffic while ensuring stable updates. The main conclusion is that DSC offers a more efficient and theoretically grounded alternative to standard approaches like Mixture-of-LoRAs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [instruction following, hindsight replay, sample-efficient RL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kongcheng Zhang, Qi Yao, Shunyu Liu, Wenjian Zhang, Min Cen, Yang Zhou, Wenkai Fang, Yiru Zhao, Baisheng Lai, Mingli Song</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Cainiao Network, Nanyang Technological University, Dalian University of Technology, University of Science and Technology of China, Alibaba Cloud Computing, Chinese Academy of Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23457" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23457</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/zhangkc97/HiR" target="_blank" rel="noopener noreferrer" class="">https://github.com/zhangkc97/HiR</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Hindsight instruction Replay (HiR), a novel RL framework that replays failed attempts as successes using a select-then-rewrite strategy to address sparse rewards. 2. Theoretically frames the RL objective as dual-preference learning at both instruction- and response-level, enabling efficient optimization with only binary rewards. 3. Demonstrates sample efficiency and promising results across various instruction following tasks with reduced computational budget.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72e272e5c14b0f640f80b3e8859a1fd3a0a8b8e8608dfacee9528d242698f15_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72e272e5c14b0f640f80b3e8859a1fd3a0a8b8e8608dfacee9528d242698f15_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of sparse rewards in RL for aligning LLMs to follow complex instructions. It proposes HiR, a sample-efficient framework that replays failed responses as successful ones based on partially satisfied constraints, framed as dual-preference learning. Experiments show HiR achieves strong performance on instruction-following tasks while being more computationally efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning from human feedback (RLHF)], [reward model, inductive bias, information bottleneck, mutual information, reward hacking]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhuo Li, Pengyu Cheng, Zhechao Yu, Feifei Tong, Anningzhe Gao, Tsung-Hui Chang, Xiang Wan, Erchao Zhao, Xiaoxi Jiang, Guanjun Jiang</p>
</li>
<li class="">
<p><strong>institution:</strong> Alibaba, The Chinese University of Hong Kong, Shenzhen Research Institute of Big Data</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23461" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23461</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Qwen-Applications/DIR" target="_blank" rel="noopener noreferrer" class="">https://github.com/Qwen-Applications/DIR</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes DIR, a novel information-theoretic debiasing method for reward models that maximizes mutual information with human preference while minimizing it with biased attributes. 2. Theoretically justifies the method&#x27;s ability to handle complex, non-linear inductive biases, extending beyond simple linear correlation models. 3. Empirically demonstrates DIR&#x27;s effectiveness in mitigating three types of biases (length, sycophancy, format) and shows it enhances RLHF performance and generalization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/522f5bbb5a5776cd8df024fb1b24faf19bb1a1ea6e0408c7951f37bfd1657846_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/522f5bbb5a5776cd8df024fb1b24faf19bb1a1ea6e0408c7951f37bfd1657846_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of inductive biases in reward models (RMs) for RLHF, which can lead to overfitting and reward hacking. It proposes DIR, an information-theoretic debiasing method inspired by the information bottleneck that optimizes mutual information to reduce bias. Experiments show DIR effectively mitigates multiple biases and improves RLHF performance and generalization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [Parameter-efficient fine-tuning, LoRA, full-rank adaptation, rotational degrees of freedom, hierarchical joint decomposition]</p>
</li>
<li class="">
<p><strong>authors:</strong> Guoan Wan, Tianyu Chen, Fangzheng Feng, Haoyi Zhou, Runhua Xu</p>
</li>
<li class="">
<p><strong>institution:</strong> Beihang University, Huazhong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23485" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23485</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Bane-Elvin/AAAI2026-FRoD" target="_blank" rel="noopener noreferrer" class="">https://github.com/Bane-Elvin/AAAI2026-FRoD</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FRoD, a novel PEFT method that combines hierarchical joint decomposition with rotational degrees of freedom for full-rank updates. 2. Introduces a globally shared basis and sparse, learnable perturbations to enhance expressiveness and efficiency beyond low-rank constraints. 3. Demonstrates that FRoD matches full fine-tuning accuracy on 20 benchmarks while using only 1.72% of trainable parameters and achieves faster convergence.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e211fe6dc2d8e782c731fff29ba32c9fe2a1f958b475d5931d50f6e8fd04fdb8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e211fe6dc2d8e782c731fff29ba32c9fe2a1f958b475d5931d50f6e8fd04fdb8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the slow convergence and limited capacity of low-rank PEFT methods like LoRA. It proposes FRoD, a method that enables full-rank updates via a shared basis and sparse perturbations, achieving faster convergence. The method matches full fine-tuning accuracy on diverse benchmarks while using only a tiny fraction of parameters.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [model selection, capability-cost frontier, constrained optimization, deployment-aware leaderboards, compliance trade-offs]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vassilis Digalakis Jr, Ramayya Krishnan, Gonzalo Martin Fernandez, Agni Orfanoudaki</p>
</li>
<li class="">
<p><strong>institution:</strong> Boston University, Carnegie Mellon University, Universitat Politècnica de Catalunya, Oxford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23487" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23487</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ML Compass, a framework that treats AI model selection as constrained optimization over a capability-cost frontier to bridge the gap between capability leaderboards and deployment decisions. 2. Characterizes optimal model configurations theoretically, showing a three-regime structure in internal measures and deriving comparative statics for budget, regulation, and technology changes. 3. Implements a practical pipeline that extracts internal measures, estimates an empirical frontier, learns task-specific utility, and validates with case studies in conversational and healthcare settings.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c28f58754a80430c57b0351355d200ab9fbfde8dd5fafc1b0d5caf4dd85bbb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c28f58754a80430c57b0351355d200ab9fbfde8dd5fafc1b0d5caf4dd85bbb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the gap between AI model capability rankings and real-world deployment decisions by introducing ML Compass, a framework that formulates model selection as constrained optimization over a capability-cost frontier. It combines theoretical analysis of optimal configurations with an implementation pipeline for recommendation, validated in conversational and healthcare case studies. The framework shows that deployment-aware rankings can differ significantly from capability-only leaderboards, clarifying trade-offs between capability, cost, and compliance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [communication &amp; networking], [URLLC, Link Adaptation, Device Scheduling, Deep Reinforcement Learning, Bayesian Optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wei Gao, Paul Zheng, Peng Wu, Yulin Hu, Anke Schmeink</p>
</li>
<li class="">
<p><strong>institution:</strong> Wuhan University, RWTH Aachen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23493" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23493</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a joint link adaptation and device scheduling design for multi-device URLLC IIoT networks under imperfect CSI, aiming to maximize total transmission rate under strict BLER constraints. 2. Introduces a novel Bayesian Optimization-driven Twin Delayed Deep Deterministic Policy Gradient (BO-TD3) method to adaptively determine device serving order and MCS based on outdated CQI. 3. Develops a BO-based training mechanism to address issues of error sample imbalance and TD3 parameter sensitivity, improving convergence speed and learning reliability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa25685331ccee6042c70838d3438022f95490a2cc609beba627f444cba8cd7c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa25685331ccee6042c70838d3438022f95490a2cc609beba627f444cba8cd7c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of joint link adaptation and device scheduling in URLLC IIoT networks with imperfect channel state information. It proposes a novel deep reinforcement learning method (BO-driven TD3) that adaptively selects the device serving order and modulation schemes, enhanced by Bayesian Optimization for faster and more reliable training. Simulation results show the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Trustworthy Machine Learning under Distribution Shifts</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [trustworthy machine learning], [distribution shift, robustness, explainability, adaptability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhuo Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Sydney</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23524" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23524</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a systematic framework for studying Trustworthy Machine Learning by categorizing three common types of distribution shifts (Perturbation, Domain, Modality). 2. Rigorously investigates trustworthiness through three key aspects: Robustness, Explainability, and Adaptability. 3. Aims to provide effective solutions and fundamental insights to enhance critical ML problems like efficiency and safety under distribution shifts.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8153efa2ea5f1ee68438cc93b22e23dd850b2f55b7ebc99a5374a4a32aea0bb4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8153efa2ea5f1ee68438cc93b22e23dd850b2f55b7ebc99a5374a4a32aea0bb4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This thesis addresses the core problem of distribution shift, which limits the reliability and trustworthiness of AI systems. The research proposes a framework that studies three types of distribution shifts and evaluates solutions through the lenses of robustness, explainability, and adaptability. The goal is to develop more reliable, versatile, and responsible machine learning models that can generalize effectively under real-world distribution shifts.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] EEG-based Graph-guided Domain Adaptation for Robust Cross-Session Emotion Recognition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [affective computing], [domain adaptation, graph regularization, EEG, emotion recognition, cross-session]</p>
</li>
<li class="">
<p><strong>authors:</strong> Maryam Mirzaei, Farzaneh Shayegh, Hamed Narimani</p>
</li>
<li class="">
<p><strong>institution:</strong> Based on author names and content, specific institution not provided. Could be inferred from typical academic affiliations in this field, but not explicitly stated in the given text.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23526" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23526</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed EGDA, a novel framework integrating domain adaptation with graph-based regularization for cross-session EEG emotion recognition. 2. Introduced a method to jointly align both marginal and conditional distributions while preserving the intrinsic data structure. 3. Demonstrated the discriminative power of the Gamma frequency band and identified critical brain regions (central-parietal and prefrontal) for emotion recognition.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97864260ef21b4d340e353b5b1666e9df5860104730e726e9067386c78cadc72_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97864260ef21b4d340e353b5b1666e9df5860104730e726e9067386c78cadc72_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of cross-session EEG emotion recognition by proposing EGDA, a framework that reduces distribution discrepancies through joint marginal and conditional alignment while using graph regularization to preserve data structure. Experiments on the SEED-IV dataset show EGDA outperforms baselines, achieving robust accuracies. The analysis further identifies the Gamma band and specific brain regions as key for reliable emotion recognition.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] VL-RouterBench: A Benchmark for Vision-Language Model Routing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [multi-modal inference], [vision-language model routing, benchmark, cost-accuracy trade-off, model selection, evaluation protocol]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhehao Huang, Baijiong Lin, Jingyuan Zhang, Jingying Wang, Yuhang Liu, Ning Lu, Tao Li, Xiaolin Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23562" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23562</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/K1nght/VL-RouterBench" target="_blank" rel="noopener noreferrer" class="">https://github.com/K1nght/VL-RouterBench</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes VL-RouterBench, the first systematic and reproducible benchmark for evaluating vision-language model (VLM) routing systems. 2. Constructs a large-scale evaluation foundation with quality and cost matrices over 519,180 sample-model pairs from 17 models and 14 datasets. 3. Introduces a comprehensive evaluation protocol that jointly measures accuracy, cost, and throughput, and uses a ranking score based on the harmonic mean for fair comparison across router configurations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/885d9087464eedead5301ad4cd041923ddee6d5773371e117abbd30fc4ae4f09_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/885d9087464eedead5301ad4cd041923ddee6d5773371e117abbd30fc4ae4f09_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces VL-RouterBench, a benchmark to systematically evaluate routing systems for vision-language models. It constructs matrices of quality and cost from extensive inference logs and uses a ranking score to compare routers. The evaluation shows current routers achieve significant gains but still fall short of an ideal Oracle, indicating room for improvement in router design.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Distribution-Free Process Monitoring with Conformal Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [anomaly detection], [Conformal Prediction, Statistical Process Control, Control Charts, Anomaly Detection, Quality Management]</p>
</li>
<li class="">
<p><strong>authors:</strong> Christopher Burger</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Mississippi</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23602" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23602</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A hybrid framework integrating Conformal Prediction&#x27;s distribution-free guarantees into Statistical Process Control (SPC). 2. Conformal-Enhanced Control Charts that visualize process uncertainty and enable proactive signals like &#x27;uncertainty spikes&#x27;. 3. Conformal-Enhanced Process Monitoring that reframes multivariate control as a formal anomaly detection problem using an intuitive p-value chart.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b25b4ce8de4803a0d1cc60a16d7221589651a9a271d92f85fb5b6426127e8b4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b25b4ce8de4803a0d1cc60a16d7221589651a9a271d92f85fb5b6426127e8b4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of traditional Statistical Process Control (SPC), which relies on often-violated statistical assumptions, by proposing a hybrid framework that integrates distribution-free Conformal Prediction. The method introduces two novel applications: enhanced control charts for visualizing uncertainty and a p-value chart for formal anomaly detection. The framework provides a more robust and statistically rigorous approach to quality control while maintaining the interpretability of classic methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [transfer learning], [Le Cam Distortion, Deficiency Distance, Directional Simulability, Unsupervised Domain Adaptation, Negative Transfer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Deniz Akdemir</p>
</li>
<li class="">
<p><strong>institution:</strong> None (Institution not specified in provided content)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23617" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23617</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a decision-theoretic framework for robust transfer learning based on Le Cam&#x27;s theory, replacing symmetric invariance with directional simulability. 2. Introduces Le Cam Distortion, quantified by the Deficiency Distance, as a rigorous upper bound for transfer risk. 3. Demonstrates the framework&#x27;s effectiveness across diverse experiments (genomics, vision, RL), showing it prevents source degradation and catastrophic negative transfer where traditional methods fail.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bd736028263c7eaaaaecae78f0df59f633374608f59295b1185c8385eea1e5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bd736028263c7eaaaaecae78f0df59f633374608f59295b1185c8385eea1e5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies a flaw in standard Unsupervised Domain Adaptation, which can cause harmful &quot;negative transfer&quot; by forcing invariance between unequally informative domains. It proposes a new framework based on Le Cam&#x27;s theory, using directional simulability and a metric called Le Cam Distortion to enable safe transfer without degrading the source domain. Experiments show this method successfully prevents information loss and catastrophic failure in safety-critical applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Regret-Based Federated Causal Discovery with Unknown Interventions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [causal discovery, unknown interventions, differential privacy, Φ-CPDAG, regret-based]</p>
</li>
<li class="">
<p><strong>authors:</strong> Federico Baldo, Charles K. Assaad</p>
</li>
<li class="">
<p><strong>institution:</strong> Sorbonne Université, INSERM, Institut Pierre Louis d&#x27;Epidémiologie et de Santé Publique</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23626" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23626</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes I-PERI, a novel federated causal discovery algorithm that handles unknown client-level interventions, 2. Introduces the Φ-Markov Equivalence Class (Φ-CPDAG), a tighter equivalence class derived from structural differences across clients, 3. Provides theoretical guarantees on convergence and privacy-preserving properties (differential privacy).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373e2e9b4041d9efb71fbe2d1095901813d7f2551cdfe37d40d79c04d7aa235d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373e2e9b4041d9efb71fbe2d1095901813d7f2551cdfe37d40d79c04d7aa235d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses federated causal discovery where client data are subject to unknown, heterogeneous interventions, a common real-world scenario overlooked by prior work. It proposes the I-PERI algorithm, which first recovers the union CPDAG and then orients additional edges by exploiting intervention-induced structural differences across clients, resulting in a tighter equivalence class called the Φ-CPDAG. Theoretical and empirical results demonstrate the algorithm&#x27;s effectiveness and privacy guarantees.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Memorization in 3D Shape Generation: An Empirical Study</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [3D shape generation], [memorization, diffusion models, latent vector-set, evaluation framework, data leakage]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shu Pu, Boya Zeng, Kaichen Zhou, Mengyu Wang, Zhuang Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> Princeton University, Harvard University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23628" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23628</a></p>
</li>
<li class="">
<p><strong>code:</strong> github.com/zlab-princeton/3d_mem</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a novel evaluation framework to quantify memorization in 3D generative models. 2. Applied the framework to benchmark and quantify memorization in existing 3D generation methods. 3. Conducted controlled experiments to identify how data and modeling factors (e.g., modality, guidance scale, augmentation) influence memorization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca470d9a610f6e7172776f63b7bfed02850fdb4a843786976b699c63c87febc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca470d9a610f6e7172776f63b7bfed02850fdb4a843786976b699c63c87febc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether 3D shape generative models memorize training data. The authors design an evaluation framework to measure memorization and conduct experiments with a latent vector-set diffusion model. They find memorization depends on data modality and diversity, peaks at moderate guidance, and can be reduced with longer latent vectors and rotation augmentation without harming quality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [multi-agent systems, hierarchical agents, bandit optimization, software engineering agents, SWE-bench]</p>
</li>
<li class="">
<p><strong>authors:</strong> Iris Xu, Guangtao Zeng, Zexue He, Charles Jin, Aldo Pareja, Dan Gutfreund, Chuang Gan, Zhang-Wei Hong</p>
</li>
<li class="">
<p><strong>institution:</strong> Massachusetts Institute of Technology, MIT-IBM Watson AI Lab, Stanford University, University of Massachusetts Amherst</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23631" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23631</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/iamxjy/BOAD-SWE-Agent" target="_blank" rel="noopener noreferrer" class="">https://github.com/iamxjy/BOAD-SWE-Agent</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulates the automatic discovery of effective hierarchical multi-agent systems for software engineering as a multi-armed bandit (MAB) problem, enabling efficient exploration under limited budgets. 2. Proposes the BOAD framework, which uses bandit optimization to coordinate specialized sub-agents (e.g., for localization, editing, validation) and attribute credit within a team. 3. Demonstrates that automatically discovered hierarchical agents outperform single-agent and manually designed multi-agent systems on challenging, out-of-distribution SWE benchmarks, including achieving second place on SWE-bench-Live with a 36B model.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67084d40c24e3869f47efa4b52c7ec1479016d613997e911c6730d7e7684254f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67084d40c24e3869f47efa4b52c7ec1479016d613997e911c6730d7e7684254f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the poor generalization of single-agent LLMs on long-horizon, out-of-distribution software engineering tasks by proposing a hierarchical multi-agent system. The core method, BOAD, automatically discovers effective agent hierarchies by formulating the search as a multi-armed bandit optimization problem. The results show that this approach significantly improves performance on SWE benchmarks, surpassing larger models like GPT-4 and Claude.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [educational ai], [generative AI, fine-tuning, randomized controlled trial, Socratic questioning, pedagogical instruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> LearnLM Team Google, Eedi, Albert Wang, Aliya Rysbek, Andrea Huber, Anjali Nambiar, Anna Kenolty, Ben Caulfield, Beth Lilley-Draper, Bibi Groot, Brian Veprek, Chelsea Burdett, Claire Willis, Craig Barton, Digory Smith, George Mu, Harriet Walters, Irina Jurenka, Iris Hulls, James Stalley-Moores, Jonathan Caton, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Liam McCafferty, Lucy Dalton, Markus Kunesch, Pauline Malubay, Rachel Kidson, Rich Wells, Sam Wheeler, Sara Wiltberger, Shakir Mohamed, Simon Woodhead, Vasco Brazão</p>
</li>
<li class="">
<p><strong>institution:</strong> Google, Eedi</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23633" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23633</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a rigorous, in-classroom exploratory RCT to evaluate the safety and efficacy of a generative AI tutor (LearnLM) in a real educational setting. 2. Demonstrated that a pedagogically fine-tuned AI model can reliably draft instructional content, with human tutors approving 76.4% of its messages with minimal or no edits. 3. Showed that AI-supported tutoring led to student performance at least equivalent to human-only tutoring, with a significant 5.5 percentage point improvement in solving novel problems on subsequent topics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b430e7c78534d660126208f226397ed95756e540bade56276301199a0114bc76_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b430e7c78534d660126208f226397ed95756e540bade56276301199a0114bc76_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether generative AI can scale effective one-to-one tutoring. The authors integrated LearnLM, a pedagogically fine-tuned AI model, into a math tutoring platform and conducted a randomized controlled trial where human tutors supervised its outputs. The results show that LearnLM was a reliable tutor, and students using it performed as well as or better than those with human tutors alone, suggesting AI can deliver effective, individualized learning support at scale.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Simultaneous Approximation of the Score Function and Its Derivatives by Deep Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [score function, approximation theory, deep neural networks, curse of dimensionality, Ornstein-Uhlenbeck process]</p>
</li>
<li class="">
<p><strong>authors:</strong> Konstantin Yakovlev, Nikita Puchkin</p>
</li>
<li class="">
<p><strong>institution:</strong> HSE University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23643" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23643</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Presents a theory for simultaneous approximation of the score function and its derivatives, extending beyond the first-order setting. 2. Derives approximation error bounds that are free from the curse of dimensionality. 3. Relaxes the common assumption of bounded data support, enabling handling of distributions with low-dimensional structure and unbounded support.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3404ba95328f082dc11706309fc0e30ca110b3f842a24921698b46a1065bfda6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3404ba95328f082dc11706309fc0e30ca110b3f842a24921698b46a1065bfda6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper develops a theoretical framework for using deep neural networks to approximate the score function and its derivatives simultaneously. The method relaxes the typical bounded support assumption and provides error bounds that avoid the curse of dimensionality. The main conclusion is that this theory enables more efficient handling of complex data distributions, which is crucial for improving the convergence of diffusion and ODE-based generative models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Random Controlled Differential Equations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time-series learning], [controlled differential equations, random features, signature kernels, reservoir computing, rough paths]</p>
</li>
<li class="">
<p><strong>authors:</strong> Francesco Piatti, Thomas Cass, William F. Turner</p>
</li>
<li class="">
<p><strong>institution:</strong> Imperial College London</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23670" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23670</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/FrancescoPiatti/RandomSigJax" target="_blank" rel="noopener noreferrer" class="">https://github.com/FrancescoPiatti/RandomSigJax</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A training-efficient framework combining random features with Controlled Differential Equations (CDEs) to create continuous-time reservoirs for time-series learning. 2. Two novel variants: Random Fourier CDEs (RF-CDEs) for kernel-free RBF approximation and Random Rough DEs (R-RDEs) for stable, efficient modeling of rough-path inputs. 3. Theoretical proof that these models induce the RBF-lifted and rough signature kernels in the infinite-width limit, unifying random-feature reservoirs, continuous-time architectures, and path-signature theory.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68926f52841c63ea8f7ccc6a25444c327f8f05d4f014a21ea65330e8b1d0af5a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68926f52841c63ea8f7ccc6a25444c327f8f05d4f014a21ea65330e8b1d0af5a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces a fast and scalable framework for time-series learning by using large, randomly parameterized Controlled Differential Equations (CDEs) as continuous-time reservoirs, with only a linear readout layer trained. Two specific model variants, RF-CDEs and R-RDEs, are proposed and shown to approximate powerful signature kernels. The methods achieve competitive or state-of-the-art performance on benchmarks, offering a practical alternative to explicit signature computations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] End-to-End Test-Time Training for Long Context</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [test-time training, meta-learning, sliding-window attention, continual learning, long-context modeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Arnuv Tandon, Karan Dalal, Xinhao Li, Daniel Koceja, Marcel Rød, Sam Buchanan, Xiaolong Wang, Jure Leskovec, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin, Jed McCaleb, Yejin Choi, Yu Sun</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University, UC Berkeley, UC San Diego, Astera Institute, NVIDIA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23675</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulates long-context modeling as a continual learning problem, enabling a standard sliding-window Transformer to learn at test time via next-token prediction, 2. Uses meta-learning during training to optimize the model&#x27;s initialization for efficient test-time learning, 3. Achieves scaling performance comparable to full-attention Transformers while maintaining constant inference latency like RNNs, resulting in significant speedups for long contexts.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a044c036c50d93caf06213291a7e5d53aecb58cc761c7738c635ddb4234a64d3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a044c036c50d93caf06213291a7e5d53aecb58cc761c7738c635ddb4234a64d3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an end-to-end test-time training method for long-context language modeling. It uses a standard sliding-window attention Transformer that learns continuously at test time via next-token prediction, with its initialization optimized via meta-learning during training. The method matches the scaling performance of full-attention Transformers while offering constant inference latency, making it 2.7x faster for 128K contexts.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Eliciting Behaviors in Multi-Turn Conversations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [llm evaluation], [behavior elicitation, multi-turn conversation, online methods, dynamic benchmarks, test case generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jing Huang, Shujian Zhang, Lun Wang, Andrew Hard, Rajiv Mathews, John Lambert</p>
</li>
<li class="">
<p><strong>institution:</strong> Google DeepMind, Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23701" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23701</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an analytical framework categorizing behavior elicitation methods into three families based on their interaction with the target model (prior knowledge, offline, online). 2. Introduces a generalized multi-turn formulation for online behavior elicitation methods, unifying single-turn and multi-turn settings. 3. Demonstrates the superior efficiency of online methods in discovering failure cases in multi-turn conversations compared to static benchmarks, advocating for a shift to dynamic evaluation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afe3305263c3b509bdd6e846cce6501d101c0ecedb788ef025ac0c9405a28103_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afe3305263c3b509bdd6e846cce6501d101c0ecedb788ef025ac0c9405a28103_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the problem of efficiently eliciting specific behaviors from large language models in multi-turn conversational settings. It introduces a framework for categorizing existing elicitation methods and proposes a generalized online method for multi-turn interactions. The key finding is that online methods can discover many more failure cases with few queries than static benchmarks, highlighting the need for dynamic evaluation approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] UniFi: Combining Irregularly Sampled CSI from Diverse Communication Packets and Frequency Bands for Wi-Fi Sensing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [Integrated Sensing and Communication (ISAC), Channel State Information (CSI), Attention Model, Irregular Sampling, Wi-Fi Sensing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gaofeng Dong, Kang Yang, Mani Srivastava</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Los Angeles (UCLA)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22143" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22143</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes UniFi, the first Wi-Fi ISAC framework that eliminates intrusive packet injection by exploiting irregularly sampled CSI from diverse communication packets across multiple bands. 2. Introduces a CSI sanitization pipeline to harmonize heterogeneous packets and a time-aware attention model that learns directly from non-uniform CSI sequences. 3. Presents CommCSI-HAR, the first dataset with irregularly sampled CSI from real-world dual-band communication traffic.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d5e6f4970d32933635a3174be0b8ba23c4b996e18b04bd75310812bd1071737_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d5e6f4970d32933635a3174be0b8ba23c4b996e18b04bd75310812bd1071737_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents UniFi, a Wi-Fi sensing framework that solves the problem of communication degradation caused by high-rate probing packets. It achieves this by directly using irregularly sampled Channel State Information from existing communication traffic across multiple frequency bands, combined with a novel sanitization pipeline and a time-aware attention model. Evaluations show that UniFi achieves state-of-the-art sensing accuracy while fully preserving communication throughput.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Training AI Co-Scientists Using Rubric Rewards</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [research plan generation, self-grading, rubric rewards]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shashwat Goel, Rishi Hazra, Dulhan Jayalath, Timon Willi, Parag Jain, William F. Shen, Ilias Leontiadis, Francesco Barbieri, Yoram Bachrach, Jonas Geiping, Chenxi Whitehouse</p>
</li>
<li class="">
<p><strong>institution:</strong> Meta Superintelligence Labs, ELLIS Institute Tübingen, Max Planck Institute for Intelligent Systems, University of Oxford, University of Cambridge</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23707" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23707</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A scalable method to automatically extract research goals and goal-specific grading rubrics from existing papers to build a training corpus. 2. A reinforcement learning framework with self-grading, where a frozen initial model acts as the grader using rubrics, enabling unsupervised improvement. 3. Demonstration of significant performance gains (12-22% relative improvement) and cross-domain generalization (e.g., to medical research) validated by human experts and frontier model juries.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/360be253dbca068ab35e1e7dcf794f5e43ae1d6fd7478850692d4a8ffc057d14_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/360be253dbca068ab35e1e7dcf794f5e43ae1d6fd7478850692d4a8ffc057d14_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of training language models to generate high-quality, constraint-following research plans. The proposed method uses reinforcement learning with self-grading, where rubrics automatically extracted from research papers provide reward signals. The approach shows significant improvements in plan quality and generalizes across domains like machine learning and medicine, validated by human expert preference.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] EEG-to-Voice Decoding of Spoken and Imagined speech Using Non-Invasive EEG</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [brain-computer interface], [EEG-to-Voice, mel-spectrogram, domain adaptation, automatic speech recognition, language model correction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hanbeot Park, Yunjeong Cho, Hunhee Kim</p>
</li>
<li class="">
<p><strong>institution:</strong> Pukyong National University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22146" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22146</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a direct, open-loop EEG-to-Voice reconstruction pipeline that generates mel-spectrograms from EEG without requiring dynamic time warping or explicit temporal alignment. 2. Applied transfer learning-based domain adaptation by pretraining a subject-specific generator on spoken speech and adapting it to imagined speech. 3. Integrated a minimal language model-based correction module to reduce ASR errors while preserving semantic structure, improving linguistic accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30fda90c01417e82377bfcdd82830a196b29afd74925e2009f1a1a1d02d4d2bd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30fda90c01417e82377bfcdd82830a196b29afd74925e2009f1a1a1d02d4d2bd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a direct EEG-to-Voice paradigm to reconstruct speech from non-invasive EEG signals for both spoken and imagined speech. The method uses a subject-specific generator to produce mel-spectrograms, followed by a vocoder and ASR, and employs domain adaptation from spoken to imagined speech. The results demonstrate the feasibility of open-loop speech reconstruction without explicit temporal alignment, with stable acoustic and linguistic performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Machine Learning-Based Basil Yield Prediction in IoT-Enabled Indoor Vertical Hydroponic Farms</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [indoor vertical hydroponics, IoT sensors, LSTM, DNN, Linear Regression]</p>
</li>
<li class="">
<p><strong>authors:</strong> Emna Bouzid, Noura Baccar, Kamran Iqbal, Yassine Chaouch, Fares Ben Youssef, Amine Regayeg, Sarra Toumi, Houda Nsir, Amina Mseddi, Leila Costelle</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Arkansas, Little Rock; Mediterranean Institute of Technology, South Mediterranean University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22151" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22151</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a prediction system for basil yield in IoT-enabled indoor vertical hydroponic farms using ML models. 2. Conducted a comparative performance analysis of Linear Regression, LSTM, and DNN models, evaluating accuracy, execution time, and RAM usage. 3. Identified DNN as offering an optimal balance between computational efficiency (speed/RAM) and high prediction accuracy (98%), making it suitable for real-world, resource-conscious deployment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db5f20edab19e2fc847a47e72931db82045d6fc345183bcd4be1eb31da0f25e0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db5f20edab19e2fc847a47e72931db82045d6fc345183bcd4be1eb31da0f25e0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses water scarcity in agriculture by proposing a machine learning-based system to predict basil yield in IoT-enabled indoor vertical hydroponic farms. It compares Linear Regression, LSTM, and DNN models using sensor data, finding that DNN provides a good trade-off between high accuracy (98%) and computational efficiency, making it suitable for practical deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Complete Anatomy of the Madden-Julian Oscillation Revealed by Artificial Intelligence</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [climate informatics], [similarity-preserving representation, latent space clustering, physics-coherent monitoring]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiao Zhou, Yuze Sun, Jie Wu, Xiaomeng Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, National Climate Centre, China Meteorological Administration</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22144" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22144</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced an &quot;AI-for-theory&quot; paradigm using a deep learning model (PhysAnchor-MJO-AE) to learn a latent representation where distance corresponds to physical-feature similarity for the MJO. 2. Objectively discovered the first complete six-phase anatomical map of the MJO life cycle, isolating two long-hypothesized transitional phases. 3. Constructed a new physics-coherent monitoring framework that decouples location and intensity, drastically reducing spurious propagation and convective misplacement compared to classical methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b09901ad992d27215117f20984faf17d508a192bf9010cc39bd8b74553ff543_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b09901ad992d27215117f20984faf17d508a192bf9010cc39bd8b74553ff543_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of objectively defining the life cycle of the Madden-Julian Oscillation (MJO) by introducing an &quot;AI-for-theory&quot; paradigm. It develops a deep learning model to learn a similarity-preserving latent representation, enabling clustering that reveals a complete six-phase anatomy of the MJO. The derived new monitoring framework significantly outperforms the classical index, demonstrating AI&#x27;s role as a discovery tool for complex systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Neural ocean forecasting from sparse satellite-derived observations: a case-study for SSH dynamics and altimetry data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [spatiotemporal forecasting], [4DVarNet, U-Net, sequence-to-sequence, sea level anomaly, neural forecast]</p>
</li>
<li class="">
<p><strong>authors:</strong> Daria Botvynko, Pierre Haslée, Lucile Gaultier, Bertrand Chapron, Clement de Boyer Montégut, Anass El Aouni, Julien Le Sommer, Ronan Fablet</p>
</li>
<li class="">
<p><strong>institution:</strong> IMT Atlantique, Ifremer, CNRS, Mercator Ocean International</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22152" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22152</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Adapts U-Net and 4DVarNet architectures for short-term forecasting of ocean dynamics from sparse satellite data. 2. Formulates the forecasting task as a sequence-to-sequence mapping using partial SLA snapshots to predict future full-field maps. 3. Demonstrates that the end-to-end neural framework outperforms an operational baseline, especially in high-variability regions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b91a25a42a5dc1b5739ae5689c604765502ed07062eadaa473e043ec5a0d288f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b91a25a42a5dc1b5739ae5689c604765502ed07062eadaa473e043ec5a0d288f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an end-to-end deep learning framework for 7-day forecasting of sea surface dynamics using sparse satellite altimetry data. It adapts U-Net and 4DVarNet models to perform sequence-to-sequence mapping from partial observations to full-field forecasts. The results show the neural model outperforms an operational ocean forecast product, demonstrating the feasibility of neural forecasting for operational oceanography under data-sparse conditions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Sampling with Shielded Langevin Monte Carlo Using Navigation Potentials</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [constrained sampling], [Langevin Monte Carlo, navigation functions, constrained sampling, non-convex support, adaptive temperature]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nicolas Zilberstein, Santiago Segarra, Luiz Chamon</p>
</li>
<li class="">
<p><strong>institution:</strong> Rice University, École Polytechnique (Institut Polytechnique de Paris)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22153" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22153</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces shielded Langevin Monte Carlo (LMC) for sampling from distributions with non-convex supports defined by convex sets with convex holes. 2. Incorporates a navigation function-inspired approach using a spatially adaptive temperature and repulsive drift to keep samples within feasible regions. 3. Demonstrates effectiveness through experiments on 2D Gaussian mixture and MIMO symbol detection, showing advantages over unconstrained methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b94714cf81960de10c530b580016be757aee25e63d9344efa57a771bb15c9bc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b94714cf81960de10c530b580016be757aee25e63d9344efa57a771bb15c9bc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes shielded Langevin Monte Carlo, a constrained sampling method that uses navigation potentials to sample from unnormalized target distributions over punctured (non-convex) supports. It modifies the Langevin diffusion with adaptive temperature and repulsive drift to avoid holes. Experiments on Gaussian mixtures and MIMO detection show it outperforms unconstrained sampling.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PaperNet: Efficient Temporal Convolutions and Channel Residual Attention for EEG Epilepsy Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [brain-computer interface (BCI)], [temporal convolution, residual attention, recurrent networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Shahriar Sajid, Abhijit Kumar Ghosh, Fariha Nusrat</p>
</li>
<li class="">
<p><strong>institution:</strong> Rajshahi University of Engineering &amp; Technology, BRAC University, University of Asia Pacific</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22172" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22172</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed PaperNet, a compact hybrid architecture combining temporal convolutions, channel-wise residual attention, and a lightweight bidirectional recurrent block for EEG classification. 2. Demonstrated high performance (macro-F1 0.96) on the BEED dataset with only ~0.6M parameters under a subject-independent protocol. 3. Provided interpretability through channel-wise attention weights to reveal electrode relevance and validated efficiency for deployment on resource-constrained systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42ceecec9d77194ecb3fce40cd41e394fe9fe473136aa06f3cec099aa5631ac0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42ceecec9d77194ecb3fce40cd41e394fe9fe473136aa06f3cec099aa5631ac0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces PaperNet, a lightweight deep learning model that integrates temporal convolutions, channel residual attention, and a bidirectional recurrent block for efficient EEG epilepsy detection. It achieves a macro-F1 score of 0.96 on the BEED dataset with only about 0.6 million parameters, showing balanced performance across classes. The results indicate that combining temporal filtering, channel reweighting, and recurrent context modeling can deliver strong classification without high computational cost.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On Fibonacci Ensembles: An Alternative Approach to Ensemble Learning Inspired by the Timeless Architecture of the Golden Ratio</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [ensemble learning], [Fibonacci weighting, Rao-Blackwell optimization, variance reduction, recursive ensemble, orthogonalization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ernest Fokoué</p>
</li>
<li class="">
<p><strong>institution:</strong> Rochester Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22284" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22284</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Fibonacci Ensembles, a novel ensemble learning framework using normalized Fibonacci weights optimized via orthogonalization and Rao-Blackwellization for systematic variance reduction. 2. Proposes a second-order recursive ensemble dynamic inspired by the Fibonacci sequence to enhance representational depth beyond classical boosting. 3. Develops a General Weighting Theory that unifies various ensemble methods (bagging, boosting, stacking, etc.) under a single mathematical framework as distributional operators.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3aac35c4c829c693f4e9607fedf9124e05465f66f615b3d6ff040b6b1d9348a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3aac35c4c829c693f4e9607fedf9124e05465f66f615b3d6ff040b6b1d9348a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Fibonacci Ensembles, a new ensemble learning method inspired by the Fibonacci sequence, which uses mathematically optimized Fibonacci weights and a recursive dynamic to reduce variance and improve model depth. Experimental results on one-dimensional regression show it can match or outperform uniform averaging and integrates effectively with orthogonal Rao-Blackwellization. The work suggests Fibonacci ensembles offer a natural and interpretable design within ensemble theory.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A review of NMF, PLSA, LBA, EMA, and LCA with a focus on the identifiability issue</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [matrix factorization], [nonnegative matrix factorization, identifiability, latent class analysis, probabilistic latent semantic analysis, end-member analysis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qianqian Qi, Peter G. M. van der Heijden</p>
</li>
<li class="">
<p><strong>institution:</strong> Hangzhou Dianzi University, Utrecht University, University of Southampton</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22282" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22282</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Highlights the similarities among five popular matrix factorization models (LBA, LCA, EMA, PLSA, NMF) that are often presented separately across different fields. 2. Proves a unified identifiability condition, showing that the solution uniqueness for LBA, EMA, LCA, and PLSA is equivalent to the uniqueness of the NMF solution. 3. Provides a brief review of algorithms for these models and illustrates their application with a social science time budget dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369a6018ccc708c5d8368b9a1290021cb1b1e9d92785fa247417c9aeb167a164_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369a6018ccc708c5d8368b9a1290021cb1b1e9d92785fa247417c9aeb167a164_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper reviews and unifies five nonnegative matrix factorization models (NMF, PLSA, LBA, EMA, LCA) from different disciplines, focusing on the identifiability issue. It proves that the uniqueness of solutions for LBA, EMA, LCA, and PLSA is equivalent to the uniqueness of the NMF solution. The work clarifies model similarities, reviews algorithms, and demonstrates application with a real-world dataset.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A General Weighting Theory for Ensemble Learning: Beyond Variance Reduction via Spectral and Geometric Structure</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [ensemble learning], [weighting theory, spectral complexity, approximation geometry, bias-variance decomposition, constrained quadratic program]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ernest Fokoué</p>
</li>
<li class="">
<p><strong>institution:</strong> Rochester Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22286" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22286</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Develops a general weighting theory for ensembles that moves beyond variance reduction, formalizing ensembles as linear operators with geometric and spectral constraints. 2. Derives a refined bias-variance-approximation decomposition showing how structured weights can outperform uniform averaging by reshaping approximation geometry and redistributing spectral complexity. 3. Provides a unified theoretical framework that subsumes classical averaging, stacking, and recent Fibonacci-based ensembles, showing optimal weights arise from constrained quadratic programs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a91a59418696aa33fbf07981fc6c57647153631a58f387a01a431ae28237ebb8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a91a59418696aa33fbf07981fc6c57647153631a58f387a01a431ae28237ebb8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a new theoretical framework for ensemble learning that explains its effectiveness beyond the traditional variance-reduction argument, particularly for stable base learners. The method formalizes ensembles as linear operators and shows how structured, non-uniform weighting can optimize performance by managing spectral complexity and approximation geometry. The main conclusion is that the principal role of aggregation for low-variance learners is the redistribution of spectral complexity, establishing a foundation for structure-driven ensemble design.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [transformer interpretability], [cross-entropy, gradient dynamics, attention mechanism, expectation-maximization, Bayesian inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra</p>
</li>
<li class="">
<p><strong>institution:</strong> Dream Sports, Columbia University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22473" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22473</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Derived an advantage-based routing law and a responsibility-weighted update rule for attention scores and values under cross-entropy training. 2. Showed that the coupled gradient dynamics induce a positive feedback loop that behaves like a two-timescale Expectation-Maximization (EM) procedure. 3. Demonstrated that these gradient dynamics sculpt the low-dimensional manifolds necessary for Bayesian inference, linking optimization to geometry and function.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed145ec9892ca4b4f8b91e5c78948ac6b81399c20bcafdccd4cb429e92da2aed_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed145ec9892ca4b4f8b91e5c78948ac6b81399c20bcafdccd4cb429e92da2aed_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes how cross-entropy training shapes the internal geometry of transformer attention heads. By deriving first-order gradient dynamics, it shows that attention score and value updates form a positive feedback loop analogous to an EM algorithm. The core conclusion is that this gradient flow sculpts the Bayesian manifolds that enable in-context probabilistic reasoning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Integrating Wide and Deep Neural Networks with Squeeze-and-Excitation Blocks for Multi-Target Property Prediction in Additively Manufactured Fiber Reinforced Composites</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-target regression], [squeeze-and-excitation blocks, wide-and-deep neural networks, Latin Hypercube Sampling, SHAP analysis, multi-input multi-target learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Behzad Parvaresh, Rahmat K. Adesunkanmi, Adel Alaeddini</p>
</li>
<li class="">
<p><strong>institution:</strong> Southern Methodist University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22397" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22397</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a data-efficient multi-input, multi-target learning approach integrating Latin Hypercube Sampling (LHS) with a squeeze-and-excitation wide and deep neural network (SE-WDNN) for predicting mechanical and manufacturing properties of additively manufactured fiber-reinforced composites. 2. Demonstrated superior performance of SE-WDNN over baseline models (e.g., feedforward neural networks, XGBoost) with the lowest overall test error (MAPE=12.33%) and statistically significant improvements for several target variables. 3. Provided interpretability through SHAP analysis, identifying reinforcement strategy as the major influence on mechanical performance, enabling guided parameter selection balancing mechanical behavior and manufacturing metrics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46886a511640f1fda32bb5caad6d94ff9e4208332562a065cf26f5a070434085_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46886a511640f1fda32bb5caad6d94ff9e4208332562a065cf26f5a070434085_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of predicting multiple properties in additively manufactured fiber-reinforced composites, where performance is sensitive to process and material parameters. The authors propose a sample-efficient method combining Latin Hypercube Sampling with a novel squeeze-and-excitation wide and deep neural network (SE-WDNN) to jointly predict mechanical and manufacturing properties. The model outperforms several baseline machine learning models, achieving the lowest test error, and SHAP analysis reveals that reinforcement strategy is the most influential factor, demonstrating the approach&#x27;s effectiveness for interpretable, multi-target prediction in this domain.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Uncertainty-Aware Flow Field Reconstruction Using SVGP Kolmogorov-Arnold Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [uncertainty quantification], [sparse variational Gaussian processes, Kolmogorov-Arnold networks, flow reconstruction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Y. Sungtaek Ju</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Los Angeles</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22426" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22426</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel machine learning framework (SVGP-KAN) for uncertainty-aware flow field reconstruction, combining sparse variational Gaussian processes with Kolmogorov-Arnold network topology. 2. Enables principled epistemic uncertainty quantification, extending classical methods like Linear Stochastic Estimation (LSE) and Spectral Analysis Modal Methods (SAMM). 3. Provides a systematic evaluation demonstrating that the method achieves accuracy comparable to established techniques while offering well-calibrated uncertainty estimates that reliably indicate prediction quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78057a4908248b84f2c93949508c1a8ed187c23a15c17c9cb4d97da3da80d1a6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78057a4908248b84f2c93949508c1a8ed187c23a15c17c9cb4d97da3da80d1a6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a machine learning framework called SVGP-KAN for reconstructing time-resolved flow fields from sparse measurements. The method combines sparse variational Gaussian processes with Kolmogorov-Arnold networks to provide accurate reconstructions along with principled uncertainty estimates. The results show the framework achieves comparable accuracy to classical methods while offering reliable uncertainty quantification, which is valuable for experimental design in periodic flows.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Likelihood-Preserving Embeddings for Statistical Inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [statistical inference], [likelihood-preserving embeddings, likelihood-ratio distortion, Hinge Theorem, approximate sufficient statistics, surrogate MLE]</p>
</li>
<li class="">
<p><strong>authors:</strong> Deniz Akdemir</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly provided; inferred from email domain as independent researcher or unspecified institution.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22638" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22638</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Likelihood-Ratio Distortion metric and the Hinge Theorem, establishing it as the necessary and sufficient condition for preserving likelihood-based inference. 2. Proves an impossibility result for universal likelihood preservation, motivating model-class-specific guarantees. 3. Provides a constructive framework using neural networks as approximate sufficient statistics with explicit bounds linking training loss to inferential guarantees.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aff5ca31934e9408fcf0755ba3d0be2604e2c3a6692b27396516614ac67f2b0a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aff5ca31934e9408fcf0755ba3d0be2604e2c3a6692b27396516614ac67f2b0a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that modern machine learning embeddings often destroy the geometric structure needed for classical likelihood-based statistical inference. It proposes a theory of likelihood-preserving embeddings, centered on controlling the Likelihood-Ratio Distortion, and proves that this control is necessary and sufficient to preserve tests, Bayes factors, and MLEs. The main conclusion is that with this framework, neural network embeddings can be made compatible with classical inference workflows under specific, provable conditions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Machine learning models for predicting catastrophe bond coupons using climate data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [financial machine learning], [catastrophe bonds, climate indicators, extremely randomized trees, gradient boosting, risk pricing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Julia Kończal, Michał Balcerek, Krzysztof Burnecki</p>
</li>
<li class="">
<p><strong>institution:</strong> Wrocław University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22660" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22660</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel integration of large-scale climate indicators (e.g., ONI, NAO, SSTs) into the prediction of catastrophe bond coupon rates. 2. Systematically compares the performance of linear regression against advanced tree-based ensemble methods (RF, GBM, ERT, XGBoost) for this financial prediction task. 3. Demonstrates that including climate variables improves predictive accuracy across all models, with Extremely Randomized Trees achieving the best performance, quantifying the influence of climate variability on CAT bond pricing.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad97e42c90e08944209aa776783ca45b8fd5114532eaa6a8db48fee77ac0f8e2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad97e42c90e08944209aa776783ca45b8fd5114532eaa6a8db48fee77ac0f8e2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the use of machine learning models to predict catastrophe bond coupons by incorporating climate data. The authors combine traditional financial features with climate indicators and compare models like linear regression, random forest, and gradient boosting. The results show that climate variables improve prediction accuracy, with extremely randomized trees performing best, indicating that climate variability significantly impacts bond pricing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [medical audio classification], [Audio Spectrogram Transformer, Sharpness-Aware Minimization, ICBHI 2017, class imbalance, loss landscape]</p>
</li>
<li class="">
<p><strong>authors:</strong> Atakan Işık, Selin Vulga Işık, Ahmet Feridun Işık, Mahşuk Taylan</p>
</li>
<li class="">
<p><strong>institution:</strong> Başkent University, Gaziantep University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22564" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22564</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel framework integrating the Audio Spectrogram Transformer (AST) with Sharpness-Aware Minimization (SAM) for robust respiratory sound classification. 2. Implements a weighted sampling strategy to effectively handle the severe class imbalance present in medical datasets like ICBHI 2017. 3. Achieves state-of-the-art performance on the ICBHI 2017 dataset, with a particular focus on improving sensitivity for reliable clinical screening.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0861fcb0d50b762d97367d04dce9ca920f2ffca74cd5112df95b11652972a9b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0861fcb0d50b762d97367d04dce9ca920f2ffca74cd5112df95b11652972a9b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenges of respiratory sound classification, such as data scarcity and class imbalance, by enhancing the Audio Spectrogram Transformer with Sharpness-Aware Minimization (SAM) to find flatter minima for better generalization. The method also employs weighted sampling and achieves a new state-of-the-art score of 68.10% and a sensitivity of 68.31% on the ICBHI 2017 dataset, demonstrating improved robustness for clinical applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Nonlinear Dynamical Modeling of Human Intracranial Brain Activity with Flexible Inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [computational neuroscience], [DFINE, state-space models, intracranial EEG, neural forecasting, brain-computer interfaces]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kiarash Vaziri, Lucine L. Oganesian, HyeongChan Jo, Roberto M.C. Vera, Charles Y. Liu, Brian Lee, Maryam M. Shanechi</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Southern California</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22785" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22785</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Extended the DFINE framework, originally for intracortical recordings, to model multisite human intracranial EEG (iEEG) signals. 2. Demonstrated that DFINE significantly outperforms linear state-space models (LSSMs) and matches or exceeds the accuracy of GRU models in forecasting future neural activity. 3. Showed that DFINE handles missing observations more robustly than baseline models, highlighting its flexible inference capability for practical BCI applications.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b25c55145e75d86ed37939fcb3e028529910a649ed86c8bd2fdb2e8fb91c496_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b25c55145e75d86ed37939fcb3e028529910a649ed86c8bd2fdb2e8fb91c496_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper extends the DFINE framework to model nonlinear dynamics in human intracranial EEG (iEEG) data. DFINE combines neural networks with a linear state-space model backbone to enable accurate neural forecasting and robust handling of missing data. The results show DFINE outperforms linear models, matches or beats GRU performance, and is particularly effective in high gamma bands, making it a promising tool for next-generation brain-computer interfaces.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Causal-Policy Forest for End-to-End Policy Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [causal inference], [policy learning, causal forest, conditional average treatment effect (CATE), end-to-end learning, random forests]</p>
</li>
<li class="">
<p><strong>authors:</strong> Masahiro Kato</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Tokyo</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22846" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22846</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Establishes an equivalence between maximizing policy value and minimizing MSE for CATE under a specific regression model, providing a theoretical foundation. 2. Proposes the causal-policy forest, a novel end-to-end algorithm that modifies the widely-used causal forest for direct policy learning. 3. Integrates policy training steps more tightly than prior methods, avoiding separate nuisance parameter estimation and improving computational efficiency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2b4476f0b9f739e0d89d20df3fd68607b0631baf53a2956bd05175daf10d348_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2b4476f0b9f739e0d89d20df3fd68607b0631baf53a2956bd05175daf10d348_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an end-to-end algorithm for policy learning in causal inference, called the causal-policy forest. It modifies the causal forest method by leveraging a theoretical equivalence between policy value maximization and CATE estimation. The method unifies policy training steps, is computationally efficient, and bridges the gap between policy learning and CATE estimation in practice.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A first-order method for nonconvex-strongly-concave constrained minimax optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [optimization theory], [minimax optimization, augmented Lagrangian method, first-order method, operation complexity, nonconvex-strongly-concave]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhaosong Lu, Sanyou Mei</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Minnesota, The Hong Kong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22909" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22909</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a first-order augmented Lagrangian method for solving nonconvex-strongly-concave constrained minimax problems. 2. Develops a new first-order method to solve the resulting unconstrained minimax subproblems by leveraging the strong concavity structure. 3. Establishes an improved operation complexity of O(ε^{-3.5} log ε^{-1}) for finding an ε-KKT solution, which is a factor of ε^{-0.5} better than the previous best-known result.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8900c67d50fc273f41601bb4bbe900a26bc3aacdb1b31495591bc5c452297c76_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8900c67d50fc273f41601bb4bbe900a26bc3aacdb1b31495591bc5c452297c76_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of nonconvex-strongly-concave constrained minimax optimization. The authors propose a novel first-order augmented Lagrangian method, where the subproblems are solved by a specially designed first-order algorithm. The main result is that their method achieves an improved operation complexity of O(ε^{-3.5} log ε^{-1}) for finding an approximate KKT solution.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Deep Learning for the Multiple Optimal Stopping Problem</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning / optimal stopping], [multiple optimal stopping, dynamic programming principle, neural network approximation, high-dimensional problems, American basket options]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mathieu Laurière, Mehdi Talbi</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Center for Data Science; NYU-ECNU Institute of Mathematical Sciences at NYU Shanghai; NYU Shanghai; Laboratoire de Probabilités, Statistiques et Modélisation, Université Paris-Cité</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22961" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22961</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel deep learning framework combining the Dynamic Programming Principle with neural networks to solve high-dimensional multiple optimal stopping problems. 2. Provides theoretical error analysis for both the discrete-time problem (neural network training error) and continuous problems (discretization error). 3. Demonstrates the method&#x27;s efficiency and scalability through numerical experiments on high-dimensional American basket options and nonlinear utility maximization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ef52bc9da77a7dcb529e7b4b3c239b993aeb4f5fc7464ce8d4f6e1c468ee677_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ef52bc9da77a7dcb529e7b4b3c239b993aeb4f5fc7464ce8d4f6e1c468ee677_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a deep learning framework to solve the challenging multiple optimal stopping problem in high dimensions. The method combines the Dynamic Programming Principle with neural network approximation of the value function. Numerical experiments show it is an efficient and scalable solution for problems like pricing American basket options.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Risk-Averse Learning with Varying Risk Levels</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [online convex optimization], [Conditional Value-at-Risk (CVaR), dynamic regret, zeroth-order optimization, risk-level variation, first-order optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Siyi Wang, Zifan Wang, Karl H. Johansson</p>
</li>
<li class="">
<p><strong>institution:</strong> KTH Royal Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22986" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22986</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a novel risk-level variation metric to capture the dynamics of changing risk preferences in online optimization. 2. Develops risk-averse learning algorithms for both first-order and zeroth-order information settings under a limited sampling budget. 3. Provides dynamic regret bounds for the proposed algorithms, analyzing their performance in terms of function variation, risk-level variation, and sample count.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19b18e34aeb85bbf659ba6da5ac559cf4534627ec075752ac91c1f532838981e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19b18e34aeb85bbf659ba6da5ac559cf4534627ec075752ac91c1f532838981e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles risk-averse online optimization in non-stationary environments where both the cost functions and the desired risk level can change over time. The authors propose new algorithms for first-order and zeroth-order settings that use Conditional Value-at-Risk (CVaR) and analyze their dynamic regret, showing adaptability to changing conditions. Numerical experiments validate the effectiveness of the proposed methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [simulation-based inference], [Bayesian adaptive design, amortized inference, diffusion models, sequential experimental design, policy learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Niels Bracher, Lars Kühmichel, Desi R. Ivanova, Xavier Intes, Paul-Christian Bürkner, Stefan T. Radev</p>
</li>
<li class="">
<p><strong>institution:</strong> Rensselaer Polytechnic Institute, TU Dortmund University, University of Oxford</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22999" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22999</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces JADAI, a framework that jointly amortizes Bayesian adaptive design and inference by training a policy, history, and inference network end-to-end., 2. Proposes a generic loss function that aggregates incremental reductions in posterior error across sequential experiments., 3. Instantiates the inference network with diffusion-based posterior estimators to handle high-dimensional and multimodal posteriors at each experimental step.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a357d49f23f5fdffd9539d05904d86150c3c7775033f4847b56afac3375ad8e6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a357d49f23f5fdffd9539d05904d86150c3c7775033f4847b56afac3375ad8e6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces JADAI, a framework that jointly learns to optimize experimental designs and perform Bayesian inference in a sequential setting. It trains a policy network, a history network, and a diffusion-based inference network end-to-end to minimize posterior error. The method achieves superior or competitive performance on standard adaptive design benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Deep Learning for Art Market Valuation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-modal learning], [multi-modal deep learning, visual embeddings, Grad-CAM, hedonic regression, repeated-sales dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jianping Mei, Michael Moses, Jan Waelty, Yucheng Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> Cheung Kong Graduate School of Business (CKGSB), University of Zurich, Swiss Finance Institute, Art Market Consultancy</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23078" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23078</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces and benchmarks multi-modal deep learning models that fuse tabular data (artist, history) with visual embeddings from artwork images for art market valuation. 2. Demonstrates that visual content provides a distinct and economically significant predictive contribution, especially for fresh-to-market works lacking prior transaction history. 3. Provides interpretability analyses using Grad-CAM and embedding visualizations to show models attend to compositional and stylistic visual cues.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes using multi-modal deep learning to improve art market valuation by incorporating visual content from artwork images alongside traditional tabular data. It finds that while artist identity and history are most predictive overall, visual features provide crucial value for first-time sales where historical data is absent. The results show deep learning offers new insights for valuation, particularly in the most challenging scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Federated Learning With L0 Constraint Via Probabilistic Gates For Sparsity</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [L0 regularization, probabilistic gates, communication efficiency, model sparsity, federated stochastic gradient descent]</p>
</li>
<li class="">
<p><strong>authors:</strong> Krishna Harsha Kovelakuntla Huthasana, Alireza Olama, Andreas Lundell</p>
</li>
<li class="">
<p><strong>institution:</strong> Åbo Akademi University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23071" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23071</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel federated learning method that enforces an L0 constraint on model parameters using probabilistic gates and their continuous relaxation to achieve target sparsity. 2. Derives the L0 constrained stochastic minimization objective from an entropy maximization problem of the stochastic gates. 3. Demonstrates that the method can achieve high target sparsity (down to ρ=0.005) under data and client heterogeneity with minimal loss in statistical performance, outperforming magnitude pruning-based methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/259d06fa8e807f154e153891f40b44308796040886ed51e218b65ee4e67a8c9c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/259d06fa8e807f154e153891f40b44308796040886ed51e218b65ee4e67a8c9c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of poor generalizability and communication inefficiency in Federated Learning due to overly dense models. It proposes a method to enforce L0 sparsity constraints via probabilistic gates, deriving the objective from entropy maximization and implementing it with federated stochastic gradient descent. The method is shown to be communication-efficient and achieves high target sparsity with better statistical performance than pruning-based baselines on synthetic and real datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] QSAR-Guided Generative Framework for the Discovery of Synthetically Viable Odorants</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [generative models], [variational autoencoder, QSAR, molecular generation, Fréchet ChemNet Distance, retrosynthesis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tim C. Pearce, Ahmed Ibrahim</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Leicester, University of Cambridge</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23080" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23080</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel generative AI framework combining a VAE with a QSAR model to design novel odorant molecules from limited training data., 2. Demonstration of effective latent space structuring for odor likelihood, enabling exploration of novel chemical scaffolds beyond simple derivatization., 3. Comprehensive validation showing generated molecules are syntactically valid, unique, thermodynamically stable, and synthetically viable.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2f78de871dd5cdb6d0663be15fa9ec9b8b77d5cae24344545c68979b5c02eaf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2f78de871dd5cdb6d0663be15fa9ec9b8b77d5cae24344545c68979b5c02eaf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a generative AI framework that combines a variational autoencoder (VAE) with a quantitative structure-activity relationship (QSAR) model to design novel odorant molecules. The method structures the VAE&#x27;s latent space based on odor probability, enabling the generation of valid, unique, and synthetically viable candidate molecules from a limited training set. The results show the model successfully explores novel chemical space, producing stable candidates with practical synthesis routes.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Why Machine Learning Models Systematically Underestimate Extreme Values II: How to Fix It with LatentNN</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [statistical machine learning], [attenuation bias, latent variable, neural networks, measurement error, joint likelihood]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuan-Sen Ting</p>
</li>
<li class="">
<p><strong>institution:</strong> The Ohio State University, Max-Planck-Institut für Astronomie</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23138" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23138</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/tingyuansen/LatentNN" target="_blank" rel="noopener noreferrer" class="">https://github.com/tingyuansen/LatentNN</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that neural networks suffer from attenuation bias, a systematic underestimation of extreme values due to input measurement errors. 2. Proposes LatentNN, a method that generalizes the latent variable solution from linear regression to neural networks by jointly optimizing network parameters and latent input values. 3. Validates the method&#x27;s effectiveness in reducing bias across various scenarios, including low signal-to-noise astronomical data, and defines its effective operational regime.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e1e58f31518d154aacb0668496d22b5dba87a170b019457f35d0fd0bfb1e03b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e1e58f31518d154aacb0668496d22b5dba87a170b019457f35d0fd0bfb1e03b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of attenuation bias, where neural networks systematically underestimate extreme values due to noisy input measurements. It introduces LatentNN, a method that treats true inputs as latent variables and jointly optimizes them with the network parameters by maximizing the joint data likelihood. The results show that LatentNN effectively reduces this bias, especially in the low signal-to-noise regimes common in astronomy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] An Inference-Based Architecture for Intent and Affordance Saturation in Decision-Making</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Kullback-Leibler divergence, decision paralysis, intent selection, affordance selection, hierarchical decision process]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wendyam Eric Lionel Ilboudo, Saori C Tanaka</p>
</li>
<li class="">
<p><strong>institution:</strong> Nara Institute of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23144" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23144</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a computational account of decision paralysis as convergence failure in a hierarchical decision process, separating intent and affordance selection. 2. Formalizes decision commitment as inference under a mixture of reverse-KL (mode-seeking) and forward-KL (mode-covering) objectives. 3. Demonstrates through simulations that forward-KL-biased inference reproduces key features of decision inertia and shutdown, framing autism as an extreme regime of this general decision-making continuum.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae0c1bd96570e940c6fa7d72cbfcec334b1a94d288ce774a384e5c60b2bfe206_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae0c1bd96570e940c6fa7d72cbfcec334b1a94d288ce774a384e5c60b2bfe206_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses decision paralysis by proposing a hierarchical inference-based model that separates intent and affordance selection. Commitment is formalized using a mixture of reverse-KL and forward-KL divergence objectives, where a bias towards forward-KL leads to slow, heavy-tailed response times and distinct failure modes. The model reproduces features of decision inertia and suggests autism represents an extreme case on this decision-making continuum.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Clipped Gradient Methods for Nonsmooth Convex Optimization under Heavy-Tailed Noise: A Refined Analysis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [stochastic optimization], [gradient clipping, heavy-tailed noise, nonsmooth convex optimization, convergence analysis, Freedman&#x27;s inequality]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zijian Liu</p>
</li>
<li class="">
<p><strong>institution:</strong> New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23178" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23178</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provided a refined analysis for Clipped SGD under heavy-tailed noise, achieving faster high-probability convergence rates that depend on a novel &quot;generalized effective dimension&quot; term. 2. Extended the refined analysis to convergence in expectation, obtaining new rates that break previously known lower bounds and proving their optimality by matching newly established lower bounds. 3. Established new lower bounds for both high-probability and in-expectation convergence, completing the theoretical landscape and confirming the optimality of the new in-expectation rates.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f14888c84379a942fea9e71d6d6a8df252ea20840f1e922a872a2fa3ee4897b2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f14888c84379a942fea9e71d6d6a8df252ea20840f1e922a872a2fa3ee4897b2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper refines the theoretical analysis of Clipped Stochastic Gradient Descent (Clipped SGD) for nonsmooth convex optimization under heavy-tailed gradient noise. By improving the use of Freedman&#x27;s inequality and providing finer bounds for clipping error, the authors derive faster high-probability convergence rates and new optimal in-expectation rates that surpass previous lower bounds. The work also establishes matching lower bounds, demonstrating the optimality of the proposed analysis for convergence in expectation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Persistent Homology via Finite Topological Spaces</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [topological data analysis], [persistent homology, finite topological spaces, posets, crosscut complexes, stability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Selçuk Kayacan</p>
</li>
<li class="">
<p><strong>institution:</strong> Bahçeşehir University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23348" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23348</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a functorial framework for persistent homology using filtrations of finite topological spaces and posets, bypassing the need for inclusion relations between simplicial complexes. 2. Demonstrates that standard simplifications at the poset level preserve persistent invariants. 3. Proves the stability of the resulting persistence diagrams under metric perturbations in a density-based instantiation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66f9ee5417a1e46f255a691f585eeb3b163bd4b47edaafbe115e358a54f4c884_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66f9ee5417a1e46f255a691f585eeb3b163bd4b47edaafbe115e358a54f4c884_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes a new functorial framework for persistent homology that starts from a finite metric space and constructs a filtration of finite topological spaces, then functorially maps these to posets and simplicial complexes via crosscut constructions. This approach decouples the metric from the homological analysis and does not require inclusion maps between complexes. The authors show that poset-level simplifications preserve persistent invariants and prove the stability of the resulting persistence diagrams.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Probabilistic Modelling is Sufficient for Causal Inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [causal inference], [probabilistic modelling, causal inference, do-operator, structural causal models, Bayesian networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Bruno Mlodozeniec, David Krueger, Richard E. Turner</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Cambridge, Max Planck Institute for Intelligent Systems, MILA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23408" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23408</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrates that standard probabilistic modelling is sufficient for answering causal inference questions without requiring special causal frameworks. 2. Provides concrete examples showing how causal problems (interventional and counterfactual) can be solved by &quot;writing down the probability of everything&quot;. 3. Reinterprets established causal tools (e.g., do-operator, do-calculus) as &quot;syntactic sugar&quot; emerging from standard probabilistic modelling, clarifying their utility.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42538108be020614ac69c6d340cfeef4400c649311210c2f7081e50dc3d98ef5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42538108be020614ac69c6d340cfeef4400c649311210c2f7081e50dc3d98ef5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper argues that causal inference questions can be fully addressed using standard probabilistic modelling and inference, without needing specialized causal tools or notation. The core method is to &quot;write down the probability of everything&quot; to model and solve both interventional and counterfactual problems. The authors conclude that causal-specific frameworks are not fundamentally necessary but can be seen as convenient abstractions built upon probabilistic foundations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [alpha screening, large language models, reinforcement learning, factor investing, economic reasoning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zuoyou Jiang, Li Zhao, Rui Sun, Ruohan Sun, Zhongjian Li, Jing Li, Daxin Jiang, Zuo Bai, Cheng Hua</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University, StepFun, FinStep</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23515" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23515</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/FinStep-AI/Alpha-R1" target="_blank" rel="noopener noreferrer" class="">https://github.com/FinStep-AI/Alpha-R1</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. 2. Introduces a method for LLMs to reason over factor logic and real-time news to evaluate alpha relevance under changing market conditions. 3. Demonstrates that the model consistently outperforms benchmarks and shows improved robustness to alpha decay across multiple asset pools.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d921912985e0858276fe1088914641df9c33c30f5de309733b2244c86c21e75e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d921912985e0858276fe1088914641df9c33c30f5de309733b2244c86c21e75e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of alpha decay in non-stationary financial markets by proposing Alpha-R1, a reasoning model trained with reinforcement learning. It uses a large language model to process factor logic and news, selectively activating factors based on contextual economic relevance. Empirical results show it outperforms benchmark strategies and is more robust to signal decay.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Adaptive Fusion Graph Network for 3D Strain Field Prediction in Solid Rocket Motor Grains</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [graph U-Net, adaptive pooling, feature fusion, strain field prediction, solid rocket motor]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jiada Huang, Hao Ma, Zhibin Shen, Yizhou Qiao, Haiyang Li</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Defense Technology, Zhengzhou University of Aeronautics</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23443" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23443</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GrainGNet, an adaptive graph network with an adaptive pooling dynamic node selection mechanism to preserve key mechanical features in critical structural regions. 2. Utilizes feature fusion to transmit deep features and enhance the model&#x27;s representational capacity for 3D strain field prediction. 3. Demonstrates significant performance improvements, including a 62.8% reduction in mean squared error and a sevenfold training efficiency gain over a baseline graph U-Net, with particular accuracy in high-strain regions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a35459d2c7214afb29d74bb1f279ddd56dc621982ef3910ac226653dcfc465f1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a35459d2c7214afb29d74bb1f279ddd56dc621982ef3910ac226653dcfc465f1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes GrainGNet, an adaptive fusion graph network, to predict the 3D strain field in solid rocket motor grains, addressing the computational expense of traditional simulations. The model uses adaptive pooling and feature fusion to accurately capture high-strain regions. It achieves a 62.8% reduction in mean squared error and improved training efficiency compared to baseline methods, offering a high-fidelity approach for structural safety evaluation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A general framework for deep learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [statistical learning theory], [deep neural networks, Bernstein-type inequality, excess risk bound, minimax optimality, mixing processes]</p>
</li>
<li class="">
<p><strong>authors:</strong> William Kengne, Modou Wade</p>
</li>
<li class="">
<p><strong>institution:</strong> Université Jean Monnet, CY Cergy Paris Université</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23425" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23425</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a general theoretical framework for deep learning that unifies analysis for data satisfying a generalized Bernstein-type inequality, encompassing independent and various dependent (mixing) observations. 2. Introduces two novel estimators: a Non-Penalized Deep Neural Network (NPDNN) and a Sparse-Penalized Deep Neural Network (SPDNN) estimator. 3. Establishes minimax optimal (up to logarithmic factors) convergence rates for the expected excess risk of both estimators on Hölder smooth and composition function classes.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4efb1121ddfae593e4dedab080501366fad55274c1b149910f149a1920a0b6e2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4efb1121ddfae593e4dedab080501366fad55274c1b149910f149a1920a0b6e2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper develops a general theoretical framework for analyzing deep neural network estimators in settings including nonparametric regression and classification. It proposes two estimators (NPDNN and SPDNN) and derives upper bounds for their expected excess risk for data satisfying a generalized Bernstein-type inequality, covering independent and various dependent data processes. The main conclusion is that both proposed estimators achieve minimax optimal convergence rates in many classical settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] From geometry to dynamics: Learning overdamped Langevin dynamics from sparse observations with geometric constraints</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [stochastic system identification], [overdamped Langevin dynamics, sparse observations, geometric constraints, stochastic control, path augmentation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dimitra Maoutsa</p>
</li>
<li class="">
<p><strong>institution:</strong> Technical University of Berlin</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23566" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23566</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A new framework that reconciles geometric and temporal perspectives by reformulating inference as a stochastic control problem. 2. A method using geometry-driven path augmentation, guided by the system&#x27;s invariant density, to reconstruct trajectories without assuming specific parametric models. 3. Demonstrating accurate recovery of stochastic dynamics from extremely undersampled data, outperforming existing methods in synthetic benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7a01c554437b016c9e156f0cb5f7047dd8973704b2623e3d218e1b9c76975c0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7a01c554437b016c9e156f0cb5f7047dd8973704b2623e3d218e1b9c76975c0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of learning stochastic dynamics from sparse temporal observations. It proposes a new framework that uses geometry-driven path augmentation within a stochastic control formulation to infer the underlying laws without parametric assumptions. The method successfully recovers overdamped Langevin dynamics from highly undersampled data, outperforming existing approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Nonstationarity-Complexity Tradeoff in Return Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [financial machine learning], [non-stationarity, model selection, adaptive window selection, return prediction, tournament procedure]</p>
</li>
<li class="">
<p><strong>authors:</strong> Agostino Capponi, Chengpiao Huang, J. Antonio Sidaoui, Kaizheng Wang, Jiacheng Zou</p>
</li>
<li class="">
<p><strong>institution:</strong> Columbia University, Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23596" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23596</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and formalizes the nonstationarity-complexity tradeoff in return prediction, where complex models reduce misspecification but require longer, more non-stationary training windows. 2. Proposes a novel model selection method that jointly optimizes model class and training window size using an adaptive tournament procedure evaluated on non-stationary validation data. 3. Provides theoretical analysis showing the method balances misspecification error, estimation variance, and non-stationarity, and demonstrates its empirical superiority with significant performance gains in out-of-sample prediction and trading strategy returns, especially during recessions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07919419a5b2bc9758efcadd74b91c11fcdca1529edf541c429c2694c06ddde0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07919419a5b2bc9758efcadd74b91c11fcdca1529edf541c429c2694c06ddde0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of predicting stock returns in non-stationary environments by identifying a tradeoff between model complexity and non-stationarity. It proposes a new model selection method that jointly chooses the model and its training window via an adaptive tournament. The method outperforms standard benchmarks, particularly during economic recessions, and generates higher cumulative returns for a trading strategy.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Calibrated Multi-Level Quantile Forecasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [online learning, forecasting], [quantile forecasting, calibration, online learning, adversarial robustness, no-regret guarantee]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tiffany Ding, Isaac Gibbs, Ryan J. Tibshirani</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Berkeley</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23671" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23671</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Multi-Level Quantile Tracker (MultiQT), a lightweight online method that wraps any existing forecaster to guarantee multi-level quantile calibration against adversarial distribution shifts. 2. Provides theoretical guarantees including calibration and a no-regret property ensuring asymptotic performance is not worse than the base forecaster. 3. Ensures the corrected forecasts are properly ordered across different quantile levels.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/133e23e4bb898bd730a1ed52ef1e6f20bce16c1d2910d09105ad3e344368fe6f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/133e23e4bb898bd730a1ed52ef1e6f20bce16c1d2910d09105ad3e344368fe6f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of producing reliable multi-level quantile forecasts that are calibrated. It proposes MultiQT, an online wrapper method that guarantees calibration and proper ordering of forecasts even under adversarial conditions, without asymptotically worsening the base forecaster&#x27;s performance. Experiments show it significantly improves calibration in epidemic and energy forecasting tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Bellman Calibration for V-Learning in Offline Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Bellman calibration, off-policy evaluation, value iteration, doubly robust estimator, Markov decision process]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lars van der Laan, Nathan Kallus</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Washington, Netflix, Cornell University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23694" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23694</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Iterated Bellman Calibration, a model-agnostic, post-hoc procedure for calibrating value predictions in infinite-horizon MDPs. 2. Adapts classical calibration methods (histogram, isotonic) to the dynamic, counterfactual setting using a doubly robust pseudo-outcome for off-policy data. 3. Provides finite-sample guarantees for calibration and prediction without requiring Bellman completeness or realizability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9300a417035535b3fec586f3ad8f9e10dae8a084d794d1d413f42e5ba2b05d37_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9300a417035535b3fec586f3ad8f9e10dae8a084d794d1d413f42e5ba2b05d37_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Iterated Bellman Calibration, a post-hoc method to improve the accuracy of long-term value predictions in offline reinforcement learning. The method repeatedly regresses fitted Bellman targets onto a model&#x27;s predictions, adapting classical calibration techniques to handle off-policy data. The analysis shows the approach provides finite-sample guarantees for calibrated predictions under weak assumptions, without needing Bellman completeness.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] INSIGHT: Spatially resolved survival modelling from routine histology crosslinked with molecular profiling reveals prognostic epithelial-immune axes in stage II/III colorectal cancer</strong></p>
<ul>
<li class=""><strong>tags:</strong> [cv], [computational pathology], [graph neural network, survival prediction, spatial transcriptomics, colorectal cancer, histology]</li>
<li class=""><strong>authors:</strong> Piotr Keller, Mark Eastwood, Zedong Hu, Aimée Selten, Ruqayya Awan, Gertjan Rasschaert, Sara Verbandt, Vlad Popovici, Hubert Piessevaux, Hayley T Morris, Petros Tsantoulis, Thomas Alexander McKee, André D&#x27;Hoore, Cédric Schraepen, Xavier Sagaert, Gert De Hertogh, Sabine Tejpar, Fayyaz Minhas</li>
<li class=""><strong>institution:</strong> KU Leuven, University of Oxford, University of Cambridge, University of Manchester, University of Bristol, University of Edinburgh, University of Glasgow, University of Sheffield, University of Southampton, University of Warwick, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strath</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22262" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22262</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e16c74c6d57e6b8f3cec00c4cc6267b2b66595c314bba0f371e27e2f86f25458_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e16c74c6d57e6b8f3cec00c4cc6267b2b66595c314bba0f371e27e2f86f25458_w640_q70.webp</a></li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-30T04:30:56.000Z" itemprop="dateModified">Dec 30, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/cslg/20251222-20251228"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251222-20251228 (cs.LG)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/category/cslo"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">cs.LO</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-29" class="table-of-contents__link toc-highlight">2025-12-29</a></li><li><a href="#2025-12-30" class="table-of-contents__link toc-highlight">2025-12-30</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/cslg/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>