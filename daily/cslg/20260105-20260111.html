<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_LG/20260105-20260111" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20260105-20260111 (cs.LG) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/cslg/20260105-20260111"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20260105-20260111 (cs.LG) | AI头条"><meta data-rh="true" name="description" content="2026-01-05"><meta data-rh="true" property="og:description" content="2026-01-05"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/cslg/20260105-20260111"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cslg/20260105-20260111" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cslg/20260105-20260111" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.LG","item":"https://jokebear666.github.io/ai_toutiao/daily/cslg"},{"@type":"ListItem","position":3,"name":"20260105-20260111 (cs.LG)","item":"https://jokebear666.github.io/ai_toutiao/daily/cslg/20260105-20260111"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.647bd1ff.css">
<script src="/ai_toutiao/assets/js/runtime~main.a5a9d6f2.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.8703b74f.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/arxiv-daily"><span title="Arxiv每日论文" class="linkLabel_WmDU">Arxiv每日论文</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Collapse sidebar category &#x27;cs.LG&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cs_LG/20251215-20251221"><span title="20251215-20251221 (cs.LG)" class="linkLabel_WmDU">20251215-20251221 (cs.LG)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cslg/20251222-20251228"><span title="20251222-20251228 (cs.LG)" class="linkLabel_WmDU">20251222-20251228 (cs.LG)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cslg/20251229-20260104"><span title="20251229-20260104 (cs.LG)" class="linkLabel_WmDU">20251229-20260104 (cs.LG)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/cslg/20260105-20260111"><span title="20260105-20260111 (cs.LG)" class="linkLabel_WmDU">20260105-20260111 (cs.LG)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csoh"><span title="cs.OH" class="categoryLinkLabel_W154">cs.OH</span></a><button aria-label="Expand sidebar category &#x27;cs.OH&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20260105-20260111"><span title="20260105-20260111" class="linkLabel_WmDU">20260105-20260111</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/daily/cslg"><span>cs.LG</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20260105-20260111 (cs.LG)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20260105-20260111 (cs.LG)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2026-01-05">2026-01-05<a href="#2026-01-05" class="hash-link" aria-label="Direct link to 2026-01-05" title="Direct link to 2026-01-05" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv260105] Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [anomaly detection], [class imbalance, synthetic dataset, generalization error, unsupervised methods, semi-supervised methods]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lesley Wheat, Martin v. Mohrenschildt, Saeid Habibi</p>
</li>
<li class="">
<p><strong>institution:</strong> McMaster University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00005" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00005</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a comprehensive, problem-agnostic evaluation of 14 anomaly detectors under simulated industrial constraints of extreme class imbalance. 2. Identified that the best-performing detector depends critically on the absolute number of faulty examples available, not just the imbalance ratio, and provided thresholds for method selection. 3. Demonstrated the nuanced impact of feature dimensionality on method performance, showing semi-supervised methods gain advantage in higher dimensions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d98d7533bc130ddf43f0469391265ce51a72fe31fdeb441a3c4b553d4e3dd35_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d98d7533bc130ddf43f0469391265ce51a72fe31fdeb441a3c4b553d4e3dd35_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper evaluates anomaly detection algorithms for industrial problems with extreme class imbalance using a synthetic dataset. It benchmarks 14 detectors across varying anomaly rates and training sizes, finding that the optimal detector depends on the absolute number of faulty examples, with unsupervised methods best for very few faults and supervised/semi-supervised methods improving with 30-50 faults. The study highlights performance drops on smaller datasets and provides practical deployment insights.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [logistics optimization], [workload balancing, evolutionary algorithms, k-means, last-mile delivery, hybrid algorithm]</p>
</li>
<li class="">
<p><strong>authors:</strong> Luis M. Moreno-Saavedra, Silvia Jimenez-Fernandez, Antonio Portilla-Figueras, David Casillas-Perez, Sancho Salcedo-Sanz</p>
</li>
<li class="">
<p><strong>institution:</strong> Universidad de Alcalá, Universidad Rey Juan Carlos</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00023" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00023</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a multi-algorithm methodology for operational human resources workload balancing in last-mile delivery, moving beyond simple geographical assignment. 2. Introduces and combines several algorithmic approaches, including different versions of k-means, evolutionary algorithms, recursive assignments, and a hybrid evolutionary ensemble. 3. Validates the proposed approach by applying it to a real-world case study of a delivery workforce in Azuqueca de Henares, Spain.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43bf05ee106d97a42fa05d1af33419810885c8fff72d16b95af434c71d43f4ff_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43bf05ee106d97a42fa05d1af33419810885c8fff72d16b95af434c71d43f4ff_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of unbalanced workload distribution among delivery workers in last-mile urban logistics. It proposes a multi-algorithm approach that uses a combination of distance and workload considerations, including k-means variants and evolutionary algorithms, to assign packages and balance daily effort. The method was successfully tested on a real-world delivery system in Spain.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [mental health language modeling], [large language models, fine-tuning, PHQ-9, Nigerian Pidgin, depression screening]</p>
</li>
<li class="">
<p><strong>authors:</strong> Isaac Iyinoluwa Olufadewa, Miracle Ayomikun Adesina, Ezekiel Ayodeji Oladejo, Uthman Babatunde Usman, Owen Kolade Adeniyi, Matthew Tolulope Olawoyin</p>
</li>
<li class="">
<p><strong>institution:</strong> Artificial Intelligence for Low-Resource Public Health Application (ALPHA) Centre, Slum and Rural Health Initiative; University of Ibadan; University of Ilorin</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00004" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00004</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Created a novel, annotated dataset of 432 Nigerian Pidgin audio responses for depression screening aligned with PHQ-9 items. 2. Fine-tuned and evaluated three LLMs (Phi-3-mini, Gemma-3-4B-it, GPT-4.1) for automated depression screening in a low-resource language. 3. Demonstrated that fine-tuned GPT-4.1 achieved high accuracy (94.5%) and cultural appropriateness for PHQ-9 severity scoring in Nigerian Pidgin.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/849aa2e2bc1566aab5f5b5e5d072677a6a66426e677648e836adf89c32b964e6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/849aa2e2bc1566aab5f5b5e5d072677a6a66426e677648e836adf89c32b964e6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of depression screening in Nigeria by fine-tuning large language models for Nigerian Pidgin English. The authors collected and annotated a dataset of audio responses, then fine-tuned three LLMs to predict PHQ-9 severity scores. The fine-tuned GPT-4.1 model achieved the best performance, providing a foundation for AI-mediated mental health tools in linguistically diverse, resource-constrained settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [policy gradient, self-play, Markov Decision Process, Advantage Actor-Critic, ablation study]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nicholas A. Pape</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Texas at Austin</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00007" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00007</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulates the classic stochastic combinatorial game Yahtzee as a Markov Decision Process and establishes it as a mid-scale RL benchmark. 2. Conducts a comprehensive empirical study comparing REINFORCE, A2C, and PPO under a fixed training budget, identifying A2C as the most robust method. 3. Achieves a median score within 5% of the optimal dynamic programming solution, while analyzing persistent challenges like long-horizon credit assignment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfca1ffc9ad8523e303bf57755dcad543948f47e5e9f3c5a9d726a359bd3fe5b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfca1ffc9ad8523e303bf57755dcad543948f47e5e9f3c5a9d726a359bd3fe5b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates using deep reinforcement learning to play the full solitaire version of Yahtzee. It trains self-play agents with policy gradient methods (REINFORCE, A2C, PPO) and performs ablation studies on various design choices. The main finding is that A2C robustly achieves near-optimal performance, while all methods struggle with long-term strategic elements like securing the upper section bonus.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [ferroelectric synapses, spiking neural networks, EEG signal processing, adaptive learning, neuromorphic computing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nikhil Garg, Anxiong Song, Niklas Plessnig, Nathan Savoia, Laura Bégon-Lours</p>
</li>
<li class="">
<p><strong>institution:</strong> ETH Zurich (Integrated Systems Laboratory, Department of Information Technology and Electrical Engineering)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00020" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00020</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrated the deployment and adaptation of Spiking Neural Networks (SNNs) on fabricated ferroelectric memristive synaptic devices for EEG-based motor imagery decoding under realistic device constraints. 2. Introduced a device-aware weight-update strategy that accumulates gradient updates digitally and triggers discrete programming events only when a threshold is exceeded, reducing programming frequency and emulating device dynamics. 3. Evaluated two complementary deployment strategies (device-aware training and transfer learning with on-device re-tuning) that achieve performance comparable to software-based SNNs and show improved accuracy through subject-specific adaptation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce95488f8704205e4a01e64825c78c800662f36da33df71b138881b21ab7b9bb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce95488f8704205e4a01e64825c78c800662f36da33df71b138881b21ab7b9bb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of adapting EEG-based brain-computer interfaces to non-stationary neural signals on resource-constrained hardware. It proposes deploying Spiking Neural Networks on ferroelectric memristive synapses with a novel device-aware update strategy and demonstrates two effective deployment methods for personalized, low-overhead adaptation. The results show that programmable ferroelectric hardware can support robust, efficient adaptation for personalized neuromorphic processing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [tokenizer transplant, model composition, supply-chain vulnerability, sparse solver, spectral mimicry]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xiaoze Liu, Weichen Yu, Matt Fredrikson, Xiaoqian Wang, Jing Gao</p>
</li>
<li class="">
<p><strong>institution:</strong> Purdue University, Carnegie Mellon University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00065" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00065</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/xz-liu/tokenforge" target="_blank" rel="noopener noreferrer" class="">https://github.com/xz-liu/tokenforge</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies tokenizer transplant as a novel attack surface in the LLM composition supply chain, 2. Introduces the concept of a &quot;breaker token&quot;—a single, engineered token that is inert in a donor model but maliciously activates after transplant, 3. Formalizes and instantiates the attack as a dual-objective optimization problem solved with a sparse solver, demonstrating its training-free nature, stealth, and persistence.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bac901d101e76e81a328892233109e7f05c25f328de683add53ccd17ae81590e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bac901d101e76e81a328892233109e7f05c25f328de683add53ccd17ae81590e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies a security vulnerability in the tokenizer transplant step required for composing different LLMs. The authors propose a method to engineer a single &quot;breaker token&quot; that, when added to a donor model, remains harmless but sabotages a base model after transplant by exploiting coefficient reuse. The attack is stealthy, training-free, and persistent, revealing a hidden risk in modular AI pipelines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] It&#x27;s Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [diffusion models], [mode collapse, noise optimization, frequency characteristics, text-to-image generation, inference-time scaling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Anne Harrington, A. Sophia Koepke, Shyamgopal Karthik, Trevor Darrell, Alexei A. Efros</p>
</li>
<li class="">
<p><strong>institution:</strong> UC Berkeley, University of Tübingen (Tübingen AI Center), Technical University of Munich (MCML)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00090" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00090</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a simple noise optimization objective to mitigate mode collapse in trained diffusion models while preserving model fidelity. 2. Analyzes the frequency characteristics of noise and demonstrates that alternative noise initializations with different frequency profiles can improve optimization and search. 3. Empirically shows that the proposed noise optimization method yields superior results in generation quality and variety compared to existing approaches.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90471f45dfeeb47a677f1a1f9518845473c93d7756e36367a947d6ebb4031c24_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90471f45dfeeb47a677f1a1f9518845473c93d7756e36367a947d6ebb4031c24_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of mode collapse in text-to-image diffusion models, where repeated sampling with the same prompt yields nearly identical images. The proposed solution is to optimize the initial noise input to the model to encourage diverse outputs, while also analyzing and leveraging the frequency characteristics of the noise for better performance. The experiments demonstrate that this noise optimization approach effectively recovers diversity without compromising the quality of the generated images.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] IMBWatch -- a Spatio-Temporal Graph Neural Network approach to detect Illicit Massage Business</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [spatio-temporal graph neural networks], [spatio-temporal graph neural network, dynamic graphs, temporal attention, graph convolutional network, network forensics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Swetha Varadarajan, Abhishek Ray, Lumina Albert</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Cape Town, George Mason University, Colorado State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00075" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00075</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://anonymous.4open.science/r/IMB-GNN-F022" target="_blank" rel="noopener noreferrer" class="">https://anonymous.4open.science/r/IMB-GNN-F022</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes IMBWatch, a novel ST-GNN framework for detecting Illicit Massage Businesses by modeling them as dynamic, heterogeneous graphs from open-source data. 2. Introduces a method combining graph convolutions with temporal attention to capture evolving spatio-temporal patterns like intercity movement and coordinated advertising. 3. Demonstrates superior performance over baseline models on real-world data and provides an interpretable, scalable tool for proactive anti-trafficking interventions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef349259f296853dd5ad0527274fd725d811fdf18b16b020222413bfeb7f6f93_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef349259f296853dd5ad0527274fd725d811fdf18b16b020222413bfeb7f6f93_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces IMBWatch, a spatio-temporal graph neural network framework designed to detect Illicit Massage Businesses by modeling their operations as dynamic graphs from online ads and records. The method uses graph convolutions and temporal attention to learn evolving patterns, and it outperforms baseline models in accuracy and F1-score on real-world data, offering a scalable tool for law enforcement.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [LLM Security], [Go-Explore, Prompt Injection, Adversarial Testing, Agent Safety, Multi-Hop Attacks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Manish Bhatt, Adrian Wood, Idan Habler, Ammar Al-Kahfah</p>
</li>
<li class="">
<p><strong>institution:</strong> OWASP, Amazon, Dropbox, CISCO, AWS</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00042" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00042</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/mbhatt1/competitionscratch" target="_blank" rel="noopener noreferrer" class="">https://github.com/mbhatt1/competitionscratch</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Adapted the Go-Explore reinforcement learning algorithm for systematic security testing of LLM agents. 2. Conducted a large-scale empirical study revealing that random seed variance dominates algorithmic parameter choices in this domain. 3. Provided actionable insights for practitioners, such as the ineffectiveness of reward shaping and the benefits of using ensembles and simple state signatures.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fd9dc0076e96f0b8282984cab768d0355248ad4e2af52c17ac7798bb446d49_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fd9dc0076e96f0b8282984cab768d0355248ad4e2af52c17ac7798bb446d49_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper adapts the Go-Explore algorithm to test the security of safety-trained LLM agents against prompt injection attacks. Through 28 experimental runs on GPT-4o-mini, the study finds that random seed variance is a major factor, reward shaping is harmful, and simple state signatures work best. The results suggest that managing seed variance and applying domain knowledge are more critical than algorithmic sophistication for effective security testing.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Exploration in the Limit</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [bandit algorithms], [best arm identification, asymptotic error control, confidence sequences, nonparametric, sample complexity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Brian M. Cho, Nathan Kallus</p>
</li>
<li class="">
<p><strong>institution:</strong> Cornell University, Netflix</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00084" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00084</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a relaxed, asymptotic formulation for fixed-confidence best arm identification (BAI) that requires valid error control only after a minimum sample size, aligning with long-horizon practical settings. 2. Develops a novel asymptotic anytime-valid confidence sequence for arm indices and uses it to design a new BAI algorithm that flexibly incorporates covariates for variance reduction in fully nonparametric settings. 3. Provides asymptotic sample complexity bounds, showing the worst-case complexity matches the best-case complexity of Gaussian BAI under exact guarantees, and demonstrates reduced average sample complexity in experiments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a27e11a98452705127752d5c92465faa383d4d4bc6f162ce25d8230c0bd85bbd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a27e11a98452705127752d5c92465faa383d4d4bc6f162ce25d8230c0bd85bbd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses limitations in existing best arm identification (BAI) methods by proposing an asymptotic framework that relaxes the requirement for exact error control to an asymptotic one, enabling the use of tighter bounds and handling nonparametric distributions with covariates. It introduces a new algorithm based on asymptotic anytime-valid confidence sequences. Experiments show this approach reduces average sample complexity while maintaining error control.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [symbolic regression], [Bayesian Optimization, Instruction Tuning, Partial Differential Equation Discovery, LLM Prompting]</p>
</li>
<li class="">
<p><strong>authors:</strong> Junqi Qu, Yan Zhang, Shangqian Gao, Shibo Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Florida State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00088" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00088</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and formalizes the problem of &quot;instruction brittleness&quot; in LLMs for equation discovery, where model outputs are highly sensitive to prompt phrasing. 2. Proposes NeuroSymBO, a novel framework that reframes prompt engineering as a sequential decision problem, using Bayesian Optimization to dynamically select optimal instructions from a library at each step. 3. Demonstrates through experiments on PDE discovery benchmarks that adaptive instruction selection significantly outperforms static prompts, achieving higher recovery rates and more parsimonious solutions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/987d0309ae3b94e9b3ab379ec45397a3f052c2722939b39c6750528988b21974_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/987d0309ae3b94e9b3ab379ec45397a3f052c2722939b39c6750528988b21974_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of &quot;instruction brittleness&quot; in LLMs used for discovering partial differential equations, where fixed prompts lead to suboptimal results. It proposes NeuroSymBO, a framework that uses Bayesian Optimization to dynamically select the best instruction at each step of the generation process. Experiments show this adaptive approach outperforms static prompting, yielding more accurate and simpler equations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Reinforcement learning with timed constraints for robotics motion planning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Metric Interval Temporal Logic (MITL), Timed Limit-Deterministic Generalized Büchi Automata (Timed-LDGBA), Q-learning, POMDP]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhaoan Wang, Junchao Li, Mahdi Mohammad, Shaoping Xiao</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Iowa, Talus Renewables, Inc., Roma Tre University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00087" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00087</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified automata-based RL framework for synthesizing policies under MITL specifications in both MDPs and POMDPs. 2. Introduces a translation of MITL formulas into Timed-LDGBA and their synchronization with decision processes to create product timed models for Q-learning. 3. Validates the framework with simulation studies showing it satisfies time-bounded requirements, scales to larger state spaces, and works in partially observable environments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/556427a99f7edc810f0aa638c014e36ca104d451bbd5edf5c81df58176b568d9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/556427a99f7edc810f0aa638c014e36ca104d451bbd5edf5c81df58176b568d9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the challenge of integrating formal temporal logic (MITL) with reinforcement learning for robotic motion planning under stochastic and partially observable dynamics. It proposes a method that translates MITL specifications into timed automata and synchronizes them with the decision process to enable policy learning via Q-learning. The results demonstrate that the learned policies successfully satisfy strict time constraints in various simulated environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [geometric reinforcement learning, Hamiltonian optimization, simultaneous navigation and mapping, local sensory observations, path differential]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aditya Sai Ellendula, Yi Wang, Minh Nguyen, Chandrajit Bajaj</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Texas at Austin</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00116" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00116</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/" target="_blank" rel="noopener noreferrer" class="">https://github.com/</a> (as per the abstract &quot;The code is publicly available on Github.&quot; The specific URL is not provided in the given text, only a placeholder link.)</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel geometric reinforcement learning framework (GRL-SNAM) for Simultaneous Navigation and Mapping that relies exclusively on local sensory observations without constructing a global map. 2. Formulates navigation and mapping as a dynamic shortest path search using controlled Hamiltonian optimization, translating sensory inputs into local energy landscapes and evolving policies via updating Hamiltonians. 3. Demonstrates that the method enables high-quality navigation with minimal exploration through local energy refinement, preserving clearance and generalizing to unseen environments in 2D navigation tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/776eec0810502d9188877dd04875e090e89eaeb9b6aaa3dec59b37da20fe81b7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/776eec0810502d9188877dd04875e090e89eaeb9b6aaa3dec59b37da20fe81b7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping (SNAM) in unknown environments. It formulates the problem as a dynamic shortest path search using Hamiltonian optimization, where local sensory data is used to update energy landscapes and refine trajectories without building a global map. The method is shown to achieve efficient, high-quality navigation with minimal exploration and good generalization in 2D tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Reinforcement Learning with Function Approximation for Non-Markov Processes</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [non-Markov processes, linear function approximation, policy evaluation, Q-learning, partially observed MDPs]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ali Devran Kara</p>
</li>
<li class="">
<p><strong>institution:</strong> Florida State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00151" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00151</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proved convergence of policy evaluation with linear function approximation under ergodic non-Markov processes, linking the limit to a fixed point of a joint projection-Bellman operator. 2. Established convergence for a special case of Q-learning with linear approximation where basis functions are based on quantization maps under similar ergodicity conditions. 3. Applied the theoretical results to Partially Observed MDPs (POMDPs) using finite-memory state representations and derived explicit error bounds for the learning algorithm limits.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e117bcd100ad5f0daa634538585e364383717d05bbbd527d7dcc2b26e2b92b1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e117bcd100ad5f0daa634538585e364383717d05bbbd527d7dcc2b26e2b92b1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies reinforcement learning with linear function approximation for non-Markov processes. It proves convergence for policy evaluation and a special case of Q-learning under ergodicity conditions, and applies the theory to POMDPs to derive error bounds.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] The Weather Paradox: Why Precipitation Fails to Predict Traffic Accident Severity in Large-Scale US Data</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [predictive modeling], [XGBoost, feature importance, class imbalance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yann Bellec, Rohan Kaman, Siwen Cui, Aarav Agrawal, Calvin Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California San Diego</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00152" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00152</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A large-scale empirical analysis using an XGBoost model to predict traffic accident severity from environmental, temporal, and spatial factors. 2. A feature importance analysis revealing that time of day, location, temperature, and wind speed are strong predictors, while precipitation and visibility show limited predictive power, suggesting driver behavioral adaptation. 3. Identification of dataset limitations (class imbalance, predominance of mid-level severity) and proposed future directions including alternative sampling and enhanced feature engineering.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27e52d086cf3d330117b8cda24994123a6a67bf935b7650263f6ca2e120a3ed2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27e52d086cf3d330117b8cda24994123a6a67bf935b7650263f6ca2e120a3ed2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study uses an XGBoost classifier on a dataset of 500,000 US traffic accidents to predict accident severity. The model achieves 78% accuracy, and feature analysis shows that while time, location, and some weather factors are predictive, precipitation and visibility are not, potentially due to driver adaptation. The findings highlight the limitations of current data for predicting extreme cases and suggest future research directions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Online Finetuning Decision Transformers with Pure RL Gradients</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Decision Transformer, online finetuning, GRPO, hindsight return relabeling, sub-trajectory optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Junkai Luo, Yinglun Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Riverside</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00167" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00167</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies hindsight return relabeling as a fundamental obstacle to using pure RL gradients for online finetuning of Decision Transformers. 2. Proposes new algorithms that adapt GRPO to DTs with key modifications like sub-trajectory optimization, sequence-level likelihood objectives, and active sampling. 3. Demonstrates state-of-the-art performance across multiple benchmarks, showing the effectiveness of pure-RL-based online finetuning for DTs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7142aabdfb71311da4fa293bcc82f4d6d24b9dddc11ccaf74c0aaea92b54d5e3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7142aabdfb71311da4fa293bcc82f4d6d24b9dddc11ccaf74c0aaea92b54d5e3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of online finetuning for Decision Transformers using pure reinforcement learning gradients, which has been largely unexplored. The authors identify and overcome the incompatibility of standard hindsight return relabeling with RL algorithms, proposing modified methods based on GRPO. Their approach outperforms existing online DT baselines, achieving new state-of-the-art results on several benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [Reservoir Computing, Sequential Architecture, Spatiotemporal Forecasting, High-dimensional Data, Training Efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ata Akbari Asanjan, Filip Wudarski, Daniel O&#x27;Connor, Shaun Geaney, Elena Strbac, P. Aaron Lott, Davide Venturelli</p>
</li>
<li class="">
<p><strong>institution:</strong> USRA Research Institute for Advanced Computer Science (RIACS), Standard Chartered Bank</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00172" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00172</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a Sequential Reservoir Computing architecture that decomposes a large reservoir into smaller, interconnected ones to reduce computational and memory costs. 2. Demonstrates superior performance with longer forecast horizons and lower error metrics on chaotic and high-dimensional physical systems compared to RNN/LSTM baselines. 3. Achieves up to three orders of magnitude lower training cost, maintaining RC&#x27;s efficiency while improving scalability for high-dimensional forecasting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80a4bf491108d4e92c2c08807229cd230f08dd3e916c820595ac48efa8952783_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80a4bf491108d4e92c2c08807229cd230f08dd3e916c820595ac48efa8952783_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Sequential Reservoir Computing, a novel architecture that breaks a large reservoir into a sequence of smaller ones to efficiently forecast high-dimensional spatiotemporal systems. It outperforms traditional RNNs and LSTMs in forecast horizon and accuracy while drastically reducing training costs, offering a path to real-time, energy-efficient forecasting.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [semantic communication], [reinforcement learning, unequal error protection, adaptive repetition coding, semantic distortion metric, per-dimension protection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Moirangthem Tiken Singh, Adnan Arif</p>
</li>
<li class="">
<p><strong>institution:</strong> Department of Computer Science and Engineering, Dibrugarh University Institute of Engineering and Technology, Dibrugarh University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00186" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00186</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel reinforcement learning framework for per-dimension unequal error protection of quantized semantic embeddings, 2. A composite semantic distortion metric that balances global embedding similarity with entity-level preservation to guide the RL agent, 3. The demonstration that simple, intelligently allocated repetition coding can outperform conventional codes like LDPC for fine-grained semantic protection</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb4eade98aeb895832aed0b8cd026ed4c334838f42e2fd62ce3cdef3c7aea4d0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb4eade98aeb895832aed0b8cd026ed4c334838f42e2fd62ce3cdef3c7aea4d0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a reinforcement learning framework to protect quantized semantic embeddings transmitted over noisy channels. The method uses adaptive repetition coding to provide unequal error protection per embedding dimension, guided by a novel semantic distortion metric. The results show that this approach significantly outperforms uniform protection, challenging traditional channel coding paradigms by aligning code structure with semantic granularity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Early Prediction of Liver Cirrhosis Up to Three Years in Advance: A Machine Learning Study Benchmarking Against the FIB-4 Score</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [clinical prediction], [XGBoost, electronic health records (EHR), FIB-4 score, liver cirrhosis, early prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhuqi Miao, Sujan Ravi, Abdulaziz Ahmed</p>
</li>
<li class="">
<p><strong>institution:</strong> Oklahoma State University, University of Alabama at Birmingham</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00175" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00175</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed and validated machine learning models for predicting liver cirrhosis 1, 2, and 3 years before diagnosis using routine EHR data. 2. Established a rigorous benchmark by comparing the ML models&#x27; performance directly against the traditional clinical FIB-4 score across multiple time horizons. 3. Demonstrated that ML models consistently outperform FIB-4, with performance gains increasing for longer-term (3-year) predictions, enabling earlier clinical risk stratification.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/10f123bbaaf0097372e87215cac6fce63e113f7c0a6ea463635fbc4532641048_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/10f123bbaaf0097372e87215cac6fce63e113f7c0a6ea463635fbc4532641048_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper develops XGBoost models using electronic health record data to predict liver cirrhosis 1-3 years before diagnosis. The models consistently outperformed the standard FIB-4 score, with performance gains increasing for longer prediction horizons. The study concludes that these ML models can be integrated into clinical workflows as automated decision-support tools for earlier and more accurate risk stratification.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [semi-supervised learning], [Generative Adversarial Network, Swin Transformer, spike classification, semi-supervised learning, Bayesian optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Danial Sharifrazi, Nouman Javed, Mojtaba Mohammadi, Seyede Sana Salehi, Roohallah Alizadehsani, Prasad N. Paradkar, U. Rajendra Acharya, Asim Bhatti</p>
</li>
<li class="">
<p><strong>institution:</strong> Deakin University, CSIRO Health and Biosecurity, Islamic Azad University, University of Southern Queensland</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00189" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00189</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a novel semi-supervised GAN architecture (SSI-GAN) with a Swin-inspired, shifted-window discriminator for neuronal spike classification. 2. Introduced a transformer-based generator and a flat, window-based transformer discriminator with multi-head self-attention to capture sparse, high-frequency spike features. 3. Demonstrated state-of-the-art performance with 99.93% accuracy using only 1-3% labeled data, reducing manual labeling effort by 97-99% compared to supervised methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49a3916713727a5d92dd8fe976e78d600a974d9217a0ccc868f8608ec8453524_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49a3916713727a5d92dd8fe976e78d600a974d9217a0ccc868f8608ec8453524_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the labor-intensive problem of classifying mosquito neuronal spikes for arboviral disease detection by proposing SSI-GAN, a semi-supervised GAN that combines a Swin-inspired discriminator with a transformer-based generator. Using only 1-3% labeled data from over 15 million spike samples, it achieved up to 99.93% accuracy in classifying Zika-infected, dengue-infected, or uninfected categories, significantly reducing labeling effort while outperforming baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [hybrid feature engineering, wavelet decomposition, graph-theoretic descriptors, linear separability, model compression]</p>
</li>
<li class="">
<p><strong>authors:</strong> Moirangthem Tiken Singh, Manibhushan Yaikhom</p>
</li>
<li class="">
<p><strong>institution:</strong> Dibrugarh University Institute of Engineering and Technology (DUIET), Dibrugarh University; Regional Institute of Medical Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00192" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00192</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A resource-efficient, data-centric framework that makes high-dimensional ECG data linearly separable through hybrid feature engineering. 2. A novel feature space integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors like PageRank centrality. 3. An ultra-lightweight model achieving state-of-the-art efficiency (8.54 KB, 0.46 µs latency) for real-time arrhythmia detection on edge devices.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a662793cb6f13133cb7159786a9b30cdc33ffe10a9ca51a20ba5d501aa94a04_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a662793cb6f13133cb7159786a9b30cdc33ffe10a9ca51a20ba5d501aa94a04_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a resource-efficient framework for arrhythmia detection on edge devices by using hybrid feature engineering (wavelet and graph descriptors) to make ECG data linearly separable, enabling the use of ultra-lightweight linear classifiers. The resulting model achieves high accuracy (98.44%) with a very small footprint (8.54 KB) and low latency, offering significant efficiency gains over compressed deep learning models for IoMT applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] StockBot 2.0: Vanilla LSTMs Outperform Transformer-based Forecasting for Stock Prices</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [LSTM, Transformer, Stock Prediction, Time Series Forecasting, Attention]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shaswat Mohanty</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00197" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00197</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Presents an enhanced StockBot architecture for systematic evaluation of modern time-series forecasting models (attention-based, convolutional, recurrent) in a unified setting. 2. Demonstrates empirically that a carefully constructed vanilla LSTM model consistently outperforms transformer-based models in stock price forecasting accuracy and decision-making stability under default hyperparameters. 3. Highlights the robustness, data efficiency, and importance of architectural inductive bias of recurrent models for financial forecasting, especially in data-limited scenarios.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/131078c30708f9e13fee0c529bee858ed184717ff3b0bf5f762eda2a9370616c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/131078c30708f9e13fee0c529bee858ed184717ff3b0bf5f762eda2a9370616c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents StockBot 2.0, a framework for evaluating time-series models for stock prediction. It finds that a vanilla LSTM model, despite its simplicity, outperforms more complex transformer-based models in forecasting accuracy and trading decision stability when trained with default settings, emphasizing the value of recurrent inductive biases for financial data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Unknown Aware AI-Generated Content Attribution</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [image attribution], [generative model attribution, constrained optimization, open world setting, CLIP features, unknown generators]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ellie Thieu, Jifan Zhang, Haoyue Bai</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Wisconsin–Madison (UW–Madison)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00218" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00218</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Establishes a strong baseline for target generator attribution using CLIP features and a linear classifier with limited labeled data. 2. Proposes a novel constrained optimization method that leverages unlabeled &quot;wild&quot; data from the internet to improve robustness to unseen generators. 3. Demonstrates that incorporating unlabeled wild data substantially improves attribution performance on challenging, unseen, and newly released generative models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53bb1bd3ca2eb8682e06279d4402b6c4441fc76d44f3b86a930479ba697b4e06_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53bb1bd3ca2eb8682e06279d4402b6c4441fc76d44f3b86a930479ba697b4e06_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of attributing AI-generated images to their specific source model in an open-world setting where new, unseen generators constantly emerge. The authors propose a constrained optimization approach that uses unlabeled data collected from the internet to encourage the classifier to treat unknown samples as non-target, while maintaining performance on known labeled data. The method significantly improves attribution accuracy on challenging, unseen generative models compared to a baseline trained only on limited labeled data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Robust Graph Fine-Tuning with Adversarial Graph Prompting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [graph neural networks], [Adversarial Graph Prompting, Parameter-Efficient Fine-Tuning, Graph Prompt Learning, Adversarial Learning, Robust Fine-Tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ziyan Zhang, Bo Jiang, Jin Tang</p>
</li>
<li class="">
<p><strong>institution:</strong> Anhui University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00229" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00229</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Adversarial Graph Prompting (AGP) framework that integrates adversarial learning into graph prompting for robust parameter-efficient fine-tuning of pre-trained GNNs. 2. Formulates AGP as a min-max optimization problem and develops an alternating optimization scheme, featuring a Joint Projected Gradient Descent (JointPGD) algorithm for generating adversarial noise and a module for learning optimal node prompts. 3. Provides theoretical analysis demonstrating that AGP can handle both graph topology and node feature noise, confirming its versatility and robustness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/337af6d04faa59592d198afa70192352bc013ab310b674dbdaed1185325d78c7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/337af6d04faa59592d198afa70192352bc013ab310b674dbdaed1185325d78c7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the vulnerability of Parameter-Efficient Fine-Tuning (PEFT) methods for Graph Neural Networks (GNNs) to noise and attacks. It proposes a novel Adversarial Graph Prompting (AGP) framework that formulates robust fine-tuning as a min-max optimization problem, using adversarial noise generation and prompt learning to counteract it. The method is theoretically sound and experimentally validated to be more robust than state-of-the-art approaches across various graph noises.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] GRIT -- Geometry-Aware PEFT with K-FACPreconditioning, Fisher-Guided Reprojection, andDynamic Rank Adaptation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [LoRA, K-FAC, Parameter-Efficient Fine-Tuning, Fisher Information, Dynamic Rank Adaptation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Pritish Saha, Chandrav Rajbangshi, Rudra Goyal, Mohit Goyal, Anurag Deo, Biswajit Roy, Ningthoujam Dhanachandra Singh, Raxit Goswami, Amitava Das</p>
</li>
<li class="">
<p><strong>institution:</strong> RAAPID Lab, Pragya Lab (BITS Pilani, Goa)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00231" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00231</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces K-FAC-based gradient preconditioning in the low-rank subspace for more geometry-aware updates. 2. Proposes periodic Fisher-guided reprojection of the LoRA basis to suppress parameter drift. 3. Implements dynamic rank adaptation to concentrate capacity on high-signal directions, reducing the number of trainable parameters.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e9515d46161014bf32c8c1a74f165b3980c9984817a7e1ca0778ce4d66219f5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e9515d46161014bf32c8c1a74f165b3980c9984817a7e1ca0778ce4d66219f5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that standard LoRA/QLoRA methods are geometry-agnostic, leading to inefficient updates and parameter drift. It proposes GRIT, a new LoRA procedure that uses K-FAC preconditioning, Fisher-guided reprojection, and dynamic rank adaptation to make updates more curvature-aware. This approach matches or surpasses baseline performance while reducing trainable parameters by an average of 46% and achieving lower drift.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [CycleGAN, YOLOv8, infrared image generation, data augmentation, PCB defect detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chao Yang, Haoyuan Zheng, Yue Ma</p>
</li>
<li class="">
<p><strong>institution:</strong> Xi’an Jiaotong Liverpool University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00237" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00237</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a cross-modal data augmentation framework using CycleGAN for unpaired translation from visible-light to infrared images to address IR data scarcity. 2. Introduces a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a YOLOv8 detector. 3. Demonstrates that the method significantly improves detection performance under low-data conditions, approaching fully supervised benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6328838143840b03119bcacf495d6f90fd2cfbf75f09c866a7d8dbf7d351c32_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6328838143840b03119bcacf495d6f90fd2cfbf75f09c866a7d8dbf7d351c32_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the problem of scarce infrared data for PCB defect detection by using CycleGAN to generate synthetic infrared images from abundant visible-light images and training a YOLOv8 detector on this augmented dataset. The proposed method enhances feature learning with limited real data, significantly outperforming models trained only on real data and nearly matching the performance of fully supervised training.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [neuromorphic computing, state-space models, sparse attention, surrogate gradients, local learning rules]</p>
</li>
<li class="">
<p><strong>authors:</strong> Osvaldo Simeone</p>
</li>
<li class="">
<p><strong>institution:</strong> Northeastern University London (Intelligent Networked Systems Institute - INSI)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00245" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00245</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel conceptual framework for analyzing modern neuromorphic AI through the lens of intra-token (feature-level) and inter-token (contextual) processing. 2. Systematically reviews the convergence of neuromorphic principles (e.g., sparse, discrete activations) with state-of-the-art AI architectures like state-space models and transformers. 3. Reviews and categorizes training methodologies for neuromorphic models, from surrogate gradients to local learning rules based on reinforcement learning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9154dccb8e64d45d3b60bd0f6bb1e84fa9423cf041633e14a8cb6272baa46_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9154dccb8e64d45d3b60bd0f6bb1e84fa9423cf041633e14a8cb6272baa46_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high energy costs of modern AI by exploring the convergence of neuromorphic computing principles with contemporary architectures. It proposes a framework distinguishing between intra-token and inter-token processing to analyze how neuromorphic ideas like sparse activations and state dynamics are embodied in models such as transformers and state-space models. The main conclusion is that modern AI is increasingly adopting brain-inspired, energy-efficient neuromorphic principles for both processing types, offering a path toward more sustainable intelligent systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Rectifying Adversarial Examples Using Their Vulnerabilities</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [adversarial defense], [adversarial examples, label rectification, re-attack, white-box attack, black-box attack]</p>
</li>
<li class="">
<p><strong>authors:</strong> Fumiya Morimoto, Ryuto Morita, Satoshi Ono</p>
</li>
<li class="">
<p><strong>institution:</strong> Kagoshima University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00270" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00270</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel adversarial example rectification method based on &quot;re-attacking&quot; AEs to move them beyond the decision boundary for correct label estimation. 2. The method is designed to be straightforward, requiring only AEs as input without parameter adjustments or preliminary training, enabling it to address diverse attack types. 3. Demonstrates consistent performance and superior stability against various attacks, including targeted and black-box attacks, compared to conventional rectification and input transformation methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c32aaba927fb42e32f98d767a04b8c60ecebe5d8f0f13c4c25f72d7d23e5138c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c32aaba927fb42e32f98d767a04b8c60ecebe5d8f0f13c4c25f72d7d23e5138c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of rectifying adversarial examples (AEs) to recover the correct labels of the original inputs, which is crucial for applications like autonomous driving. The proposed method works by &quot;re-attacking&quot; the AEs to push them across the model&#x27;s decision boundary. The results show that this method performs consistently across different attack types and is more stable than existing approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [quantization, self-explanations, faithfulness, natural language explanations, counterfactual examples]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qianli Wang, Nils Feldhus, Pepa Atanasova, Fedor Splitt, Simon Ostermann, Sebastian Möller, Vera Schmitt</p>
</li>
<li class="">
<p><strong>institution:</strong> Technische Universität Berlin, German Research Center for Artificial Intelligence (DFKI), University of Copenhagen</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00282" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00282</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. First comprehensive study on the impact of quantization on the quality and faithfulness of LLM self-explanations. 2. Empirical evaluation across multiple quantization techniques, bit widths, and model sizes, revealing moderate but consistent degradation in explanation metrics. 3. Provides practical recommendations for validating self-explanations in quantized models, highlighting the greater sensitivity of natural language explanations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8576a4dae59be54ef8b0a451a49893f5b9d59eceafecb4a697aed2b3fd4a470b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8576a4dae59be54ef8b0a451a49893f5b9d59eceafecb4a697aed2b3fd4a470b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates how quantization affects the quality and faithfulness of self-explanations generated by large language models. The authors evaluate multiple quantization methods and find they cause moderate declines in explanation metrics, with larger models showing better faithfulness preservation. They conclude that while quantization degrades self-explanations, the impact is relatively minor and does not negate its benefits for model compression.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Task-Driven Kernel Flows: Label Rank Compression and Laplacian Spectral Filtering</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [theoretical machine learning], [kernel evolution, spectral truncation, label rank compression, Laplacian spectral filtering, Neural Tangent Kernel]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hongxi Li, Chunlin Huang</p>
</li>
<li class="">
<p><strong>institution:</strong> Sun Yat-sen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00276" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00276</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Derives a kernel ODE revealing a &quot;water-filling&quot; spectral law for supervised learning, showing the kernel is compressed into a low-rank subspace bounded by the number of classes. 2. Proves that any stable steady state under L2-regularization and linear readout inherently exhibits label-driven rank compression, independent of the fast-readout approximation. 3. Demonstrates that SGD noise is confined to a low-rank subspace (O(C)), unifying deterministic and stochastic views of alignment, and contrasts this with expansive self-supervised representations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03dc1352545d41fe155dce9a675ce3e4dfa7f83f33a1829acc745f5fa2954776_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03dc1352545d41fe155dce9a675ce3e4dfa7f83f33a1829acc745f5fa2954776_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper develops a kernel-centric theory for feature learning in wide neural networks. It shows that supervised learning inherently compresses the kernel into a low-rank subspace bounded by the number of classes, and that SGD noise is similarly confined, contrasting this compressive behavior with the expansive representations of self-supervised learning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Can Optimal Transport Improve Federated Inverse Reinforcement Learning?</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [Inverse Reinforcement Learning, Federated Learning, Optimal Transport, Wasserstein Barycenter, Maximum Entropy IRL]</p>
</li>
<li class="">
<p><strong>authors:</strong> David Millard, Ali Baheri</p>
</li>
<li class="">
<p><strong>institution:</strong> Rochester Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00309" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00309</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces an optimal transport-based approach for federating learned reward functions in Inverse Reinforcement Learning (IRL). 2. Proposes using a Wasserstein barycenter for reward fusion, which accounts for the geometric structure of the reward landscape, as opposed to simple parameter averaging. 3. Provides a theoretical proof that the barycentric fusion yields a more faithful global reward estimate than conventional federated averaging methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18b7dd99db471c1c90bc7b908611843e09230b1dcad68713f2c1d393a1fa86bb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18b7dd99db471c1c90bc7b908611843e09230b1dcad68713f2c1d393a1fa86bb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of learning a shared reward function across heterogeneous agents in privacy-sensitive, communication-limited settings. It proposes a federated IRL framework where agents perform local Maximum Entropy IRL and then fuse their reward functions via a Wasserstein barycenter. The authors prove this method provides a more accurate global reward estimate than standard parameter averaging, offering a principled and efficient solution for multi-agent systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Quantum King-Ring Domination in Chess: A QAOA Approach</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [quantum optimization], [QAOA, benchmark, constraint-preserving mixers, warm-start, CVaR]</p>
</li>
<li class="">
<p><strong>authors:</strong> Gerhard Stenzel, Michael Kölle, Tobias Rohe, Julian Hager, Leo Sünkel, Maximilian Zorn, Claudia Linnhoff-Popien</p>
</li>
<li class="">
<p><strong>institution:</strong> LMU Munich</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00318" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00318</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduction of the Quantum King-Ring Domination (QKRD) benchmark, a structured, NISQ-scale testbed derived from chess with 5,000 instances. 2. Systematic evaluation of QAOA design choices, showing the advantages of constraint-preserving mixers and warm-start strategies. 3. Demonstration that structured benchmarks reveal performance insights for problem-informed QAOA techniques that are obscured in random synthetic instances.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0b58913db95e3acf390bd659716f38782d77c5b0f4c58736c855c0b4940575b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0b58913db95e3acf390bd659716f38782d77c5b0f4c58736c855c0b4940575b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces a new structured benchmark called Quantum King-Ring Domination (QKRD) based on chess to evaluate the Quantum Approximate Optimization Algorithm (QAOA). Using this benchmark, the authors systematically test various QAOA design choices and find that constraint-preserving mixers and warm-start strategies significantly improve performance. The results show that structured benchmarks are crucial for revealing the advantages of problem-informed quantum optimization techniques.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Smart Fault Detection in Nanosatellite Electrical Power System</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [fault diagnosis], [neural network, PCA classification, decision tree, KNN]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alireza Rezaee, Niloofar Nobahari, Amin Asgarifar, Farshid Hajati</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Tehran, University of New England</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00335" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00335</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a new fault detection method for nanosatellite electrical power systems operating without an Attitude Determination Control Subsystem (ADCS) in LEO orbit. 2. Uses a neural network to simulate the fault-free system behavior using solar radiation and panel temperature as inputs to predict current and load. 3. Applies multiple machine learning classifiers (neural network, PCA, decision tree, KNN) to diagnose specific fault patterns and types in the power subsystem.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab383b2f3d676c3a09faf3b55b3c7d4dd74c574ecc0a93d40c28f82897ffe1bf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab383b2f3d676c3a09faf3b55b3c7d4dd74c574ecc0a93d40c28f82897ffe1bf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a machine learning-based method for detecting faults in nanosatellite electrical power systems. It first simulates normal system behavior using a neural network and then employs various classifiers to identify specific fault types. The approach aims to diagnose common faults like line-to-line shorts and open circuits without relying on an ADCS.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [multilingual representation learning], [Joint Embedding Predictive Architecture (JEPA), BERT, CLS token, language-agnostic embedding, multilingual benchmarks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Taj Gillin, Adam Lalani, Kenneth Zhang, Marcel Mateos Salles</p>
</li>
<li class="">
<p><strong>institution:</strong> Brown University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00366" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00366</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces BERT-JEPA (BEPA), a novel training paradigm that adds a JEPA objective to BERT-style models to reorganize the [CLS] embedding space. 2. Demonstrates that BEPA finetuning transforms the [CLS] embedding space into a semantic-first, language-agnostic space, shifting its PCA representation from low-rank to fuller-rank. 3. Shows that this reorganization improves performance on multilingual tasks with little to no loss in English performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cd5b4a28e22d003dd88f59a4d1a1a55a97fa54c9c398a578c710764342d3180_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cd5b4a28e22d003dd88f59a4d1a1a55a97fa54c9c398a578c710764342d3180_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that BERT&#x27;s [CLS] embeddings fail to capture language-invariant semantics. It proposes BERT-JEPA (BEPA), a method that adds a Joint Embedding Predictive Architecture (JEPA) objective during training to reorganize the [CLS] embedding space into a language-agnostic &quot;thought space&quot;. The main conclusion is that this approach significantly improves performance on multilingual benchmarks while maintaining English task performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Deterministic Coreset for Lp Subspace</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [randomized algorithms, numerical linear algebra, data summarization], [coreset, subspace embedding, ℓp regression, deterministic algorithm, iterative algorithm]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rachit Chhaya, Anirban Dasgupta, Dan Feldman, Supratim Shit</p>
</li>
<li class="">
<p><strong>institution:</strong> Dhirubhai Ambani University, IIT Gandhinagar, University of Haifa, IIIT-Delhi</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00361" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00361</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the first iterative algorithm for constructing a deterministic ε-coreset for ℓp subspace embedding for any p in [1,∞). 2. Achieves an optimal coreset size of O(d^{max(1,p/2)}/ε²), removing long-standing logarithmic factors. 3. Provides a deterministic guarantee for the coreset, enabling its use for approximately solving ℓp regression deterministically.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b23fc2c49cea022bba3ab19cb79b7de330860628aab1b669177840bc826ecda1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b23fc2c49cea022bba3ab19cb79b7de330860628aab1b669177840bc826ecda1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a new iterative algorithm for constructing a deterministic coreset that provides an ℓp subspace embedding for any p≥1. The method ensures bounded loss in each iteration, leading to a coreset whose size is optimal and free of logarithmic factors. The result solves a long-standing open problem and enables deterministic approximate solutions to ℓp regression.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Engineering Attack Vectors and Detecting Anomalies in Additive Manufacturing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [cyber-physical systems security], [intrusion detection system, anomaly detection, G-code manipulation, transformer encoder, self-attention autoencoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Mahbub Hasan, Marcus Sternhagen, Krishna Chandra Roy</p>
</li>
<li class="">
<p><strong>institution:</strong> New Mexico Institute of Mining and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00384" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00384</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Investigation of stealthy Man-in-the-Middle (MitM) attack vectors targeting the CAD-to-machine interface in Fused Deposition Modeling (FDM) 3D printers. 2. Proposal of an unsupervised Intrusion Detection System (IDS) that uses a frozen Transformer-based encoder and contrastive learning to create anomaly-sensitive embeddings from machine logs. 3. Demonstration of effective anomaly classification using a combination of clustering and a self-attention autoencoder on real 3D printing systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b296e2b64944be1e0ab79e116131c11acbb4a662a6a9c3e8adaf377db0c3190e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b296e2b64944be1e0ab79e116131c11acbb4a662a6a9c3e8adaf377db0c3190e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates stealthy cyberattacks that manipulate G-code in additive manufacturing systems, leading to structurally defective parts. To detect these attacks, the authors propose an unsupervised intrusion detection system that uses a Transformer-based encoder and contrastive learning to analyze machine logs. Their method successfully distinguishes between normal and compromised printing executions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Real-Time Human Detection for Aerial Captured Video Sequences via Deep Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [aerial video, optical flow, convolutional neural network, hierarchical extreme learning machine, UCF-ARG dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nouar AlDahoul, Aznul Qalid Md Sabri, Ali Mohammed Mansoor</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Malaya</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00391" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00391</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a framework combining optical flow with three different deep learning models (S-CNN, pretrained CNN, H-ELM) for human detection in challenging aerial videos. 2. Conducts a comparative performance analysis of the models on the UCF-ARG dataset, evaluating accuracy and training speed across five human actions. 3. Demonstrates the effectiveness of automatic feature learning over handcrafted features for handling dynamic events like camera jitter and scale variation in aerial footage.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/104ed4332f4aebfb8e2617a89656868c3e4e5fae6201be63d4509d5b080a2f50_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/104ed4332f4aebfb8e2617a89656868c3e4e5fae6201be63d4509d5b080a2f50_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of human detection in aerial videos with a non-static camera. It proposes using optical flow combined with three deep learning models (S-CNN, a pretrained CNN, and H-ELM) for automatic feature learning. The experiments on the UCF-ARG dataset show that the pretrained CNN achieves the highest accuracy (98.09%), successfully demonstrating the method&#x27;s robustness to dynamic conditions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] NOS-Gate: Queue-Aware Streaming IDS for Consumer Gateways under Timing-Controlled Evasion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [network intrusion detection], [timing-controlled evasion, weighted fair queueing (WFQ), network-optimised spiking (NOS), metadata-only detection, streaming IDS]</p>
</li>
<li class="">
<p><strong>authors:</strong> Muhammad Bilal, Omer Tariq, Hasan Ahmed</p>
</li>
<li class="">
<p><strong>institution:</strong> Lancaster University, Korea Advanced Institute of Science and Technology (KAIST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00389" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00389</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed NOS-Gate, a lightweight, streaming IDS for consumer gateways that uses a two-state unit derived from Network-Optimised Spiking dynamics per flow. 2. Introduced a queue-aware, reversible mitigation action that temporarily reduces a flagged flow&#x27;s weight under Weighted Fair Queueing (WFQ). 3. Developed an executable &#x27;worlds&#x27; benchmark for evaluating IDS under timing-controlled evasion, specifying benign processes, attacker budgets, and enabling packet-level WFQ replay.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16e859896027b80a597ad555df2beab42dc7cf683d8aef57af2a60ff1820126c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16e859896027b80a597ad555df2beab42dc7cf683d8aef57af2a60ff1820126c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of detecting intrusions in encrypted traffic on resource-constrained consumer gateways, where attackers can evade detection by manipulating timing patterns. It proposes NOS-Gate, a lightweight streaming IDS that uses metadata features and a novel mitigation strategy integrated with queue management. The evaluation shows NOS-Gate achieves higher detection recall and reduces queueing delays compared to baselines, with low computational overhead.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [time-warp emulation, CUDA interception, virtual time coordination, performance modeling, discrete-event simulation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amey Agrawal, Mayank Yadav, Sukrit Kumar, Anirudha Agrawal, Garv Ghai, Souradeep Bera, Elton Pinto, Sirish Gambhira, Mohammad Adain, Kasra Sohrab, Chus Antonanzas, Alexey Tumanov</p>
</li>
<li class="">
<p><strong>institution:</strong> Georgia Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00397" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00397</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A time-warp emulator that enables performance modeling by directly executing real serving system code without physical GPUs, eliminating the need to re-implement complex control logic. 2. A system that intercepts CUDA API calls to virtualize device management and performs time jumps by fast-forwarding virtual time based on predicted kernel durations. 3. A coordination protocol that synchronizes time jumps across distributed processes while preserving causality, ensuring accurate emulation of parallel execution.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87798fc4e00db06b5558e8552991b262a89ec7eea8a194407a93b93519f3357e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87798fc4e00db06b5558e8552991b262a89ec7eea8a194407a93b93519f3357e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents Revati, a time-warp emulator for efficient LLM serving configuration testing. It directly executes real serving system code by intercepting CUDA calls and performing virtual time jumps instead of running GPU kernels, achieving less than 5% prediction error while running 5-17x faster than real GPU execution on frameworks like vLLM and SGLang.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [Privacy-preserving data aggregation], [unanimous-release confidentiality, consensus locking, malicious deviation detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Prajwal Panth, Sahaj Raj Malla</p>
</li>
<li class="">
<p><strong>institution:</strong> KIIT University, Kathmandu University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00418" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00418</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the CPPDD framework, a lightweight protocol for secure multi-client data aggregation using per-client affine masking and priority-driven sequential consensus locking to enforce unanimous-release confidentiality. 2. Introduces decentralized integrity verification via step and data checksums (σ_S, σ_D) enabling autonomous malicious deviation detection and atomic abort without persistent coordination. 3. Formally proves the framework&#x27;s properties (correctness, CDIF, IND-CPA security) and empirically demonstrates linear scalability up to 500 clients with significantly lower computational overhead compared to MPC and HE baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a94aa63b5803d44299e228782541c1d2830c11c784cb2a9d0a55df2b0c6765_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a94aa63b5803d44299e228782541c1d2830c11c784cb2a9d0a55df2b0c6765_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes the CPPDD framework to address the problem of secure and verifiable multi-client data sharing. The method combines affine masking and consensus locking for privacy, and uses checksums for integrity verification, enabling efficient, scalable aggregation with malicious security. The framework is proven secure and shown to be orders of magnitude more efficient than traditional cryptographic approaches like MPC and HE.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] RMAAT: Astrocyte-Inspired Memory Compression and Replay for Efficient Long-Context Transformers</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [efficient transformers], [astrocyte-inspired computing, long-term plasticity (LTP), short-term plasticity (STP), memory compression, Long Range Arena (LRA)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Md Zesun Ahmed Mia, Malyaban Bal, Abhronil Sengupta</p>
</li>
<li class="">
<p><strong>institution:</strong> Pennsylvania State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00426" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00426</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces RMAAT, a novel Transformer architecture that integrates abstracted astrocyte functionalities for efficient long-context processing. 2. Proposes an adaptive memory compression mechanism governed by a novel retention factor derived from simulated astrocyte long-term plasticity (LTP). 3. Develops Astrocytic Memory Replay Backpropagation (AMRB), a novel training algorithm designed for memory efficiency in recurrent networks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48db0bbab3b8ca0fcbec4bfde72ebb384d63651cf925a575ef2f9e06a070abc5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48db0bbab3b8ca0fcbec4bfde72ebb384d63651cf925a575ef2f9e06a070abc5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the quadratic complexity problem of Transformer self-attention for long sequences by proposing RMAAT, an architecture inspired by astrocyte functions in biological memory. The method uses recurrent segment-based processing with adaptive memory compression and a linear-complexity attention mechanism. Evaluations on the Long Range Arena benchmark show that RMAAT achieves competitive accuracy with substantial improvements in computational and memory efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Deep Delta Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [neural network architecture], [residual networks, geometric transformation, spectral analysis, rank-1 perturbation, dynamic gating]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yifan Zhang, Yifeng Liu, Mengdi Wang, Quanquan Gu</p>
</li>
<li class="">
<p><strong>institution:</strong> Princeton University, University of California, Los Angeles</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00417" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00417</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/yifanzhang-pro/deep-delta-learning" target="_blank" rel="noopener noreferrer" class="">https://github.com/yifanzhang-pro/deep-delta-learning</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Deep Delta Learning (DDL), a novel architecture that generalizes residual connections with a learnable, data-dependent geometric transformation called the Delta Operator. 2. Provides a spectral analysis of the Delta Operator, showing it can dynamically interpolate between identity mapping, orthogonal projection, and geometric reflection via a gating scalar. 3. Restructures the residual update as a synchronous rank-1 injection, unifying feature erasure and writing under a dynamic step size to enable complex, non-monotonic dynamics while preserving stable training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e9151cd8b5ec94dcb21276489c4319c73f4c1a7295a6715a6c2a36d36f9a9b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e9151cd8b5ec94dcb21276489c4319c73f4c1a7295a6715a6c2a36d36f9a9b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies that the strictly additive inductive bias of standard residual networks limits their capacity to model complex state transitions. To address this, it proposes Deep Delta Learning (DDL), which modulates the identity shortcut with a learnable, data-dependent geometric transformation (the Delta Operator). This allows the network to explicitly control its layer-wise transition spectrum, enabling the modeling of complex dynamics like oscillations while maintaining stable training.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning for human feedback], [reinforcement learning from human feedback (RLHF), flow matching, stochastic differential equations (SDE), group relative policy optimization (GRPO), entropy-aware sampling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shengjun Zhang, Zhang Zhang, Chensheng Dai, Yueqi Duan</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00423" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00423</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/shengjun-zhang/VisualGRPO" target="_blank" rel="noopener noreferrer" class="">https://github.com/shengjun-zhang/VisualGRPO</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identified that high-entropy denoising steps are crucial for effective exploration in RL for flow models, while low-entropy steps lead to ambiguous rewards. 2. Proposed E-GRPO, an entropy-aware method that consolidates consecutive low-entropy steps into a single high-entropy step for SDE sampling and uses ODE sampling elsewhere. 3. Introduced a multi-step group normalized advantage calculation that computes advantages relative to samples sharing the same consolidated SDE step.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/517610c9d7d0b5f72ab6ea2e2c36dda14fb3879dc1ff5c4a8ae66c97dd3a6457_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/517610c9d7d0b5f72ab6ea2e2c36dda14fb3879dc1ff5c4a8ae66c97dd3a6457_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of sparse and ambiguous reward signals when applying reinforcement learning to flow models over multiple denoising steps. It proposes E-GRPO, an entropy-aware method that strategically uses SDE sampling on high-entropy steps and ODE sampling on others, along with a group-relative advantage calculation. Experiments show this approach is more effective for aligning flow models with human preferences.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] A Comparative Analysis of Interpretable Machine Learning Methods</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [interpretable machine learning], [Explainable Boosting Machines (EBMs), Symbolic Regression (SR), Generalized Optimal Sparse Decision Trees (GOSDT), Interpretable Generalized Additive Neural Networks (IGANNs), tabular data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mattia Billa, Giovanni Orlandi, Veronica Guidetti, Federica Mandreoli</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Modena and Reggio Emilia</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00428" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00428</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Conducted a large-scale comparative evaluation of 16 inherently interpretable methods across 216 real-world tabular datasets. 2. Stratified performance analysis based on structural dataset characteristics (dimensionality, sample size, linearity, class imbalance) and assessed training time and robustness under distributional shifts. 3. Provided empirical findings on performance hierarchies and context-dependent model suitability, offering practical guidance for balancing interpretability and predictive performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b45f5348139a7a9be289787000b2099d616e153113ae1190dc4749f573df65bb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b45f5348139a7a9be289787000b2099d616e153113ae1190dc4749f573df65bb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the scarcity of systematic evaluations for inherently interpretable machine learning models on tabular data by conducting a large-scale benchmark of 16 methods. The study analyzes performance across 216 datasets, considering structural characteristics and robustness. The results show that EBMs are strong for regression, while SR and IGANNs excel in non-linear settings, and GOSDT is sensitive to class imbalance, providing context-dependent guidance for practitioners.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series anomaly detection], [time series foundation models, parameter-efficient fine-tuning, anomaly detection, LoRA, AUC-PR]</p>
</li>
<li class="">
<p><strong>authors:</strong> Miseon Park, Kijung Yoon</p>
</li>
<li class="">
<p><strong>institution:</strong> Hanyang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00446" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00446</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Systematically evaluated the use of Time Series Foundation Models (TSFMs) as universal backbones for anomaly detection, showing they outperform task-specific models. 2. Compared multiple adaptation strategies (zero-shot, full fine-tuning, PEFT) for TSFMs across benchmarks, highlighting their versatility. 3. Demonstrated that Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA can match or exceed full fine-tuning performance while being computationally cheaper.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/94e39b065d01be71b9920224b4a74b3db8c40a04646e3c3b4b83dce81d7aa0c3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/94e39b065d01be71b9920224b4a74b3db8c40a04646e3c3b4b83dce81d7aa0c3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether pretrained Time Series Foundation Models (TSFMs) can be effectively adapted for anomaly detection. It compares different adaptation strategies, including zero-shot inference and parameter-efficient fine-tuning (PEFT). The results show that TSFMs, especially when adapted with PEFT methods like LoRA, outperform traditional task-specific models, offering a scalable and efficient solution for time series anomaly detection.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Controllable Concept Bottleneck Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [explainable ai], [Concept Bottleneck Models, Model Editing, Influence Functions, Machine Unlearning, Incremental Learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hongbin Lin, Chenyang Ren, Juangui Xu, Zhengyu Hu, Cheng-Long Wang, Yao Shu, Hui Xiong, Jingfeng Zhang, Di Wang, Lijie Hu</p>
</li>
<li class="">
<p><strong>institution:</strong> Based on the author list and affiliations (Hui Xiong, Fellow, IEEE), the primary institution is likely Rutgers University. Other affiliations may be present but are not explicitly listed in the provided content.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00451" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00451</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Controllable Concept Bottleneck Models (CCBMs) that support three granularities of model editing (concept-label, concept, and data-level) for dynamic maintenance. 2. Derives mathematically rigorous closed-form approximations for editing operations using influence functions, eliminating the need for retraining from scratch. 3. Demonstrates the efficiency and adaptability of CCBMs through experiments, validating their practical value for creating dynamic and trustworthy models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8610f217ce88f4c2d5513fec62b4fe3c5b8a65762f2a3fae9fcf60d3e539a686_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8610f217ce88f4c2d5513fec62b4fe3c5b8a65762f2a3fae9fcf60d3e539a686_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of efficiently editing Concept Bottleneck Models (CBMs) in dynamic real-world scenarios without retraining. It proposes Controllable Concept Bottleneck Models (CCBMs), which use influence functions to provide closed-form approximations for edits at concept-label, concept, and data levels. Experimental results show that CCBMs are efficient and adaptable, making them practical for maintaining trustworthy AI systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Imitation from Observations with Trajectory-Level Generative Embeddings</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [imitation learning], [imitation learning from observations, offline learning, diffusion models, trajectory embedding, surrogate reward]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yongtao Qu, Shangzhe Li, Weitong Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> University of North Carolina at Chapel Hill</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00452" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00452</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes TGE, a trajectory-level generative embedding method for offline imitation learning from observations. 2. Introduces a dense, smooth surrogate reward by estimating expert state density in the latent space of a temporal diffusion model. 3. Demonstrates that the method effectively bridges distributional gaps and outperforms prior methods on D4RL benchmarks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8023941a359742c0280304bc5c6d6f3a9e7332f5f298e3867930992f450af6fb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8023941a359742c0280304bc5c6d6f3a9e7332f5f298e3867930992f450af6fb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses offline imitation learning from observations where expert data is scarce and offline data is suboptimal. It proposes TGE, a method that uses a temporal diffusion model to create a smooth trajectory embedding for robust reward estimation. The approach outperforms existing methods on standard locomotion and manipulation benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Deep Networks Learn Deep Hierarchical Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [theoretical machine learning], [hierarchical models, residual networks, layerwise SGD, efficient learnability, teacher-student framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amit Daniely</p>
</li>
<li class="">
<p><strong>institution:</strong> Hebrew University of Jerusalem, Google Research Tel Aviv</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00455" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00455</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proves that layerwise SGD on residual networks can efficiently learn a class of hierarchical models with polynomial depth, surpassing previous learnable models limited to log-depth. 2. Introduces a formal model where the existence of human teachers, providing granular labels, naturally reveals a hierarchical structure that facilitates learning. 3. Suggests that the learnability of deep hierarchical models could form a theoretical basis for understanding why deep learning works.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1faa5ee1ce6f2d929632472a3fef6f2a37f968b78b881ef42a244f56de38679_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1faa5ee1ce6f2d929632472a3fef6f2a37f968b78b881ef42a244f56de38679_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper shows that layerwise stochastic gradient descent (SGD) on residual networks can efficiently learn a class of hierarchical models where labels are structured in increasingly complex levels. This class is more expressive, requiring polynomial depth, than previously known learnable models. The authors argue that this learnability, supported by a formal model of teaching, provides a potential theoretical foundation for understanding deep learning&#x27;s success.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Laplacian Kernelized Bandit</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-armed bandits], [graph Laplacian, reproducing kernel Hilbert space (RKHS), Gaussian process, regret bound, multi-user contextual bandits]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuang Wu, Arash A. Amini</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Los Angeles (UCLA)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00461" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00461</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a principled joint penalty combining graph smoothness and individual roughness, proving it is equivalent to the squared norm in a unified multi-user RKHS. 2. Explicitly derived the reproducing kernel for this RKHS, which fuses the graph Laplacian with a base arm kernel. 3. Designed two algorithms (LK-GP-UCB and LK-GP-TS) based on this kernel and provided theoretical regret bounds scaling with the kernel&#x27;s effective dimension.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef4eb4360cdf936fab9711bd6722ad2b67b34b282bf409e4e900beb17c23a0b2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef4eb4360cdf936fab9711bd6722ad2b67b34b282bf409e4e900beb17c23a0b2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses multi-user contextual bandits where users are connected by a graph and have non-linear reward functions. It proposes a new kernel that combines graph structure with arm features, enabling the design of efficient Gaussian Process-based algorithms for exploration. The method provides strong theoretical regret guarantees and outperforms baselines in non-linear settings.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [Mixture-of-Experts, Orthogonality Regularization, Weight-Activation Gap, Sparse Activation, Expert Diversity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hyunjun Kim</p>
</li>
<li class="">
<p><strong>institution:</strong> Korea Advanced Institute of Science and Technology (KAIST)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00457" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00457</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Showed that orthogonality regularization fails to reduce weight-space overlap and yields inconsistent effects on model performance across different datasets. 2. Identified a significant disconnect between weight-space and activation-space orthogonality, with no significant correlation between the two. 3. Demonstrated that weight-space regularization is an unreliable optimization target for improving expert diversity in MoE models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/297630182c2a41472ac5ae250b1200161c3b50705c9ba5130f166dda38b1a979_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/297630182c2a41472ac5ae250b1200161c3b50705c9ba5130f166dda38b1a979_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the effectiveness of applying orthogonality loss to enforce expert diversity in Mixture-of-Experts (MoE) models. The analysis reveals that this geometric regularization fails to reduce weight-space overlap, does not translate to activation-space orthogonality, and leads to inconsistent performance changes. The findings demonstrate that weight-space regularization is unsuitable for achieving MoE diversity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Detecting Spike Wave Discharges (SWD) using 1-dimensional Residual UNet</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [biomedical signal processing], [EEG, UNet, data augmentation, spike wave discharges, seizure detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Saurav Sengupta, Scott Kilianski, Suchetha Sharma, Sakina Lashkeri, Ashley McHugh, Mark Beenhakker, Donald E. Brown</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Virginia</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00459" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00459</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Comprehensive comparison of 14 machine learning classifiers on a large, manually annotated EEG dataset for SWD detection, identifying a 1D UNet as the best performer. 2. Enhancement of the 1D UNet model through data augmentation, with scaling identified as the most beneficial augmentation technique, resulting in the AugUNet1D model. 3. Public release of the AugUNet1D model (both pretrained and untrained) and demonstration of its superior performance against a state-of-the-art algorithmic method (&quot;Twin Peaks&quot;).</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/284cd89a49b52c6de51bc92974e7ebd74725aa53109914c27ce628c85db7b46c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/284cd89a49b52c6de51bc92974e7ebd74725aa53109914c27ce628c85db7b46c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the time-consuming manual labeling of spike wave discharges (SWDs) in EEG recordings by proposing an automated detection method. The authors developed and evaluated a 1D UNet model enhanced with data augmentation (AugUNet1D) on a large mouse EEG dataset, finding it outperformed other classifiers and a recent algorithmic approach. The main conclusion is that AugUNet1D provides a superior, publicly available tool for accurate SWD detection.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Neural Chains and Discrete Dynamical Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [scientific machine learning], [neural chains, physics-informed neural networks (PINNs), finite-difference methods, Burgers equation, Eikonal equation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sauro Succi, Abhisek Ganguly, Santosh Ansumali</p>
</li>
<li class="">
<p><strong>institution:</strong> Italian Institute of Technology, Jawaharlal Nehru Centre for Advanced Scientific Research (JNCASR), University of Roma Tre, Harvard University, Cornell University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00473" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00473</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes and analyzes the analogy between transformer-based neural chains (without self-attention) and discrete dynamical systems from discretized neural integral/PDEs. 2. Conducts a comparative analysis between standard numerical discretization (finite-difference) and PINN learning for solving Burgers and Eikonal equations, showing they converge to similar dynamical knowledge. 3. Identifies that PINNs explore a vast space of random matrices, unlike the structured matrices of finite-difference methods, leading to more parameters, higher training costs, and reduced explainability for 1D problems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/484d1f1718d109a31c34806ec956587c6c88440887cfe665bb5795e2c2cad940_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/484d1f1718d109a31c34806ec956587c6c88440887cfe665bb5795e2c2cad940_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the connection between neural chains (transformers without self-attention) and discrete dynamical systems. It compares solving PDEs like Burgers and Eikonal equations using standard finite-difference methods versus Physics-Informed Neural Networks (PINNs), finding both methods yield similar solutions but PINNs use many more random, less interpretable parameters. The authors conclude that for these 1D problems, PINNs offer no efficiency advantage over traditional methods, though their potential for high-dimensional problems remains open.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Noise-Aware Named Entity Recognition for Historical VET Documents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [named entity recognition], [Noise-Aware Training (NAT), OCR Noise, Data Augmentation, Transfer Learning, Multi-stage Fine-tuning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alexander M. Esser, Jens Dörpinghaus</p>
</li>
<li class="">
<p><strong>institution:</strong> Federal Institute for Vocational Education and Training (BIBB), University of Koblenz</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00488" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00488</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a robust NER approach for historical VET documents using Noise-Aware Training with synthetic OCR errors. 2. Systematically compares three complementary training strategies (noisy, clean, and artificial data). 3. Demonstrates that domain-specific and noise-aware fine-tuning significantly improves robustness and accuracy under noisy conditions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/beb3e9604f5f72e56feeecdc196004299094bc184a404fe77959f2a61d9e4da2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/beb3e9604f5f72e56feeecdc196004299094bc184a404fe77959f2a61d9e4da2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles Named Entity Recognition in noisy, historical Vocational Education and Training documents by proposing a method using Noise-Aware Training with synthetic OCR errors, transfer learning, and multi-stage fine-tuning. The approach, one of the first to recognize multiple entity types in this domain, shows that domain-specific and noise-aware fine-tuning substantially increases model robustness and accuracy. The method is applied to German but is designed to be transferable to other languages.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Improving LLM-Assisted Secure Code Generation through Retrieval-Augmented-Generation and Multi-Tool Feedback</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [Retrieval-Augmented Generation, CodeQL, KLEE, Self-Repair, Symbolic Execution]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vidyut Sriram, Sawan Pandita, Achintya Lakshmanan, Aneesh Shamraj, Suman Saha</p>
</li>
<li class="">
<p><strong>institution:</strong> Pennsylvania State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00509" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00509</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a retrieval-augmented, multi-tool repair workflow integrating compiler diagnostics, CodeQL security scanning, and KLEE symbolic execution for iterative LLM self-repair. 2. Utilizes a lightweight embedding model for semantic retrieval of security-focused repair examples to guide code generation. 3. Demonstrates significant robustness improvements, reducing security vulnerabilities by up to 96% for DeepSeek-Coder and from 58.55% to 22.19% for CodeLlama.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cdcdc91c311cf046454507445fa7f6baef39a437738ec8e32b2eb087f6fbfa91_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cdcdc91c311cf046454507445fa7f6baef39a437738ec8e32b2eb087f6fbfa91_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of LLM-generated code containing security vulnerabilities and errors. It proposes a method where a code-generating LLM iteratively refines its output using feedback from multiple tools (compiler, CodeQL, KLEE) and retrieval of past successful repairs. The results show this approach significantly reduces security defects, even for larger, more stubborn models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [reasoning integrity, retrieval-augmented generation, process verification, small language models, neural classifier]</p>
</li>
<li class="">
<p><strong>authors:</strong> Laksh Advani</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher (affiliation inferred from email domain: University of Colorado Boulder)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00513" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00513</a></p>
</li>
<li class="">
<p><strong>contributions:</strong>  1. Introduces the &quot;Right-for-Wrong-Reasons&quot; (RWR) phenomenon and the Reasoning Integrity Score (RIS), a novel process-based metric for evaluating the trustworthiness of small language model agents, validated with high inter-rater agreement. 2. Empirically demonstrates that while retrieval-augmented generation (RAG) significantly improves reasoning integrity, meta-cognitive interventions like self-critique often degrade it in small models, providing mechanistic explanations for these effects. 3. Distills the verification capability into a lightweight neural classifier that achieves high performance (0.86 F1-score) and efficiency (100x speedup), enabling practical process-based verification for deployment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05b874fd2b8e4599623bc1f3efd5491de7179346cb166566b9907ab13137ae7a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05b874fd2b8e4599623bc1f3efd5491de7179346cb166566b9907ab13137ae7a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies a critical reliability problem where small language models (7-9B parameters) often produce correct answers based on flawed reasoning, a phenomenon invisible to standard accuracy metrics. To address this, the authors introduce a process-based evaluation metric (RIS) and analyze the impact of interventions like RAG and self-critique, finding RAG improves reasoning while self-critique harms it in small models. They conclude that process verification is essential for trustworthy agents and demonstrate a fast neural classifier for this purpose.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [Siamese Recurrent Autoencoder, hybrid loss, real-time anomaly detection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Laksh Advani</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher (affiliation inferred from email domain: University of Colorado Boulder)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00516" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00516</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Demonstrated the ineffectiveness of standard anomaly detection methods for agent trajectory validation, establishing the need for specialized models. 2. Proposed a novel, sequence-aware Siamese Recurrent Autoencoder with a hybrid loss function for real-time trajectory anomaly detection. 3. Demonstrated that the approach is over 17x faster than LLM Judge baselines, making it suitable for real-time deployment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de97af87ef40e2bb7fa3067f0d8a7f8e2dc7d8fd40b77747e64af793669009c2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de97af87ef40e2bb7fa3067f0d8a7f8e2dc7d8fd40b77747e64af793669009c2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of detecting anomalous action plans in autonomous LLM agents, where existing methods fail to capture sequential structure and context. It proposes Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss combining contrastive learning and reconstruction for unified anomaly detection. The method achieves high F1-scores (0.88-0.94) and significantly faster inference (32 ms) than LLM-based baselines, enabling real-time safety verification.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Optimizing LSTM Neural Networks for Resource-Constrained Retail Sales Forecasting: A Model Compression Study</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [LSTM compression, model efficiency, retail forecasting, edge computing, hidden units]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ravi Teja Pagidoju</p>
</li>
<li class="">
<p><strong>institution:</strong> Campbellsville University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00525" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00525</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/RaviTeja444/sales-forecast-LSTM" target="_blank" rel="noopener noreferrer" class="">https://github.com/RaviTeja444/sales-forecast-LSTM</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Systematic evaluation of LSTM network sizes from 16 to 128 hidden units on real retail data. 2. Discovery that moderate compression (to 64 units) actually improves forecast accuracy. 3. Practical guidelines for model selection based on the accuracy-efficiency trade-off.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bda8da07103a6f89178a84bdce230c27d6cea8328b8c1fcb749e7912c516181_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bda8da07103a6f89178a84bdce230c27d6cea8328b8c1fcb749e7912c516181_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies LSTM model compression for resource-constrained retail sales forecasting by reducing the number of hidden units. The method involves systematically pruning the LSTM from 128 to 16 hidden units. The main conclusion is that reducing the model to 64 units not only makes it 73% smaller but also improves accuracy by 47%, showing larger models are not always better.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] A Sparse-Attention Deep Learning Model Integrating Heterogeneous Multimodal Features for Parkinson&#x27;s Disease Severity Profiling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multimodal learning], [sparse-attention, cross-attention, class-balanced focal loss, multimodal fusion, explainable AI]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dristi Datta, Tanmoy Debnath, Minh Chau, Manoranjan Paul, Gourab Adhikary, Md Geaur Rahman</p>
</li>
<li class="">
<p><strong>institution:</strong> Charles Sturt University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00519" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00519</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed the Class-Weighted Sparse-Attention Fusion Network (SAFN), an interpretable deep learning framework for multimodal Parkinson&#x27;s disease profiling. 2. Introduced a sparsity-constrained attention-gating fusion layer to dynamically prioritize informative modalities from heterogeneous data (MRI, clinical, demographic). 3. Employed a Class-Balanced Focal Loss to effectively handle dataset imbalance without synthetic oversampling, achieving high performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0104d1bebca16cde5f74be8dbe6bfbcfb976784343fbe1f1f8025e57eed10_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0104d1bebca16cde5f74be8dbe6bfbcfb976784343fbe1f1f8025e57eed10_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes SAFN, an interpretable deep learning model that integrates MRI and clinical data using sparse-attention and cross-attention mechanisms for Parkinson&#x27;s disease severity profiling. It addresses challenges in multimodal fusion and class imbalance. The model achieves high accuracy and interpretability, aligning clinical assessment importance with diagnostic principles.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Federated Customization of Large Models: Approaches, Experiments, and Insights</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [federated learning, prefix-tuning, large model customization, efficient fine-tuning, retrieval-augmented generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuchuan Ye, Ming Ding, Youjia Chen, Peng Cheng, Dusit Niyato</p>
</li>
<li class="">
<p><strong>institution:</strong> Fuzhou University, Data61 CSIRO, La Trobe University, Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00526" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00526</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a comprehensive review of large model customization techniques and discusses their implementation within a federated learning framework. 2. Proposes and experimentally validates federated prefix-tuning, which is the first application of prefix-tuning in a federated learning setting. 3. Demonstrates through comparative experiments that federated prefix-tuning achieves competitive performance, satisfactory efficiency, and consistent robustness compared to other federated customization methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02fea5cfedb112a13fd696a58bea7fb932671198f51d78a63540d7922a373af9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02fea5cfedb112a13fd696a58bea7fb932671198f51d78a63540d7922a373af9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper explores the federated customization of large models, which aims to adapt pre-trained models for specialized tasks using decentralized, private data. It proposes and validates federated prefix-tuning as a novel method, showing its performance is close to centralized approaches and competitive with other federated techniques. The work provides insights into implementing various customization methods within a federated learning framework to address privacy and data decentralization challenges.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Cloud-Native Generative AI for Automated Planogram Synthesis: A Diffusion Model Approach for Multi-Store Retail Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [diffusion models, cloud-native architecture, edge deployment, constraint satisfaction, planogram generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ravi Teja Pagidoju, Shriya Agarwal</p>
</li>
<li class="">
<p><strong>institution:</strong> Campbellsville University, University of the Cumberlands</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00527" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00527</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel cloud-native architecture for automated planogram synthesis using diffusion models, combining AWS for training and edge deployment for real-time inference. 2. A diffusion model that integrates retail-specific constraints through a modified loss function to generate store-specific layouts. 3. A comprehensive simulation-based and economic analysis demonstrating significant reductions in design time (98.3%) and cost (97.5%) with high constraint satisfaction (94.4%) and linear scalability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b868081df5651946090e341fbb8526b8b99531ee775ea4b316dc9682b146e22e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b868081df5651946090e341fbb8526b8b99531ee775ea4b316dc9682b146e22e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a cloud-native system using diffusion models to automate the generation of retail planograms. The method learns from successful shelf arrangements across multiple stores and incorporates business constraints into the model. The results show it drastically reduces design time and cost while maintaining high constraint satisfaction, proving the viability of generative AI for retail space optimization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Entropy Production in Machine Learning Under Fokker-Planck Probability Flow</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [Fokker-Planck equation, Kullback-Leibler divergence, entropy production, data drift, retraining trigger]</p>
</li>
<li class="">
<p><strong>authors:</strong> Lennon Shikhman</p>
</li>
<li class="">
<p><strong>institution:</strong> Florida Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00554" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00554</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel entropy-based retraining framework for machine learning models grounded in nonequilibrium statistical physics, modeling data drift as probability flow governed by a Fokker-Planck equation. 2. Derives a dynamical interpretation of model-data mismatch by showing its time derivative decomposes into an entropy-balance equation featuring a nonnegative entropy production term. 3. Introduces and validates an entropy-triggered retraining strategy that maintains high predictive performance while significantly reducing retraining frequency compared to daily or label-based policies.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a53bb0b8d5d5942ee17365e19e7b865b31c2701e9fcbcde1a5d6b9dc2ccc5b0e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a53bb0b8d5d5942ee17365e19e7b865b31c2701e9fcbcde1a5d6b9dc2ccc5b0e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses performance degradation in ML models due to data drift in nonstationary environments. It proposes a principled retraining framework based on nonequilibrium dynamics, using a Fokker-Planck model of drift and an entropy-production trigger. The method achieves performance comparable to frequent retraining while drastically reducing the number of retraining events.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Adversarial Samples Are Not Created Equal</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [adversarial robustness], [adversarial samples, non-robust features, adversarial bugs, ensemble-based metric, sharpness-aware minimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jennifer Crawford, Amol Khanna, Fred Lu, Amy R. Wagoner, Stella Biderman, Andre T. Nguyen, Edward Raff</p>
</li>
<li class="">
<p><strong>institution:</strong> Scale AI, CrowdStrike, Booz Allen Hamilton, EleutherAI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00577" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00577</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an ensemble-based metric to measure the manipulation of non-robust features by adversarial perturbations. 2. Introduces the concept of &quot;adversarial bugs&quot; to differentiate adversarial samples that do not rely on non-robust features. 3. Uses this new perspective to re-examine phenomena like the impact of sharpness-aware minimization and the robustness gap in adversarially trained models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e880259ebf04545c0ce64ed1e802b2caa6a3b401669986a52d0c3ccb0f063371_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e880259ebf04545c0ce64ed1e802b2caa6a3b401669986a52d0c3ccb0f063371_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper argues that not all adversarial samples are created equal, differentiating between those that exploit brittle &quot;non-robust&quot; features and those that do not (&quot;adversarial bugs&quot;). It proposes an ensemble-based metric to identify this distinction and uses it to analyze adversarial attacks, offering a new lens to re-examine existing robustness phenomena.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Learning to be Reproducible: Custom Loss Design for Robust Neural Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [training stability &amp; reproducibility], [custom loss function, training robustness, reproducibility, stochastic factors, weight initialization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Waqas Ahmed, Sheeba Samuel, Kevin Coakley, Birgitta Koenig-Ries, Odd Erik Gundersen</p>
</li>
<li class="">
<p><strong>institution:</strong> Friedrich Schiller University Jena, University of Technology Chemnitz, Norwegian University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00578" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00578</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies and empirically analyzes the critical gap in ensuring consistent performance across training runs due to stochastic factors like weight initialization and data shuffling. 2. Proposes a novel Custom Loss Function (CLF) designed to explicitly balance predictive accuracy with training stability, reducing sensitivity to these stochastic factors. 3. Demonstrates through extensive experiments on diverse architectures and tasks (image classification, time series forecasting) that CLF significantly improves training robustness without sacrificing predictive performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0accb01febbfd00a3b2c4650b921cc29a716544cb9a8fbc39d5a8ebe82c21bf6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0accb01febbfd00a3b2c4650b921cc29a716544cb9a8fbc39d5a8ebe82c21bf6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of inconsistent model performance across training runs due to stochastic factors. It proposes a Custom Loss Function (CLF) to explicitly balance accuracy and stability, which is shown to improve training robustness without harming predictive performance in experiments on image classification and time series forecasting.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [Mixture-of-Experts, federated fine-tuning, resource-aware, expert selection, sparsity-aware aggregation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zihan Fang, Zheng Lin, Senkang Hu, Yanan Ma, Yihang Tao, Yiqin Deng, Xianhao Chen, Yuguang Fang</p>
</li>
<li class="">
<p><strong>institution:</strong> City University of Hong Kong, The University of Hong Kong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00583" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00583</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a method to identify expert importance based on contributions to fine-tuning performance, enabling informed expert selection. 2. Proposes an adaptive expert subset selection mechanism from an information bottleneck perspective to align with heterogeneous client computing budgets. 3. Designs a sparsity-aware model aggregation strategy that weights updates from actively fine-tuned experts and gating parameters to mitigate destructive interference during global aggregation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a4349d6a95221a61360e261ad370cd60546e55c0ae19bef19bfe4f05de72ff4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a4349d6a95221a61360e261ad370cd60546e55c0ae19bef19bfe4f05de72ff4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes HFedMoE, a heterogeneous federated learning framework for fine-tuning large language models using Mixture-of-Experts. It addresses challenges in expert selection, resource heterogeneity, and aggregation interference by customizing expert subsets per client and using importance-weighted aggregation. Experiments show HFedMoE outperforms state-of-the-art methods in accuracy and convergence speed.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Cycling Race Time Prediction: A Personalized Machine Learning Approach Using Route Topology and Training Load</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [sports analytics], [Lasso regression, training load (CTL/ATL), N-of-1 study, feature engineering, route topology]</p>
</li>
<li class="">
<p><strong>authors:</strong> Francisco Aguilera Moreno</p>
</li>
<li class="">
<p><strong>institution:</strong> None specified (Inferred from author&#x27;s email domain: gmail.com)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00604" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00604</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a personalized machine learning model for cycling time prediction that uses route topology and athlete fitness state, avoiding complex physics-based parameters. 2. Implements and validates the approach using an N-of-1 study design with rigorous feature engineering to prevent data leakage. 3. Demonstrates that integrating fitness metrics (CTL, ATL) reduces prediction error by 14% compared to using route features alone, highlighting the importance of physiological state.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9697eaafe078b4416e1e63fcfa2f99ad60e0031eff1e90380ad4c7d7042e046_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9697eaafe078b4416e1e63fcfa2f99ad60e0031eff1e90380ad4c7d7042e046_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a personalized machine learning approach to predict cycling race times by combining route topology features with the athlete&#x27;s training load-derived fitness state. The method, evaluated on a single-athlete dataset, uses Lasso regression and achieves high accuracy (MAE=6.60 min, R²=0.922), showing that fitness metrics significantly improve predictions over topology alone.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Graph Neural Network, Q-learning, traffic-aware optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sonia Khetarpaul, P Y Sharan</p>
</li>
<li class="">
<p><strong>institution:</strong> Shiv Nadar Institution of Eminence</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00607" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00607</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel traffic-aware, graph-based reinforcement learning framework for optimal taxi placement that integrates real-time traffic data (e.g., congestion scores) with historical demand patterns. 2. Employs Graph Neural Network (GNN) embeddings to encode spatial-temporal dependencies within the urban road network, enhancing the agent&#x27;s understanding of network topology and dynamics. 3. Designs a multi-objective reward mechanism that jointly optimizes passenger waiting time, driver travel distance, and congestion avoidance, leading to significant performance improvements over a baseline.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d1b24d654126d64fa77f6ff66cd948e1830612a785483db227d8986e431770d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d1b24d654126d64fa77f6ff66cd948e1830612a785483db227d8986e431770d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of inefficient taxi supply-demand matching in smart cities by proposing a framework that models the urban road network as a graph and uses Graph Neural Networks combined with Q-learning to recommend optimal taxi placement hotspots. The method integrates real-time traffic conditions and historical data to optimize for passenger waiting time and driver travel distance. Experiments on a simulated Delhi dataset show the model reduces passenger waiting time by 56% and travel distance by 38% compared to a stochastic baseline.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Stronger Approximation Guarantees for Non-Monotone γ-Weakly DR-Submodular Maximization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [submodular optimization], [weakly DR-submodular, continuous-greedy, Frank-Wolfe, approximation algorithm, down-closed convex body]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hareshkumar Jadav, Ranveer Singh, Vaneet Aggarwal</p>
</li>
<li class="">
<p><strong>institution:</strong> IIT Indore, Purdue University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00611" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00611</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel approximation algorithm for maximizing non-monotone γ-weakly DR-submodular functions over down-closed convex bodies. 2. A smooth approximation guarantee that recovers the 0.401 factor for DR-submodular (γ=1) and degrades gracefully for γ&lt;1, improving upon prior bounds. 3. A hybrid algorithmic framework combining Frank-Wolfe-guided continuous-greedy with a γ-aware double-greedy step to handle non-monotonicity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3fb1d9788f036c8398d4ed9b82b8201cf4616c9a51d4916ebe91884a9996b77_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3fb1d9788f036c8398d4ed9b82b8201cf4616c9a51d4916ebe91884a9996b77_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of maximizing non-monotone, nonnegative γ-weakly DR-submodular functions over down-closed convex bodies. The authors propose a new algorithm that integrates a Frank-Wolfe-guided continuous-greedy approach with a γ-aware double-greedy step. This method achieves state-of-the-art approximation guarantees that depend smoothly on the parameter γ, improving upon previous results for this class of functions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Do Chatbot LLMs Talk Too Much? The YapBench Benchmark</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [llm evaluation], [verbosity, benchmark, brevity, over-generation, evaluation metric]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vadim Borisov, Michael Gröger, Mina Mikhael, Richard H. Schreiber</p>
</li>
<li class="">
<p><strong>institution:</strong> tabularis.ai</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00624" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00624</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://huggingface.co/datasets/tabularisai/yapbench" target="_blank" rel="noopener noreferrer" class="">https://huggingface.co/datasets/tabularisai/yapbench</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces YapBench, a benchmark with over 300 prompts to quantify LLM over-generation in brevity-ideal scenarios. 2. Proposes YapScore, a tokenizer-agnostic metric based on character count to measure excess response length. 3. Establishes a live leaderboard and provides analysis revealing an order-of-magnitude spread in verbosity across 76 evaluated LLMs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87b7bd8c8557d407f813af012f276e8f3d15707b3fbc649fe498992f3cbf4d01_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87b7bd8c8557d407f813af012f276e8f3d15707b3fbc649fe498992f3cbf4d01_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of LLMs generating unnecessarily long and verbose responses to simple prompts. It introduces the YapBench benchmark and YapScore metric to measure this over-generation. The evaluation of 76 models shows significant variation in verbosity, highlighting a common failure mode in current assistant LLMs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image analysis], [Hypergraph Neural Network, Learning Using Privileged Information, Knowledge Distillation, Severed Graph Strategy, dual-stream distillation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuren Gabriel Yu, Sikang Ren, Yongji Tian</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Beijing Tiantan Hospital</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00626" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00626</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information framework for preoperative ependymoma prognosis. 2. Introduces a Severed Graph Strategy with a shared encoder to process both a Teacher graph (with post-surgery text) and a Student graph (with pre-op MRI only). 3. Employs dual-stream distillation to enable the Student model to hallucinate semantic community structures from visual features alone, transferring expert knowledge without requiring text at inference.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b9ab4eaefd7ae386c2d1758797c14d52ec57c898fa468bb73264675d58297cb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b9ab4eaefd7ae386c2d1758797c14d52ec57c898fa468bb73264675d58297cb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of preoperative ependymoma prognosis where post-operative text reports are unavailable at inference. It proposes HyperPriv-EPN, a hypergraph learning framework that uses a severed graph strategy and dual-stream distillation to transfer knowledge from privileged text data to a model that only uses MRI. The method achieves state-of-the-art diagnostic accuracy and survival stratification on a multi-center cohort, enabling the use of historical post-operative data for new patient diagnosis.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [interpretable machine learning], [Bi-objective Optimization, Temporal Integrated Gradients, Optimal Path Oracle, Directed Acyclic Graph, Structured Regularization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kasra Fouladi, Hamta Rahmani</p>
</li>
<li class="">
<p><strong>institution:</strong> Not explicitly stated; inferred from email domains as independent researchers.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00655" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00655</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the IGBO framework that trains interpretable models by formalizing the task as a bi-objective optimization problem, jointly optimizing for accuracy and adherence to domain knowledge constraints. 2. Introduces an Optimal Path Oracle to generate data-manifold-aware integration paths, addressing the Out-of-Distribution problem in Temporal Integrated Gradients computation. 3. Provides theoretical analysis proving convergence properties and robustness to mini-batch noise, and demonstrates empirical effectiveness on time-series data with minimal accuracy loss.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/797ceec0d9225c3c9714a73c2ff308494df8996ce6022916738679549e999bd1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/797ceec0d9225c3c9714a73c2ff308494df8996ce6022916738679549e999bd1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains models to be both accurate and interpretable by jointly optimizing a task loss and an interpretability loss derived from domain knowledge encoded as a DAG. It addresses a key challenge in gradient-based attribution (the OOD problem) by learning an Optimal Path Oracle. Empirical results show IGBO effectively enforces interpretability constraints with minimal impact on accuracy, outperforming standard regularization methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [talking head generation], [diffusion forcing, direct preference optimization, real-time interaction, low latency, multimodal inputs]</p>
</li>
<li class="">
<p><strong>authors:</strong> Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang</p>
</li>
<li class="">
<p><strong>institution:</strong> KAIST, NTU Singapore, DeepAuto.ai</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00664" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00664</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://taekyungki.github.io/AvatarForcing" target="_blank" rel="noopener noreferrer" class="">https://taekyungki.github.io/AvatarForcing</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Avatar Forcing, a framework using diffusion forcing for real-time interactive head avatar generation that processes multimodal user inputs with low latency. 2. Introduces a label-free direct preference optimization method using synthetic losing samples to learn expressive interactions. 3. Demonstrates real-time performance (~500ms latency, 6.8x speedup) and generates avatars preferred over 80% against the baseline for expressiveness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the lack of truly interactive and emotionally engaging talking head avatars by proposing Avatar Forcing, a framework that uses diffusion forcing for real-time, low-latency generation and a direct preference optimization method for label-free learning of expressive reactions. The method achieves a significant speedup and produces avatar motions that are strongly preferred by users in evaluations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Three factor delay learning rules for spiking neural networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [spiking neural networks, delay learning, three-factor learning, online learning, neuromorphic processors]</p>
</li>
<li class="">
<p><strong>authors:</strong> Luke Vassallo, Nima Taherinejad</p>
</li>
<li class="">
<p><strong>institution:</strong> Heidelberg University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00668" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00668</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced learnable synaptic and axonal delays into LIF-based SNNs and proposed novel three-factor learning rules for online, simultaneous learning of both weights and delays. 2. Employed a smooth Gaussian surrogate gradient exclusively for eligibility trace calculation to enable gradient-equivalent delay parameter updates. 3. Demonstrated significant improvements in model efficiency, achieving up to 6.6x model size reduction and 67% lower inference latency with minimal accuracy loss, enabling on-device learning for resource-constrained environments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58cabcbc7350cb70af9599a2c224309ae64c370ecf64ef754054d42693a8f504_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58cabcbc7350cb70af9599a2c224309ae64c370ecf64ef754054d42693a8f504_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limited temporal learning capability of Spiking Neural Networks (SNNs) by introducing learnable synaptic and axonal delays and proposing online three-factor learning rules to train them. The method uses a Gaussian surrogate gradient for eligibility traces and achieves competitive accuracy on temporal tasks like speech recognition while drastically reducing model size and latency. The findings facilitate efficient, on-device learning for power and area-constrained neuromorphic hardware.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Sparse FEONet: A Low-Cost, Memory-Efficient Operator Network via Finite-Element Local Sparsity for Parametric PDEs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [operator learning, finite element methods, sparse networks, computational efficiency, stability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Seungchan Ko, Jiyeon Kim, Dongwook Shin</p>
</li>
<li class="">
<p><strong>institution:</strong> Inha University, Ajou University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00672" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00672</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel sparse network architecture for FEONet, leveraging finite-element local sparsity to reduce computational cost and memory usage. 2. Provides theoretical analysis demonstrating the sparse architecture&#x27;s approximation capability and stability for reliable training. 3. Validates the method through extensive numerical experiments, showing substantial efficiency gains while maintaining accuracy comparable to the original FEONet.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ca7dbad16ad9ca12a77087fa33228fbe27facf787daefb7dcf7928c77caf2bd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ca7dbad16ad9ca12a77087fa33228fbe27facf787daefb7dcf7928c77caf2bd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Sparse FEONet, a memory-efficient variant of the Finite Element Operator Network, to address the scalability issues of the original model. By incorporating a sparse architecture inspired by finite element structures, it significantly reduces computational cost and memory footprint while preserving accuracy. Theoretical and experimental results confirm its effectiveness and stability for solving parametric PDEs.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [post-training (sft/rlhf)], [Generative Reward Models, Bradley-Terry Model, Group Relative Policy Optimization, Reinforcement Learning from Human Feedback, Pointwise Scoring]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haonan Song, Qingchen Xie, Huan Zhu, Feng Xiao, Luxi Xing, Fuzhen Li, Liu Kang, Feng Jiang, Zhiyong Zheng, Fan Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> HUJING Digital Media &amp; Entertainment Group (XingYun Lab), Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00677" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00677</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes IRPO, a novel RL framework that integrates the Bradley-Terry model into GRPO to scale pairwise reward models for RL training. 2. Introduces a pointwise scoring mechanism that enables efficient evaluation of many candidate responses during RL, overcoming the O(n^2) computational bottleneck. 3. Demonstrates state-of-the-art performance among pointwise GRMs and shows significant advantages over pairwise GRMs in post-training evaluations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5cc90e0083ca0244ea3fbdd445ab9a0b8ff4478f444643d38c88fecae22744f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5cc90e0083ca0244ea3fbdd445ab9a0b8ff4478f444643d38c88fecae22744f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the computational bottleneck of pairwise Generative Reward Models (GRMs) in reinforcement learning by proposing Intergroup Relative Preference Optimization (IRPO). IRPO incorporates the Bradley-Terry model into Group Relative Policy Optimization to generate pointwise scores, enabling efficient evaluation of many candidates. The method achieves state-of-the-art performance among pointwise GRMs and outperforms pairwise GRMs in post-training evaluations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [quantization, spike-driven language models (SLMs), memory footprint, tiered search, embedded systems]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rachmad Vidya Wicaksana Putra, Pasindu Wickramasinghe, Muhammad Shafique</p>
</li>
<li class="">
<p><strong>institution:</strong> New York University (NYU) Abu Dhabi</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00679" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00679</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes QSLM, an automated quantization framework for compressing pre-trained Spike-driven Language Models (SLMs) to meet performance and memory constraints. 2. Introduces a tiered quantization strategy (global-, block-, and module-level) guided by network hierarchy and layer sensitivity analysis. 3. Leverages a multi-objective performance-and-memory trade-off function to select the final quantization setting, achieving significant memory and power reduction while maintaining high task performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/911838f93e33219fcf586121369dd081b9908c86a1169c367cfdd1bb7e50139b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/911838f93e33219fcf586121369dd081b9908c86a1169c367cfdd1bb7e50139b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes QSLM, an automated framework for quantizing Spike-driven Language Models (SLMs) to reduce their memory footprint for embedded deployment. It uses a tiered search strategy based on network hierarchy and layer sensitivity, along with a multi-objective trade-off function, to find optimal quantization settings. Experimental results show QSLM can reduce memory by up to 86.5% and power by up to 20% while maintaining performance close to the original model.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Cost Optimization in Production Line Using Genetic Algorithm</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [combinatorial optimization], [genetic algorithm, task scheduling, production line, chromosome encoding, JGAP]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alireza Rezaee</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Tehran</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00689" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00689</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes and compares two chromosome encoding strategies (station-based and task-based) for a GA applied to a production line scheduling problem. 2. Adapts standard GA operators (crossover, mutation, etc.) to preserve solution feasibility under precedence and capacity constraints. 3. Empirically demonstrates that the task-based encoding yields smoother convergence and more reliable cost minimization, especially for problems with a large number of valid schedules.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d46632061e4499187a195e89a09ba37b6eff28c44f9881da5b237c2b781953b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d46632061e4499187a195e89a09ba37b6eff28c44f9881da5b237c2b781953b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper applies a genetic algorithm to optimize task scheduling in a production line to minimize cost. It investigates two different ways to represent the schedule (encoding) within the algorithm and finds that a task-based encoding performs better, converging more smoothly to lower-cost solutions. The study shows GAs are advantageous for this type of complex, constrained scheduling problem.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [rag (retrieval-augmented generation)], [ticket troubleshooting, retrieval-augmented generation, instruction-tuning, domain-specific ranking, large language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mohamed Trabelsi, Huseyin Uzunalioglu</p>
</li>
<li class="">
<p><strong>institution:</strong> Nokia Bell Labs</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00691" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00691</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes TeleDoCTR, an end-to-end system for telecom ticket troubleshooting integrating classification, retrieval, and generation tasks. 2. Introduces a domain-specific and contextual approach combining ranking and generative models tailored for the telecom domain. 3. Demonstrates superior performance over state-of-the-art methods on a real-world telecom dataset, enhancing troubleshooting accuracy and efficiency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/151b34be88d61fea64f35727dce1917583308996e63a1822af1e6ad8863fe6a8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/151b34be88d61fea64f35727dce1917583308996e63a1822af1e6ad8863fe6a8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes TeleDoCTR, a system that automates telecom ticket troubleshooting by integrating domain-specific models for ticket classification, retrieval of similar historical tickets, and generation of fault analysis reports. It is evaluated on a real-world telecom dataset and shows improved performance over existing methods, making the troubleshooting process more accurate and efficient.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] ARISE: Adaptive Reinforcement Integrated with Swarm Exploration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [swarm intelligence, policy gradient, adaptive exploration, non-stationary rewards, particle swarm]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rajiv Chaitanya M, D R Ramesh Babu</p>
</li>
<li class="">
<p><strong>institution:</strong> Dayananda Sagar College of Engineering</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00693" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00693</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces ARISE, a lightweight framework that augments standard policy-gradient RL methods with a swarm-based exploration layer., 2. Proposes an adaptive mechanism that modulates exploration intensity based on reward-variance cues., 3. Demonstrates significant performance improvements and robustness, particularly in challenging and non-stationary environments, without altering core algorithmic structures.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da08cf28812cd5b0330c83f593897cfcd5e8e953ff273742e122d3262910e186_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da08cf28812cd5b0330c83f593897cfcd5e8e953ff273742e122d3262910e186_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces ARISE, a framework that enhances reinforcement learning by integrating a swarm-based exploration layer with standard policy-gradient methods to improve exploration. It adaptively blends policy actions with particle-driven proposals and modulates exploration using reward variance. The method shows substantial performance gains on complex tasks and improved robustness in non-stationary environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [time series forecasting], [B-spline tokenization, adaptive segmentation, Rotary Positional Embedding (RoPE), long-term forecasting, transformer efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Maximilian Reinwardt, Michael Eichelbeck, Matthias Althoff</p>
</li>
<li class="">
<p><strong>institution:</strong> Technical University of Munich</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00698" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00698</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the B-Spline Adaptive Tokenizer (BSAT), a parameter-free method for adaptively segmenting time series by fitting B-splines and placing tokens in high-curvature regions. 2. Proposes a hybrid positional encoding strategy combining additive learnable encoding with a novel L-RoPE (layer-wise learnable base Rotary Positional Embedding). 3. Demonstrates that the model achieves competitive performance at high compression rates, making it suitable for memory-constrained use cases.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6206fb30f6a9eddbc2ada96d43c221bc41515951535d8d579b2df705186641a9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6206fb30f6a9eddbc2ada96d43c221bc41515951535d8d579b2df705186641a9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the inefficiency of transformers for long-term time series forecasting by introducing BSAT, an adaptive tokenizer that uses B-splines to create variable-length tokens aligned with data semantics, and a novel hybrid positional encoding. The method achieves strong performance with high compression, making it effective under memory constraints.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Bayesian Inverse Games with High-Dimensional Multi-Modal Observations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [inverse reinforcement learning], [Bayesian inference, variational autoencoder, Nash equilibrium, inverse games, multimodal observations]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yash Jain, Xinjie Liu, Lasse Peters, David Fridovich-Keil, Ufuk Topcu</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Texas at Austin, Delft University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00696" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00696</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Bayesian inference framework for inverse games to quantify uncertainty in estimating agent objectives, addressing the overconfidence of point-estimate methods. 2. Introduces a structured variational autoencoder with an embedded differentiable Nash game solver, enabling posterior sampling without requiring labeled objective data. 3. Demonstrates that multimodal inference reduces uncertainty when trajectory data is insufficient, leading to safer downstream planning decisions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9263c0ad6078bf92eb8f7a7ca21579f0a55a6e710fa80a06f40a66a735ce24a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9263c0ad6078bf92eb8f7a7ca21579f0a55a6e710fa80a06f40a66a735ce24a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of inferring agents&#x27; hidden objectives in multi-agent interactions, where existing maximum likelihood methods produce overconfident point estimates. The authors propose a Bayesian inverse game framework using a structured variational autoencoder with a differentiable Nash solver to generate posterior samples from multimodal observations. Experiments show the method improves inference quality, quantifies uncertainty, and enables safer autonomous decision-making compared to prior approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [model compression (quantization/pruning)], [reinforcement learning, precision autotuning, contextual bandit, mixed-precision, linear solvers]</p>
</li>
<li class="">
<p><strong>authors:</strong> Erin Carson, Xinye Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Charles University, Sorbonne Université, CNRS, LIP6</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00728" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00728</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel reinforcement learning framework formulated as a contextual bandit problem for adaptive precision tuning of numerical algorithms. 2. Applies the framework to iterative refinement for linear solvers, using a Q-table and epsilon-greedy strategy to dynamically select precision configurations based on system features. 3. Demonstrates the framework&#x27;s effectiveness and generalization, reducing computational cost while maintaining accuracy comparable to double-precision baselines on unseen data.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7720c36c9ae5fbf03bb75ea2bb7142862906819bb41cf70b683252fc884718e2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7720c36c9ae5fbf03bb75ea2bb7142862906819bb41cf70b683252fc884718e2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a reinforcement learning framework for adaptive precision tuning, formulated as a contextual bandit problem, to optimize the trade-off between computational cost and accuracy in linear solvers. The method dynamically selects precision configurations based on system features using a Q-learning approach. Empirical results show it reduces cost while maintaining accuracy, and it generalizes well to unseen data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [actor-critic, overestimation, aleatoric uncertainty, distributional critic, dropout]</p>
</li>
<li class="">
<p><strong>authors:</strong> Uğurcan Özalp</p>
</li>
<li class="">
<p><strong>institution:</strong> Turkish Aerospace</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00737" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00737</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Stochastic Actor-Critic (STAC), a novel algorithm that uses temporal aleatoric uncertainty (from stochastic transitions, rewards, and policy) to scale pessimistic bias in TD updates, instead of relying on epistemic uncertainty. 2. Demonstrates that a single distributional critic network modeling return uncertainty is sufficient to mitigate overestimation and induce risk-averse behavior. 3. Shows that applying dropout for regularization in both actor and critic networks further improves training stability and performance, enhancing computational efficiency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602bf8125f08dc0296e56b4a9e156cd6089d329c07cd83909cc1b3c96effc1bf_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602bf8125f08dc0296e56b4a9e156cd6089d329c07cd83909cc1b3c96effc1bf_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of overestimation bias in off-policy actor-critic reinforcement learning methods. It proposes the Stochastic Actor-Critic (STAC) algorithm, which mitigates overestimation by using a single distributional critic to model temporal aleatoric uncertainty for scaling pessimistic updates, and employs dropout for regularization. The results show that this approach effectively reduces overestimation, leads to risk-averse behavior, and improves computational efficiency and training stability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [distributional creative reasoning, diversity collapse, gradient flow, variational objective, reasoning paths]</p>
</li>
<li class="">
<p><strong>authors:</strong> Max Ruiz Luyten, Mihaela van der Schaar</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Cambridge</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00747" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00747</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Distributional Creative Reasoning (DCR), a unified variational framework that models training as gradient flow through probability measures on solution traces, encompassing methods like STaR, GRPO, and DPO as special cases. 2. Proves the diversity decay theorem, which explains how correctness-focused objectives lead to distinct modes of diversity collapse in different algorithms. 3. Provides principled designs and actionable recipes to ensure convergence to a stable and diverse policy, preventing creative collapse while maintaining correctness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9fbb639faaebe1f394144b6e1a9a7bbb2ea6b005c8194a324964adf46349c164_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9fbb639faaebe1f394144b6e1a9a7bbb2ea6b005c8194a324964adf46349c164_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that bootstrapped reasoning loops in LLMs, which optimize for correctness, lead to a collapse in the diversity of reasoning paths, harming creative problem-solving. To address this, it proposes Distributional Creative Reasoning (DCR), a unified theoretical framework that explains this collapse and offers designs to prevent it. The main conclusion is that DCR provides the first principled recipe for training LLMs that are both correct and creative.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [sports analytics], [covariate-dependent Hidden Markov Model, defensive credit attribution, role-conditioned ghosting]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sean Groom, Shuo Wang, Francisco Belo, Axl Rice, Liam Anderson</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Birmingham, Nottingham Forest Football Club</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00748" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00748</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a covariate-dependent Hidden Markov Model (CDHMM) to infer time-resolved defensive role assignments (man-marking and zonal) from player tracking data during corner kicks. 2. Proposes a novel framework for defensive credit attribution based on the inferred role assignments. 3. Develops a role-conditioned ghosting method for context-aware counterfactual analysis of off-ball defensive performance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/829cb6bc9dbc2a288f24bae908e436437a33ee4cddc07525ba2d5196a191500c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/829cb6bc9dbc2a288f24bae908e436437a33ee4cddc07525ba2d5196a191500c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of evaluating off-ball defensive performance in football, which is not captured by traditional metrics. The authors propose a covariate-dependent Hidden Markov Model to automatically infer defensive roles from tracking data and use this to create a new framework for credit attribution and counterfactual analysis. The method provides an interpretable, context-aware evaluation of defensive contributions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Memory Bank Compression for Continual Adaptation of Large Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [memory &amp; caching], [memory bank compression, codebook optimization, online resetting mechanism, Key-Value Low-Rank Adaptation (KV-LoRA)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Thomas Katraouras, Dimitrios Rafailidis</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Thessaly</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00756" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00756</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Thomkat/MBC" target="_blank" rel="noopener noreferrer" class="">https://github.com/Thomkat/MBC</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed MBC, a model that compresses the memory bank for continual learning via a codebook optimization strategy. 2. Introduced an online resetting mechanism to prevent codebook collapse and ensure stable learning. 3. Employed Key-Value Low-Rank Adaptation (KV-LoRA) in the LLM&#x27;s attention layers to efficiently utilize the compressed memory representations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4b16d7a0bdd1f6fa0e71da5c21a92629d07342bdc56905e10612ee07549b8dd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4b16d7a0bdd1f6fa0e71da5c21a92629d07342bdc56905e10612ee07549b8dd_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of memory bank growth in continual learning for LLMs by proposing MBC, which compresses the memory bank using codebook optimization and an online resetting mechanism. The method integrates KV-LoRA for efficient adaptation and achieves a 99.7% reduction in memory bank size while maintaining high accuracy on question-answering tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Categorical Reparameterization with Denoising Diffusion models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [diffusion models], [categorical reparameterization, gradient estimation, denoising diffusion, continuous relaxation, Gumbel-Softmax]</p>
</li>
<li class="">
<p><strong>authors:</strong> Samson Gourevitch, Alain Durmus, Eric Moulines, Jimmy Olsson, Yazid Janati</p>
</li>
<li class="">
<p><strong>institution:</strong> CMAP, Ecole polytechnique; Mohamed Bin Zayed University of AI; KTH Royal Institute of Technology; Institute of Foundation Models, MBZUAI</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00781" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00781</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a novel diffusion-based soft reparameterization for categorical distributions, extending the family of continuous relaxations. 2. Shows that the denoiser for categorical distributions under a Gaussian noising process has a closed-form, efficient solution, enabling a training-free diffusion sampler. 3. Demonstrates that the proposed method yields competitive or improved optimization performance on various benchmarks compared to existing gradient estimators.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07551087da54975e4db6aaddfb0b58a659ed27c7a4b438249f27613f2153d75f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07551087da54975e4db6aaddfb0b58a659ed27c7a4b438249f27613f2153d75f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of gradient-based optimization with categorical variables by proposing a new diffusion-based reparameterization method. It leverages the closed-form denoiser for categorical distributions to create a differentiable, training-free sampler. Experimental results show this approach provides effective gradient estimates, outperforming or matching existing methods on several benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [nlp], [reasoning verification], [spectral graph analysis, attention patterns, Fiedler value, high-frequency energy ratio, sliding window attention]</p>
</li>
<li class="">
<p><strong>authors:</strong> Valentin Noël</p>
</li>
<li class="">
<p><strong>institution:</strong> Devoteam</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00791" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00791</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced a training-free method for detecting valid mathematical reasoning in LLMs by performing spectral analysis on attention matrices treated as dynamic graphs. 2. Identified four interpretable spectral diagnostics (Fiedler value, HFER, smoothness, entropy) that show significant statistical differences between valid and invalid proofs across multiple model families. 3. Discovered that the method captures logical coherence rather than formal verifier acceptance and revealed an architectural dependency where different attention mechanisms (e.g., Sliding Window Attention) shift the primary discriminative spectral feature.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b1f2999ddcf2e05565198a4f51efee7b71422414b78038f132537978df2e4e9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b1f2999ddcf2e05565198a4f51efee7b71422414b78038f132537978df2e4e9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes a training-free method to detect valid mathematical reasoning in large language models by analyzing the spectral properties of attention patterns. The method identifies key spectral signatures that effectively distinguish between valid and invalid proofs with high accuracy. The findings show the method captures logical coherence and its effectiveness depends on the model&#x27;s attention architecture.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [hypernetwork, conditional VAE, differential privacy, MMD alignment, client heterogeneity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sunny Gupta, Amit Sethi</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology Bombay</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00785" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00785</a></p>
</li>
<li class="">
<p><strong>code:</strong> github.com/sunnyinAI/FedHypeVAE</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A bi-level framework using a shared hypernetwork to generate personalized, client-aware decoders and class-conditional priors for a conditional VAE, decoupling local data from shared parameters. 2. Incorporation of differential privacy during hypernetwork optimization with noise-perturbed, clipped gradients to provide formal privacy guarantees against gradient leakage. 3. Introduction of a local MMD alignment loss and Lipschitz regularization to enhance stability and distributional coherence under non-IID data conditions, along with a neutral meta-code for domain-agnostic synthesis.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726225d41733e7b16755efae5f3b9ec28771c58479913c7f5581e9843368fd0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726225d41733e7b16755efae5f3b9ec28771c58479913c7f5581e9843368fd0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper proposes FedHypeVAE, a federated learning framework that uses a differentially private hypernetwork to generate personalized conditional VAEs for synthesizing embedding-level data across decentralized clients. It addresses challenges of non-IID data and privacy by decoupling local data from shared parameters and ensuring formal privacy guarantees. The method establishes a foundation for privacy-preserving data synthesis in federated settings by unifying personalization, privacy, and distribution alignment at the generator level.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image segmentation], [U-Net, layer normalization, instance-batch normalization, left ventricle segmentation, cardiac MRI]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wenhui Chu, Nikolaos V. Tsekos</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Houston</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00794" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00794</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed LNU-Net, a novel segmentation architecture derived from U-Net that applies layer normalization in each convolutional block. 2. Proposed IBU-Net, another novel architecture that incorporates instance and batch normalization together in the first convolutional block. 3. Demonstrated that the proposed methods outperform state-of-the-art approaches on a dataset of 805 MRI images using metrics like dice coefficient and average perpendicular distance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45a19599df20601908ea938c8bea28356e54685c5ce08dbe58a85a26dda95834_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45a19599df20601908ea938c8bea28356e54685c5ce08dbe58a85a26dda95834_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes two new deep learning models, LNU-Net and IBU-Net, for automated segmentation of the left ventricle in cardiac MRI images. The models are based on the U-Net architecture but incorporate different normalization strategies—layer normalization and a combined instance-batch normalization—to improve segmentation performance. Experimental results show that both proposed approaches outperform existing state-of-the-art methods on key evaluation metrics.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Active learning for data-driven reduced models of parametric differential systems with Bayesian operator inference</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [scientific machine learning], [Bayesian operator inference, active learning, reduced-order models, parametric systems, adaptive sampling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shane A. McQuarrie, Mengwu Guo, Anirban Chaudhuri</p>
</li>
<li class="">
<p><strong>institution:</strong> Brigham Young University, Lund University, The University of Texas at Austin</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00038" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00038</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a probabilistic version of parametric operator inference by casting the learning problem as Bayesian linear regression. 2. Designed a sequential adaptive sampling scheme that uses prediction uncertainties from the probabilistic ROM to select new training parameters. 3. Demonstrated through numerical experiments that the active learning framework yields more stable and accurate ROMs than random sampling under the same computational budget.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c29bb4bdd9fe6e4385983b4dab88a4de7d95bf126d6c1b64568426082175c1b6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c29bb4bdd9fe6e4385983b4dab88a4de7d95bf126d6c1b64568426082175c1b6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an active learning framework to improve data-driven reduced-order models (ROMs) for parametric dynamical systems. The method uses Bayesian operator inference to create probabilistic ROMs and leverages their prediction uncertainties to adaptively select the most informative training parameters. The results show this approach consistently produces more stable and accurate ROMs compared to training with random parameter samples.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Automated electrostatic characterization of quantum dot devices in single- and bilayer heterostructures</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [quantum computing], [quantum dot, charge stability diagram, automated characterization, machine learning, image processing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Merritt P. R. Losert, Dario Denora, Barnaby van Straaten, Michael Chan, Stefan D. Oosterhout, Lucas Stehouwer, Giordano Scappucci, Menno Veldhorst, Justyna P. Zwolak</p>
</li>
<li class="">
<p><strong>institution:</strong> National Institute of Standards and Technology (NIST), Delft University of Technology (QuTech)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00067" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00067</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed an automated protocol combining ML, image processing, and object detection to extract capacitive properties from charge stability diagrams without manual labeling. 2. Demonstrated the method&#x27;s effectiveness on complex bilayer germanium heterostructures, which feature interlayer tunneling and distinct loading lines. 3. Enabled statistical estimation of physically relevant device parameters, such as lever arms and capacitive couplings, facilitating rapid device characterization.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c536889ababa23206b42f144321036dbbfd329505ea45c505aa53c2a04bf7815_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c536889ababa23206b42f144321036dbbfd329505ea45c505aa53c2a04bf7815_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents an automated method for characterizing quantum dot devices by analyzing charge stability diagrams. The protocol integrates machine learning and image processing to identify charge transitions and extract capacitive properties from experimental data. It enables rapid, scalable extraction of key device parameters, which is critical for advancing large-scale quantum dot arrays.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Group Cross-Correlations with Faintly Constrained Filters</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [group theory, harmonic analysis, topological dynamics], [group cross-correlations, G-equivariant filters, non-compact stabilizers, non-transitive actions, unimodularity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Benedikt Fluhr</p>
</li>
<li class="">
<p><strong>institution:</strong> Cannot be inferred from the provided content.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00045" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00045</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a new notion of group cross-correlations with less constrained filters, 2. Resolves incompatibilities for group actions with non-compact stabilizers, 3. Generalizes results to non-transitive group actions and weakens the unimodularity assumption.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/140a9a6a7cb64898feb16f84d9ad4ac9bc745bc82682b538beb6dd66d1f871c8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/140a9a6a7cb64898feb16f84d9ad4ac9bc745bc82682b538beb6dd66d1f871c8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a generalized framework for group cross-correlations by relaxing the constraints on the filters used. This new approach resolves previous theoretical limitations for group actions with non-compact stabilizers and extends applicability to non-transitive actions without requiring unimodularity. The work provides a more flexible mathematical foundation for equivariant transformations on vector bundles.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Deep Learning Approach for the Diagnosis of Pediatric Pneumonia Using Chest X-ray Imaging</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical image classification], [Convolutional Neural Network, Transfer Learning, Chest X-ray, Pediatric Pneumonia, RegNet]</p>
</li>
<li class="">
<p><strong>authors:</strong> Fatemeh Hosseinabadi, Mohammad Mojtaba Rohani</p>
</li>
<li class="">
<p><strong>institution:</strong> Zahedan University of Medical Sciences, Guilan University of Medical Sciences</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00041" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00041</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Evaluated and compared the performance of state-of-the-art CNN architectures (ResNetRS, RegNet, EfficientNetV2) for automated pediatric pneumonia diagnosis. 2. Applied transfer learning with ImageNet-pretrained weights to a curated dataset of pediatric chest X-rays to address data and expertise limitations. 3. Demonstrated that the RegNet model achieved the highest accuracy and sensitivity for this specific binary classification task.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbf67414c0b3072bcb906df1f0ca6e5942968a2374f2ac09a701ed20efb78f09_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbf67414c0b3072bcb906df1f0ca6e5942968a2374f2ac09a701ed20efb78f09_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes using transfer learning with deep convolutional neural networks to automate the diagnosis of pediatric pneumonia from chest X-ray images. The authors fine-tuned and compared three CNN models (ResNetRS, RegNet, EfficientNetV2) on a curated dataset, finding that RegNet performed best with 92.4% accuracy. The study concludes that such deep learning approaches can provide reliable diagnostic support, especially in settings with limited radiological expertise.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [neural fields, signal processing], [Neural Radiance Fields (NeRF), EEG, brain-computer interfaces, signal reconstruction, continuous representation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shahar Ain Kedem, Itamar Zimerman, Eliya Nachmani</p>
</li>
<li class="">
<p><strong>institution:</strong> Ben Gurion University of the Negev, Tel Aviv University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00012" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00012</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Shaharak88/neural-brain-fields" target="_blank" rel="noopener noreferrer" class="">https://github.com/Shaharak88/neural-brain-fields</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel NeRF-inspired method to learn a continuous representation of brain activity from discrete EEG electrode data. 2. Enables rendering of EEG signals at unseen time steps and spatial electrode positions, including simulating non-existent electrodes. 3. Demonstrates that the reconstructed signals can be used to improve the performance of standard EEG processing networks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79dda8675c25fe2c8906136ddc20e55e51c835cb08bed2a4e02388b7dadc16da_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79dda8675c25fe2c8906136ddc20e55e51c835cb08bed2a4e02388b7dadc16da_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces Neural Brain Fields, a method inspired by Neural Radiance Fields (NeRF) to model EEG data. It trains a neural network on a single EEG sample to produce a fixed-size weight vector that encodes the continuous neural activity, allowing for signal reconstruction at any time or scalp location. The approach effectively generates data for non-existent electrodes, which can enhance downstream EEG analysis tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Modeling Day-Long ECG Signals to Predict Heart Failure Risk with Explainable AI</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [health informatics], [deep learning, Holter ECG, explainable AI, time series analysis, risk prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Eran Zvuloni, Ronit Almog, Michael Glikson, Shany Brimer Biton, Ilan Green, Izhar Laufer, Offer Amir, Joachim A. Behar</p>
</li>
<li class="">
<p><strong>institution:</strong> Technion - Israel Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00014" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00014</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed DeepHHF, a deep learning model that uses full 24-hour single-lead ECG recordings for heart failure risk prediction, outperforming models using short segments and clinical scores. 2. Created and utilized the large-scale Technion-Leumit Holter ECG (TLHE) dataset, comprising 69,663 recordings from 47,729 patients collected over 20 years. 3. Provided explainability analysis showing the model focuses on arrhythmias and heart abnormalities, with key attention patterns during daytime hours (8 AM to 3 PM), linking model decisions to clinically relevant features.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfa3d768712a10f4b2d45732f0f8e0d64f70a07add2581bf81c12c996da11b77_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfa3d768712a10f4b2d45732f0f8e0d64f70a07add2581bf81c12c996da11b77_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes DeepHHF, a deep learning model that analyzes 24-hour single-lead ECG data to predict the 5-year risk of heart failure. The model achieved an AUC of 0.80, outperforming baseline methods, and identified high-risk individuals with a two-fold increased chance of hospitalization or death. The study demonstrates the feasibility of using long-term, continuous ECG data and explainable AI for non-invasive and accessible heart failure risk prediction.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Cuffless, calibration-free hemodynamic monitoring with physics-informed machine learning models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [biomedical sensing and instrumentation], [electrical bioimpedance, physics-informed neural network, cuffless blood pressure, hemodynamic monitoring, wearable device]</p>
</li>
<li class="">
<p><strong>authors:</strong> Henry Crandall, Tyler Schuessler, Filip Bělík, Albert Fabregas, Barry M. Stults, Alexandra Boyadzhiev, Huanan Zhang, Jim S. Wu, Aylin R. Rodan, Stephen P. Juraschek, Ramakrishna Mukkamala, Alfred K. Cheung, Stavros G. Drakos, Christel Hohenegger, Braxton Osting, Benjamin Sanchez</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Utah, University of Illinois Chicago</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00081" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00081</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a smartwatch device with real-time electrical bioimpedance (BioZ) sensing for cuffless hemodynamic monitoring. 2. Established a multiscale biophysical modeling framework to elucidate the relationship between BioZ and blood pressure, identifying key influencing parameters. 3. Created a signal-tagged physics-informed neural network that incorporates fluid dynamics principles for calibration-free estimation of blood pressure and blood velocity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ba7d002ced9d6e8a70d39b806f9e6640ce2f6db3f9aa85c2884e1ce15b38a91_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ba7d002ced9d6e8a70d39b806f9e6640ce2f6db3f9aa85c2884e1ce15b38a91_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper develops a smartwatch-based system using electrical bioimpedance sensing and a physics-informed machine learning model to enable cuffless, calibration-free monitoring of blood pressure and blood velocity. The method addresses the theoretical limitations of existing wearable technologies by grounding the model in biophysical principles. The approach was successfully validated across diverse populations and settings, demonstrating its clinical feasibility.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Detecting Unobserved Confounders: A Kernelized Regression Approach</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [causal inference], [unobserved confounders, kernel regression, reproducing kernel hilbert space, single-environment, causal effect estimation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yikai Chen, Yunxin Mao, Chunyuan Zheng, Hao Zou, Shanzhi Gu, Shixuan Liu, Yang Shi, Wenjing Yang, Kun Kuang, Haotian Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Defense Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00200" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00200</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a novel method (KRCD) for detecting unobserved confounders in nonlinear observational data under single-environment conditions, bridging a key gap in existing methods. 2. Provided theoretical guarantees, proving that in infinite samples, regression coefficients coincide if and only if no unobserved confounders exist, and that finite-sample differences converge to a zero-mean Gaussian distribution. 3. Demonstrated superior performance and computational efficiency over existing baselines through extensive experiments on synthetic benchmarks and the Twins dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d34f526abd65dea1ab2c42ba3db6b7f0a13399cb09990eded28483079693cf4f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d34f526abd65dea1ab2c42ba3db6b7f0a13399cb09990eded28483079693cf4f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Kernel Regression Confounder Detection (KRCD), a method to detect unobserved confounders in nonlinear, single-environment observational data by comparing standard and higher-order kernel regressions. Theoretically, it proves key properties about the test statistic&#x27;s behavior. Experiments show KRCD outperforms existing baselines in both detection performance and computational efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Neural Minimum Weight Perfect Matching for Quantum Error Codes</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [Quantum Error Correction, Minimum Weight Perfect Matching, Graph Neural Networks, Transformer, Hybrid Decoder]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yotam Peled, David Zenati, Eliya Nachmani</p>
</li>
<li class="">
<p><strong>institution:</strong> Ben-Gurion University of the Negev</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00242" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00242</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a hybrid decoder (NMWPM) that integrates GNNs and Transformers to predict dynamic edge weights for the MWPM algorithm. 2. Formulated a novel proxy loss function to enable end-to-end training through the non-differentiable MWPM algorithm. 3. Demonstrated a significant reduction in Logical Error Rate (LER) compared to standard baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e507723ea614845b9a945c6d086d7f6413ab64a2e5cbfa21b28ceaa1777ffa60_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e507723ea614845b9a945c6d086d7f6413ab64a2e5cbfa21b28ceaa1777ffa60_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a neural-enhanced decoder for quantum error correction called Neural Minimum Weight Perfect Matching (NMWPM). It uses a hybrid architecture of Graph Neural Networks and Transformers to predict dynamic edge weights for the classical MWPM decoder, trained with a novel proxy loss. The method significantly reduces the Logical Error Rate, showing the advantage of combining neural networks with classical algorithms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Combining datasets with different ground truths using Low-Rank Adaptation to generalize image-based CNN models for photometric redshift prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [astronomical image analysis], [Low-Rank Adaptation (LoRA), photometric redshift, spectroscopic redshift, CNN, transfer learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Vikram Seenivasan, Srinath Saikrishnan, Andrew Lizarraga, Jonathan Soriano, Bernie Boscoe, Tuan Do</p>
</li>
<li class="">
<p><strong>institution:</strong> University of California, Los Angeles (UCLA), Southern Oregon University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00146" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00146</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Applied Low-Rank Adaptation (LoRA), a technique from large language models, to fine-tune CNN-based regression models for photometric redshift prediction in astrophysics. 2. Demonstrated that LoRA effectively combines datasets with different ground truths (photometric and spectroscopic redshifts) to improve model performance, outperforming traditional transfer learning with significantly reduced bias and scatter. 3. Showed that LoRA provides a computationally efficient middle ground between full model retraining and no retraining, enabling the leveraging of pre-trained models for data-sparse tasks in astrophysics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7c8dfd9d9ff319ca34f37730c3de7d103e6a39edc2131c3e4ca2711ce7fd7e3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7c8dfd9d9ff319ca34f37730c3de7d103e6a39edc2131c3e4ca2711ce7fd7e3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper applies Low-Rank Adaptation (LoRA) to fine-tune a CNN model for predicting galaxy redshifts, combining a less accurate but broad photometric redshift dataset with a more accurate but limited spectroscopic redshift dataset. The LoRA-based method outperforms traditional transfer learning, achieving lower bias and scatter, and offers a computationally efficient compromise between full retraining and no adaptation. The work demonstrates LoRA&#x27;s utility for improving regression models in astrophysics by efficiently integrating datasets with different quality ground truths.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Solving nonlinear subsonic compressible flow in infinite domain via multi-stage neural networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [computational fluid dynamics], [physics-informed neural networks, multi-stage training, unbounded domain, coordinate transformation, compressible potential equation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xuehui Qian, Hongkai Tao, Yongji Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Washington University in St. Louis, University of Notre Dame, Central South University, Stanford University, New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00342" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00342</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel PINN framework to solve the full nonlinear compressible potential equation in an infinite domain, overcoming a key limitation of traditional methods. 2. Introduces a coordinate transformation and embeds physical asymptotic constraints to address the unbounded-domain and convergence challenges inherent in standard PINNs. 3. Employs a Multi-Stage PINN (MS-PINN) approach to iteratively minimize residuals, achieving solution accuracy approaching machine precision.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70afc10edc6a8a0b3606afe357472e00dfb9e25867b67948c402e19271e24608_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70afc10edc6a8a0b3606afe357472e00dfb9e25867b67948c402e19271e24608_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a novel framework using Physics-Informed Neural Networks (PINNs) to solve the full nonlinear subsonic compressible flow equation in an infinite domain. The method addresses domain and convergence challenges via coordinate transformation and a multi-stage training process. The results demonstrate high-fidelity solutions and quantify errors from traditional domain truncation and linearization, especially at higher Mach numbers.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Interpretable Machine Learning for Quantum-Informed Property Predictions in Artificial Sensing Materials</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [interpretable machine learning], [quantum-mechanical properties, CatBoost, explainable AI, binding features, electronic descriptors]</p>
</li>
<li class="">
<p><strong>authors:</strong> Li Chen, Leonardo Medrano Sandonas, Shirong Huang, Alexander Croy, Gianaurelio Cuniberti</p>
</li>
<li class="">
<p><strong>institution:</strong> TUD Dresden University of Technology, Friedrich Schiller University Jena</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00503" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00503</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Development of the MORE-ML computational framework integrating quantum-mechanical data with machine learning for predicting sensing properties. 2. Expansion of the dataset to MORE-QX, providing extensive electronic binding features for body odor volatilome adsorption. 3. Demonstration that CatBoost models with explainable AI methods offer superior performance and mechanistic insights for rational receptor design.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e19c57b3a84042598ddfad14eee8131675c83c4b83807b904281ff6859ca8850_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e19c57b3a84042598ddfad14eee8131675c83c4b83807b904281ff6859ca8850_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of designing sensing materials for complex body odor detection by developing MORE-ML, a framework that uses quantum-mechanical property data of molecular building blocks as inputs to train tree-based ML models (like CatBoost) to predict binding features. The approach, validated on an expanded dataset, shows that CatBoost models perform well, especially on unseen compounds, and explainable AI methods identify key quantum properties influencing predictions, providing a foundation for rational design of artificial sensing materials.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [machine learning interatomic potential], [TensorNet2, force field, drug-like compounds, charged states, DFT-level accuracy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Stephen E. Farr, Stefan Doerr, Antonio Mirarchi, Francesc Sabanes Zariquiey, Gianni De Fabritiis</p>
</li>
<li class="">
<p><strong>institution:</strong> Acellera Labs, Universitat Pompeu Fabra, ICREA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00581" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00581</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/acellera/aceff-paper" target="_blank" rel="noopener noreferrer" class="">https://github.com/acellera/aceff-paper</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces AceFF, a pre-trained MLIP specifically optimized for small molecule drug discovery. 2. Employs a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds to improve generalizability. 3. Achieves a balance of high-throughput inference speed and DFT-level accuracy, explicitly supporting essential medicinal chemistry elements and charged states.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf03a6da5bff3c9952736adc7ba7fd161d48187c1623ade05d2174437fd823b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf03a6da5bff3c9952736adc7ba7fd161d48187c1623ade05d2174437fd823b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces AceFF, a machine learning interatomic potential designed for small molecule drug discovery. It uses a refined TensorNet2 architecture trained on drug-like compounds to achieve DFT-level accuracy with fast inference. The authors demonstrate its state-of-the-art performance on organic molecules through rigorous benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260105] Generative Conditional Missing Imputation Networks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [missing data imputation], [Generative Neural Network, Missing Imputation, Multiple Imputation by Chained Equations, MCAR, MAR]</p>
</li>
<li class="">
<p><strong>authors:</strong> George Sun, Yi-Hui Zhou</p>
</li>
<li class="">
<p><strong>institution:</strong> North Carolina State University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.00517" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.00517</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Generative Conditional Missing Imputation Networks (GCMI) with a theoretical foundation for MCAR and MAR mechanisms., 2. Enhances GCMI&#x27;s robustness and accuracy by integrating a multiple imputation framework using chained equations., 3. Empirically demonstrates the superior performance of the proposed method compared to other leading imputation techniques.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c2fe27f0abc6f988f3b6e7f4d03378bacf1dab0ce55f725ea0b5d5b1991f7df_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c2fe27f0abc6f988f3b6e7f4d03378bacf1dab0ce55f725ea0b5d5b1991f7df_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes Generative Conditional Missing Imputation Networks (GCMI), a generative method for imputing missing data. The method is theoretically grounded for MCAR and MAR scenarios and is further improved by integrating a multiple imputation approach. Experiments show it outperforms other state-of-the-art imputation techniques.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2026-01-05T03:17:11.000Z" itemprop="dateModified">Jan 5, 2026</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/cslg/20251229-20260104"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251229-20260104 (cs.LG)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/category/cslo"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">cs.LO</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2026-01-05" class="table-of-contents__link toc-highlight">2026-01-05</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2026-01</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><div class="calendar-cell calendar-empty"></div><div class="calendar-cell calendar-empty"></div><div class="calendar-cell calendar-empty"></div><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251229-20260104#2026-01-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251229-20260104#2026-01-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251229-20260104#2026-01-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20251229-20260104#2026-01-04">4</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/cslg/20260105-20260111#2026-01-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260105-20260111#2026-01-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260105-20260111#2026-01-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260105-20260111#2026-01-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260105-20260111#2026-01-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260105-20260111#2026-01-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260105-20260111#2026-01-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260112-20260118#2026-01-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260112-20260118#2026-01-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260112-20260118#2026-01-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260112-20260118#2026-01-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260112-20260118#2026-01-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260112-20260118#2026-01-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260112-20260118#2026-01-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260119-20260125#2026-01-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260119-20260125#2026-01-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260119-20260125#2026-01-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260119-20260125#2026-01-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260119-20260125#2026-01-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260119-20260125#2026-01-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260119-20260125#2026-01-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260126-20260201#2026-01-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260126-20260201#2026-01-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260126-20260201#2026-01-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260126-20260201#2026-01-29">29</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260126-20260201#2026-01-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cslg/20260126-20260201#2026-01-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>