<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_RO/20251222-20251228" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251222-20251228 (cs.RO) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/csro/20251222-20251228"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251222-20251228 (cs.RO) | AI头条"><meta data-rh="true" name="description" content="2025-12-22"><meta data-rh="true" property="og:description" content="2025-12-22"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/csro/20251222-20251228"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/csro/20251222-20251228" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/csro/20251222-20251228" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.RO","item":"https://jokebear666.github.io/ai_toutiao/daily/csro"},{"@type":"ListItem","position":3,"name":"20251222-20251228 (cs.RO)","item":"https://jokebear666.github.io/ai_toutiao/daily/csro/20251222-20251228"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.bc6dbd00.css">
<script src="/ai_toutiao/assets/js/runtime~main.a6c4f209.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.4d8b05d9.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/daily">Daily</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/paper">Paper</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Collapse sidebar category &#x27;cs.RO&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cs_RO/20251215-20251221"><span title="20251215-20251221 (cs.RO)" class="linkLabel_WmDU">20251215-20251221 (cs.RO)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/csro/20251222-20251228"><span title="20251222-20251228 (cs.RO)" class="linkLabel_WmDU">20251222-20251228 (cs.RO)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/daily/csro"><span>cs.RO</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251222-20251228 (cs.RO)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251222-20251228 (cs.RO)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-22">2025-12-22<a href="#2025-12-22" class="hash-link" aria-label="Direct link to 2025-12-22" title="Direct link to 2025-12-22" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251222] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [reinforcement learning, model predictive control, MPPI, hierarchical planning, adaptive sampling]</li>
<li class=""><strong>authors:</strong> Toshiaki Hori, Jonathan DeCastro, Deepak Gopinath, Avinash Balachandran, Guy Rosman</li>
<li class=""><strong>institution:</strong> Toyota Research Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17091" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17091</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method that fuses reinforcement learning and model-predictive control (MPC) into an adaptive hierarchical framework. It uses RL actions to guide the MPPI sampler and adaptively aggregates MPPI samples to improve value estimation, leading to more robust and sample-efficient policies. The approach demonstrates improved data efficiency, performance, and convergence speed in domains like race driving and Lunar Lander.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] DiffeoMorph: Learning to Morph 3D Shapes Using Differentiable Agent-Based Simulations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [differentiable simulation, graph neural network, SE(3)-equivariance, attention mechanism, 3D Zernike polynomials, shape-matching loss, implicit differentiation, bilevel optimization]</li>
<li class=""><strong>authors:</strong> Seong Ho Pahng, Guoye Guan, Benjamin Fefferman, Sahand Hormoz</li>
<li class=""><strong>institution:</strong> Harvard University, Harvard Medical School, Dana-Farber Cancer Institute, Broad Institute of MIT and Harvard</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17129" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17129</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces DiffeoMorph, a differentiable framework that uses an attention-based SE(3)-equivariant graph neural network to train agents to collectively morph into target 3D shapes. It employs a novel shape-matching loss based on 3D Zernike polynomials and uses implicit differentiation to handle a bilevel optimization problem for rotation alignment. The method successfully generates complex shapes from simple ellipsoids using minimal spatial cues.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [interactive reinforcement learning, multi-teacher learning, Q-learning, teacher selection, concept drift]</li>
<li class=""><strong>authors:</strong> Maher Mesto, Francisco Cruz</li>
<li class=""><strong>institution:</strong> University of New South Wales, Universidad Central de Chile</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17180" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17180</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a multi-teacher interactive reinforcement learning framework where agents can select advice from teachers with different reward structures. The core finding is that agents exhibit a strong conservative bias, overwhelmingly preferring low-reward but consistent teachers over high-reward ones, which challenges traditional reward-maximization assumptions in RL.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Research on Dead Reckoning Algorithm for Self-Propelled Pipeline Robots in Three-Dimensional Complex Pipelines</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [robotics and navigation], [extended kalman filter, inertial measurement unit, wheel odometer, dead reckoning]</li>
<li class=""><strong>authors:</strong> Yan Gao, Jiliang Wang, Minghan Wang, Xiaohua Chen, Demin Chen, Zhiyong Ren, Tian-Yun Huang</li>
<li class=""><strong>institution:</strong> Peking University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17215" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17215</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52e181750dd60029c711d4a23702ec5e96a39d86b1ce8c7f9c34de75f853c369_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52e181750dd60029c711d4a23702ec5e96a39d86b1ce8c7f9c34de75f853c369_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a dead reckoning method for a self-propelled pipeline robot, using an IMU for initial attitude estimation, refining it with an Extended Kalman Filter, and combining it with wheel odometer data for localization. The method was tested in a rectangular loop pipeline, and the results verified the effectiveness of the proposed algorithm for navigating complex three-dimensional pipelines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [imitation learning, dataset aggregation, direct preference optimization, closed-loop evaluation, expert takeover data]</li>
<li class=""><strong>authors:</strong> Deqing Liu, Yinfeng Gao, Deheng Qian, Qichao Zhang, Xiaoqing Ye, Junyu Han, Yupeng Zheng, Xueyi Liu, Zhongpu Xia, Dawei Ding, Yifeng Pan, Dongbin Zhao</li>
<li class=""><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences; University of Science and Technology Beijing; Chongqing Chang&#x27;an Technology Co., Ltd.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17370" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17370</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74b74ccffb4336d971aedcab583ac81f676e7f75bb85a137f4255da4a692fafe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74b74ccffb4336d971aedcab583ac81f676e7f75bb85a137f4255da4a692fafe_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes TakeAD, a framework that fine-tunes a pre-trained imitation learning policy for autonomous driving using expert takeover data. The method combines iterative Dataset Aggregation (DAgger) for imitation with Direct Preference Optimization (DPO) for preference alignment to improve closed-loop performance. Experiments show it effectively mitigates the open-loop gap and outperforms pure imitation learning methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [sensor fusion and filtering], [Error-State Extended Kalman Filter, Scaled Unscented Kalman Filter, visual-inertial odometry, quaternion estimation, adaptive covariance, loosely coupled architecture]</li>
<li class=""><strong>authors:</strong> Ufuk Asil, Efendi Nasibov</li>
<li class=""><strong>institution:</strong> Dokuz Eylul University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17505" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17505</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0950fd135a7ca6f717b1ee73ea881e6e27ca5075f9e2e7ca11cb3c42ff286cab_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0950fd135a7ca6f717b1ee73ea881e6e27ca5075f9e2e7ca11cb3c42ff286cab_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid VIO method that combines an Error-State EKF with a targeted Scaled UKF step for orientation refinement, while dynamically adjusting visual measurement noise based on image quality metrics. The approach achieves significant improvements in accuracy over ESKF-based methods and reduces computational cost compared to a full UKF, balancing efficiency and performance in challenging UAV environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning Safe Autonomous Driving Policies Using Predictive Safety Representations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [safe reinforcement learning], [safe reinforcement learning, predictive safety representations, constrained markov decision processes, waymo open motion dataset, nuplan, srpl]</li>
<li class=""><strong>authors:</strong> Mahesh Keswani, Raunak Bhattacharyya</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology Delhi</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17586" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17586</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40dd4c52ca67a3f1ecc6ec7068de338a3616940aa4e51935c5ce5f30430ebbfe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40dd4c52ca67a3f1ecc6ec7068de338a3616940aa4e51935c5ce5f30430ebbfe_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates the Safety Representations for Safer Policy Learning (SRPL) framework, which augments SafeRL agents with a predictive model of future constraint violations to improve the safety-performance trade-off in autonomous driving. Experiments on real-world datasets (Waymo Open Motion Dataset and NuPlan) show that SRPL can lead to statistically significant improvements in success rate and cost reduction, and enhances robustness to noise and generalization in cross-dataset evaluation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Vidarc: Embodied Video Diffusion Model for Closed-loop Control</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [video diffusion, autoregressive generation, masked inverse dynamics model, closed-loop control, cross-embodiment pre-training, KV cache]</li>
<li class=""><strong>authors:</strong> Yao Feng, Chendong Xiang, Xinyi Mao, Hengkai Tan, Zuyue Zhang, Shuhe Huang, Kaiwen Zheng, Haitian Liu, Hang Su, Jun Zhu</li>
<li class=""><strong>institution:</strong> Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17661" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17661</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7977f39cb797f71021c4776c090587d8f5e8e7c33c06e677b445877e8ad4c5d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7977f39cb797f71021c4776c090587d8f5e8e7c33c06e677b445877e8ad4c5d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Vidarc, a method for robotic control that combines an autoregressive video diffusion model with a masked inverse dynamics model to enable fast, closed-loop operation. It is pre-trained on a large dataset of diverse robotic episodes and achieves state-of-the-art performance, including higher success rates and significantly lower latency compared to baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [energy-based models, gradient-based refinement, hindsight goal relabeling, latent-space planning]</li>
<li class=""><strong>authors:</strong> Carlos Vélez García, Miguel Cazorla, Jorge Pomares</li>
<li class=""><strong>institution:</strong> INESCOP, University of Alicante</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17846" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17846</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Planning as Descent (PaD), a method for offline goal-conditioned reinforcement learning that learns an energy function over latent trajectories and performs planning via gradient-based refinement in this energy landscape. It achieves state-of-the-art 95% success on cube manipulation tasks, demonstrating that verification-driven trajectory synthesis outperforms direct policy learning, especially when trained on noisy data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [ViPR, ViPR-Eureka, ViPR-RL, behavior cloning, VLM-in-the-loop Parallel Refinement, LLM-guided contact sampling, sim-to-real transfer, GPU simulation]</li>
<li class=""><strong>authors:</strong> Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper</li>
<li class=""><strong>institution:</strong> Robotics and AI Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17853" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17853</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3552d146c759bb8b81dd4441230819f44e04ae7530ed1ec49cc21133ed3f116_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3552d146c759bb8b81dd4441230819f44e04ae7530ed1ec49cc21133ed3f116_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents AnyTask, an automated framework that uses massively parallel GPU simulation and foundation models to generate diverse robot manipulation tasks and expert demonstration data. It introduces three agents (ViPR, ViPR-Eureka, ViPR-RL) for synthesizing demonstrations, which are used to train behavior cloning policies. These policies achieve a 44% average success rate when deployed directly on real robot hardware for various manipulation tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] RadarGen: Automotive Radar Point Cloud Generation from Cameras</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [diffusion model, bird&#x27;s-eye-view, radar cross section, Doppler, point cloud generation, foundation models]</li>
<li class=""><strong>authors:</strong> Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany</li>
<li class=""><strong>institution:</strong> Technion, MIT, NVIDIA, University of Toronto, Vector Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17897" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17897</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RadarGen is a diffusion model that generates realistic automotive radar point clouds from multi-view camera images by representing radar data in bird&#x27;s-eye-view and conditioning on visual cues. It uses a lightweight recovery step to reconstruct point clouds from the generated maps. Evaluations show it captures real radar statistics and reduces the performance gap for perception models trained on synthetic data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Diffusion Forcing for Multi-Agent Interaction Sequence Modeling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multi-agent motion generation], [diffusion forcing, autoregressive diffusion, transformer, multi-agent interaction, denoising]</li>
<li class=""><strong>authors:</strong> Vongani H. Maluleke, Kie Horiuchi, Lea Wilken, Evonne Ng, Jitendra Malik, Angjoo Kanazawa</li>
<li class=""><strong>institution:</strong> UC Berkeley, Sony Group Corporation, Meta</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17900" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17900</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cd37ce47a0e9e2e02887aa6dab039bf60ed28e99e8eaf07d8c6aaa610b08b8a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cd37ce47a0e9e2e02887aa6dab039bf60ed28e99e8eaf07d8c6aaa610b08b8a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MAGNet, a unified autoregressive diffusion framework for generating multi-agent human motion sequences. It extends Diffusion Forcing to explicitly model inter-agent coupling, enabling coherent coordination for both synchronized and loosely structured social interactions. The method performs on par with specialized dyadic benchmarks and naturally scales to polyadic scenarios with three or more agents.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-23">2025-12-23<a href="#2025-12-23" class="hash-link" aria-label="Direct link to 2025-12-23" title="Direct link to 2025-12-23" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251223] Untethered thin dielectric elastomer actuated soft robot</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xi Wang, Jing Liu, Siqian Li, Hengtai Dai, Jung-Che Chang, Adam Rushworth, Xin Dong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17940" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17940</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/147e184fb3d08f337e1d702b64cecb4a8209fe095f79478259483975e982eb39_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/147e184fb3d08f337e1d702b64cecb4a8209fe095f79478259483975e982eb39_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Untethered thin dielectric elastomer actuated soft robot</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Farida Mohsen, Ali Safa</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17958" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17958</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf18ccdf18f30e0e5c324fc709aa108b4706cb9c6a2b56366e90ad3a8bd1a32c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf18ccdf18f30e0e5c324fc709aa108b4706cb9c6a2b56366e90ad3a8bd1a32c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Robotic VLA Benefits from Joint Learning with Motion Image Diffusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yu Fang, Kanchana Ranasinghe, Le Xue, Honglu Zhou, Juntao Tan, Ran Xu, Shelby Heinecke, Caiming Xiong, Silvio Savarese, Daniel Szafir, Mingyu Ding, Michael S. Ryoo, Juan Carlos Niebles</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18007" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18007</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b59654686bbf5b51e6cb8f755af962b88b4d3f2a65f9c991f5f10b64604828_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b59654686bbf5b51e6cb8f755af962b88b4d3f2a65f9c991f5f10b64604828_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Robotic VLA Benefits from Joint Learning with Motion Image Diffusion</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18028" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18028</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30065ffa9a2b8c3aa56f4dd49b67477e394ef33e769980a2e7968c85edfa3346_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30065ffa9a2b8c3aa56f4dd49b67477e394ef33e769980a2e7968c85edfa3346_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Design and Integration of Thermal and Vibrotactile Feedback for Lifelike Touch in Social Robots</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jacqueline Borgstedt, Jake Bhattacharyya, Matteo Iovino, Frank E. Pollick, Stephen Brewster</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18032" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18032</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab5ccc615f8631e1cb919fee6836987e7793d4803a41430b431092ecd0b25ec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab5ccc615f8631e1cb919fee6836987e7793d4803a41430b431092ecd0b25ec_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Design and Integration of Thermal and Vibrotactile Feedback for Lifelike Touch in Social Robots</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Unifying Deep Predicate Invention with Pre-trained Foundation Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Qianwei Wang, Bowen Li, Zhanpeng Luo, Yifan Xu, Alexander Gray, Tom Silver, Sebastian Scherer, Katia Sycara, Yaqi Xie</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17992" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17992</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a6ef8216b049bb762fabd2e4dcf908ec9b4a2cc0dfdfed0068704f85aa89e8a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a6ef8216b049bb762fabd2e4dcf908ec9b4a2cc0dfdfed0068704f85aa89e8a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Unifying Deep Predicate Invention with Pre-trained Foundation Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Design of a Polymer-based Steerable Cannula for Neurosurgical Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nidhi Malhotra, Amber K. Rothe, Revanth Konda, Jaydev P. Desai</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18048" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18048</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb51c5c2d914d049ca0f0ea5fe80a670a7185b2b3b169a2816bc271d98f67cde_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb51c5c2d914d049ca0f0ea5fe80a670a7185b2b3b169a2816bc271d98f67cde_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Design of a Polymer-based Steerable Cannula for Neurosurgical Applications</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Juo-Tung Chen, XinHao Chen, Ji Woong Kim, Paul Maria Scheikl, Richard Jaepyeong Cha, Axel Krieger</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18068" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18068</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afad84fb7ac4ae6f6bdc263133a9e2d4acce99d04476e05fcc665011a0948574_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afad84fb7ac4ae6f6bdc263133a9e2d4acce99d04476e05fcc665011a0948574_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Towards Autonomous Navigation in Endovascular Interventions</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tudor Jianu, Anh Nguyen, Sebastiano Fichera, Pierre Berthet-Rayne</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18081" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18081</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f911d278f0ab6bda3ee8c65e700063ac44b97c8d600de90a9982d9c8c3b33f81_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f911d278f0ab6bda3ee8c65e700063ac44b97c8d600de90a9982d9c8c3b33f81_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards Autonomous Navigation in Endovascular Interventions</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] On Swarm Leader Identification using Probing Policies</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Stergios E. Bachoumas, Panagiotis Artemiadis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18146" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18146</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/584c84764226eb21b4c2c555cf6432ccfe102a3aa9dcc530809f19d78d76d7ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/584c84764226eb21b4c2c555cf6432ccfe102a3aa9dcc530809f19d78d76d7ac_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> On Swarm Leader Identification using Probing Policies</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Alternating Minimization for Time-Shifted Synergy Extraction in Human Hand Coordination</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Trevor Stepp, Parthan Olikkal, Ramana Vinjamuri, Rajasekhar Anguluri</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18206" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18206</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16675ffdd8bab8dfbdd7e63041cd5c55425ee187db948a8a16692b6b569f0516_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16675ffdd8bab8dfbdd7e63041cd5c55425ee187db948a8a16692b6b569f0516_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Alternating Minimization for Time-Shifted Synergy Extraction in Human Hand Coordination</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Fractional-order Modeling for Nonlinear Soft Actuators via Particle Swarm Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wu-Te Yang, Masayoshi Tomizuka</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18213" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18213</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16dc328a115226b3e0e0e21126d10a6edacc4f54483536e9b10d3091f0de254b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16dc328a115226b3e0e0e21126d10a6edacc4f54483536e9b10d3091f0de254b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Fractional-order Modeling for Nonlinear Soft Actuators via Particle Swarm Optimization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yudong Liu, Spencer Hallyburton, Jiwoo Kim, Yueqian Lin, Yiming Li, Qinsi Wang, Hui Ye, Jingwei Sun, Miroslav Pajic, Yiran Chen, Hai Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18211" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18211</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9eea86ac8cc855b1a0e43febafd0f1e08626f900ba3ded294c1d99b0072a7da3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9eea86ac8cc855b1a0e43febafd0f1e08626f900ba3ded294c1d99b0072a7da3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shahram Najam Syed, Yitian Hu, Yuchao Yao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18237" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18237</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b04b692d1acf5e8524839243e2fe0778331620858ff93eb02166948c9684c4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b04b692d1acf5e8524839243e2fe0778331620858ff93eb02166948c9684c4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] On The Computational Complexity for Minimizing Aerial Photographs for Full Coverage of a Planar Region</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Si Wei Feng</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18268" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18268</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d836c87e7156449cac14d8856f6ca606bb4ddeea577e006c464614eac364fe2d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d836c87e7156449cac14d8856f6ca606bb4ddeea577e006c464614eac364fe2d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> On The Computational Complexity for Minimizing Aerial Photographs for Full Coverage of a Planar Region</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Youssef Mahran, Zeyad Gamal, Ayman El-Badawy</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18333" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18333</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f5c0778f6eace42568ad8fc221a69e1cf4360df09bb1bb286e34299351febe0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f5c0778f6eace42568ad8fc221a69e1cf4360df09bb1bb286e34299351febe0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Youssef Mahran, Zeyad Gamal, Ayman El-Badawy</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18336" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18336</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67d046b180bb4bec9247b6bb525bd30b5814a0c5e4673bd03aea3eb64b9797e7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67d046b180bb4bec9247b6bb525bd30b5814a0c5e4673bd03aea3eb64b9797e7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Learning Semantic Atomic Skills for Multi-Task Robotic Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yihang Zhu, Weiqing Wang, Shijie Wu, Ye Shi, Jingya Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18368" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18368</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/004564326e17c1e67b90e1c7ff4c2148031c0e38d68983ac3a5150236f6fa495_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/004564326e17c1e67b90e1c7ff4c2148031c0e38d68983ac3a5150236f6fa495_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning Semantic Atomic Skills for Multi-Task Robotic Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yulu Wu, Jiujun Cheng, Haowen Wang, Dengyang Suo, Pei Ren, Qichao Mao, Shangce Gao, Yakun Huang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18396" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18396</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/336a4848582e30bb097fa913acf8a86a750fe8f43097ffae12c485a2ff45723d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/336a4848582e30bb097fa913acf8a86a750fe8f43097ffae12c485a2ff45723d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] When Robots Say No: The Empathic Ethical Disobedience Benchmark</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dmytro Kuzmenko, Nadiya Shvai</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18474" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18474</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/083ac27bf9c1ba565d26a4c6417b20f994336e4ffb3cb410b3b93d3ef36fb6aa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/083ac27bf9c1ba565d26a4c6417b20f994336e4ffb3cb410b3b93d3ef36fb6aa_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> When Robots Say No: The Empathic Ethical Disobedience Benchmark</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] STORM: Search-Guided Generative World Models for Robotic Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wenjun Lin, Jensen Zhang, Kaitong Cai, Keze Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18477" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18477</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79949cb474434bd024983169a211bb0a029e6412a974a6dd6f4ee7a51cf05349_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79949cb474434bd024983169a211bb0a029e6412a974a6dd6f4ee7a51cf05349_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> STORM: Search-Guided Generative World Models for Robotic Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Systematic Benchmarking of SUMO Against Data-Driven Traffic Simulators</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Erdao Liang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18537" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18537</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c63a2c59e4cb34d27c44febba963f6d35bc48277e311a14bca062e28fbe5aee_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c63a2c59e4cb34d27c44febba963f6d35bc48277e311a14bca062e28fbe5aee_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Systematic Benchmarking of SUMO Against Data-Driven Traffic Simulators</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pengcheng Li, Qiang Fang, Tong Zhao, Yixing Lan, Xin Xu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18583" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18583</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fdabaf80e15fc440c53464fb3134095f9121f39b7d6d83850d4cb921dec3fda_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fdabaf80e15fc440c53464fb3134095f9121f39b7d6d83850d4cb921dec3fda_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhenhao Zhou, Dan Negrut</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18619" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18619</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c6d919e0fe62510cfe25cf2ce73cce1bed581dc021d27540c0588fbf495702_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c6d919e0fe62510cfe25cf2ce73cce1bed581dc021d27540c0588fbf495702_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Offline Reinforcement Learning for End-to-End Autonomous Driving</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chihiro Noguchi, Takaki Yamamoto</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18662" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18662</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48070c89f8318c57738214d175fd04c9e38d7e43e2838b599453ae0e31d8c27f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48070c89f8318c57738214d175fd04c9e38d7e43e2838b599453ae0e31d8c27f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Offline Reinforcement Learning for End-to-End Autonomous Driving</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Geometric-Photometric Event-based 3D Gaussian Ray Tracing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kai Kohyama, Yoshimitsu Aoki, Guillermo Gallego, Shintaro Shiba</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18640" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18640</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85632c631e489e2f789f1b5319b05b69e8e883a12a7b626de475c98e6066ef45_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85632c631e489e2f789f1b5319b05b69e8e883a12a7b626de475c98e6066ef45_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Geometric-Photometric Event-based 3D Gaussian Ray Tracing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Cailin Lei, Haiyang Wu, Yuxiong Ji, Xiaoyu Cai, Yuchuan Du</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18703" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18703</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ec9f06c274a9525b199fad25bdb142bfd63b4a43030adbafd554411a02c2fe1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ec9f06c274a9525b199fad25bdb142bfd63b4a43030adbafd554411a02c2fe1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] DSO-VSA: a Variable Stiffness Actuator with Decoupled Stiffness and Output Characteristics for Rehabilitation Robotics</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Maozeng Zhang, Ke Shi, Huijun Li, Tongshu Chen, Jiejun Yan, Aiguo Song</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18712" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18712</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/788f7143f635c52e41543f64d13fef2124889bb6d62b5ae4656ad27e5705081b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/788f7143f635c52e41543f64d13fef2124889bb6d62b5ae4656ad27e5705081b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DSO-VSA: a Variable Stiffness Actuator with Decoupled Stiffness and Output Characteristics for Rehabilitation Robotics</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Multimodal Classification Network Guided Trajectory Planning for Four-Wheel Independent Steering Autonomous Parking Considering Obstacle Attributes</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jingjia Teng, Yang Li, Jianqiang Wang, Yingbai Hu, Songyuan Tang, Manjiang Hu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18836" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18836</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7a54132191e50063831023e6eae9aa9357ab29b75e7078d8784bd1328c2d0f0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7a54132191e50063831023e6eae9aa9357ab29b75e7078d8784bd1328c2d0f0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multimodal Classification Network Guided Trajectory Planning for Four-Wheel Independent Steering Autonomous Parking Considering Obstacle Attributes</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] InDRiVE: Reward-Free World-Model Pretraining for Autonomous Driving via Latent Disagreement</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Feeza Khan Khanzada, Jaerock Kwon</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18850" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18850</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6819b32f0d91763f868ca298c9e844896fa38b755e6a9bf4eed0851e5ae3cbf1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6819b32f0d91763f868ca298c9e844896fa38b755e6a9bf4eed0851e5ae3cbf1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> InDRiVE: Reward-Free World-Model Pretraining for Autonomous Driving via Latent Disagreement</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Construction and deformation of P-hedra using control polylines</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Georg Nawratil</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18869" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18869</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c054a6aa06f719232431f4b3e82e7945ca7f3f642dc9e2322fdc9185878ba5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c054a6aa06f719232431f4b3e82e7945ca7f3f642dc9e2322fdc9185878ba5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Construction and deformation of P-hedra using control polylines</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Optimizing Robotic Placement via Grasp-Dependent Feasibility Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tianyuan Liu, Richard Dazeley, Benjamin Champion, Akan Cosgun</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18922" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18922</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb36c038274937dd40786c8ac78288222c12acc29b7aee73573c5a6331a6145_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb36c038274937dd40786c8ac78288222c12acc29b7aee73573c5a6331a6145_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Optimizing Robotic Placement via Grasp-Dependent Feasibility Prediction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Point What You Mean: Visually Grounded Instruction Policy</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hang Yu, Juntu Zhao, Yufeng Liu, Kaiyu Li, Cheng Ma, Di Zhang, Yingdong Hu, Guang Chen, Junyuan Xie, Junliang Guo, Junqiao Zhao, Yang Gao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18933" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18933</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0cc4da48f9bff61ef73842c7f9922cf58cd10179d0b3d6cb1241fbc052909cc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0cc4da48f9bff61ef73842c7f9922cf58cd10179d0b3d6cb1241fbc052909cc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Point What You Mean: Visually Grounded Instruction Policy</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Framework for Deploying Learning-based Quadruped Loco-Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yadong Liu, Jianwei Liu, He Liang, Dimitrios Kanoulas</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18938" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18938</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed6b45b02a235e7607255d7b5204abe59d049aea04c8c4caa01b35f1f4f309d9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed6b45b02a235e7607255d7b5204abe59d049aea04c8c4caa01b35f1f4f309d9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Framework for Deploying Learning-based Quadruped Loco-Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yanding Yang, Weitao Zhou, Jinhai Wang, Xiaomin Guo, Junze Wen, Xiaolong Liu, Lang Ding, Zheng Fu, Jinyu Miao, Kun Jiang, Diange Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18988" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18988</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b25603fac1726ddbcf4804fa919bd28a3ae6d25cea4e62760e5e98ac335e570_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b25603fac1726ddbcf4804fa919bd28a3ae6d25cea4e62760e5e98ac335e570_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ryosuke Korekata, Quanting Xie, Yonatan Bisk, Komei Sugiura</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18987" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18987</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/456f5beecc1c2c45f598578f8491d702dc76e9f500bae44e01454f783dc10b05_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/456f5beecc1c2c45f598578f8491d702dc76e9f500bae44e01454f783dc10b05_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sihao Lin, Zerui Li, Xunyi Zhao, Gengze Zhou, Liuyi Wang, Rong Wei, Rui Tang, Juncheng Li, Hanqing Wang, Jiangmiao Pang, Anton van den Hengel, Jiajun Liu, Qi Wu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19021" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19021</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73e25aea53ce484be801e13db8703f7ac91dcd519e03aa8f1f11869324466841_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73e25aea53ce484be801e13db8703f7ac91dcd519e03aa8f1f11869324466841_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] EGM: Efficiently Learning General Motion Tracking Policy for High Dynamic Humanoid Whole-Body Control</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chao Yang, Yingkai Sun, Peng Ye, Xin Chen, Chong Yu, Tao Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19043" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19043</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7fe11e8953d7da559e3d75bda7b4d386de464da3c906331d4f0b2c94d096ac4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7fe11e8953d7da559e3d75bda7b4d386de464da3c906331d4f0b2c94d096ac4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EGM: Efficiently Learning General Motion Tracking Policy for High Dynamic Humanoid Whole-Body Control</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xu Liu, Yu Liu, Hanshuo Qiu, Yang Qirong, Zhouhui Lian</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19024" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19024</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6756a03a5e10a12bbcf7c372024e83600363def7ecc93793c6ea6abf5fb1e097_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6756a03a5e10a12bbcf7c372024e83600363def7ecc93793c6ea6abf5fb1e097_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pengyu Chen, Tao Ouyang, Ke Luo, Weijie Hong, Xu Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19083" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19083</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92e42fabfcde19e98a2f6371e650103f1dd9895fac5630cbb65993eda35b116a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92e42fabfcde19e98a2f6371e650103f1dd9895fac5630cbb65993eda35b116a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pengxuan Yang, Ben Lu, Zhongpu Xia, Chao Han, Yinfeng Gao, Teng Zhang, Kun Zhan, XianPeng Lang, Yupeng Zheng, Qichao Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19133" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19133</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95928df29dd1d6db8d9cc4946cd472f8722d19286d4bfa47dd919093172f5948_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95928df29dd1d6db8d9cc4946cd472f8722d19286d4bfa47dd919093172f5948_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Flexible Field-Based Policy Learning Framework for Diverse Robotic Systems and Sensors</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jose Gustavo Buenaventura Carreon, Floris Erich, Roman Mykhailyshyn, Tomohiro Motoda, Ryo Hanai, Yukiyasu Domae</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19148" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19148</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/956d6e879c4f50e7adb31c9c246b0ef696f20c3d25b5e10073242ec049305b97_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/956d6e879c4f50e7adb31c9c246b0ef696f20c3d25b5e10073242ec049305b97_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Flexible Field-Based Policy Learning Framework for Diverse Robotic Systems and Sensors</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Vision-Language-Policy Model for Dynamic Robot Task Planning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jin Wang, Kim Tien Ly, Jacques Cloete, Nikos Tsagarakis, Ioannis Havoutis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19178" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19178</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61b83585238832b8c3decda632e24be3e08e065673710008da8a24e7fc11820b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61b83585238832b8c3decda632e24be3e08e065673710008da8a24e7fc11820b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Vision-Language-Policy Model for Dynamic Robot Task Planning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Vision-Aided Relative State Estimation for Approach and Landing on a Moving Platform with Inertial Measurements</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tarek Bouazza, Alessandro Melis, Soulaimane Berkane, Robert Mahony, Tarek Hamel</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19245" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19245</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d513f5a88f2a529f413e52c4a1f8a346c5bbe6ab056bc09090ccb37d3041a044_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d513f5a88f2a529f413e52c4a1f8a346c5bbe6ab056bc09090ccb37d3041a044_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Vision-Aided Relative State Estimation for Approach and Landing on a Moving Platform with Inertial Measurements</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Are All Data Necessary? Efficient Data Pruning for Large-scale Autonomous Driving Dataset via Trajectory Entropy Maximization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhaoyang Liu, Weitao Zhou, Junze Wen, Cheng Jing, Qian Cheng, Kun Jiang, Diange Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19270" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19270</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/956fe529e5541a0272de8e8183c92a6a5f010136fae9741d64e0049d6c4ea14c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/956fe529e5541a0272de8e8183c92a6a5f010136fae9741d64e0049d6c4ea14c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Are All Data Necessary? Efficient Data Pruning for Large-scale Autonomous Driving Dataset via Trajectory Entropy Maximization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Translating Flow to Policy via Hindsight Online Imitation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yitian Zheng, Zhangchen Ye, Weijun Dong, Shengjie Wang, Yuyang Liu, Chongjie Zhang, Chuan Wen, Yang Gao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19269" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19269</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d1e38857114c4e6bdd80a03e175ccdec39b489a4a7abb23d37f7f4402ba68fc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d1e38857114c4e6bdd80a03e175ccdec39b489a4a7abb23d37f7f4402ba68fc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Translating Flow to Policy via Hindsight Online Imitation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Comparison and Evaluation of Different Simulation Environments for Rigid Body Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Longxiang Shao, Ulrich Dahmen, Juergen Rossmann</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19289" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19289</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cd64634c88e8117da55124a532a6234cc3d8703ceb30084a311c5e28bb6ba516_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cd64634c88e8117da55124a532a6234cc3d8703ceb30084a311c5e28bb6ba516_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Comparison and Evaluation of Different Simulation Environments for Rigid Body Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] OMP: One-step Meanflow Policy with Directional Alignment</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Han Fang, Yize Huang, Yuheng Zhao, Paul Weng, Xiao Li, Yutong Ban</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19347" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19347</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bebf7d7dda43e94f094030d3f33a13922da836a13a10a2f445b742b579a24684_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bebf7d7dda43e94f094030d3f33a13922da836a13a10a2f445b742b579a24684_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> OMP: One-step Meanflow Policy with Directional Alignment</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hongwei Fan, Hang Dai, Jiyao Zhang, Jinzhou Li, Qiyang Yan, Yujie Zhao, Mingju Gao, Jinghang Wu, Hao Tang, Hao Dong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19390" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19390</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf1e9d4003951dba668a306d606416b006b75689695b9667afa58a3ba76ea89_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf1e9d4003951dba668a306d606416b006b75689695b9667afa58a3ba76ea89_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Mixed formulation and structure-preserving discretization of Cosserat rod dynamics in a port-Hamiltonian framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Philipp L. Kinon, Simon R. Eugster, Peter Betsch</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19408" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19408</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2011f29ba413592cebb090b3281d18ea6b552fbd91b93ffc64d7c6a5c901ca5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2011f29ba413592cebb090b3281d18ea6b552fbd91b93ffc64d7c6a5c901ca5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Mixed formulation and structure-preserving discretization of Cosserat rod dynamics in a port-Hamiltonian framework</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yujie Zhao, Hongwei Fan, Di Chen, Shengcong Chen, Liliang Chen, Xiaoqi Li, Guanghui Ren, Hao Dong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19402" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19402</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3585a78a8a01def680be454d4e0abbbbcf24961bd331f0f18d30d4fa2409128_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3585a78a8a01def680be454d4e0abbbbcf24961bd331f0f18d30d4fa2409128_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Sign Language Recognition using Parallel Bidirectional Reservoir Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nitin Kumar Singh, Arie Rachmad Syulistyo, Yuichiro Tanaka, Hakaru Tamukoh</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19451" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19451</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/554347db8f25765f90ed0de383600422b9f8c928bbd223207e7df0e299098ce4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/554347db8f25765f90ed0de383600422b9f8c928bbd223207e7df0e299098ce4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Sign Language Recognition using Parallel Bidirectional Reservoir Computing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhenglong Guo, Yiming Zhao, Feng Jiang, Heng Jin, Zongbao Feng, Jianbin Zhou, Siyuan Xu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19453" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19453</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf7556ca61a6911a607e95a1a25dcd238b160d37af7403e1dcec7e54c013afa6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf7556ca61a6911a607e95a1a25dcd238b160d37af7403e1dcec7e54c013afa6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Martin Sedlacek, Pavlo Yefanov, Georgy Ponimatkin, Jai Bardhan, Simon Pilc, Mederic Fourmy, Evangelos Kazakos, Cees G. M. Snoek, Josef Sivic, Vladimir Petrik</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19562" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19562</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cb99fa05ab76ebf7aa83d9c99a1cab512063ae25e6fb2fa7beee0f0c7246905_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cb99fa05ab76ebf7aa83d9c99a1cab512063ae25e6fb2fa7beee0f0c7246905_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yanliang Huang, Xia Yan, Peiran Yin, Zhenduo Zhang, Zeyan Shao, Youran Wang, Haoliang Huang, Matthias Althoff</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19564" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19564</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd9b53a38761715bf725e8473ae990f27df4fc7048b67ea4fde9ad955b4ac95d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd9b53a38761715bf725e8473ae990f27df4fc7048b67ea4fde9ad955b4ac95d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kirill Djebko, Tom Baumann, Erik Dilger, Frank Puppe, Sergio Montenegro</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19576" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19576</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/559c74a4e132b0428b11e1b742ace3b49e9292ec3c666ac9dd536d79ee6c2a1f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/559c74a4e132b0428b11e1b742ace3b49e9292ec3c666ac9dd536d79ee6c2a1f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] LIMOncello: Revisited IKFoM on the SGal(3) Manifold for Fast LiDAR-Inertial Odometry</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Carlos Pérez-Ruiz, Joan Solà</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19567" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19567</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ca49f17edae17bf6ccc3e14c30cbfe1362d605146c63dbbff4a97cc1a226407_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ca49f17edae17bf6ccc3e14c30cbfe1362d605146c63dbbff4a97cc1a226407_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LIMOncello: Revisited IKFoM on the SGal(3) Manifold for Fast LiDAR-Inertial Odometry</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yinhuai Wang, Runyi Yu, Hok Wai Tsui, Xiaoyi Lin, Hui Zhang, Qihan Zhao, Ke Fan, Miao Li, Jie Song, Jingbo Wang, Qifeng Chen, Ping Tan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19583" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19583</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50064ac2ceaf4f963ab285d471e35de23cbb546a9b072cfb14bd945c6439276e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50064ac2ceaf4f963ab285d471e35de23cbb546a9b072cfb14bd945c6439276e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jiaqi Peng, Wenzhe Cai, Yuqiang Yang, Tai Wang, Yuan Shen, Jiangmiao Pang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19629" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19629</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c789e71ef34ed4c917c96dfb4b01a72b2ebbbaf1b31aedd33c19890927a051c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c789e71ef34ed4c917c96dfb4b01a72b2ebbbaf1b31aedd33c19890927a051c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Zero-shot Reconstruction of In-Scene Object Manipulation from Video</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dixuan Lin, Tianyou Wang, Zhuoyang Pan, Yufu Wang, Lingjie Liu, Kostas Daniilidis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19684" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19684</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/967b97ff200c76a7e13dbcc9b4b2be1132807200b91f7312ad2eb4984f48bb88_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/967b97ff200c76a7e13dbcc9b4b2be1132807200b91f7312ad2eb4984f48bb88_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Zero-shot Reconstruction of In-Scene Object Manipulation from Video</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Deterministic Reconstruction of Tennis Serve Mechanics: From Aerodynamic Constraints to Internal Torques via Rigid-Body Dynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sun-Hyun Youn</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18320" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18320</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb9a9476c629b61f7574cea9ece28357bfe67bff74e75f1e094face2582eef9c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb9a9476c629b61f7574cea9ece28357bfe67bff74e75f1e094face2582eef9c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Deterministic Reconstruction of Tennis Serve Mechanics: From Aerodynamic Constraints to Internal Torques via Rigid-Body Dynamics</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] PalpAid: Multimodal Pneumatic Tactile Sensor for Tissue Palpation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Devi Yuliarti, Ravi Prakash, Hiu Ching Cheung, Amy Strong, Patrick J. Codd, Shan Lin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19010" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19010</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a46f8c0e00e5fc7cc3fa225c9fc8a67e630549137470ebd50de3e6bd418c8a5b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a46f8c0e00e5fc7cc3fa225c9fc8a67e630549137470ebd50de3e6bd418c8a5b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PalpAid: Multimodal Pneumatic Tactile Sensor for Tissue Palpation</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-24">2025-12-24<a href="#2025-12-24" class="hash-link" aria-label="Direct link to 2025-12-24" title="Direct link to 2025-12-24" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251224] Gaussian Variational Inference with Non-Gaussian Factors for State Estimation: A UWB Localization Case Study</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Andrew Stirling, Mykola Lukashchuk, Dmitry Bagaev, Wouter Kouw, James R. Forbes</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19855" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19855</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9016f998c96e245ed8aaa845a90618946ec0f9528b50b0585f4ee2e13dca70ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9016f998c96e245ed8aaa845a90618946ec0f9528b50b0585f4ee2e13dca70ac_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Gaussian Variational Inference with Non-Gaussian Factors for State Estimation: A UWB Localization Case Study</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] A Class of Axis-Angle Attitude Control Laws for Rotational Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Francisco M. F. R. Gonçalves, Ryan M. Bena, Néstor O. Pérez-Arancibia</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19846" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19846</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/052f19775c0d61e53c6124e8478d4e7ee400cc921f19dbc0a3ec62a387c79f86_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/052f19775c0d61e53c6124e8478d4e7ee400cc921f19dbc0a3ec62a387c79f86_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Class of Axis-Angle Attitude Control Laws for Rotational Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sujan Warnakulasooriya, Andreas Willig, Xiaobing Wu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19914" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19914</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/188da8ba7b74e5b961c0e61a49776e60da92670b4dd0b522d0c74e156682ec49_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/188da8ba7b74e5b961c0e61a49776e60da92670b4dd0b522d0c74e156682ec49_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sangoh Lee, Sangwoo Mo, Wook-Shin Han</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20014" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20014</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbb9757af838ced105aac295c936d5bf2fa52c5f79d162d34ed41c9c961ae9e0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbb9757af838ced105aac295c936d5bf2fa52c5f79d162d34ed41c9c961ae9e0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Learning Skills from Action-Free Videos</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hung-Chieh Fang, Kuo-Han Hung, Chu-Rong Chen, Po-Jung Chou, Chun-Kai Yang, Po-Chen Ko, Yu-Chiang Wang, Yueh-Hua Wu, Min-Hung Chen, Shao-Hua Sun</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20052" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20052</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/def215474b82a6d04f6a6f79dc99c74e3b1159e34a80855d18649f29781a36fc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/def215474b82a6d04f6a6f79dc99c74e3b1159e34a80855d18649f29781a36fc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning Skills from Action-Free Videos</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Detecting Non-Optimal Decisions of Embodied Agents via Diversity-Guided Metamorphic Testing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wenzhao Wu, Yahui Tang, Mingfei Cheng, Wenbing Tang, Yuan Zhou, Yang Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20083" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20083</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2f6a93b0ca9601c38b73ad3e8c062333303a8f9538415f2546b2a923116462a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2f6a93b0ca9601c38b73ad3e8c062333303a8f9538415f2546b2a923116462a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Detecting Non-Optimal Decisions of Embodied Agents via Diversity-Guided Metamorphic Testing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Robert van de Ven, Trim Bresilla, Bram Nelissen, Ard Nieuwenhuizen, Eldert J. van Henten, Gert Kootstra</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20148" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20148</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0476ec59995b5f3259a9b2ab1456d67ddbbb750902c5acd3d684600b97c47598_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0476ec59995b5f3259a9b2ab1456d67ddbbb750902c5acd3d684600b97c47598_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LoLA: Long Horizon Latent Action Learning for General Robot Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiaofan Wang, Xingyu Gao, Jianlong Fu, Zuolei Li, Dean Fortier, Galen Mullins, Andrey Kolobov, Baining Guo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20166" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20166</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9cca5d5cc494ac98e7e5083e1b8aeea89e51f1de0242a845197c5e3b3df56b17_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9cca5d5cc494ac98e7e5083e1b8aeea89e51f1de0242a845197c5e3b3df56b17_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LoLA: Long Horizon Latent Action Learning for General Robot Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Teqiang Zou, Hongliang Zeng, Yuxuan Nong, Yifan Li, Kehui Liu, Haotian Yang, Xinyang Ling, Xin Li, Lianyang Ma</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20188" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20188</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2574b3e0fe38bea23055b8f04a72d9d1bf8fedc2ee3f2dcf7a4e5a2f16af3c51_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2574b3e0fe38bea23055b8f04a72d9d1bf8fedc2ee3f2dcf7a4e5a2f16af3c51_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] UrbanV2X: A Multisensory Vehicle-Infrastructure Dataset for Cooperative Navigation in Urban Areas</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Qijun Qin, Ziqi Zhang, Yihan Zhong, Feng Huang, Xikun Liu, Runzhi Hu, Hang Chen, Wei Hu, Dongzhe Su, Jun Zhang, Hoi-Fung Ng, Weisong Wen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20224" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20224</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eafa66d44e2633f1e1623b3bfe8d87a949ce8da88e1e0430473ab41769747405_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eafa66d44e2633f1e1623b3bfe8d87a949ce8da88e1e0430473ab41769747405_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> UrbanV2X: A Multisensory Vehicle-Infrastructure Dataset for Cooperative Navigation in Urban Areas</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Finite-Time Control Based on Differential Flatness for Wheeled Mobile Robots with Experimental Validation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Imtiaz Ur Rehman, Moussa Labbadi, Amine Abadi, Lew Lew Yan Voon</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20229" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20229</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fcb428fb736e259aa4637b3927a792e0cf3fb0c6c2182598360d56c1081362b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fcb428fb736e259aa4637b3927a792e0cf3fb0c6c2182598360d56c1081362b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Finite-Time Control Based on Differential Flatness for Wheeled Mobile Robots with Experimental Validation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuntao Dai, Hang Gu, Teng Wang, Qianyu Cheng, Yifei Zheng, Zhiyong Qiu, Lei Gong, Wenqi Lou, Xuehai Zhou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20276" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20276</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b076c18a7407a2fa23f502051cf18d209fb696f05cbca7af89a2db3703250585_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b076c18a7407a2fa23f502051cf18d209fb696f05cbca7af89a2db3703250585_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhongyu Xia, Wenhao Chen, Yongtao Wang, Ming-Hsuan Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20299" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20299</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0961ae49fbc925ad5eacb6602aeec6cfdfabae07b252a044cd181bdb5a47746_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0961ae49fbc925ad5eacb6602aeec6cfdfabae07b252a044cd181bdb5a47746_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Pneumatic bladder links with wide range of motion joints for articulated inflatable robots</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Katsu Uchiyama, Ryuma Niiyama</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20322" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20322</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/358991788a6fb301b2e9eea033e19d4fd9eecadc34cda99479fccc3d85fd1601_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/358991788a6fb301b2e9eea033e19d4fd9eecadc34cda99479fccc3d85fd1601_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Pneumatic bladder links with wide range of motion joints for articulated inflatable robots</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hao Wei, Peiji Wang, Qianhao Wang, Tong Qin, Fei Gao, Yulin Si</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20355" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20355</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b1319141213b2b28f550b4d7cf9685c47d5f6e12e3ee5a934bec5a817f95189_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b1319141213b2b28f550b4d7cf9685c47d5f6e12e3ee5a934bec5a817f95189_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Maulana Bisyir Azhari, Donghun Han, Je In You, Sungjun Park, David Hyunchul Shim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20475" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20475</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d47bd82e4c97fdc34a8d98cf68ba2bc04d4ee3480b229430db92dd47b53b90e1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d47bd82e4c97fdc34a8d98cf68ba2bc04d4ee3480b229430db92dd47b53b90e1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Long Nguyen, Micha Fauth, Bernhard Jaeger, Daniel Dauner, Maximilian Igl, Andreas Geiger, Kashyap Chitta</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20563" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20563</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f62c50da026de5eec286e01930af394650fb8b9c309ff48cd8f733ee9ca220b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f62c50da026de5eec286e01930af394650fb8b9c309ff48cd8f733ee9ca220b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Changyi Lin, Boda Huo, Mingyang Yu, Emily Ruppel, Bingqing Chen, Jonathan Francis, Ding Zhao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20591" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20591</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87e6d8b8d3a87a06671a80e30688f46eb142134f0545d66682f305f4219edec9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87e6d8b8d3a87a06671a80e30688f46eb142134f0545d66682f305f4219edec9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Design and Modeling of a Simple-Structured Continuously Variable Transmission Utilizing Shape Memory Alloy Superelasticity for Twisted String Actuator</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chanchan Xu, Shuai Dong, Xiaojie Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20342" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20342</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373ffac6f0235699aed45ecfc76ef062a0f91e5868ec23a1feee94ffde2e1eed_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373ffac6f0235699aed45ecfc76ef062a0f91e5868ec23a1feee94ffde2e1eed_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Design and Modeling of a Simple-Structured Continuously Variable Transmission Utilizing Shape Memory Alloy Superelasticity for Twisted String Actuator</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Contingency Model-based Control (CMC) for Communicationless Cooperative Collision Avoidance in Robot Swarms</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Georg Schildbach</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20391" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20391</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1dffba56daf9a83e73971cbad3df8a558f4f8bd39358567cc9675935db77fa4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1dffba56daf9a83e73971cbad3df8a558f4f8bd39358567cc9675935db77fa4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Contingency Model-based Control (CMC) for Communicationless Cooperative Collision Avoidance in Robot Swarms</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-24T07:41:26.000Z" itemprop="dateModified">Dec 24, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/cs_RO/20251215-20251221"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251215-20251221 (cs.RO)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/category/cssc"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">cs.SC</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-22" class="table-of-contents__link toc-highlight">2025-12-22</a></li><li><a href="#2025-12-23" class="table-of-contents__link toc-highlight">2025-12-23</a></li><li><a href="#2025-12-24" class="table-of-contents__link toc-highlight">2025-12-24</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/csro/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>