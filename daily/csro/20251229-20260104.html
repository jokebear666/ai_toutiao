<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_RO/20251229-20260104" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251229-20260104 (cs.RO) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/csro/20251229-20260104"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251229-20260104 (cs.RO) | AI头条"><meta data-rh="true" name="description" content="2025-12-29"><meta data-rh="true" property="og:description" content="2025-12-29"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/csro/20251229-20260104"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/csro/20251229-20260104" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/csro/20251229-20260104" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.RO","item":"https://jokebear666.github.io/ai_toutiao/daily/csro"},{"@type":"ListItem","position":3,"name":"20251229-20260104 (cs.RO)","item":"https://jokebear666.github.io/ai_toutiao/daily/csro/20251229-20260104"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.9ae66a68.css">
<script src="/ai_toutiao/assets/js/runtime~main.bd476283.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.b2da8712.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/arxiv-daily"><span title="Arxiv每日论文" class="linkLabel_WmDU">Arxiv每日论文</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csoh"><span title="cs.OH" class="categoryLinkLabel_W154">cs.OH</span></a><button aria-label="Expand sidebar category &#x27;cs.OH&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Collapse sidebar category &#x27;cs.RO&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cs_RO/20251215-20251221"><span title="20251215-20251221 (cs.RO)" class="linkLabel_WmDU">20251215-20251221 (cs.RO)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/csro/20251222-20251228"><span title="20251222-20251228 (cs.RO)" class="linkLabel_WmDU">20251222-20251228 (cs.RO)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/csro/20251229-20260104"><span title="20251229-20260104 (cs.RO)" class="linkLabel_WmDU">20251229-20260104 (cs.RO)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/daily/csro"><span>cs.RO</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251229-20260104 (cs.RO)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251229-20260104 (cs.RO)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-29">2025-12-29<a href="#2025-12-29" class="hash-link" aria-label="Direct link to 2025-12-29" title="Direct link to 2025-12-29" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251229] Safe Path Planning and Observation Quality Enhancement Strategy for Unmanned Aerial Vehicles in Water Quality Monitoring Tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [robotic perception and planning], [Interfered Fluid Dynamical System (IFDS), Model Predictive Control (MPC), Dynamic Flight Altitude Adjustment (DFAA)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuanshuang Fu, Qianyao Wang, Qihao Wang, Bonan Zhang, Jiaxin Zhao, Yiming Cao, Zhijun Li</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Electronic Science and Technology of China, North China University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21375" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21375</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a dynamic prediction model that transforms time-varying light and shadow disturbances (e.g., sun glint) into 3D virtual obstacles for path planning. 2. Introduces an improved IFDS algorithm combined with an MPC framework to generate smooth, safe, and dynamically feasible real-time trajectories for UAVs. 3. Designs a Dynamic Flight Altitude Adjustment (DFAA) mechanism to actively lower flight altitude in narrow observable areas, enhancing spatial resolution and data quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5582bc8dd27c45953b75b142df4da9d25f5164a9ed81f8842a846572fbb8a2f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5582bc8dd27c45953b75b142df4da9d25f5164a9ed81f8842a846572fbb8a2f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of UAV water quality monitoring being hindered by dynamic illumination disturbances like shadows and sun glint, which degrade spectral data. The proposed method actively plans safe flight paths by modeling disturbances as obstacles, using an improved IFDS and MPC for real-time trajectory optimization, and dynamically adjusting altitude to improve data quality. Simulation results show the method achieves a 98% obstacle avoidance success rate and increases effective observation data volume by approximately 27%.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Fast Navigation Through Occluded Spaces via Language-Conditioned Map Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robot navigation], [language-conditioned planning, map forecasting, Log-MPPI]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rahul Moorthy Mahesh, Oguzhan Goktug Poyrazoglu, Yukang Cao, Volkan Isler</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Minnesota (inferred from author &quot;Volkan Isler&quot;)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21398" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21398</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces PaceForecaster, a novel architecture that integrates co-pilot language instructions into local motion planning. 2. Predicts a forecasted map (Level-2) of occluded areas and an instruction-conditioned subgoal within it. 3. Demonstrates a 36% improvement in navigation performance by integrating PaceForecaster with a Log-MPPI controller.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e1b55661a25fc48b61cda55eadcd0e6f2f90bf3fac6514d8b38c5f3883d102b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e1b55661a25fc48b61cda55eadcd0e6f2f90bf3fac6514d8b38c5f3883d102b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the speed-safety trade-off in robot navigation in occluded environments by introducing PaceForecaster, a method that uses language instructions to forecast occluded map regions and generate conditioned subgoals. Integrating this with a Log-MPPI controller allows for more decisive and goal-directed planning. The approach improves navigation performance by 36% over a baseline that uses only the local sensor map.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Developing a Fundamental Diagram for Urban Air Mobility Based on Physical Experiments</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [Transportation Systems, Robotics], [Fundamental Diagram, Urban Air Mobility, Traffic Flow Theory, Drone Control, Physical Experiments]</p>
</li>
<li class="">
<p><strong>authors:</strong> Hang Zhou, Yuhui Zhai, Shiyu Shen, Yanfeng Ouyang, Xiaowei Shi, Xiaopeng</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Wisconsin-Madison, University of Illinois Urbana-Champaign, University of Wisconsin-Milwaukee</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21425" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21425</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/CATS-Lab/UAM-FD" target="_blank" rel="noopener noreferrer" class="">https://github.com/CATS-Lab/UAM-FD</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel framework integrating theory and physical experiments to construct a Fundamental Diagram for Urban Air Mobility traffic. 2. Develops and validates the first UAM Fundamental Diagram using real-world physical test data from a reduced-scale drone testbed. 3. Creates and releases the UAMTra2Flow dataset containing simulation and physical test trajectory data for UAM traffic analysis.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fcb69b60c91e88b8ff8fdb72d4cfe7a72f7f1c82ac3f0ed8afd4f7ca60297d6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fcb69b60c91e88b8ff8fdb72d4cfe7a72f7f1c82ac3f0ed8afd4f7ca60297d6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study addresses the lack of understanding of Urban Air Mobility (UAM) traffic flow by proposing a framework to construct its Fundamental Diagram (FD) through theoretical modeling and physical experiments using drones. The results show that classical ground traffic FD structures are applicable to UAM, but physical experiments reveal deviations from simulation, underscoring the need for experimental validation. The findings and a public dataset provide practical insights for future UAM system design.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] EVE: A Generator-Verifier System for Generative Policies</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robot learning], [generative policies, test-time compute, vision-language models, verifier agents, embodied control]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yusuf Ali, Gryphon Patlin, Karthik Kothuri, Muhammad Zubair Irshad, Wuwei Liang, Zsolt Kira</p>
</li>
<li class="">
<p><strong>institution:</strong> Georgia Institute of Technology, Toyota Research Institute, Symbotic Inc.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21430" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21430</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes EVE, a modular generator-verifier framework that improves pretrained generative visuomotor policies at test time without additional training. 2. Introduces a system of multiple zero-shot VLM-based verifier agents that propose action refinements, coupled with an action incorporator to fuse these suggestions. 3. Provides a systematic analysis and practical guidelines for designing scalable generator-verifier systems for embodied control through extensive ablations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a88f407920b463e24f81294c5ecb39e2cf549c376fd258eaa250ec03d1cdbed1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a88f407920b463e24f81294c5ecb39e2cf549c376fd258eaa250ec03d1cdbed1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem that generative visuomotor policies degrade under distribution shifts and lack recovery capabilities. It proposes EVE, a framework that uses additional inference-time compute to refine a frozen base policy&#x27;s actions via multiple zero-shot VLM verifiers. The method consistently improves task success rates across diverse manipulation tasks without any retraining.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Planetary Terrain Datasets and Benchmarks for Rover Path Planning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [path planning], [global path planning, digital terrain models, autonomous navigation, rover exploration, benchmark datasets]</p>
</li>
<li class="">
<p><strong>authors:</strong> Marvin Chancán, Avijit Banerjee, George Nikolakopoulos</p>
</li>
<li class="">
<p><strong>institution:</strong> Luleå University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21438" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21438</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/mchancan/PlanetaryPathBench" target="_blank" rel="noopener noreferrer" class="">https://github.com/mchancan/PlanetaryPathBench</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the first two large, space mission-derived planetary benchmark datasets for rover path planning (MarsPlanBench and MoonPlanBench). 2. Establishes a unified framework for evaluating both classical and learning-based path planning algorithms on these planetary terrains. 3. Provides new empirical insights into algorithm performance, showing classical methods achieve high success rates on challenging terrains while learning-based methods struggle to generalize.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbb45e523b7fc6a07c19646449f84300acdad583bbecaad64993aae93fddcf33_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbb45e523b7fc6a07c19646449f84300acdad583bbecaad64993aae93fddcf33_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of standardized planetary datasets and benchmarks for rover path planning by introducing MarsPlanBench and MoonPlanBench, two large datasets derived from high-resolution digital terrain models of Mars and the Moon. The authors evaluate classical and learning-based path planning algorithms in a unified framework on these new benchmarks. The key finding is that classical algorithms achieve very high success rates (up to 100%) on challenging planetary terrains, explaining their practical use by agencies like NASA, while learning-based models still face generalization difficulties in these domains.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Spatiotemporal Tubes for Probabilistic Temporal Reach-Avoid-Stay Task in Uncertain Dynamic Environment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [control theory], [spatiotemporal tube, probabilistic safety, real-time control, uncertain dynamic environment, reach-avoid-stay]</p>
</li>
<li class="">
<p><strong>authors:</strong> Siddhartha Upadhyay, Ratnangshu Das, Pushpak Jagtap</p>
</li>
<li class="">
<p><strong>institution:</strong> Robert Bosch Centre for Cyber-Physical Systems, Indian Institute of Science (IISc), Bengaluru</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21497" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21497</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Extension of the Spatiotemporal Tube (STT) framework to handle Probabilistic Temporal Reach-Avoid-Stay (PrT-RAS) tasks in environments with time-varying uncertain obstacles. 2. Development of a real-time tube synthesis procedure that provides formal probabilistic safety guarantees. 3. Derivation of a closed-form, model-free, approximation-free, and optimization-free control law that confines the system within the tube for efficient real-time execution.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90c986ba71dbe6e082ded217bbbe89ccda980dde23fb1eaaed32b9bf8e4aa780_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90c986ba71dbe6e082ded217bbbe89ccda980dde23fb1eaaed32b9bf8e4aa780_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an extension of the Spatiotemporal Tube framework to solve Probabilistic Temporal Reach-Avoid-Stay tasks in uncertain dynamic environments. The method synthesizes a time-varying safe tube online and provides a closed-form control law to keep the system inside it, offering formal probabilistic safety and task completion guarantees. The approach is validated through simulations and hardware experiments on various robotic platforms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] A Novel Robotic Variable Stiffness Mechanism Based on Helically Wound Structured Electrostatic Layer Jamming</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [soft robotics, variable stiffness actuators], [electrostatic layer jamming, helical winding, variable stiffness robotic finger, voltage-driven control]</p>
</li>
<li class="">
<p><strong>authors:</strong> Congrui Bai, Zhenting Du, Weibang Bai</p>
</li>
<li class="">
<p><strong>institution:</strong> ShanghaiTech University, King&#x27;s College London</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21534" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21534</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel Helically Wound Structured Electrostatic Layer Jamming (HWS-ELJ) mechanism for variable stiffness., 2. Demonstrates that the helical configuration provides exponentially greater stiffness adjustment and a reduced footprint compared to conventional planar designs., 3. Develops and validates a functional robotic finger prototype that confirms the feasibility of voltage-driven stiffness modulation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/287c4eb37a5ab5af01f325e34fecea5969ff071b0b31c3e96ae601a3ad2f8fad_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/287c4eb37a5ab5af01f325e34fecea5969ff071b0b31c3e96ae601a3ad2f8fad_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a novel variable stiffness mechanism for robotics called Helically Wound Structured Electrostatic Layer Jamming (HWS-ELJ). It uses a helical configuration and electrostatic attraction to achieve tunable stiffness, offering superior performance and a smaller footprint than traditional planar designs. The work is validated through experiments and a functional robotic finger prototype, confirming its feasibility.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] World-Coordinate Human Motion Retargeting via SAM 3D Body</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [human motion capture and retargeting], [SAM 3D Body, Momentum HumanRig, contact-aware optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhangzheng Tum, Kailun Su, Shaolong Zhu, Yukun Zheng</p>
</li>
<li class="">
<p><strong>institution:</strong> Dalian University of Technology, Shenzhen University, Harbin Institute of Technology, Shenzhen</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21573" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21573</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a lightweight framework using a frozen SAM 3D Body backbone and Momentum HumanRig representation for world-coordinate human motion recovery from monocular video. 2. Introduces temporal consistency enforcement via identity/scale locking and efficient sliding-window smoothing in a low-dimensional latent space. 3. Recovers physically plausible global root trajectories using a differentiable soft foot-ground contact model and contact-aware optimization for reliable robot retargeting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ddf0876edffc654fb4ec059aced2eb58f78ddcb89551be7a907e2f5bdee145c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ddf0876edffc654fb4ec059aced2eb58f78ddcb89551be7a907e2f5bdee145c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a lightweight framework for recovering world-coordinate human motion from monocular video and retargeting it to a humanoid robot. The method leverages SAM 3D Body as a frozen backbone, enforces temporal consistency, smooths predictions, and uses a contact model for plausible global trajectories. Results show the method produces stable, robot-ready motion from monocular input.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [novel view synthesis], [diffusion models, 3D Gaussian Splatting, auto-regressive restoration, context-aware inpainting, view synthesis]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhiyuan Liu, Daocheng Fu, Pinlong Cai, Lening Wang, Ying Liu, Yilong Ren, Botian Shi, Jianqiang Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Shanghai Artificial Intelligence Laboratory, Beihang University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21618" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21618</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SymDrive, a unified diffusion-based framework for joint high-quality rendering and scene editing in autonomous driving simulation. 2. Introduces a Symmetric Auto-regressive Online Restoration paradigm for recovering fine-grained details and generating consistent lateral views. 3. Leverages the restoration capability for a training-free harmonization mechanism, treating vehicle insertion as context-aware inpainting to ensure lighting and shadow consistency.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> SymDrive addresses the challenge of creating high-fidelity and controllable 3D driving simulators by proposing a unified diffusion-based framework. It uses a symmetric auto-regressive online restoration method to enhance novel-view synthesis and a training-free harmonization mechanism for realistic vehicle insertion. The method achieves state-of-the-art performance in both rendering quality and scene editing realism.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] AstraNav-Memory: Contexts Compression for Long Memory</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [memory &amp; caching], [visual context compression, image-centric memory, lifelong embodied navigation, DINOv3, Qwen2.5-VL]</p>
</li>
<li class="">
<p><strong>authors:</strong> Botao Ren, Junjun Hu, Xinda Xue, Minghua Luo, Jintao Chen, Haochen Bai, Liangliang You, Mu Xu</p>
</li>
<li class="">
<p><strong>institution:</strong> Alibaba Group (Amap), Tsinghua University, Peking University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21627" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21627</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://astra-amap.github.io/AstraNav-Memory.github.io/" target="_blank" rel="noopener noreferrer" class="">https://astra-amap.github.io/AstraNav-Memory.github.io/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed an image-centric memory framework for lifelong embodied navigation that uses an efficient visual context compression module to achieve long-term implicit memory. 2. Introduced a configurable visual tokenizer built on a ViT backbone with frozen DINOv3 features and lightweight PixelUnshuffle+Conv blocks, enabling high compression rates (e.g., 16x) to expand context capacity. 3. Demonstrated state-of-the-art navigation performance on benchmarks (GOAT-Bench, HM3D-OVON), showing improved exploration in unfamiliar environments and shorter paths in familiar ones, with ablation studies validating the efficiency-accuracy trade-off.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce3c5628c478fa221373bddffcb2cb72bd35c261cc9b555152b34063fccbb9f2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce3c5628c478fa221373bddffcb2cb72bd35c261cc9b555152b34063fccbb9f2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of building long-term memory for lifelong embodied navigation. It proposes AstraNav-Memory, an image-centric framework that compresses visual contexts using a configurable tokenizer to efficiently store hundreds of images, coupled with a Qwen2.5-VL-based navigation policy. The method achieves state-of-the-art performance, balancing efficiency and accuracy for scalable lifelong agents.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [swarm intelligence], [Ant Colony Optimization, structural prior, load-aware objective, overlap suppression, multi-robot path planning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zikun Guo, Adeyinka P. Adedigba, Rammohan Mallipeddi, Heoncheol Lee</p>
</li>
<li class="">
<p><strong>institution:</strong> Kyungpook National University, Kumoh National Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21654" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21654</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a structure-induced exploration framework that integrates structural priors into ACO initialization to constrain the search space. 2. Designs a pheromone update rule that emphasizes structurally meaningful connections and incorporates a load-aware objective to balance total travel distance with individual robot workload. 3. Introduces an explicit overlap suppression strategy to ensure distinct and balanced task allocation across the robot team.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca4635b64758650f78e762004770f2a7ac8eb62cd36aa2db98d90af82d3f6eae_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca4635b64758650f78e762004770f2a7ac8eb62cd36aa2db98d90af82d3f6eae_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of scalable and balanced multi-robot path planning. It proposes a new framework that integrates structural priors into Ant Colony Optimization, along with a load-aware objective and overlap suppression, to improve route compactness, stability, and workload distribution. The method demonstrates consistent improvements over metaheuristic baselines and offers a scalable solution for applications like logistics and search-and-rescue.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] MAction-SocialNav: Multi-Action Socially Compliant Navigation via Reasoning-enhanced Prompt Tuning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [socially compliant navigation], [meta-cognitive prompt, vision language model, multi-action generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zishuo Wang, Xinyu Zhang, Zhuonan Liu, Tomohito Kawabata, Daeun Song, Xuesu Xiao, Ling Xiao</p>
</li>
<li class="">
<p><strong>institution:</strong> Hokkaido University, George Mason University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21722" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21722</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes MAction-SocialNav, a vision language model for socially compliant navigation that generates multiple plausible actions to handle real-world ambiguity. 2. Introduces a novel meta-cognitive prompt (MCP) method to enhance the model&#x27;s reasoning capability. 3. Curates a new multi-action socially compliant navigation dataset with diverse conditions and dual human annotations, and designs five evaluation metrics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08864b9cd92dd7e9f4041ef22b9caf54fe559ad301e22a19b24957d3cb331538_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08864b9cd92dd7e9f4041ef22b9caf54fe559ad301e22a19b24957d3cb331538_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of action ambiguity in socially compliant robot navigation by proposing MAction-SocialNav, an efficient vision language model that generates multiple plausible actions per scenario using a novel meta-cognitive prompt tuning method. The method is evaluated on a newly curated dataset and shows superior decision quality, safety alignment, and real-time efficiency compared to large language models like GPT-4o and Claude.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] HELP: Hierarchical Embodied Language Planner for Household Tasks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [embodied agent, hierarchical planning, large language model, household tasks, open source LLM]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alexandr V. Korchemnyi, Anatoly O. Onishchenko, Eva A. Bakaeva, Alexey K. Kovalev, Aleksandr I. Panov</p>
</li>
<li class="">
<p><strong>institution:</strong> MIRAI, Cognitive AI Systems Lab</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21723" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21723</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Hierarchical Embodied Language Planner (HELP) architecture using multiple LLM-based agents for decomposing and grounding natural language instructions. 2. Demonstrates the approach on a real-world household task using an embodied agent. 3. Focuses on the use of relatively small, open-source LLMs to enable autonomous deployment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of planning for embodied agents following ambiguous natural language instructions in complex environments. It proposes HELP, a hierarchical planner using multiple LLM-based agents to decompose high-level instructions into grounded, executable subtasks. The method is evaluated on a household task with a real robot, showing the feasibility of using smaller, open-source LLMs for autonomous operation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] MoonBot: Modular and On-Demand Reconfigurable Robot Toward Moon Base Construction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [space robotics], [modular robot, reconfigurable robot, lunar construction, field demonstration, connector design]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kentaro Uno, Elian Neppel, Gustavo H. Diaz, Ashutosh Mishra, Shamistan Karimov, A. Sejal Jain, Ayesha Habib, Pascal Pama, Hazal Gozbasi, Shreya Santra, Kazuya Yoshida</p>
</li>
<li class="">
<p><strong>institution:</strong> Space Robotics Laboratory (SRL), Department of Aerospace Engineering, Graduate School of Engineering, Tohoku University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21853" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21853</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MoonBot, a modular and reconfigurable robotic system designed for lunar payload constraints and task adaptability. 2. Details the system&#x27;s design and development, including a field demonstration simulating lunar infrastructure tasks like civil engineering and component deployment. 3. Systematically summarizes lessons learned, particularly on connector design, to inform future modular robotic systems for lunar missions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7ceec836e29be790b6ca39392714dc64e19923b0842210e75988ad1c5fdeeb2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7ceec836e29be790b6ca39392714dc64e19923b0842210e75988ad1c5fdeeb2_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces MoonBot, a modular and reconfigurable robot designed for constructing lunar bases under strict mass constraints. It details the robot&#x27;s design and validates its concept through field demonstrations of simulated construction tasks. The work concludes with lessons learned, especially regarding connector design, to guide future lunar robotic systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Optimal Trajectory Planning for Orbital Robot Rendezvous and Docking</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [space robotics], [trajectory planning, nonlinear optimization, dynamic keep-out sphere, ON/OFF thrusters, rendezvous and docking]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kenta Iizuka, Akiyoshi Uchida, Kentaro Uno, Kazuya Yoshida</p>
</li>
<li class="">
<p><strong>institution:</strong> Tohoku University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21882" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21882</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A trajectory planning method based on nonlinear optimization for close-range rendezvous with a tumbling target. 2. The introduction of a dynamic keep-out sphere that adapts to approach conditions for safer access. 3. A control strategy to reproduce the optimized trajectory using discrete ON/OFF thrusters, considering practical implementation constraints.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0619d27a582b3a86da2ec2f90d52c42f5327490ba9737fc6b241e03569d2be3b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0619d27a582b3a86da2ec2f90d52c42f5327490ba9737fc6b241e03569d2be3b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of safely approaching a tumbling space debris object for capture. It proposes a trajectory planning method using nonlinear optimization with a dynamic safety boundary and a control strategy for ON/OFF thrusters. The method enables closer and safer access as a preliminary step for robotic capture.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Online Inertia Parameter Estimation for Unknown Objects Grasped by a Manipulator Towards Space Applications</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [robotics, dynamics and control], [inertia parameter estimation, online identification, momentum conservation, floating-base robots, space robotics]</p>
</li>
<li class="">
<p><strong>authors:</strong> Akiyoshi Uchida, Antonine Richard, Kentaro Uno, Miguel Olivares-Mendez, Kazuya Yoshida</p>
</li>
<li class="">
<p><strong>institution:</strong> Tohoku University, University of Luxembourg</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21886" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21886</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Extended an existing online inertia identification method to be applicable to floating-base robots by incorporating momentum conservation. 2. Validated the proposed method through numerical simulations for space applications like on-orbit servicing. 3. Demonstrated accurate estimation of unknown object inertia parameters during manipulation in a simulated microgravity environment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56413666705f0b959b1c7926a7846dc2d7421e2c52ee8ac46620538562104026_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56413666705f0b959b1c7926a7846dc2d7421e2c52ee8ac46620538562104026_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of estimating the inertia parameters of an unknown object grasped by a manipulator on a free-floating space robot. The authors extend an existing online identification method by incorporating momentum conservation to handle the floating base. Numerical simulations validate the method, showing accurate identification and highlighting its potential for on-orbit servicing missions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [visual navigation], [world model, future frame projection, 4-dof uav, long-horizon visual generation, aerial navigation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Weichen Zhang, Peizhi Tang, Xin Zeng, Fanhang Man, Shiquan Yu, Zichao Dai, Baining Zhao, Hongjin Chen, Yu Shang, Wei Wu, Chen Gao, Xinlei Chen, Xin Wang, Yong Li, Wenwu Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21887" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21887</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes ANWM, an aerial navigation world model for predicting future visual observations to incorporate high-level semantics into UAV path planning. 2. Introduces a physics-inspired Future Frame Projection (FFP) module to provide coarse geometric priors and mitigate uncertainty in long-distance visual generation. 3. Demonstrates superior performance in long-distance visual forecasting and improves UAV navigation success rates in large-scale 3D environments.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02f69e3df37002aba4354016667950073739b574c3c8044ad15f65d9721e62db_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02f69e3df37002aba4354016667950073739b574c3c8044ad15f65d9721e62db_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes ANWM, an aerial navigation world model that predicts future visual observations for UAVs using a novel Future Frame Projection module. It addresses the challenges of complex 4-DoF action spaces and long-horizon visual generation. The model outperforms existing methods in visual forecasting and enhances navigation success in large-scale environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Flexible Multitask Learning with Factorized Diffusion Policy</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [diffusion policy, modular architecture, multitask learning, imitation learning, mixture-of-experts]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chaoqi Liu, Haonan Chen, Sigmund H. Høeg, Shaoxiong Yao, Yunzhu Li, Kris Hauser, Yilun Du</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Illinois at Urbana-Champaign, Harvard University, Norwegian University of Science and Technology, Columbia University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21898" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21898</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a novel modular diffusion policy framework (FDP) that factorizes complex action distributions into a composition of specialized diffusion models. 2. Proposes continuous score aggregation via an observation-conditioned router for stable training and clear component specialization, addressing issues in standard MoE. 3. Demonstrates that the modular structure enables flexible policy adaptation to new tasks and mitigates catastrophic forgetting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b261f496908cd892964760d9f52edb76c57a6126f2c6a9969b7e36d9d43b048e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b261f496908cd892964760d9f52edb76c57a6126f2c6a9969b7e36d9d43b048e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of multitask imitation learning in robotics, where complex action distributions are difficult to model. It proposes a Factorized Diffusion Policy (FDP) that decomposes the policy into specialized diffusion components and composes them via a router. The method outperforms baselines in simulation and real-world manipulation and supports flexible adaptation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [robotic vision], [stereo vision, vision-language-action models, geometric-semantic fusion, depth estimation, robotic manipulation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shengliang Deng, Mi Yan, Yixin Zheng, Jiayi Su, Wenhao Zhang, Xiaoguang Zhao, Heming Cui, Zhizheng Zhang, He Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University, The University of Hong Kong, Institute of Automation, Chinese Academy of Sciences, Beijing Academy of Artificial Intelligence</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21970" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21970</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://shengliangd.github.io/StereoVLA-Webpage" target="_blank" rel="noopener noreferrer" class="">https://shengliangd.github.io/StereoVLA-Webpage</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed StereoVLA, a novel Vision-Language-Action model that leverages stereo vision for enhanced spatial perception. 2. Introduced a Geometric-Semantic Feature Extraction module to fuse geometric cues from stereo differences with semantic features from a monocular view. 3. Designed an auxiliary Interaction-Region Depth Estimation task to improve spatial understanding and accelerate model convergence.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fbd07a25a843958488215748e8162f92542370bba173316326de44d4be3c65f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fbd07a25a843958488215748e8162f92542370bba173316326de44d4be3c65f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of single-view input in Vision-Language-Action (VLA) models for robotic manipulation by introducing StereoVLA, which utilizes stereo vision. The core method involves a novel module to extract and fuse geometric and semantic features, along with an auxiliary depth estimation task. Experiments show the model significantly outperforms baselines in stereo-based tasks and is robust to camera pose variations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Bab_Sak Robotic Intubation System (BRIS): A Learning-Enabled Control Framework for Safe Fiberoptic Endotracheal Intubation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [medical robotics], [robotic intubation, closed-loop control, shape sensing, depth estimation, human-in-the-loop]</p>
</li>
<li class="">
<p><strong>authors:</strong> Saksham Gupta, Sarthak Mishra, Arshad Ayub, Kamran Farooque, Spandan Roy, Babita Gupta</p>
</li>
<li class="">
<p><strong>institution:</strong> International Institute of Information Technology Hyderabad (IIIT-H), All India Institute of Medical Sciences, New Delhi (AIIMS)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21983" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21983</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A compact, integrated robotic platform (BRIS) for fiberoptic-guided intubation, featuring a steerable bronchoscope, an independent tube advancement mechanism, and a camera-augmented mouthpiece. 2. A learning-enabled closed-loop control framework that uses real-time shape sensing to map joystick inputs to precise bronchoscope tip motion, ensuring stable teleoperation despite tendon nonlinearities. 3. The use of monocular endoscopic depth estimation to classify airway regions and provide anatomy-aware guidance for safe endotracheal tube positioning relative to the carina.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c843dca48b820a7b40108d200bec7f3af89493cac93374b9ac37ec6874acfd9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c843dca48b820a7b40108d200bec7f3af89493cac93374b9ac37ec6874acfd9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents the Bab Sak Robotic Intubation System (BRIS), a human-in-the-loop platform designed to assist with safe fiberoptic endotracheal intubation. It integrates a learning-enabled control framework for stable scope navigation and uses monocular depth estimation for anatomy-aware tube placement guidance. The system was validated on airway mannequins, demonstrating reliable performance as a step toward safer and more consistent robotic airway management.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-30">2025-12-30<a href="#2025-12-30" class="hash-link" aria-label="Direct link to 2025-12-30" title="Direct link to 2025-12-30" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251230] HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [FPGA, HLS, Point Cloud, Model Compression, Fixed-Point]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amur Saqib Pal, Muhammad Mohsin Ghaffar, Faisal Shafait, Christian Weis, Norbert Wehn</p>
</li>
<li class="">
<p><strong>institution:</strong> National University of Sciences and Technology (Pakistan), RPTU Kaiserslautern-Landau (Germany)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22139" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22139</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/dll-ncai/HLS4PC" target="_blank" rel="noopener noreferrer" class="">https://github.com/dll-ncai/HLS4PC</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed HLS4PC, a parameterizable HLS framework for accelerating point-based 3D point cloud models on FPGA. 2. Introduced PointMLP-Lite, a 4x less complex model variant created via hardware-aware compression techniques (URS, quantization, pruning, fusion). 3. Demonstrated FPGA acceleration achieving 3.56x higher throughput than prior work and outperforming GPU/CPU implementations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21433bfda0767bbdfcee46f13fb3acd9373d13bb741d87a755643767c9ad74f9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21433bfda0767bbdfcee46f13fb3acd9373d13bb741d87a755643767c9ad74f9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of real-time 3D point cloud processing by proposing HLS4PC, a parameterizable FPGA acceleration framework. The method combines algorithmic optimizations and hardware-aware model compression to create an efficient fixed-point implementation, which significantly outperforms previous accelerators and GPU/CPU baselines in throughput.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Joint UAV-UGV Positioning and Trajectory Planning via Meta A3C for Reliable Emergency Communications</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Meta-A3C, UAV-UGV deployment, trajectory planning, road graph, Markov Decision Process]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ndagijimana Cyprien, Mehdi Sookhak, Hosein Zarini, Chandra N Sekharan, Mohammed Atiquzzaman</p>
</li>
<li class="">
<p><strong>institution:</strong> Texas A&amp;M University-Corpus Christi, University of Oklahoma</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22187" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22187</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a joint UAV-UGV positioning and trajectory planning framework to guarantee optimal QoS for ground users in disaster scenarios. 2. Introduces a road graph model to constrain and direct UGV mobility according to real-world road network constraints. 3. Formulates the problem as an MDP and develops a novel Meta-A3C algorithm for rapid adaptation to new environments and dynamic conditions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c161f6205934be4fb52432c71ffd2a960c50e866e6e382c489af38b90ca39b44_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c161f6205934be4fb52432c71ffd2a960c50e866e6e382c489af38b90ca39b44_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of ensuring good QoS with minimal UAVs in disaster recovery by proposing a joint UAV-UGV deployment framework. The core method involves modeling UGV mobility with a road graph and solving the optimization problem using a novel Meta-A3C reinforcement learning algorithm. The results show that Meta-A3C outperforms baseline methods, achieving higher throughput and faster execution while meeting QoS requirements.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On Extending Semantic Abstraction for Efficient Search of Hidden Objects</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [semantic abstraction, relevancy maps, 3D localization, hidden objects, unstructured search]</p>
</li>
<li class="">
<p><strong>authors:</strong> Tasha Pais, Nikhilesh Belulkar</p>
</li>
<li class="">
<p><strong>institution:</strong> Columbia University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22220" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22220</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Extends the Semantic Abstraction framework to the novel domain of localizing hidden (occluded) objects. 2. Proposes using historical placement data to efficiently guide the unstructured search for hidden objects. 3. Demonstrates a model that can accurately identify a hidden object&#x27;s complete 3D location faster than a naive random search.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb1e039b06f845acfd3a644354907fc35eec78f060cf605799b7607c8a20ba8_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb1e039b06f845acfd3a644354907fc35eec78f060cf605799b7607c8a20ba8_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of efficiently finding hidden or lost objects by extending the Semantic Abstraction framework. The method uses 2D VLM relevancy maps as abstract object representations to learn 3D localization and leverages historical placement data to optimize the search. The result is a model that can locate hidden objects significantly faster than random search, aiming to improve the capabilities of household robots.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [object detection], [RGB-LWIR fusion, multispectral imagery, YOLO, adaptive framework, illumination conditions]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aahan Sachdeva, Dhanvinkumar Ganeshkumar, James E. Gallagher, Tyler Treat, Edward J. Oughton</p>
</li>
<li class="">
<p><strong>institution:</strong> George Mason University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22263" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22263</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An adaptive framework that dynamically selects the optimal RGB-LWIR fusion ratio and detection model based on real-time illumination conditions. 2. Creation of a comprehensive dataset and model set, training 33 YOLO models on over 22,000 annotated images across three light levels with eleven fusion ratios. 3. Demonstrated significant performance improvements over RGB-only and thermal-only baselines, particularly in full-light and dim-light conditions, enhancing detection reliability for autonomous systems.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitations of RGB and thermal-only object detection in variable lighting by proposing an adaptive framework that fuses RGB and LWIR video streams at multiple ratios and selects the best model for the current illumination. The method, evaluated on a large dataset, showed that optimized fusion models significantly outperformed baseline models in full and dim light, improving detection confidence and reliability for autonomous robotic vision.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [embodied navigation], [interactive navigation, active dialog, benchmark dataset]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wensi Huang, Shaohao Zhu, Meng Wei, Jinming Xu, Xihui Liu, Hanqing Wang, Tai Wang, Feng Zhao, Jiangmiao Pang</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai AI Laboratory, University of Science and Technology of China, Zhejiang University, The University of Hong Kong</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22342" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22342</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://0309hws.github.io/VL-LN.github.io/" target="_blank" rel="noopener noreferrer" class="">https://0309hws.github.io/VL-LN.github.io/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes the Interactive Instance Object Navigation (IION) task, which requires agents to navigate and resolve ambiguity through active dialog. 2. Introduces the VL-LN benchmark, a large-scale, automatically generated dataset with over 41k dialog-augmented trajectories for training and evaluation. 3. Demonstrates that a navigation model trained on VL-LN achieves significant improvements over baselines, validating the benchmark&#x27;s effectiveness.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/963768d906e1031332e12b4d315a28062a7236f00bbf7e3b88c636a5d3422ff7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/963768d906e1031332e12b4d315a28062a7236f00bbf7e3b88c636a5d3422ff7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the gap in embodied navigation where real-world instructions are often vague. It proposes a new task (IION) and a large-scale benchmark (VL-LN) for training and evaluating agents that can navigate and ask clarifying questions. The results show that models trained with this dialog-enabled approach significantly outperform baseline methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [Heterogeneous Computing, ROS 2, FreeRTOS, PID Control, AWS IoT]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amro Gamar, Ahmed Abduljalil, Alargam Mohammed, Ali Elhenidy, Abeer Tawakol</p>
</li>
<li class="">
<p><strong>institution:</strong> Mansoura University, Egypt</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22408" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22408</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a heterogeneous computing architecture combining a Raspberry Pi 5 with ROS 2 for high-level AI perception/path planning and an ESP32 with FreeRTOS for real-time motor control. 2. Implemented a low-latency, reliable communication link between the ROS 2 host and the embedded controller to ensure system coordination. 3. Enhanced system reliability through deterministic PID-based motor control with static memory allocation and integrated AWS IoT monitoring with a firmware-level motor shutdown failsafe.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c49a4266e78db3945d26829ffd5e030ccc9fa68be1c543cca653fc81df446dfe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c49a4266e78db3945d26829ffd5e030ccc9fa68be1c543cca653fc81df446dfe_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents the development of an autonomous delivery robot using a unified, multi-disciplinary approach. It employs a heterogeneous computing architecture to handle AI-based navigation on a Raspberry Pi and real-time motor control on an ESP32, addressing challenges like algorithm optimization and inter-processor communication. The result is a robust, operational system demonstrated to be capable of real-world deployment.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Emergence of Human to Robot Transfer in Vision-Language-Action Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robot learning], [vision-language-action models, human-to-robot transfer, co-training, emergent capability, embodiment-agnostic representations]</p>
</li>
<li class="">
<p><strong>authors:</strong> Simar Kareer, Karl Pertsch, James Darpinian, Judy Hoffman, Danfei Xu, Sergey Levine, Chelsea Finn, Suraj Nair</p>
</li>
<li class="">
<p><strong>institution:</strong> Physical Intelligence, Georgia Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22414" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22414</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a simple co-training recipe for training Vision-Language-Action (VLA) models on a mix of human video and robot data. 2. Discovers and demonstrates that the ability to transfer skills from human videos to robot policies is an emergent property that appears with sufficient scale and diversity in robot pre-training data. 3. Provides analysis suggesting the emergent capability arises from the model learning embodiment-agnostic representations through diverse pre-training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c14383fd88b5d16af8c13ba59202c2143ecd1d25b65a10248ec614491595a50_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c14383fd88b5d16af8c13ba59202c2143ecd1d25b65a10248ec614491595a50_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates whether Vision-Language-Action (VLA) models can learn to transfer skills from human videos to robots, a task that is typically challenging. The authors propose a simple co-training method and find that this human-to-robot transfer capability emerges as a property of scale when the model is pre-trained on a sufficiently large and diverse dataset of robot tasks. Their experiments show that with diverse pre-training, leveraging human data can nearly double performance on tasks seen only in human videos.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Bugs with Features: Vision-Based Fault-Tolerant Collective Motion Inspired by Nature</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [swarm robotics], [collective motion, visual perception, fault-tolerance, intermittent locomotion, distance estimation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Peleg Shefi, Amir Ayali, Gal A. Kaminka</p>
</li>
<li class="">
<p><strong>institution:</strong> Bar Ilan University, Tel Aviv University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22448" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22448</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A robust distance estimation method for vision-based swarms that combines perceived horizontal and vertical sizes of neighbors. 2. The introduction of intermittent locomotion as a mechanism for reliably detecting faulty peers that disrupt swarm motion. 3. A fault-avoidance strategy that is robust to errors in classifying robots as faulty, improving swarm resilience in both Avoid-Attract and Alignment-based models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/456a9beab88e762dd3d3dfcafbb4d0007f77b2154cd9a7b657dcdf5df21407db_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/456a9beab88e762dd3d3dfcafbb4d0007f77b2154cd9a7b657dcdf5df21407db_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the brittleness of artificial swarms using vision by proposing two bio-inspired mechanisms. It introduces a robust visual distance estimation method and an intermittent locomotion strategy for fault detection and avoidance. Extensive simulations show these techniques dramatically improve swarm resilience across different collective motion models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [motion generation and editing], [residual vector quantization (RVQ), pose code, transformer, text-to-motion, motion editing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sukhyun Jeong, Yong-Hoon Choi</p>
</li>
<li class="">
<p><strong>institution:</strong> Kwangwoon University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22464" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22464</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/jayze3736/PGR2M" target="_blank" rel="noopener noreferrer" class="">https://github.com/jayze3736/PGR2M</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a hybrid motion representation (PGR²M) that augments interpretable pose codes with residual codes learned via RVQ to capture both coarse structure and fine-grained details. 2. Introduces a pose-guided RVQ tokenizer and a two-stage Transformer architecture (base and refine) for generating and refining motion from text. 3. Demonstrates improved performance in generation and editing tasks over baselines through quantitative metrics and user studies, while preserving semantic alignment and editability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fc4d2cdfd610913fc29db703161a253780380e59619b49c5b160c01670eabac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fc4d2cdfd610913fc29db703161a253780380e59619b49c5b160c01670eabac_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the limitation of pose-code-based motion generation in capturing subtle temporal dynamics by introducing PGR²M, a hybrid representation combining interpretable pose codes with residual codes via RVQ. The method uses a two-stage Transformer to generate pose codes and then refine them with residual details, conditioned on text. Experiments show it outperforms baselines in both generation and editing while enabling intuitive, structure-preserving motion edits.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Asymmetric Friction in Geometric Locomotion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [geometric mechanics, robotics], [Finsler metrics, sub-Finslerian, motility map, asymmetric friction, geometric locomotion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ross L. Hatton, Yousef Salaman, Shai Revzen</p>
</li>
<li class="">
<p><strong>institution:</strong> Oregon State University (inferred from author Ross L. Hatton&#x27;s affiliation)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22484" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22484</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Extends the geometric locomotion framework from systems with symmetric (Riemannian) friction to systems with asymmetric (Finsler) friction. 2. Demonstrates that the sub-Riemannian construction of the motility map naturally generalizes to a sub-Finslerian approach. 3. Identifies system properties analogous to constraint curvature that characterize motion capabilities under asymmetric friction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7325c56bd6ecd61b2e5288756589f2b41cc9256cdcdb37b465ecd8b058f30c3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7325c56bd6ecd61b2e5288756589f2b41cc9256cdcdb37b465ecd8b058f30c3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper extends geometric models of locomotion to systems with asymmetric friction, where drag coefficients differ for forward and backward motion. The authors propose a sub-Finslerian framework to construct the system&#x27;s motility map, generalizing the traditional sub-Riemannian approach. The main conclusion is that this new framework allows for the characterization of locomotion capabilities in a broader class of systems with non-reciprocal environmental interactions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Topology-Preserving Scalar Field Optimization for Boundary-Conforming Spiral Toolpaths on Multiply Connected Freeform Surfaces</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [computer-aided manufacturing (CAM)], [spiral toolpath planning, scalar field optimization, topology-preserving deformation, conformal slit mapping, boundary-conforming]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shen Changqing, Xu Bingzhou, Qi Bosong, Zhang Xiaojian, Yan Sijie, Ding Han</p>
</li>
<li class="">
<p><strong>institution:</strong> Huazhong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22502" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22502</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a strategy to enforce boundary conformity and eliminate zero-gradient singularities in scalar-field-based toolpath optimization for multiply connected surfaces. 2. Reformulates the optimization as a topology-preserving mesh deformation with boundary-synchronous updates to achieve globally optimized spacing and smooth transitions. 3. Demonstrates significant improvements in machining efficiency, scallop-height uniformity, and vibration reduction compared to a state-of-the-art method.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8db268a37d6b76e48aa2571b6519b562d7e55bfc3d4e8f7c07120bc4dbfc1315_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8db268a37d6b76e48aa2571b6519b562d7e55bfc3d4e8f7c07120bc4dbfc1315_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of generating continuous, boundary-conforming spiral toolpaths for ball-end milling on complex freeform surfaces. The proposed method uses conformal slit mapping to create an initial singularity-free scalar field and then optimizes it via a topology-preserving mesh deformation process. Experimental results show the approach increases machining efficiency by over 14%, improves surface finish uniformity, and reduces vibrations compared to existing methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [robotic manipulation], [Vision-Language-Action (VLA), object-centric grounding, geometric grounding, perception module, clutter robustness]</p>
</li>
<li class="">
<p><strong>authors:</strong> Khoa Vo, Taisei Hanyu, Yuki Ikebe, Trong Thang Pham, Nhat Chung, Minh Nhat Vu, Duy Nguyen Ho Minh, Anh Nguyen, Anthony Gunderman, Chase Rainwater, Ngan Le</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Arkansas, National University of Singapore, TU Wien, Max Planck Research School for Intelligent Systems / University of Stuttgart, University of Liverpool</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22519" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22519</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://uark-aicv.github.io/OBEYED" target="_blank" rel="noopener noreferrer" class="">https://uark-aicv.github.io/OBEYED</a> VLA</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes OBEYED-VLA, a framework that explicitly disentangles perceptual grounding from action reasoning in VLA models. 2. Introduces a perception module with a VLM-based object-centric grounding stage and a complementary geometric grounding stage to create task-conditioned, object-centric, and geometry-aware observations. 3. Demonstrates substantial real-world robustness improvements in cluttered scenarios, including handling distractors, absent targets, background changes, and unseen objects.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7643cf1feb83697be4818d833bf42eecb6c488ed1ac9917cb58299ddd69c4cfe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7643cf1feb83697be4818d833bf42eecb6c488ed1ac9917cb58299ddd69c4cfe_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies that monolithic Vision-Language-Action (VLA) models suffer from poor grounding in cluttered real-world scenes. To solve this, it proposes OBEYED-VLA, which adds an explicit perception module for object-centric and geometric grounding before action prediction. The method significantly improves robustness on a real-world tabletop robot across various challenging clutter and distraction scenarios.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [embodied ai / robot learning], [vision-language-action models, benchmark, generalization, robustness, structured task design]</p>
</li>
<li class="">
<p><strong>authors:</strong> Borong Zhang, Jiahao Li, Jiachen Shen, Yishuai Cai, Yuhao Zhang, Yuanpei Chen, Juntao Dai, Jiaming Ji, Yaodong Yang</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University, State Key Laboratory of General Artificial Intelligence (Peking University), PKU-PsiBot Joint Lab, Beijing Academy of Artificial Intelligence</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22539" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22539</a></p>
</li>
<li class="">
<p><strong>code:</strong> this https URL (from abstract)</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces VLA-Arena, a comprehensive benchmark with a novel structured task design framework to quantify difficulty across three orthogonal axes (Task Structure, Language Command, Visual Observation). 2. Provides systematic robustness evaluation via decoupled language and visual perturbations, enabling precise analysis of model failure modes. 3. Releases a complete open-source framework including an end-to-end toolchain, datasets (VLA-Arena-S/M/L), and a leaderboard to foster reproducible research.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72fbe811b1b247e0cabad5619ed5d23d6155ddda1e84493fde30e54c1e9e1092_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72fbe811b1b247e0cabad5619ed5d23d6155ddda1e84493fde30e54c1e9e1092_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces VLA-Arena, an open-source benchmark and framework designed to systematically evaluate the capabilities and failure modes of Vision-Language-Action models. It proposes a structured task design with fine-grained difficulty levels across four dimensions and orthogonal perturbations to measure model robustness. The evaluation reveals critical limitations in current VLAs, such as memorization over generalization and poor safety consideration, and the released framework aims to address these challenges.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] ParaMaP: Parallel Mapping and Collision-free Motion Planning for Reactive Robot Manipulation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [motion planning], [Euclidean Distance Transform, sampling-based model predictive control, GPU parallelization, collision-free planning, SE(3) pose tracking]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xuewei Zhang, Bailing Tian, Kai Zheng, Yulin Hui, Junjie Lu, Zhiyu Li</p>
</li>
<li class="">
<p><strong>institution:</strong> Tianjin University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22575" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22575</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://zxw610.github.io/ParaMaP" target="_blank" rel="noopener noreferrer" class="">https://zxw610.github.io/ParaMaP</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A parallel framework tightly integrating GPU-based Euclidean Distance Transform mapping with a sampling-based MPC planner for real-time replanning. 2. A robot-masked update mechanism for the distance field to prevent false self-collision detections during online perception. 3. Formulating motion generation as a stochastic optimization with a unified objective and a geometrically consistent SE(3) pose tracking metric for fast, accurate convergence.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dc2c02919a284bed96ccdbd82c4e1bdf169e2e161c4adb9bfc19e68ccb0dc21_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dc2c02919a284bed96ccdbd82c4e1bdf169e2e161c4adb9bfc19e68ccb0dc21_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes ParaMaP, a framework for real-time, collision-free motion planning in unknown environments. It combines a GPU-accelerated distance field mapping system with a parallel sampling-based model predictive control planner. The method is validated through simulations and real-world experiments on a 7-DoF manipulator, demonstrating effective high-frequency replanning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Modeling of UAV Tether Aerodynamics for Real-Time Simulation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [robotics simulation], [tether aerodynamics, real-time simulation, catenary theory, lumped mass model, CasADi]</p>
</li>
<li class="">
<p><strong>authors:</strong> Max Beffert, Andreas Zell</p>
</li>
<li class="">
<p><strong>institution:</strong> Cognitive Systems Group, University of Tübingen</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22588" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22588</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed an analytical tether model based on catenary theory with uniform drag for fast (&lt;1ms) real-time simulation. 2. Developed a flexible numerical tether model using segment discretization and optimization (CasADi/IPOPT) achieving real-time performance (5ms). 3. Validated both models with real-world experiments, providing a framework for offline optimization and online tasks like control and planning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a5999cc3e2fc5ef1aa99519ee5adb890754d8b494519879e512c601ea3c9b6a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a5999cc3e2fc5ef1aa99519ee5adb890754d8b494519879e512c601ea3c9b6a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of modeling aerodynamic forces on a tether for tethered UAVs to enable continuous operation from a moving base. It proposes two complementary real-time methods: a fast analytical model and a more flexible numerical model. The work concludes that the analytical model is sufficiently accurate for most applications, while the numerical model offers higher physical fidelity when needed.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Sistema de navegación de cobertura para vehículos no holonómicos en ambientes de exterior</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [mobile robotics, coverage path planning], [coverage navigation, non-holonomic vehicle, obstacle recovery, outdoor environments, mining automation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Michelle Valenzuela, Francisco Leiva, Javier Ruiz-del-Solar</p>
</li>
<li class="">
<p><strong>institution:</strong> Advanced Mining Technology Center, Universidad de Chile</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22734" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22734</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Development of a coverage navigation system for non-holonomic robots in outdoor environments. 2. Incorporation of recovery behaviors to handle dynamic or unmapped obstacles, ensuring complete coverage. 3. Validation of the system in both simulated and real outdoor settings, achieving near 90% coverage.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d51f00a4abaf62f479ca3c4a4186de6c2ffb93b0fee4c3e43193c92bebff4d9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d51f00a4abaf62f479ca3c4a4186de6c2ffb93b0fee4c3e43193c92bebff4d9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents a coverage navigation system for non-holonomic vehicles, designed for outdoor tasks like mining. The system plans routes to cover an area and includes recovery behaviors to handle unexpected obstacles. It was tested in simulation and real-world environments, achieving near 90% coverage, with plans to scale up to industrial mining vehicles.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Active Constraint Learning in High Dimensions from Demonstrations</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robot learning], [active learning, constraint inference, Gaussian processes, learning from demonstration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zheng Qiu, Chih-Yuan Chiu, Glen Chou</p>
</li>
<li class="">
<p><strong>institution:</strong> Georgia Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22757" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22757</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an iterative active constraint learning (ACL) algorithm that intelligently queries for new demonstrations to reduce constraint uncertainty. 2. Integrates a Gaussian process (GP) model within the learning from demonstrations (LfD) paradigm to represent and infer unknown constraints. 3. Demonstrates superior performance over a random-sampling baseline in recovering nonlinear constraints from sparse, informative demonstrations in high-dimensional settings with nonlinear dynamics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5acbf135d5aa431114b6b72db2fb101427cb6bd1e55fb1a8afd3028c0874cb4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5acbf135d5aa431114b6b72db2fb101427cb6bd1e55fb1a8afd3028c0874cb4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the data inefficiency of learning unknown constraints from demonstrations by proposing an active learning algorithm. The method iteratively trains a Gaussian process on demonstration data to model constraints and uses the model&#x27;s uncertainty to query for new, informative start/goal states to generate more demonstrations. Experiments show the approach outperforms a random-sampling baseline in accurately inferring constraints from fewer demonstrations in high-dimensional, nonlinear environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Two-Robot Computational Landscape: A Complete Characterization of Model Power in Minimal Mobile Robot Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [distributed computing], [autonomous mobile robots, Look-Compute-Move (LCM), computational power hierarchy, finite-state robots, robots with lights]</p>
</li>
<li class="">
<p><strong>authors:</strong> Naoki Kitamura, Yuichi Sudo, Koichi Wada</p>
</li>
<li class="">
<p><strong>institution:</strong> The University of Osaka, Hosei University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22770" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22770</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proves that under full synchrony, the FSTA (finite-state) and LUMI (robots with lights) models coincide for two robots, showing perfect synchrony can substitute for memory and communication at this minimal scale. 2. Shows that the FSTA and FCOM (finite-communication) models are orthogonal (bidirectionally incomparable), completing the landscape of incomparability. 3. Provides the first complete and exact characterization of the computational power hierarchy for two robots across all major models and schedulers using a novel simulation-free method.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddce34ada1bbaebc271a0c17bbc6cf8413606d2887ebd22bfe77cc4c0e90d34a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddce34ada1bbaebc271a0c17bbc6cf8413606d2887ebd22bfe77cc4c0e90d34a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper provides the first complete characterization of the computational power of two autonomous mobile robots across major models (OBLOT, FSTA, FCOM, LUMI) and schedulers. Using a novel simulation-free method, it reveals a landscape distinct from the general n-robot case, showing that perfect synchrony can substitute for memory and communication for two robots, and that FSTA and FCOM are orthogonal. This yields the first exact computational hierarchy for minimal robot systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The body is not there to compute: Comment on &quot;Informational embodiment: Computational role of information structure in codes and robots&quot; by Pitti et al</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [embodied cognition, robotics], [morphological computation, embodiment, information theory, passive dynamic walker]</p>
</li>
<li class="">
<p><strong>authors:</strong> Matej Hoffmann</p>
</li>
<li class="">
<p><strong>institution:</strong> Czech Technical University in Prague</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22868" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22868</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Critiques the application of computational and informational frameworks to biological and robotic bodies, arguing it is a misleading metaphor. 2. Distinguishes between the physical, non-computational role of body morphology and the metaphorical concept of &quot;morphological computation&quot;. 3. Proposes that the primary function of bodies is not to compute, challenging a core premise of the target article.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c58366db344796ab1e3ca689f71030b6079280073a3b1974c0cb683e87c3918c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c58366db344796ab1e3ca689f71030b6079280073a3b1974c0cb683e87c3918c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This commentary argues against the central thesis of a target article that applies computational and informational concepts to understand animal and robot bodies. The author contends that the concept of &quot;morphological computation&quot; is merely a metaphor and that the body&#x27;s main role is physical, not computational. The core conclusion is that bodies are not fundamentally for computing, challenging an informational embodiment perspective.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MUSON: A Reasoning-oriented Multimodal Dataset for Socially Compliant Navigation in Urban Environments</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [embodied navigation], [socially compliant navigation, multimodal dataset, chain-of-thought, vision language models, benchmark]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhuonan Liu, Xinyu Zhang, Zishuo Wang, Tomohito Kawabata, Xuesu Xiao, Ling Xiao</p>
</li>
<li class="">
<p><strong>institution:</strong> Hokkaido University, George Mason University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22867" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22867</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MUSON, a new multimodal dataset for socially compliant navigation with structured five-step Chain-of-Thought annotations (perception, prediction, reasoning, action, explanation). 2. Addresses limitations of prior datasets by explicitly modeling static physical constraints and providing a rationally balanced discrete action space to overcome long-tailed action distributions. 3. Establishes MUSON as an effective benchmark, demonstrating its utility by benchmarking state-of-the-art Small Vision Language Models, with Qwen2.5-VL-3B achieving the highest decision accuracy.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5248e75c8017605d3d39791503fba58a4cce5e655f4d900abeee425ea863b824_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5248e75c8017605d3d39791503fba58a4cce5e655f4d900abeee425ea863b824_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces MUSON, a reasoning-oriented multimodal dataset designed to address the lack of explicit reasoning supervision and imbalanced action distributions in existing social navigation datasets. It features structured Chain-of-Thought annotations and a balanced action space. Benchmarking results show that MUSON serves as an effective benchmark, with Qwen2.5-VL-3B achieving the highest accuracy, demonstrating its utility for training and evaluating socially compliant navigation models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] P-FABRIK: A General Intuitive and Robust Inverse Kinematics Method for Parallel Mechanisms Using FABRIK Approach</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [robotics], [inverse kinematics, parallel mechanisms, FABRIK, workspace robustness, topological decomposition]</p>
</li>
<li class="">
<p><strong>authors:</strong> Daqian Cao, Quan Yuan, Weibang Bai</p>
</li>
<li class="">
<p><strong>institution:</strong> ShanghaiTech University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22927" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22927</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes P-FABRIK, a novel inverse kinematics method for parallel mechanisms based on the FABRIK algorithm. 2. Introduces a new topological decomposition strategy to break down parallel mechanisms into serial sub-chains for iterative solution. 3. Demonstrates the method&#x27;s generality, computational efficiency, and robustness in handling targets outside the workspace.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b554beb83a556bb3fea7bd9931b48b310df4fe75825b63e9466dd5c70a26a7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b554beb83a556bb3fea7bd9931b48b310df4fe75825b63e9466dd5c70a26a7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes P-FABRIK, a general and robust inverse kinematics method for parallel mechanisms. It adapts the FABRIK algorithm by decomposing the mechanism into serial sub-chains and iteratively revising end targets. The method is shown to be effective, efficient, and capable of handling out-of-workspace targets for various parallel mechanisms.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PreGME: Prescribed Performance Control of Aerial Manipulators based on Variable-Gain ESO</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [robotics control], [aerial manipulator, prescribed performance control, variable-gain extended state observer, dynamic coupling, motion control]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mengyu Ji, Shiliang Guo, Zhengzhen Li, Jiahao Shen, Huazi Cao, Shiyu Zhao</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University, Westlake University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22957" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22957</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel prescribed performance motion control framework (PreGME) for aerial manipulators. 2. The use of variable-gain extended state observers (ESOs) for accurate real-time estimation of rapidly varying dynamic coupling. 3. A control strategy that generates a preset error trajectory to ensure tracking errors remain within a prescribed performance envelope for high-precision control.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f2803479d85cfb232ae59d1133082b1c37608a63bed7f68577a5675d15b2598_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f2803479d85cfb232ae59d1133082b1c37608a63bed7f68577a5675d15b2598_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes PreGME, a new control framework for aerial manipulators that combines variable-gain extended state observers to estimate dynamic coupling with prescribed performance control to constrain error trajectories. The method enables high-precision control even during aggressive arm motions. Experiments, including aerial mixology and cart-pulling, validate its effectiveness under significant dynamic disturbances.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robotic manipulation], [robotic foundation models, high-level planning, low-level control, imitation learning, reinforcement learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shuanghao Bai, Wenxuan Song, Jiayi Chen, Yuheng Ji, Zhide Zhong, Jin Yang, Han Zhao, Wanqi Zhou, Zhe Li, Pengxiang Ding, Cheng Chi, Chang Xu, Xiaolong Zheng, Donglin Wang, Haoang Li, Shanghang Zhang, Badong Chen</p>
</li>
<li class="">
<p><strong>institution:</strong> Xi&#x27;an Jiaotong University, Hong Kong University of Science and Technology (Guangzhou), Chinese Academy of Sciences, Westlake University, Zhejiang University, University of Sydney, BAAI, Peking University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22983" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22983</a></p>
</li>
<li class="">
<p><strong>code:</strong> Awesome-Robotics-Manipulation</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a unified algorithmic abstraction for robot manipulation, organizing approaches into high-level planning and low-level control. 2. Extends classical task planning to include reasoning over language, code, motion, affordances, and 3D representations. 3. Introduces a training-paradigm-oriented taxonomy for learning-based control, categorizing methods by input modeling, latent representation learning, and policy learning.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22135ed5ae9ef66789b2dbe7f9c4c6e6a9de91ca6dd4b2025e75cfd5d4a89d02_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22135ed5ae9ef66789b2dbe7f9c4c6e6a9de91ca6dd4b2025e75cfd5d4a89d02_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This survey paper organizes recent learning-based approaches to robot manipulation within a unified framework of high-level planning and low-level control. It extends task planning to include multimodal reasoning and proposes a new taxonomy for learning-based control. The analysis aims to clarify the design space and identifies key challenges like scalability and safety for future robotic foundation models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Embodied Learning of Reward for Musculoskeletal Control with Vision Language Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [musculoskeletal control, vision-language models, embodied learning, reward function discovery, motion representation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Saraswati Soedarmadji, Yunyue Wei, Chen Zhang, Yisong Yue, Yanan Sui</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, California Institute of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23077" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23077</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces MoVLR, a framework that uses Vision-Language Models (VLMs) to bridge high-level goal specification and low-level movement control for musculoskeletal systems. 2. Proposes an iterative method to explore the reward space by combining control optimization with VLM feedback, avoiding reliance on handcrafted rewards. 3. Demonstrates the framework&#x27;s ability to discover and refine reward functions for complex locomotion and manipulation tasks, grounding abstract language descriptions in physical control principles.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e43a6e8e2c1f39257ec8f88c3695335f5287e369c9a159892f3f3cd6efc7ce61_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e43a6e8e2c1f39257ec8f88c3695335f5287e369c9a159892f3f3cd6efc7ce61_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of designing reward functions for high-dimensional musculoskeletal control by proposing MoVLR, a framework that leverages Vision-Language Models to iteratively align control policies with high-level goals described in language. The method transforms language and vision-based assessments into structured guidance for embodied learning. The results show that VLMs can effectively ground abstract motion descriptions in the implicit principles of physiological motor control.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] APOLLO Blender: A Robotics Library for Visualization and Animation in Blender</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [robotics visualization], [URDF, Blender, keyframing, Python scripting, 3D animation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Peter Messina, Daniel Rakita</p>
</li>
<li class="">
<p><strong>institution:</strong> Yale University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23103" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23103</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A library for importing robots and environments directly from standardized descriptions like URDF into Blender, 2. Python-based scripting tools for keyframing robot states and visual attributes, 3. Convenient generation of primitive 3D shapes for creating schematic figures and animations.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6cba129e043d858f09043e81dde4891fc5ef6711639795d93574734cb37f17c5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6cba129e043d858f09043e81dde4891fc5ef6711639795d93574734cb37f17c5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces APOLLO Blender, a lightweight Python library that simplifies creating high-quality robotics visualizations and animations within Blender by providing robotics-focused scripting tools. It bridges the gap between powerful 3D graphics software and robotics research needs, enabling researchers to generate publication-ready content without deep Blender expertise. The work demonstrates the library&#x27;s utility through examples and discusses future extensions.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Beyond URDF: The Universal Robot Description Directory for Shared, Extensible, and Standardized Robot Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [robotics systems], [URDD, robot description, JSON/YAML modules, Bevy visualization, Three.js viewer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Roshan Klein-Seetharaman, Daniel Rakita</p>
</li>
<li class="">
<p><strong>institution:</strong> Yale University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23135" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23135</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced the Universal Robot Description Directory (URDD), a modular representation that organizes derived robot information into structured JSON and YAML modules to reduce redundancy and improve standardization. 2. Provided an open-source toolkit with a Rust implementation to automatically generate URDDs from URDFs, including Bevy-based visualization capabilities. 3. Developed a JavaScript/Three.js viewer for web-based inspection of URDDs, enabling interactive visualization and verification across platforms.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f3c8fda99c3364911696d021db2c35cc62671784fcdbf685fd62934de409e5f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f3c8fda99c3364911696d021db2c35cc62671784fcdbf685fd62934de409e5f_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of redundant computations and fragmented implementations in robotics due to basic robot specification files like URDF. It proposes the Universal Robot Description Directory (URDD), a modular representation that organizes richer derived information into JSON/YAML files, along with tools for automatic generation and visualization. The work concludes that URDD efficiently encapsulates extensive robot data, enabling shared standards and reducing redundancy across frameworks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A New Software Tool for Generating and Visualizing Robot Self-Collision Matrices</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [robotics], [self-collision matrix, robot self-collision, proximity query, shape representation, interactive visualization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Roshan Klein-Seetharama, Daniel Rakita</p>
</li>
<li class="">
<p><strong>institution:</strong> Yale University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23140" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23140</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An interactive tool for generating and visualizing self-collision matrices that overcomes limitations of static tools like MoveIt Setup Assistant. 2. Support for multiple shape representations (spheres, OBBs, convex hulls, convex decompositions) and dynamic inspection/filtering. 3. Implementation in Rust with Bevy engine for high-quality visualization and export to JSON/YAML for easy integration.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea10a80ffc42162cc3aac7a557e3310d2bcf065680b6fdc22c295d40ed8b5ec4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea10a80ffc42162cc3aac7a557e3310d2bcf065680b6fdc22c295d40ed8b5ec4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper introduces a new interactive software tool for generating and visualizing robot self-collision matrices. It supports multiple shape representations and enables dynamic inspection and refinement, leading to faster and more accurate self-collision and self-proximity queries. The tool is implemented in Rust using the Bevy game engine and outputs results in standard formats for integration into planning frameworks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Pole-centric Descriptors for Robust Robot Localization: Evaluation under Pole-at-Distance (PaD) Observations using the Small Pole Landmark (SPL) Dataset</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [robot localization], [pole-centric descriptors, contrastive learning, small-scale observations, landmark distinctiveness, pole-at-distance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Wuhao Xie, Kanji Tanaka</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Fukui</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23141" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23141</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Construction of the Small Pole Landmark (SPL) dataset via an automated tracking-based pipeline for evaluating landmark identification under sparse observations. 2. Empirical comparative analysis demonstrating that Contrastive Learning (CL) induces a more robust feature space for sparse geometry than Supervised Learning (SL). 3. Systematic robustness breakdown analyzing the trade-off between observation distance and descriptor reliability, identifying effective operational ranges.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/10a74c5a1b74d238ad9ddaabd43dc96605a8f33839f93907c39978f07db411a7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/10a74c5a1b74d238ad9ddaabd43dc96605a8f33839f93907c39978f07db411a7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the degradation of pole landmark identification reliability under long-distance, sparse observations in urban environments. It proposes an evaluation framework using the SPL dataset and compares Contrastive Learning (CL) and Supervised Learning (SL) paradigms, finding that CL achieves superior retrieval performance, especially in the 5-10m range, by learning more robust features.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Towards the Automation in the Space Station: Feasibility Study and Ground Tests of a Multi-Limbed Intra-Vehicular Robot</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [space robotics], [multi-limbed robot, intra-vehicular activity, motion planning, microgravity simulation, autonomous operation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Seiko Piotr Yamaguchi, Kentaro Uno, Yasumaru Fujii, Masazumi Imai, Kazuki Takada, Taku Okawara, Kazuya Yoshida</p>
</li>
<li class="">
<p><strong>institution:</strong> Japan Aerospace Exploration Agency (JAXA), Tohoku University, Hamano Products Co., Ltd.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23153" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23153</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A feasibility study and design for a multi-limbed intra-vehicular robot (MLIVR) to automate logistical tasks on the ISS. 2. Development and simulation of 3D motion planning for the robot&#x27;s transportation capabilities in a space station environment. 3. Execution of prototype ground tests on a 2D table to validate autonomous operation in a simulated microgravity setting.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/282a695eb4eaf1576b9e4ce6a2fca61f3d48e62d714d0b32d51e22f5a403fbd6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/282a695eb4eaf1576b9e4ce6a2fca61f3d48e62d714d0b32d51e22f5a403fbd6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the use of an autonomous multi-limbed robot to assist astronauts with logistical tasks on the International Space Station. The method involved simulating 3D motion planning and testing a prototype on a 2D table to mimic microgravity. The results show that such tasks can be performed with minimal human intervention, enhancing operational efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Sequential Hermaphrodite Coupling Mechanism for Lattice-based Modular Robots</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [modular robotics], [coupling mechanism, sequential hermaphrodite, shape-matching, lattice-based, modular self-reconfigurable robot (MSR)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Keigo Torii, Kentaro Uno, Shreya Santra, Kazuya Yoshida</p>
</li>
<li class="">
<p><strong>institution:</strong> Tohoku University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23154" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23154</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel shape-matching mechanical coupling mechanism that meets complex requirements for heterogeneous structural modules. 2. A design enabling controlled, sequential transitions between male and female states to facilitate single-sided operations. 3. The capability for single-sided decoupling from both the male and female sides by forcibly switching the opposite mechanism&#x27;s state.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e13d98b23f8f9371fe7ea6dda54625a0d3bd211a4b265a46a96f7e03a5eba13_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e13d98b23f8f9371fe7ea6dda54625a0d3bd211a4b265a46a96f7e03a5eba13_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the complex design requirements for coupling mechanisms in lattice-based modular robots, such as single-sided operation and flat uncoupled surfaces. It proposes a novel sequential hermaphrodite coupling mechanism that dynamically switches between male and female states to meet these needs. The mechanism is concluded to be applicable to various modular robot systems and tool changers.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Breaking Symmetry-Induced Degeneracy in Multi-Agent Ergodic Coverage via Stochastic Spectral Control</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent systems], [ergodic coverage, spectral multiscale coverage, stochastic perturbation, gradient cancellation, mean-square boundedness]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kooktae Lee, Julian Martinez</p>
</li>
<li class="">
<p><strong>institution:</strong> New Mexico Institute of Mining and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23158" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23158</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Rigorously characterizes the initial conditions and symmetry-induced invariant manifolds that cause directional degeneracy (stalling/axis-constrained motion) in classical Spectral Multiscale Coverage (SMC). 2. Introduces a novel stochastic spectral control method that combines a stochastic perturbation with a contraction term to address the degeneracy. 3. Provides theoretical proofs that the proposed dynamics ensure almost-sure escape from zero-gradient manifolds while maintaining mean-square boundedness of agent trajectories.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a357d9e67e490796584c8650824889a0abbafdd39b17b5ae994eec026cc68f38_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a357d9e67e490796584c8650824889a0abbafdd39b17b5ae994eec026cc68f38_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of gradient cancellation and agent stalling in multi-agent ergodic coverage when agents start near symmetry points of a target distribution. The authors propose a stochastic spectral control method that adds perturbation and contraction to the dynamics, proving it escapes degenerate states while keeping trajectories bounded. Simulations confirm the method mitigates stalling and axis-constrained motion effectively.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robotic imitation learning], [world model, vision-language-action (VLA) model, inverse dynamics model, surgical robotics, synthetic data generation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yufan He, Pengfei Guo, Mengya Xu, Zhaoshuo Li, Andriy Myronenko, Dillan Imans, Bingjie Liu, Dongren Yang, Mingxue Gu, Yongnan Ji, Yueming Jin, Ren Zhao, Baiyong Shen, Daguang Xu</p>
</li>
<li class="">
<p><strong>institution:</strong> NVIDIA, The Chinese University of Hong Kong, Sung Kyun Kwan University, Wenzhou Medical University, National University of Singapore, Ruijin Hospital</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23162" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23162</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Curated the Surgical Action-Text Alignment (SATA) dataset with detailed text descriptions for surgical robot actions. 2. Built SurgWorld, a generative world model capable of producing diverse and realistic synthetic surgical videos. 3. Pioneered the use of an inverse-dynamics model to infer pseudo-kinematics from synthetic videos, creating synthetic paired video-action data for training.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd5e2ca51f8df31fc2fe20ced16d79b01d4a091736ac738cdfcd0c8fda793a16_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd5e2ca51f8df31fc2fe20ced16d79b01d4a091736ac738cdfcd0c8fda793a16_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the data scarcity problem in autonomous surgical robotics by proposing SurgWorld, a world model that generates realistic synthetic surgical videos. The method uses an inverse dynamics model to infer robot actions from these videos, creating a large-scale paired dataset to train a Vision-Language-Action policy. The resulting policy significantly outperforms models trained only on real demonstrations on a real surgical robot platform.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Human-Oriented Cooperative Driving Approach: Integrating Driving Intention, State, and Conflict</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [cooperative driving, human-machine conflict, intention-aware planning, authority allocation, shared control]</p>
</li>
<li class="">
<p><strong>authors:</strong> Qin Wang, Shanmin Pang, Jianwu Fang, Shengye Dong, Fuhao Liu, Jianru Xue, Chen Lv</p>
</li>
<li class="">
<p><strong>institution:</strong> Xi&#x27;an Jiaotong University, Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23220" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23220</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/i-Qin/HOCD" target="_blank" rel="noopener noreferrer" class="">https://github.com/i-Qin/HOCD</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Human-Oriented Cooperative Driving (HOCD) approach that minimizes human-machine conflict by prioritizing driver intention and state. 2. Designs an intention-aware trajectory planning method at the tactical level, using an intention consistency cost to align the trajectory with driver intention. 3. Develops a reinforcement learning-based control authority allocation strategy at the operational level to achieve consistency between driver state and authority allocation.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d5e5ef9e0e93b645fd2997c604be86cc36eb4f1a7f88af7219f0cc6a908523b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d5e5ef9e0e93b645fd2997c604be86cc36eb4f1a7f88af7219f0cc6a908523b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a Human-Oriented Cooperative Driving (HOCD) approach to improve human-vehicle interaction by minimizing conflict. The method integrates intention-aware trajectory planning and a reinforcement learning-based authority allocation strategy. Simulation and human-in-the-loop experiments show the approach aligns with driver intention, ensures reasonable authority allocation, and enhances driving performance compared to other methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Beyond Coverage Path Planning: Can UAV Swarms Perfect Scattered Regions Inspections?</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [coverage path planning], [UAV Swarms, Multi-Agent, Coverage Path Planning, Remote Sensing, Trajectory Optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Socratis Gkelios, Savvas D. Apostolidis, Pavlos Ch. Kapoutsis, Elias B. Kosmatopoulos, Athanasios Ch. Kapoutsis</p>
</li>
<li class="">
<p><strong>institution:</strong> Democritus University of Thrace, Information Technologies Institute, Centre for Research &amp; Technology Hellas</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23257" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23257</a></p>
</li>
<li class="">
<p><strong>code:</strong> github.com/soc12/mUDAI</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formally introduces the Fast Inspection of Scattered Regions (FISR) problem, focusing on inspecting multiple non-connected areas. 2. Proposes the multi-UAV Disjoint Areas Inspection (mUDAI) method, a two-fold optimization for image capture positions and UAV trajectories. 3. Validates the method through simulations and real-world deployments, demonstrating improved operational efficiency while maintaining data quality.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eadfe6ac5c135ae77cf9cbeeaa979770a93369ca56f253722ad9679e402d746a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eadfe6ac5c135ae77cf9cbeeaa979770a93369ca56f253722ad9679e402d746a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the inefficiency of traditional coverage path planning for inspecting multiple scattered regions using UAVs. It proposes the mUDAI method, which optimizes both image capture positions and UAV trajectories to minimize time and resource use. The method is validated through simulations and real-world tests, showing it enables rapid, efficient inspections.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement Using Deep Learning-Based Dynamic Object Filtering</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [visual SLAM], [ORB-SLAM3, YOLOv8, dynamic object filtering, point cloud refinement, CUDA acceleration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sheng-Kai Chen, Jie-Yu Chao, Jr-Yu Chang, Po-Lien Wu, Po-Chiang Lin</p>
</li>
<li class="">
<p><strong>institution:</strong> Yuan Ze University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23318" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23318</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes PCR-ORB, an enhanced ORB-SLAM3 framework that integrates deep learning-based point cloud refinement to filter dynamic objects. 2. Implements a multi-stage filtering strategy combining semantic segmentation (YOLOv8), ground plane estimation, sky removal, edge filtering, and temporal consistency for robust dynamic object removal. 3. Achieves real-time performance through CUDA-accelerated processing and demonstrates significant accuracy improvements in specific dynamic sequences on the KITTI dataset.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e72d53f96b383c73428b67d24fbf2d4163ac5820be1bfed6f370c529922f919_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e72d53f96b383c73428b67d24fbf2d4163ac5820be1bfed6f370c529922f919_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces PCR-ORB, an enhanced visual SLAM system that improves ORB-SLAM3&#x27;s robustness in dynamic environments by integrating YOLOv8 for semantic segmentation and a multi-stage point cloud refinement process to filter moving objects. The method achieves real-time performance with CUDA acceleration. Evaluation on KITTI shows scenario-dependent effectiveness, with notable accuracy improvements in some sequences but mixed results overall.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [explainable ai (xai)], [inverse kinematics, shapley additive explanations (SHAP), InterpretML, obstacle avoidance, neural network]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sheng-Kai Chen, Yi-Ling Tsai, Chun-Chih Chang, Yan-Chen Chen, Po-Chiang Lin</p>
</li>
<li class="">
<p><strong>institution:</strong> Yuan Ze University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23312" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23312</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an explainability-centered workflow integrating SHapley Additive exPlanations (SHAP) with physics-based obstacle avoidance evaluation for neural inverse kinematics. 2. Introduces and trains two lightweight variants of IKNet (Improved IKNet with residual connections and Focused IKNet with position-orientation decoupling) on a synthetic dataset. 3. Demonstrates through simulation that neural IK architectures with more balanced feature importance attribution tend to maintain wider safety margins without sacrificing accuracy, linking XAI insights to robotic safety.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ab3055f2df7d03d7972b536c04fab2618a41cdd21ee682cf0f1ab9c0c6610f6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ab3055f2df7d03d7972b536c04fab2618a41cdd21ee682cf0f1ab9c0c6610f6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This study addresses the lack of transparency in neural network-based inverse kinematics (IK) solvers by proposing an explainable AI workflow. It integrates SHAP analysis with physics-based simulation to evaluate two new IKNet variants on obstacle avoidance tasks. The key finding is that architectures with more evenly distributed feature importance achieve better safety performance, showing how XAI can guide the development of trustworthy robotic manipulation systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Optimal Scalability-Aware Allocation of Swarm Robots: From Linear to Retrograde Performance via Marginal Gains</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent systems], [task allocation, swarm robotics, scalability functions, marginal gains, collective decision-making]</p>
</li>
<li class="">
<p><strong>authors:</strong> Simay Atasoy Bingöl, Tobias Töpfer, Sven Kosub, Heiko Hamann, Andreagiovanni Reina</p>
</li>
<li class="">
<p><strong>institution:</strong> Universität Konstanz, Max Planck Institute of Animal Behavior</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23431" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23431</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A computationally efficient algorithm for optimal agent allocation based on marginal performance gains. 2. The algorithm handles tasks with concave scalability functions, including linear, saturating, and retrograde scaling. 3. Validation of the algorithm in a simulated robot swarm performing collective decision-making tasks with varying difficulty.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65eaf636a462023cd662333c71ac9a0b2b73588d8278f1b001d8292a37ecc630_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65eaf636a462023cd662333c71ac9a0b2b73588d8278f1b001d8292a37ecc630_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the problem of optimally allocating a finite number of agents across multiple tasks where performance scales differently. It proposes an efficient algorithm based on marginal gains to handle concave scalability functions, including retrograde scaling where too many agents degrade performance. The method is validated in robot swarm simulations for collective decision-making, showing its utility for future multi-robot systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Assessing behaviour coverage in a multi-agent system simulation for autonomous vehicle testing</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent systems], [Model Predictive Control, Coverage-Based Testing, Edge-Case Exploration, Multi-Agent Simulation, Behaviour Coverage]</p>
</li>
<li class="">
<p><strong>authors:</strong> Manuel Franco-Vivo</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Bristol</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23445" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23445</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A systematic approach to measure and assess behaviour coverage within a multi-agent simulation for autonomous vehicle testing. 2. The proposal of a Model Predictive Control (MPC) pedestrian agent designed to generate interesting tests and realistic behaviour. 3. Insights and analysis for improving and optimizing simulation frameworks through behaviour coverage metrics.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385ed91e6ca24b7cc0e8d369cc587a3dd677cf4643f89f27d85aaadf4cd4ea70_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385ed91e6ca24b7cc0e8d369cc587a3dd677cf4643f89f27d85aaadf4cd4ea70_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the need for comprehensive testing of autonomous vehicles by analyzing behaviour coverage in multi-agent simulations. It proposes a systematic method to measure coverage and introduces an MPC-based pedestrian agent to generate more realistic and challenging test scenarios. The research concludes that assessing behaviour coverage is crucial for validating the robustness of autonomous systems and improving simulation frameworks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Theory of Mind for Explainable Human-Robot Interaction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [human-robot interaction], [Theory of Mind, Explainable AI, XAI evaluation, human-centered explanation, VXAI framework]</p>
</li>
<li class="">
<p><strong>authors:</strong> Marie Bauer, Julia Gachot, Matthias Kerzel, Cornelius Weber, Stefan Wermter</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Hamburg</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23482" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23482</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes to conceptualize Theory of Mind (ToM) in Human-Robot Interaction as a form of Explainable AI (XAI), 2. Identifies a critical gap in ToM-HRI research regarding the fidelity of explanations to the robot&#x27;s actual internal reasoning, 3. Advocates for integrating ToM principles into XAI frameworks to shift focus towards user-centered explanations and enable evaluation using frameworks like VXAI.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16cba35efa080097fd84afa40a6d270891cd44dfcf26d46fde5d29edb9bb1541_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16cba35efa080097fd84afa40a6d270891cd44dfcf26d46fde5d29edb9bb1541_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper identifies that Theory of Mind (ToM) in human-robot interaction and Explainable AI (XAI) share the goal of making AI reasoning understandable. It proposes to treat ToM as a form of XAI and argues for integrating ToM&#x27;s user-centered perspective into XAI frameworks to address the lack of explanation fidelity and user-centered evaluation in current research.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Robust Deep Learning Control with Guaranteed Performance for Safe and Reliable Robotization in Heavy-Duty Machinery</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robust control, learning-based control], [robust control, deep learning, safety guarantees, heavy-duty machinery, hierarchical control]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mehdi Heydari Shahna</p>
</li>
<li class="">
<p><strong>institution:</strong> Based on the author name &quot;Mehdi Heydari Shahna&quot;, a common affiliation for this research area would be a university with a strong robotics program, such as Aalto University, Tampere University, or the University of Oulu in Finland. However, the provided text does not explicitly state the institution. A reasonable inference from the context of heavy-duty machinery and robotics in Europe could be &quot;Tampere University&quot; or &quot;Aalto University&quot;, but without explicit data, the safest answer is &quot;Not explicitly stated in provided content&quot;.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23505" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23505</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A generic modular control framework for electrified heavy-duty mobile machines that is energy-source independent and simplifies design. 2. Hierarchical control policies that integrate AI/learning strategies while providing formal guarantees for safety, performance, and stability. 3. Methods to interpret and verify black-box learning components to ensure compliance with international safety standards.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66f78ddebe7bd223957b3334fed0e1b9489d7be34c9c804202f8cfb9e774548a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66f78ddebe7bd223957b3334fed0e1b9489d7be34c9c804202f8cfb9e774548a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This dissertation addresses the challenge of safely integrating AI into heavy-duty mobile machines undergoing electrification. It proposes a robust, modular control framework that combines deep learning with formal guarantees for performance and stability. The framework is validated through multiple case studies, advancing control methods for safer and more reliable robotic systems in heavy industry.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Act2Goal: From World Model To General Goal-conditioned Policy</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [goal-conditioned policy, world model, multi-scale temporal hashing, hindsight goal relabeling, LoRA]</p>
</li>
<li class="">
<p><strong>authors:</strong> Pengfei Zhou, Liliang Chen, Shengcong Chen, Di Chen, Wenzhi Zhao, Rongjun Jin, Guanghui Ren, Jianlan Luo</p>
</li>
<li class="">
<p><strong>institution:</strong> Agibot Research</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23541" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23541</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://act2goal.github.io/" target="_blank" rel="noopener noreferrer" class="">https://act2goal.github.io/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control for long-horizon tasks. 2. Introduces Multi-Scale Temporal Hashing (MSTH) to decompose imagined visual trajectories into dense proximal and sparse distal frames for fine-grained control and global consistency. 3. Enables reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c80e64501db97d2a806134a5544d87a826563f38be2334e8bdec4a9d7f9cf78_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c80e64501db97d2a806134a5544d87a826563f38be2334e8bdec4a9d7f9cf78_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of long-horizon robotic manipulation by proposing Act2Goal, a policy that uses a goal-conditioned world model to generate visual plans and a multi-scale temporal control mechanism for robust execution. The method achieves strong zero-shot generalization and allows for rapid online adaptation. Real-robot experiments show it significantly improves success rates on out-of-distribution tasks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Soft Robotic Technological Probe for Speculative Fashion Futures</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [human-robot interaction], [soft robotics, wearable technology, speculative design, pneumatic actuation, technological probe]</p>
</li>
<li class="">
<p><strong>authors:</strong> Amy Ingold, Loong Yi Lee, Richard Suphapol Diteesawat, Ajmal Roshan, Yael Zekaria, Edith-Clare Hall, Enrico Werner, Nahian Rahman, Elaine Czech, Jonathan Rossiter</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Bristol</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23570" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23570</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. The design and fabrication of &quot;Sumbrella,&quot; a novel soft robotic garment integrating origami-inspired bistable units, fabric pneumatic actuators, and computer vision. 2. The use of Sumbrella as a technological probe in a focus group study to explore public interpretation, interaction, and ethical concerns regarding future soft robotic wearables. 3. The contribution of key considerations for HRI, including kinesic communication, social dynamics, and ethical guidelines, and a reflection on the value of speculative design for evaluating social acceptability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/961ff66eaf860636f7951016c0465ba6020b516e266f501287c6b78b23a9fafa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/961ff66eaf860636f7951016c0465ba6020b516e266f501287c6b78b23a9fafa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents &quot;Sumbrella,&quot; a soft robotic garment designed as a speculative fashion probe to explore the social implications of wearable robotics. Through a focus group study, the authors used the prototype to gather insights on how people imagine future interactions with such technology, revealing both expressive potential and significant ethical concerns. The work contributes design considerations and a methodological reflection on using speculative design in Human-Robot Interaction research to address social meaning alongside functionality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Unsupervised Learning for Detection of Rare Driving Scenarios</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [anomaly detection], [Deep Isolation Forest, t-SNE, naturalistic driving data]</p>
</li>
<li class="">
<p><strong>authors:</strong> Dat Le, Thomas Manhardt, Moritz Venator, Johannes Betz</p>
</li>
<li class="">
<p><strong>institution:</strong> Technical University of Munich (TUM), CARIAD SE</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23585" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23585</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an unsupervised learning framework using Deep Isolation Forest for detecting rare driving scenarios without labeled data. 2. Introduces a preprocessing pipeline that extracts structured statistical features from perception data using sliding windows. 3. Incorporates t-SNE for dimensionality reduction and visualization to improve the interpretability of detected anomalies.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22d1396541ab0766bc09a3986c6b01acba4cc137b0167e70049e1da565e969a3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22d1396541ab0766bc09a3986c6b01acba4cc137b0167e70049e1da565e969a3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of detecting rare and hazardous driving scenarios for autonomous systems. It proposes an unsupervised framework using Deep Isolation Forest on naturalistic driving data, which effectively identifies anomalies without relying on labeled datasets. The method provides a scalable solution, though it depends on proxy ground truth and manually defined features.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Kalman Filter-Based Disturbance Observer for Steer-by-Wire Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [control systems], [Kalman Filter, Disturbance Observer, Steer-by-Wire, Driver Impedance, Torque Estimation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Nikolai Beving, Jonas Marxen, Steffen Mueller, Johannes Betz</p>
</li>
<li class="">
<p><strong>institution:</strong> Technical University of Munich, Technical University of Berlin</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23593" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23593</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Designed a Kalman filter-based disturbance observer to estimate high-frequency driver torque without using direct torque sensors. 2. Modeled the driver&#x27;s passive torque as an extended state using a PT1-lag approximation within both linear and nonlinear system models. 3. Demonstrated that a nonlinear extended Kalman Filter outperforms a linear one in handling frictional nonlinearities during static-to-dynamic friction transitions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae20fc48cc0d99b531523c76958cd606e506aeb5f6e913a69a693c9835f5d2eb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae20fc48cc0d99b531523c76958cd606e506aeb5f6e913a69a693c9835f5d2eb_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of high-frequency driver-induced disturbances in Steer-by-Wire systems. It proposes a Kalman filter-based disturbance observer that estimates driver torque using only motor state measurements, eliminating the need for costly direct torque sensors. The method was validated via simulation, showing accurate disturbance reconstruction with minimal delay (~14ms) and improved performance using a nonlinear extended Kalman filter for handling friction.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Interactive Robot Programming for Surface Finishing via Task-Centric Mixed Reality Interfaces</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [human-robot interaction], [surface segmentation, mixed reality interface, task-centric programming, robot trajectory generation, user study]</p>
</li>
<li class="">
<p><strong>authors:</strong> Christoph Willibald, Lugh Martensen, Thomas Eiband, Dongheui Lee</p>
</li>
<li class="">
<p><strong>institution:</strong> German Aerospace Center (DLR), University of Lübeck, TU Wien</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23616" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23616</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel interactive robot programming approach using task-centric workflows for non-experts. 2. A new surface segmentation algorithm that incorporates human input and provides continuous visual feedback for iterative refinement. 3. An optimal mixed reality interface design, validated through user studies, that reduces workload and improves usability for surface finishing tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a561f172d18a77b56aecf8017c7bffbc65ae8d01334f01ff6a1aa8ff7c050bf3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a561f172d18a77b56aecf8017c7bffbc65ae8d01334f01ff6a1aa8ff7c050bf3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the barrier of complex robot programming for surface finishing tasks in small-scale manufacturing. It proposes a task-centric mixed reality interface where non-experts can intuitively program a robot by interactively segmenting workpiece surfaces and receiving visual feedback, with the system then generating the corresponding robot trajectory. User studies showed that this approach significantly reduces user workload and improves usability for effective task programming.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The N-5 Scaling Law: Topological Dimensionality Reduction in the Optimal Design of Fully-actuated Multirotors</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [other], [aerial robotics, control systems, geometric optimization], [fully-actuated multirotors, topological optimization, isotropy metric, phase locking, N-5 scaling law]</p>
</li>
<li class="">
<p><strong>authors:</strong> Antonio Franchi</p>
</li>
<li class="">
<p><strong>institution:</strong> Based on the author name &quot;Antonio Franchi&quot;, the institution is not explicitly stated in the provided text. Common affiliations for this author include CNRS (Centre national de la recherche scientifique) and LAAS (Laboratoire d&#x27;analyse et d&#x27;architecture des systèmes) in Toulouse, France.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23619" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23619</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulates the geometric design of fully-actuated multirotors as a topological optimization problem on the product manifold of Projective Lines, moving beyond parametric optimization. 2. Discovers and formulates the &quot;N-5 Scaling Law&quot;, an empirical relationship describing the topology of optimal configurations for regular polyhedral chassis. 3. Identifies a design redundancy enabling optimality-preserving morphing, allowing continuous vehicle reconfiguration without loss of isotropic control authority.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/867b87b0001ff54cd94ab728db5acbc6219646d0a24ca04d8913e9c5baa68162_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/867b87b0001ff54cd94ab728db5acbc6219646d0a24ca04d8913e9c5baa68162_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the optimal geometric design of fully-actuated multirotor drones. It formulates the problem as a topological optimization on a manifold and discovers that for symmetric chassis, the optimal solutions form continuous curves following an &quot;N-5 Scaling Law&quot;. This reveals a design redundancy that allows the vehicle to morph its shape while maintaining optimal control performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robot learning], [video-to-locomotion, visual motion intent, diffusion policy]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Academy of Artificial Intelligence (BAAI), University of Sydney, Hong Kong University of Science and Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23649" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23649</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes RoboMirror, the first retargeting-free framework that directly generates humanoid locomotion from raw videos by understanding visual motion intents. 2. Introduces a method that leverages Vision-Language Models (VLMs) to distill videos into semantic motion intents, which condition a diffusion-based policy, bypassing explicit pose estimation. 3. Demonstrates the framework&#x27;s effectiveness for both egocentric (telepresence) and third-person video control, significantly reducing control latency and improving task success rates.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb8087d8ec53ebe9a14dbea08d918cbe27838f7e4dc5d178ed65513c16703cd7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb8087d8ec53ebe9a14dbea08d918cbe27838f7e4dc5d178ed65513c16703cd7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the gap between visual understanding and control in humanoid locomotion by proposing RoboMirror, a framework that first understands visual motion intents from raw videos and then uses them to condition a diffusion policy for generating physically plausible actions. The method eliminates the need for explicit pose reconstruction and retargeting. Experiments show it enables effective telepresence, reduces control latency by 80%, and achieves higher task success than baselines.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Do You Have Freestyle? Expressive Humanoid Locomotion via Audio Control</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robot learning], [audio-to-locomotion, diffusion policy, retargeting-free]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> BAAI (Beijing Academy of Artificial Intelligence), University of Sydney, Harbin Institute of Technology, Hong Kong University of Science and Technology, Shanghai Jiao Tong University, Peking University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23650" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23650</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes RoboPerform, the first unified framework for directly generating music-driven dance and speech-driven co-speech gestures from audio for humanoid robots. 2. Introduces a novel &quot;motion = content + style&quot; principle, treating audio as implicit style signals to eliminate the need for explicit motion reconstruction and retargeting. 3. Designs a policy architecture integrating a ResMoE teacher for diverse motion patterns and a diffusion-based student for audio style injection, ensuring low latency and high fidelity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/384b6201b72bb7358f39e97637b20f414d5429fd79013e12be43d148e4e91c65_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/384b6201b72bb7358f39e97637b20f414d5429fd79013e12be43d148e4e91c65_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the lack of expressive, audio-reactive locomotion in humanoid robots by proposing RoboPerform, a unified framework that directly generates dance and co-speech gestures from audio without explicit motion reconstruction. The method treats audio as a style signal and uses a teacher-student policy with diffusion for style injection. Experiments show the approach achieves physically plausible and audio-aligned motions, enabling responsive robot performance.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] The Bulldozer Technique: Efficient Elimination of Local Minima Traps for APF-Based Robot Navigation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [robot navigation], [Artificial Potential Field, local minima, path planning, backfilling mechanism, ramp-based enhancement]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mohammed Baziyad, Manal Al Shohna, Tamer Rabie</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Sharjah</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23672" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23672</a></p>
</li>
<li class="">
<p><strong>contributions:</strong>  1. Proposed the novel &quot;Bulldozer&quot; technique, which introduces a backfilling mechanism to systematically identify and eliminate local minima regions in APF-based navigation by increasing their potential values. 2. Incorporated a ramp-based enhancement to assist robots in escaping trap areas when they start within a local minimum, improving robustness. 3. Provided comprehensive experimental validation on a physical mobile robot, demonstrating superior execution speed and competitive path quality compared to standard APF, adaptive APF, A*, PRM, and RRT.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/102dc24d2d8cfbc8e6d1d7ea2397f8d21537a37a1ed1946ffbedecc40d34a0a3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/102dc24d2d8cfbc8e6d1d7ea2397f8d21537a37a1ed1946ffbedecc40d34a0a3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the local minima trap problem in Artificial Potential Field (APF) based robot navigation by proposing the &quot;Bulldozer&quot; technique. The method uses a backfilling mechanism to eliminate local minima and a ramp-based enhancement to escape traps, preserving APF&#x27;s advantages. Experimental results show it effectively solves the local minima issue while achieving fast execution and good path quality suitable for real-world use.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Process Reward Model, Policy-Invariant Reward Shaping, Multi-Perspective Reward Fusion]</p>
</li>
<li class="">
<p><strong>authors:</strong> Huajie Tan, Sixiang Chen, Yijie Xu, Zixiao Wang, Yuheng Ji, Cheng Chi, Yaoxu Lyu, Zhongxia Zhao, Xiansheng Chen, Peterson Co, Shaoxuan Xie, Guocai Yao, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Peking University, Beijing Academy of Artificial Intelligence</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23703" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23703</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://robo-dopamine.github.io" target="_blank" rel="noopener noreferrer" class="">https://robo-dopamine.github.io</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces Dopamine-Reward, a method for learning a step-aware, general-purpose process reward model (GRM) from multi-view inputs to overcome perceptual limitations. 2. Proposes a theoretically-sound Policy-Invariant Reward Shaping method within the Dopamine-RL framework to enable efficient policy learning without altering the optimal policy. 3. Demonstrates high efficiency and generalization, where a one-shot adapted GRM enables policy learning to achieve 95% success with only 150 online rollouts.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbdde8d7dea23f224b530580752926db8c72c9f5768172278573c890a3c6b0c6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbdde8d7dea23f224b530580752926db8c72c9f5768172278573c890a3c6b0c6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of designing effective reward functions for real-world robotic RL by introducing Robo-Dopamine. It proposes a general, step-aware reward model trained on a large dataset and a robust policy learning framework with theoretically-sound reward shaping. Experiments show the approach achieves state-of-the-art reward accuracy and significantly improves policy learning efficiency with strong generalization.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-30T09:42:39.000Z" itemprop="dateModified">Dec 30, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/csro/20251222-20251228"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251222-20251228 (cs.RO)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/category/cssc"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">cs.SC</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-29" class="table-of-contents__link toc-highlight">2025-12-29</a></li><li><a href="#2025-12-30" class="table-of-contents__link toc-highlight">2025-12-30</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/csro/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/csro/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>