{
  "categories": [
    {
      "label": "cs.AI",
      "slug": "csai",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.AR",
      "slug": "csar",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.CC",
      "slug": "cscc",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.CE",
      "slug": "csce",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.CG",
      "slug": "cscg",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.CL",
      "slug": "cscl",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.CR",
      "slug": "cscr",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.CV",
      "slug": "cscv",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.CY",
      "slug": "cscy",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.DB",
      "slug": "csdb",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.DC",
      "slug": "csdc",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.DL",
      "slug": "csdl",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.DM",
      "slug": "csdm",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.DS",
      "slug": "csds",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.ET",
      "slug": "cset",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.FL",
      "slug": "csfl",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.GR",
      "slug": "csgr",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.GT",
      "slug": "csgt",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.HC",
      "slug": "cshc",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.IR",
      "slug": "csir",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.IT",
      "slug": "csit",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.LG",
      "slug": "cslg",
      "week": "20251229-20260104",
<<<<<<< HEAD
      "items": []
=======
      "items": [
        {
          "title": "Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum",
          "authors": "Dimitrios Amaxilatis, Themistoklis Sarantakos, Nikolaos Tsironis, Souvik Sengupta, Kostas Ramantas, Jhofre Ojeda",
          "institution": "Spark Works Ltd., IONOS SE, Iquadrat Informática S.L.",
          "link": "https://arxiv.org/pdf/2512.21340",
          "code": null,
          "tags": [
            "on-device ai",
            "data spaces",
            "cloud-edge continuum",
            "containerized microservices",
            "edge AI",
            "intelligent infrastructure monitoring"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ae37cae2dac445e3682b661f116dd30e5d680105ac5070eb7b48c049ab9aff3_w640_q70.webp",
          "contributions": "1. A real-world implementation of intelligent infrastructure monitoring within a data space-enabled cloud-edge framework, demonstrating practical integration. 2. Leveraging edge computing, containerized microservices, and interoperable data sharing to address challenges like sensor integration, data privacy, and scalability. 3. Showcasing the training and deployment of AI/ML services directly at the edge for optimized resource use and timely decision-making in smart city applications.",
          "summary": "This paper presents a real-world use case for smart cities, implementing an intelligent climate monitoring system within a cloud-edge continuum framework enabled by data spaces. The method combines edge computing, containerized microservices, and secure data sharing to facilitate localized analytics and AI deployment. The conclusion highlights the transformative potential of integrating AI, edge computing, and data spaces for building efficient and resilient smart city infrastructures.",
          "mindmap": "graph TB\n        A[Harnessing Data Spaces for Smart City Infrastructures] --> B[核心问题/Problem: Enhancing smart city efficiency, sustainability, and resilience with secure, interoperable data exchange]\n        A --> C[主要方法/Method: Data space-enabled cloud-edge framework with edge computing, containerized microservices, and edge AI/ML]\n        A --> D[关键结果/Results: Demonstrates practical use case for intelligent monitoring, enabling localized analytics, real-time inference, and trusted data collaboration]"
        },
        {
          "title": "Physics-Informed Neural Solvers for Periodic Quantum Eigenproblems",
          "authors": "Haaris Mian",
          "institution": "Columbia University (Program in Applied Mathematics, Department of Applied Physics and Applied Mathematics)",
          "link": "https://arxiv.org/pdf/2512.21349",
          "code": null,
          "tags": [
            "physics-informed machine learning",
            "physics-informed neural networks",
            "Floquet-Bloch eigenvalue problem",
            "honeycomb lattice",
            "band structure",
            "transfer learning"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18928753702626417cc700c2238d86a4711adfe422a6c1995a68fbcbf3401d4f_w640_q70.webp",
          "contributions": "1. A physics-informed machine learning framework for solving the Floquet-Bloch eigenvalue problem for particles in 2D periodic potentials, using neural networks to learn Bloch functions and eigenvalues simultaneously. 2. A mesh-free solver that enforces the Schrödinger equation, Bloch periodicity, and normalization via a composite loss function without supervision. 3. Demonstration of transfer learning to adapt the solver from nearly-free to strongly varying potentials, capturing changes in band topology.",
          "summary": "This thesis proposes a physics-informed neural network framework to solve quantum eigenproblems for particles in periodic potentials, specifically targeting the honeycomb lattice. The method learns Bloch wavefunctions and their energies by training over the Brillouin zone with a loss function that encodes the governing physics. It is validated against traditional methods and shows promise for exploring band structure topology through transfer learning.",
          "mindmap": "graph TB\n        A[Physics-Informed Neural Solvers for Periodic Quantum Eigenproblems] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[求解二维周期势中的Floquet-Bloch本征值问题/Solve Floquet-Bloch eigenvalue problem in 2D periodic potentials]\n        B --> B2[关注石墨烯等材料的蜂窝晶格和能带拓扑/Focus on honeycomb lattice & band topology for materials like graphene]\n        C --> C1[使用神经网络同时学习布洛赫函数和本征值/Use neural networks to learn Bloch functions & eigenvalues]\n        C --> C2[通过复合损失函数强制执行薛定谔方程和周期性/Enforce Schrödinger eq. & periodicity via composite loss]\n        C --> C3[在布里渊区训练并探索迁移学习/Train over Brillouin zone & explore transfer learning]\n        D --> D1[数值验证与传统方法一致/Numerical validation against traditional methods]\n        D --> D2[迁移学习捕捉能带拓扑变化/Transfer learning captures changes in band topology]"
        },
        {
          "title": "A Reinforcement Learning Approach to Synthetic Data Generation",
          "authors": "Natalia Espinosa-Dice, Nicholas J. Jackson, Chao Yan, Aaron Lee, Bradley A. Malin",
          "institution": "Princeton University, Vanderbilt University Medical Center, Washington University in St. Louis",
          "link": "https://arxiv.org/pdf/2512.21395",
          "code": null,
          "tags": [
            "reinforcement learning",
            "synthetic data generation",
            "reinforcement learning",
            "proximal policy optimization",
            "privacy",
            "biomedical data"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75d9e0c3d2cba88654d16f9652042680f0037508065d6d94fba27208a6199969_w640_q70.webp",
          "contributions": "1. Reframes synthetic data generation (SDG) as a reinforcement learning problem, introducing a novel perspective. 2. Proposes RLSyn, a framework that models the data generator as a stochastic policy optimized via Proximal Policy Optimization with discriminator-derived rewards for stable, data-efficient training. 3. Demonstrates the effectiveness of the RL approach, showing it performs comparably to or better than GANs and diffusion models, especially in data-scarce regimes on biomedical datasets.",
          "summary": "This paper proposes RLSyn, a reinforcement learning framework for generating synthetic biomedical data by modeling the generator as a policy optimized with PPO. It shows that this approach achieves performance comparable to or better than GANs and diffusion models, particularly when training data is limited, offering a stable and data-efficient alternative for privacy-preserving data sharing.",
          "mindmap": "graph TB\n        A[A Reinforcement Learning Approach to Synthetic Data Generation] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: State-of-the-art generative models need large datasets and complex training, limiting use in small-sample settings.]\n        C[主要方法/Method: Reframe SDG as RL; introduce RLSyn (stochastic policy optimized via PPO with discriminator rewards).]\n        D[关键结果/Results: RLSyn performs comparably to/better than GANs & diffusion models, especially on smaller datasets.]"
        },
        {
          "title": "Learning to Reconfigure: Using Device Status to Select the Right Constrained Coding Scheme",
          "authors": "Doğukan Özbayrak, Ahmed Hareedy",
          "institution": "Middle East Technical University",
          "link": "https://arxiv.org/pdf/2512.21396",
          "code": null,
          "tags": [
            "storage systems",
            "constrained coding",
            "LOCO codes",
            "linear programming",
            "code reconfiguration",
            "two-dimensional magnetic recording (TDMR)"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602c83173fa9054dd0b0d4fea2c1b1c7d235aa475323c43a09847363ee851c20_w640_q70.webp",
          "contributions": "1. Proposes offline and online learning methods to reconfigure constrained coding schemes based on actual device status, moving beyond predetermined time stamps. 2. Models the reconfiguration problem as a linear programming optimization to maximize storage capacity and/or minimize decoding complexity, proving global optimality. 3. Demonstrates the effectiveness of the approach through experimental results in Two-Dimensional Magnetic Recording (TDMR) systems.",
          "summary": "This paper addresses the problem of optimally reconfiguring constrained coding schemes in storage devices as they age. The authors propose offline and online learning methods that fit device status data to polynomial models and formulate the reconfiguration decision as a linear programming problem to maximize capacity or minimize complexity. Experimental results show the proposed method is effective for TDMR systems and can be extended to other storage or transmission systems.",
          "mindmap": "graph TB\n        Root(”Learning to Reconfigure: Using Device Status to Select the Right Constrained Coding Scheme”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”设备老化需要不同等级的数据保护/Device aging requires different levels of data protection”)\n        Problem --> P2(”基于预定时间戳的重配置忽略实际状态/Reconfiguration based on timestamps neglects actual device status”)\n        Method --> M1(”提出离线和在线学习方法/Propose offline and online learning methods”)\n        Method --> M2(”将训练数据拟合为多项式方程/Fit training data to polynomial equations”)\n        Method --> M3(”将重配置建模为线性规划问题/Model reconfiguration as a linear programming problem”)\n        Results --> R1(”解决方案是全局最优的/Solution is globally optimal”)\n        Results --> R2(”实验证明在TDMR系统中有效/Experiments demonstrate effectiveness in TDMR systems”)"
        },
        {
          "title": "kooplearn: A Scikit-Learn Compatible Library of Algorithms for Evolution Operator Learning",
          "authors": "Giacomo Turri, Grégoire Pacreau, Giacomo Meanti, Timothée Devergne, Daniel Ordonez, Erfan Mirzaei, Bruno Belucci, Karim Lounici, Vladimir Kostic, Massimiliano Pontil, Pietro Novelli",
          "institution": "Italian Institute of Technology, École Polytechnique, Inria, University College London",
          "link": "https://arxiv.org/pdf/2512.21409",
          "code": "https://github.com/Machine-Learning-Dynamical-Systems/kooplearn",
          "tags": [
            "dynamical systems learning",
            "Koopman operator",
            "transfer operator",
            "spectral decomposition",
            "scikit-learn API",
            "reduced-order models"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6abf4d9f1d18bb64fbf627b5a4cc8d5b0c12b9f7fda20d8f6811df3f96602779_w640_q70.webp",
          "contributions": "1. Provides a unified library implementing linear, kernel, and deep-learning estimators for both discrete-time (Koopman/Transfer) and continuous-time evolution operators. 2. Offers a scikit-learn compatible API for easy integration into existing ML workflows. 3. Includes curated benchmark datasets to support experimentation, reproducibility, and fair algorithm comparison.",
          "summary": "The paper introduces kooplearn, a Python library for learning evolution operators (like Koopman and transfer operators) from dynamical systems data. It provides a suite of estimators and a scikit-learn compatible interface to enable spectral analysis, reduced-order modeling, and forecasting. The library aims to standardize and facilitate research in data-driven dynamical systems modeling.",
          "mindmap": "graph TB\n        Root[kooplearn: A Scikit-Learn Compatible Library] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Need for a unified, easy-to-use library for learning evolution operators from dynamical systems data] --> Problem_Sub1[应用/Application: Analyze systems, forecast, reduce model dimension]\n        Method[主要方法/Method: Implement linear, kernel, and deep-learning estimators] --> Method_Sub1[接口/Interface: Scikit-learn compatible API]\n        Method --> Method_Sub2[模型/Model: Discrete-time (Koopman/Transfer) & continuous-time operators]\n        Results[关键结果/Results: kooplearn library released] --> Results_Sub1[特性/Features: Includes benchmark datasets for fair comparison]\n        Results --> Results_Sub2[目标/Goal: Facilitate integration, experimentation, and reproducibility]"
        },
        {
          "title": "A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding",
          "authors": "Christina Liu, Alan Q. Wang, Joy Hsu, Jiajun Wu, Ehsan Adeli",
          "institution": "California Institute of Technology, Stanford University",
          "link": "https://arxiv.org/pdf/2512.21414",
          "code": "https://github.com/christinaliu2020/tool-bottleneck-framework",
          "tags": [
            "medical image analysis",
            "tool-use framework",
            "vision-language model",
            "interpretability",
            "data-efficiency",
            "tool bottleneck model"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp",
          "contributions": "1. Proposed the Tool Bottleneck Framework (TBF) for medical image understanding, which uses a learned Tool Bottleneck Model (TBM) to compose tool outputs instead of text-based composition. 2. Introduced a strategy for the TBM to handle arbitrary VLM tool selections, enabling flexible and robust prediction. 3. Demonstrated that the framework improves performance, interpretability, and data-efficiency, matching or surpassing deep learning classifiers and other tool-use frameworks, especially in data-limited scenarios.",
          "summary": "This paper addresses the problem that existing tool-use frameworks for image understanding perform poorly on medical images due to their reliance on text to compose specialized tools. The authors propose the Tool Bottleneck Framework (TBF), which uses a vision-language model to select tools and a learned neural network (the Tool Bottleneck Model) to fuse their outputs. Evaluations on histopathology and dermatology tasks show TBF performs on par with or better than existing methods, offering gains in data-limited settings and more interpretable predictions.",
          "mindmap": "graph TB\n        A[Tool Bottleneck Framework for Medical Image Understanding] --> B[核心问题/Problem: Text-based tool composition fails for medical images with localized features]\n        A --> C[主要方法/Method: TBF uses VLM to select tools, TBM (neural network) to fuse outputs]\n        A --> D[关键结果/Results: Matches or beats baselines, more interpretable, data-efficient]"
        },
        {
          "title": "A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning",
          "authors": "Alimu Alibotaiken, Suyang Wang, Oluwaseun T. Ajayi, Yu Cheng",
          "institution": "Illinois Institute of Technology, California State University, San Bernardino",
          "link": "https://arxiv.org/pdf/2512.21412",
          "code": null,
          "tags": [
            "wireless networking",
            "Age of Information (AoI)",
            "reinforcement learning",
            "freshness optimization",
            "wireless networks",
            "multi-agent systems"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b58e6fe193d4299ab0beca4eb949d8b13e21e8198c223c3dd6c0ca64896bbf7d_w640_q70.webp",
          "contributions": "1. Proposes a novel taxonomy for Age of Information (AoI) and its variants, categorizing them into native, function-based, and application-oriented families to clarify freshness modeling for B5G/6G systems. 2. Introduces a policy-centric taxonomy for reinforcement learning in freshness-aware networks, consisting of update-control RL, medium-access RL, risk-sensitive RL, and multi-agent RL. 3. Synthesizes recent RL-driven freshness control progress and highlights open challenges like delayed decision processes and cross-layer design to establish a unified foundation for learning-based freshness optimization.",
          "summary": "This survey addresses the gap between classical Age of Information (AoI) studies and broad reinforcement learning (RL) discussions in wireless networks by examining RL specifically for freshness optimization. It organizes AoI variants and introduces a policy-centric RL taxonomy to provide a coherent framework for freshness-aware decision-making in next-generation wireless systems. The paper aims to establish a unified foundation for learning-based freshness control and highlights key open challenges for future research.",
          "mindmap": "graph TB\n        A[A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有综述的不足: 经典AoI与泛化RL研究分离 / Gap: Classical AoI vs. Broad RL]\n        C --> C1[提出以AoI为中心的RL综述框架 / Propose AoI-centric RL Survey Framework]\n        C --> C2[构建AoI变体分类与策略中心分类法 / Build AoI Variant & Policy-Centric Taxonomies]\n        D --> D1[为B5G/6G建立学习式新鲜度优化的统一基础 / Establish Unified Foundation for Learning-based Freshness Optimization]\n        D --> D2[识别开放挑战: 延迟决策、随机性、跨层设计 / Identify Open Challenges: Delayed Decisions, Stochasticity, Cross-layer]"
        },
        {
          "title": "Scalable Deep Subspace Clustering Network",
          "authors": "Nairouz Mrabah, Mohamed Bouguessa, Sihem Sami",
          "institution": "University of Quebec at Montreal",
          "link": "https://arxiv.org/pdf/2512.21434",
          "code": null,
          "tags": [
            "subspace clustering",
            "landmark-based approximation",
            "self-expression",
            "spectral clustering",
            "linear complexity",
            "convolutional auto-encoder"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp",
          "contributions": "1. Proposes a deep subspace clustering framework (SDSNet) that achieves linear O(n) computational complexity, breaking the traditional O(n^3) bottleneck. 2. Introduces a landmark-based approximation to avoid constructing the full n×n affinity matrix, combined with joint optimization of auto-encoder reconstruction and self-expression objectives. 3. Enables direct spectral clustering on factorized representations, integrating convolutional auto-encoders with subspace-preserving constraints for efficient and accurate clustering.",
          "summary": "The paper addresses the high computational cost (O(n^3)) of traditional subspace clustering methods. It proposes SDSNet, a scalable deep learning framework that uses landmark approximation and joint optimization to achieve linear O(n) complexity. Experiments show SDSNet maintains clustering quality comparable to state-of-the-art methods while being significantly more efficient.",
          "mindmap": "graph TB\n        Root[”Scalable Deep Subspace Clustering Network”] --> Problem[”核心问题/Problem: O(n^3) 计算复杂度 / O(n^3) Computational Complexity”]\n        Root --> Method[”主要方法/Method: 地标近似与联合优化 / Landmark Approximation & Joint Optimization”]\n        Root --> Results[”关键结果/Results: 线性复杂度与可比性能 / Linear Complexity & Comparable Performance”]"
        },
        {
          "title": "Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors",
          "authors": "Hridya Dhulipala, Xiaokai Rong, Tien N. Nguyen",
          "institution": "University of Texas at Dallas",
          "link": "https://arxiv.org/pdf/2512.21431",
          "code": null,
          "tags": [
            "software testing",
            "runtime error detection",
            "coverage-guided testing",
            "multi-agent reasoning",
            "large language models",
            "static analysis"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d41379a4c8476f7ed1a8a02b193c5fe427e6a274d56beccd85313ce47ba5e76_w640_q70.webp",
          "contributions": "1. Proposes Cerberus, a novel predictive, execution-free coverage-guided testing framework that uses LLMs for input generation, coverage prediction, and error detection without code execution. 2. Introduces a two-phase feedback loop that first maximizes code coverage and detects errors, then focuses solely on error detection after coverage is maximized, improving performance over single-phase prompting. 3. Empirically demonstrates that Cerberus outperforms conventional and learning-based testing frameworks for both complete and incomplete code snippets by generating high-coverage test cases more efficiently and discovering more runtime errors.",
          "summary": "The paper proposes Cerberus, a framework that uses Large Language Models (LLMs) to statically detect runtime errors in code snippets without execution. It employs a multi-agent reasoning approach with a two-phase, coverage-guided feedback loop to generate test inputs and predict errors. The evaluation shows Cerberus is more efficient and effective at finding runtime errors than existing testing methods.",
          "mindmap": "graph TB\n        A[Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors] --> B(核心问题/Problem: Detecting runtime errors in code snippets without execution is crucial for software safety.)\n        A --> C(主要方法/Method: Uses LLMs for execution-free, coverage-guided testing with a two-phase feedback loop.)\n        A --> D(关键结果/Results: Outperforms conventional and learning-based frameworks by generating high-coverage tests and finding more errors.)"
        },
        {
          "title": "DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction",
          "authors": "Khondoker Mirazul Mumenin, Robert Underwood, Dong Dai, Jinzhen Wang, Sheng Di, Zarija Lukić, Franck Cappello",
          "institution": "University of North Carolina at Charlotte, Argonne National Laboratory, University of Delaware, Lawrence Berkeley National Laboratory",
          "link": "https://arxiv.org/pdf/2512.21433",
          "code": null,
          "tags": [
            "model compression (quantization/pruning)",
            "lossy compression",
            "quality prediction",
            "deep-surrogate",
            "mixture-of-experts",
            "feature-extraction"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06e831f83754144a92a117a184fdcc51da0584d4163390cf94b34d4175b77a1_w640_q70.webp",
          "contributions": "1) A generalizable surrogate model for predicting compression quality across different compressors, quality metrics, and datasets. 2) A novel two-stage design that decouples expensive feature extraction from lightweight prediction for efficient training and modular inference. 3) A mixture-of-experts design to optimize performance and robustness for time-evolving scientific data.",
          "summary": "This paper proposes DeepCQ, a deep-surrogate framework to efficiently predict the quality of data after lossy compression, which is traditionally computationally expensive to assess. The method uses a two-stage and mixture-of-experts design for generalizability and robustness across different compressors, metrics, and time-evolving datasets. The framework achieves high predictive accuracy (errors under 10%) on real-world applications, enabling informed compression decisions and reducing I/O and computational overhead.",
          "mindmap": "graph TB\n        A[DeepCQ: 通用深度代理框架用于有损压缩质量预测] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[评估压缩后数据质量的计算成本高/Expensive to assess post-compression data quality]\n        C --> C1[两阶段设计: 特征提取 + 轻量预测/Two-stage design: feature extraction + lightweight prediction]\n        C --> C2[专家混合设计处理时变数据/Mixture-of-experts for time-evolving data]\n        D --> D1[预测误差普遍低于10%/Prediction errors generally under 10%]\n        D --> D2[显著优于现有方法/Significantly outperforms existing methods]"
        },
        {
          "title": "Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models",
          "authors": "Geoffroy Morlat, Marceau Nahon, Augustin Chartouny, Raja Chatila, Ismael T. Freire, Mehdi Khamassi",
          "institution": "Institute of Intelligent Systems and Robotics, Sorbonne University",
          "link": "https://arxiv.org/pdf/2512.21439",
          "code": null,
          "tags": [
            "computational ethics",
            "moral context",
            "probabilistic clustering",
            "LLM semantics",
            "interpretable prediction",
            "human judgment"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f4abc9f666c2d29dadd77869bddf3f159d0bbc8839c7c0f65bbdb4c29ad40c_w640_q70.webp",
          "contributions": "1. An empirically grounded dataset of 300 moral scenarios with human ternary judgments. 2. A reproducible pipeline (COMETH) combining human judgments, probabilistic context learning, and LLM-based semantic abstraction. 3. An interpretable, context-sensitive moral prediction model that outperforms end-to-end LLM prompting.",
          "summary": "The paper addresses the problem that moral judgments depend heavily on context. It proposes the COMETH framework, which uses probabilistic clustering on human judgment data and LLM-based semantic abstraction to learn and explain action-specific moral contexts. The main conclusion is that COMETH significantly outperforms direct LLM prompting in aligning with human majority judgments while providing interpretable predictions.",
          "mindmap": "graph TB\n        A[COMETH: Learning Interpretable Moral Contexts] --> B[核心问题/Problem: Moral judgments are context-dependent]\n        A --> C[主要方法/Method: Probabilistic clustering + LLM semantics + Human judgments]\n        A --> D[关键结果/Results: Doubles alignment with human judgments vs. LLM prompting]"
        },
        {
          "title": "Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing",
          "authors": "Hridya Dhulipala, Xiaokai Rong, Aashish Yadavally, Tien N. Nguyen",
          "institution": "University of Texas at Dallas",
          "link": "https://arxiv.org/pdf/2512.21440",
          "code": null,
          "tags": [
            "fuzz testing",
            "initial corpus generation",
            "large language models",
            "multi-agent framework",
            "predictive code coverage",
            "mutation-based fuzzing"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcb8eafec283e39ae04e0274dc4688aced924346193560581e8469f1151507f6_w640_q70.webp",
          "contributions": "1. Proposes FuzzWise, a novel method that integrates initial corpus generation and minimization into a single, streamlined process using an LLM-based multi-agent framework., 2. Introduces a predictive code coverage module (an LLM agent) that assesses new test cases without requiring actual program execution, saving computational resources., 3. Demonstrates empirically that FuzzWise generates a smaller, higher-quality initial corpus that achieves higher code coverage and triggers more runtime errors more efficiently than baseline methods.",
          "summary": "The paper addresses the problem of generating a high-quality initial seed corpus for mutation-based fuzzing. It proposes FuzzWise, a method that uses a multi-agent LLM framework to generate and intelligently select test cases based on predicted coverage without execution. The evaluation shows FuzzWise produces a smaller, more effective corpus that achieves higher coverage and finds more bugs efficiently.",
          "mindmap": "graph TB\n        A[FuzzWise: Intelligent Initial Corpus Generation for Fuzzing] --> B[核心问题/Problem: 为模糊测试生成高质量的初始种子语料库/Generating high-quality initial seed corpus for fuzzing]\n        A --> C[主要方法/Method: 基于LLM的多智能体框架，集成生成与预测性覆盖评估/LLM-based multi-agent framework integrating generation and predictive coverage assessment]\n        A --> D[关键结果/Results: 用更少的测试用例实现更高的代码覆盖率和错误发现率/Achieves higher code coverage and bug detection with fewer test cases]"
        },
        {
          "title": "dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning",
          "authors": "Shirui Chen, Jiantao Jiao, Lillian J. Ratliff, Banghua Zhu",
          "institution": "University of Washington, University of California, Berkeley",
          "link": "https://arxiv.org/pdf/2512.21446",
          "code": null,
          "tags": [
            "diffusion models",
            "masked diffusion language models",
            "reinforcement learning",
            "parallel decoding",
            "on-policy optimization",
            "unmasking planner"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f1e67e2dde4b6b9e98e4e3c5574326f0e2d63114afe47049e17c2ae04bb41b_w640_q70.webp",
          "contributions": "1. Proposes dUltra, an on-policy RL framework (GRPO-based) for learning efficient unmasking strategies in MDLMs. 2. Introduces a joint optimization scheme for the base diffusion model and a new unmasking planner head using a composite reward. 3. Demonstrates improved accuracy-efficiency trade-off over heuristic and distillation baselines in reasoning and code generation tasks.",
          "summary": "The paper addresses the slow sampling speed of masked diffusion language models (MDLMs) by proposing dUltra, a reinforcement learning framework that learns an optimal strategy for parallel token unmasking. The method jointly optimizes the diffusion model and a planner head using rewards for correctness, distillation, and step count. The results show dUltra achieves a better trade-off between accuracy and efficiency than existing methods, advancing towards \"diffusion supremacy\" over autoregressive models.",
          "mindmap": "graph TB\n        A[dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[MDLMs解码慢，速度优势有限/MDLMs decode slowly, limiting speed advantage]\n        C --> C1[基于GRPO的在线强化学习框架/On-policy RL framework based on GRPO]\n        C --> C2[联合优化扩散模型与解掩码规划器/Jointly optimize diffusion model & unmasking planner]\n        D --> D1[提升精度-效率权衡/Improves accuracy-efficiency trade-off]\n        D --> D2[迈向”扩散霸权”/Moving towards ”diffusion supremacy”]"
        },
        {
          "title": "An Equivariance Toolbox for Learning Dynamics",
          "authors": "Yongyi Yang, Liu Ziyin",
          "institution": "University of Michigan, Massachusetts Institute of Technology, NTT Research",
          "link": "https://arxiv.org/pdf/2512.21447",
          "code": null,
          "tags": [
            "learning theory",
            "equivariance",
            "Noether's theorem",
            "Hessian constraints",
            "learning dynamics",
            "symmetry"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69f03ddf87a64feb67cd904eadbbfc83a393f0b282be28c74e5bfeac489db50b_w640_q70.webp",
          "contributions": "1. Developed a general equivariance framework that unifies first-order constraints (conservation laws, implicit bias) into a single identity. 2. Extended the analysis to second-order, providing structural predictions about curvature, flat/sharp directions, and gradient-Hessian alignment. 3. Generalized classical symmetry analyses from continuous transformations to include general equivariance and discrete transformations.",
          "summary": "The paper develops a unified equivariance toolbox for analyzing learning dynamics in neural networks. It extends classical symmetry-based analyses to derive coupled first- and second-order constraints, connecting transformation structure to loss landscape geometry. The framework recovers known results and provides new characterizations linking equivariance to modern optimization phenomena.",
          "mindmap": "graph TB\n        Root[An Equivariance Toolbox for Learning Dynamics] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[现有分析是特定问题的/Existing analyses are problem-specific]\n        Problem --> P2[二阶结构理解不足/Second-order structure less understood]\n        Method[主要方法/Method] --> M1[构建等变性工具箱/Build general equivariance toolbox]\n        Method --> M2[扩展诺特定理分析/Extend Noether-type analyses]\n        Results[关键结果/Results] --> R1[统一一阶约束/Unify first-order constraints]\n        Results --> R2[提供二阶结构预测/Provide second-order structural predictions]\n        Results --> R3[连接变换结构与几何/Connect transformation structure to geometry]"
        },
        {
          "title": "RLLaVA: An RL-central Framework for Language and Vision Assistants",
          "authors": "Lei Zhao, Zihao Ma, Boyu Lin, Yuhe Liu, Wenjun Wu, Lei Huang",
          "institution": "Beihang University",
          "link": "https://arxiv.org/pdf/2512.21450",
          "code": "https://github.com/TinyLoopX/RLLaVA",
          "tags": [
            "multi-modal training",
            "reinforcement learning",
            "vision-language models",
            "Markov decision process",
            "resource-efficient training",
            "modular framework"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69c1815245107c8b5c717a57c722eeb64e0b9b4b77c5a989f0d2b9f4353b36df_w640_q70.webp",
          "contributions": "1. Proposes a modular RL framework (RLLaVA) that decouples algorithmic logic from model architecture and distributed execution, enabling easy implementation of new RL algorithms. 2. Formulates the joint visual-textual sequential decision of VLMs as a unified Markov Decision Process (MDP), providing a principled foundation for multi-modal RL. 3. Achieves resource-efficient training, enabling end-to-end full-parameter updates for up to 4B-scale models on a single 24GB GPU.",
          "summary": "The paper introduces RLLaVA, a lightweight and modular reinforcement learning framework specifically designed for training vision-language models. It decouples RL logic from system components to simplify algorithm development and enables efficient training of large models on common GPUs. Experiments show that models trained with RLLaVA improve over base models and are competitive with other specialized RL frameworks.",
          "mindmap": "graph TB\n        A[RLLaVA: An RL-central Framework for Language and Vision Assistants] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[缺乏专门的多模态RL框架 / Lack of specialized multi-modal RL framework]\n        B --> B2[现有框架资源消耗大 / Existing frameworks are resource-intensive]\n        C --> C1[解耦RL逻辑与架构 / Decouple RL logic from architecture & execution]\n        C --> C2[统一MDP建模 / Unified MDP formulation for VLMs]\n        C --> C3[支持多种算法与模型 / Supports broad RL methods & VLMs]\n        D --> D1[资源高效训练 / Resource-efficient training (e.g., 4B model on 24GB GPU)]\n        D --> D2[性能提升 / Models show improved performance]\n        D --> D3[任务可扩展性 / Task extensibility demonstrated]"
        },
        {
          "title": "CCAD: Compressed Global Feature Conditioned Anomaly Detection",
          "authors": "Xiao Jin, Liang Diao, Qixin Xiao, Yifan Hu, Ziqi Zhang, Yuchen Liu, Haisong Gu",
          "institution": "Columbia University, University of Michigan, University of California at Berkeley, Stevens Institute of Technology, VisionX LLC",
          "link": "https://arxiv.org/pdf/2512.21459",
          "code": "https://github.com/chloeqxq/CCAD",
          "tags": [
            "anomaly detection",
            "global feature conditioning",
            "adaptive compression",
            "reconstruction-based anomaly detection"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp",
          "contributions": "1. Proposes CCAD, a novel method that synergizes reconstruction-based and representation-based anomaly detection by using compressed global features as a conditioning modality for the reconstruction model. 2. Introduces an adaptive compression mechanism to enhance model generalization and training efficiency. 3. Contributes a reorganized and re-annotated version of the DAGM 2007 dataset to validate the method's effectiveness.",
          "summary": "This paper addresses the challenges of anomaly detection in industrial settings with limited anomalous data by proposing CCAD, a method that combines the strengths of reconstruction-based and representation-based approaches. CCAD conditions a reconstruction model on adaptively compressed global features, leading to improved generalization and training efficiency. Experiments show that CCAD outperforms state-of-the-art methods in AUC and achieves faster convergence.",
          "mindmap": "graph TB\n        A[CCAD: Compressed Global Feature Conditioned Anomaly Detection] --> B[核心问题/Problem: 异常检测在有限异常数据下的挑战，现有方法在泛化性、效率和约束上的不足]\n        A --> C[主要方法/Method: 提出CCAD，融合重建与表征方法，使用压缩的全局特征作为重建模型的条件]\n        A --> D[关键结果/Results: 在AUC上超越SOTA，收敛更快，贡献了重新标注的DAGM 2007数据集]"
        },
        {
          "title": "Statistical vs. Deep Learning Models for Estimating Substance Overdose Excess Mortality in the US",
          "authors": "Sukanya Krishna, Marie-Laure Charpignon, Maimuna Majumder",
          "institution": "Harvard University, Boston Children's Hospital, Broad Institute of MIT and Harvard, Massachusetts Institute of Technology",
          "link": "https://arxiv.org/pdf/2512.21456",
          "code": null,
          "tags": [
            "time series forecasting",
            "LSTM",
            "SARIMA",
            "conformal prediction",
            "counterfactual estimation",
            "uncertainty quantification"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f7ae15bb7d75d1d53ce8df2d4924f6311b8bfce998ef81244521cd5679c4225_w640_q70.webp",
          "contributions": "1. Conducted a systematic empirical comparison showing LSTM outperforms SARIMA in point estimation and uncertainty calibration for counterfactual mortality projection under regime change. 2. Provided analysis that attention-based models (Seq2Seq, Transformer) underperform in this task due to overfitting to historical means. 3. Developed a reproducible pipeline incorporating conformal prediction intervals and extensive convergence analysis, providing an open-source framework deployable for public health planning.",
          "summary": "This paper compares traditional statistical (SARIMA) and deep learning models (LSTM, Seq2Seq, Transformer) for estimating substance overdose excess mortality in the US during the COVID-19 pandemic. The study finds that LSTM provides more accurate and better-calibrated counterfactual estimates than SARIMA, establishing that carefully validated deep learning models can be more reliable for public health planning under structural disruptions.",
          "mindmap": "graph TB\n        A[Statistical vs. Deep Learning Models for Estimating Substance Overdose Excess Mortality in the US] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[估计疫情导致的药物过量超额死亡率/Estimating pandemic-attributable excess mortality]\n        B --> B2[传统方法在结构性变化下可能失效/Traditional methods may fail under structural change]\n        C --> C1[系统比较SARIMA与深度学习模型/Systematic comparison of SARIMA vs. DL models]\n        C --> C2[使用CDC数据(2015-2019)进行训练/Using CDC data (2015-2019) for training]\n        C --> C3[预测2020-2023年的反事实轨迹/Projecting counterfactual trajectories for 2020-2023]\n        D --> D1[LSTM在点估计和不确定性校准上表现最佳/LSTM achieves best point estimation & uncertainty calibration]\n        D --> D2[注意力模型因过拟合历史均值而表现不佳/Attention models underperform due to overfitting to historical means]\n        D --> D3[提供可部署的开源框架/Providing a deployable open-source framework]"
        },
        {
          "title": "When Bayesian Tensor Completion Meets Multioutput Gaussian Processes: Functional Universality and Rank Learning",
          "authors": "Siyuan Li, Shikai Fang, Lei Cheng, Feng Yin, Yik-Chung Wu, Peter Gerstoft, Sergios Theodoridis",
          "institution": "Zhejiang University, The Chinese University of Hong Kong, Shenzhen, The University of Hong Kong, Technical University of Denmark, University of California, San Diego, Athena R.C.",
          "link": "https://arxiv.org/pdf/2512.21486",
          "code": "https://github.com/OceanSTARLab/RR-FBTC",
          "tags": [
            "tensor decomposition",
            "Bayesian tensor completion",
            "multioutput Gaussian processes",
            "variational inference",
            "rank learning",
            "functional universality"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac46f14d99db0300a24117fcc6a1f29a54c4ad4ae1586354da1e156d0b048995_w640_q70.webp",
          "contributions": "1. Proposes a rank-revealing functional Bayesian tensor completion (RR-FBTC) method that handles tensors with real-valued indices and automatically determines the tensor rank during inference. 2. Establishes the universal approximation property of the model, demonstrating its expressive power for continuous multi-dimensional signals. 3. Derives an efficient variational inference algorithm with closed-form updates for learning the model.",
          "summary": "The paper addresses the challenge of determining the optimal rank in functional tensor decomposition, which is NP-hard. It proposes a new method called RR-FBTC that uses multioutput Gaussian processes to model latent functions, enabling automatic rank learning and functional universality. Experiments show the method is effective and superior to state-of-the-art approaches.",
          "mindmap": "graph TB\n        A[When Bayesian Tensor Completion Meets Multioutput Gaussian Processes: Functional Universality and Rank Learning] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法需已知张量秩/Existing methods require known tensor rank]\n        B --> B2[确定最优秩是NP难问题/Determining optimal rank is NP-hard]\n        C --> C1[提出RR-FBTC方法/Propose RR-FBTC method]\n        C --> C2[使用多输出高斯过程建模/Model with multioutput Gaussian processes]\n        C --> C3[变分推断与闭式更新/Variational inference with closed-form updates]\n        D --> D1[证明泛函逼近能力/Prove functional universal approximation]\n        D --> D2[实现自动秩学习/Achieve automatic rank learning]\n        D --> D3[实验验证有效性/Experiments validate effectiveness]"
        },
        {
          "title": "MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding",
          "authors": "Aiwei Zhang, Arvind Pillai, Andrew Campbell, Nicholas C. Jacobson",
          "institution": "Dartmouth College",
          "link": "https://arxiv.org/pdf/2512.21506",
          "code": null,
          "tags": [
            "multi-modal training",
            "wearable sensing",
            "actigraphy encoder",
            "projection module",
            "frozen LLM",
            "behavioral summarization"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp",
          "contributions": "1. Introduces MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs) for free-text generation of daily behavioral summaries. 2. Constructs a novel, large-scale dataset of 54,383 (actigraphy, text) pairs derived from real-world NHANES recordings. 3. Demonstrates superior performance over prompt-based baselines in semantic fidelity and lexical accuracy, with qualitative analysis showing the model captures circadian structure and behavioral transitions.",
          "summary": "The paper addresses the challenge of generating natural language summaries from raw physiological signals like actigraphy. It proposes MotionTeller, a framework that integrates a pretrained actigraphy encoder with a frozen LLM via a projection module. The model, trained on a novel dataset, outperforms baselines in generating fluent, human-centered descriptions of daily behavior.",
          "mindmap": "graph TB\n        A[MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs] --> B[核心问题/Problem: How to generate natural language summaries from raw physiological signals like actigraphy?]\n        A --> C[主要方法/Method: Combines a pretrained actigraphy encoder and a projection module to map behavioral embeddings into a frozen LLM's token space.]\n        A --> D[关键结果/Results: Achieves high semantic fidelity (BERTScore-F1=0.924) and lexical accuracy (ROUGE-1=0.722), outperforming baselines by 7%.]"
        },
        {
          "title": "Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering",
          "authors": "Wenyuan Yang, Jie Xu, Hongqing He, Jiangzhang Gan, Xiaofeng Zhu",
          "institution": "University of Electronic Science and Technology of China, Hainan University, Singapore University of Technology and Design, Guangxi Normal University",
          "link": "https://arxiv.org/pdf/2512.21510",
          "code": null,
          "tags": [
            "multi-view clustering",
            "incomplete multi-view clustering",
            "missing pattern tree",
            "decision ensemble",
            "knowledge distillation"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp",
          "contributions": "1. Proposes a missing-pattern tree model to group data into decision sets for fully utilizing available multi-view pairs, addressing the pair under-utilization issue. 2. Introduces a multi-view decision ensemble module that uses uncertainty-based weighting to aggregate robust clustering decisions from different sets. 3. Designs an ensemble-to-individual knowledge distillation module to transfer ensemble knowledge to view-specific models, promoting mutual optimization between ensemble and individual modules.",
          "summary": "This paper addresses the problem of incomplete multi-view clustering (IMVC) where inconsistent missing patterns hinder performance. It proposes a framework called TreeEIC that groups data by missing pattern, performs clustering within each group, ensembles the results with uncertainty weighting, and uses knowledge distillation to refine individual models. Experiments show TreeEIC achieves state-of-the-art performance and robustness.",
          "mindmap": "graph TB\n        Root[”Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering”] --> Problem[”核心问题/Problem: Inconsistent missing patterns in multi-view data limit clustering performance.”]\n        Root --> Method[”主要方法/Method: TreeEIC framework with missing-pattern tree grouping, decision ensemble, and knowledge distillation.”]\n        Root --> Results[”关键结果/Results: Achieves state-of-the-art IMVC performance and superior robustness.”]"
        },
        {
          "title": "First Provable Guarantees for Practical Private FL: Beyond Restrictive Assumptions",
          "authors": "Egor Shulgin, Grigory Malinovsky, Sarit Khirirat, Peter Richtárik",
          "institution": "King Abdullah University of Science and Technology (KAUST)",
          "link": "https://arxiv.org/pdf/2512.21521",
          "code": null,
          "tags": [
            "federated learning",
            "differential privacy",
            "convergence guarantees",
            "partial client participation",
            "local updates",
            "clipping bias"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffad28b47b2302842557d1ba4a799ee36a241e4ab2529611ee61b38dd671f76d_w640_q70.webp",
          "contributions": "1. Introduces Fed-α-NormEC, the first DP FL framework with provable convergence under standard assumptions (without bounded gradients or heterogeneity). 2. Fully supports practical FL features like multiple local updates and, crucially, partial client participation. 3. Provides theoretical analysis showing how partial client participation is essential for real-world deployment and vital for privacy amplification.",
          "summary": "The paper addresses the gap between theoretical private federated learning methods, which rely on unrealistic assumptions, and practical deployment needs. It proposes Fed-α-NormEC, a new framework that provides provable differential privacy and convergence guarantees while supporting key practical features like local updates and partial client participation. Experiments on private deep learning tasks validate the theoretical findings.",
          "mindmap": "graph TB\n        Root[First Provable Guarantees for Practical Private FL] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[现有私有FL方法依赖不现实假设/Existing private FL relies on unrealistic assumptions]\n        Problem --> P2[忽略本地更新与部分客户端参与/Neglects local updates & partial participation]\n        Method[主要方法/Method] --> M1[提出Fed-α-NormEC框架/Propose Fed-α-NormEC framework]\n        Method --> M2[集成本地更新与独立学习率/Integrates local updates & separate stepsizes]\n        Method --> M3[支持部分客户端参与/Supports partial client participation]\n        Results[关键结果/Results] --> R1[提供可证明的收敛与DP保证/Provides provable convergence & DP guarantees]\n        Results --> R2[实验验证理论/Experiments corroborate theory]"
        },
        {
          "title": "Perplexity-Aware Data Scaling Law: Perplexity Landscapes Predict Performance for Continual Pre-training",
          "authors": "Lei Liu, Hao Zhu, Yue Shen, Zhixuan Chu, Jian Wang, Jinjie Gu, Kui Ren",
          "institution": "Ant Group, Zhejiang University",
          "link": "https://arxiv.org/pdf/2512.21515",
          "code": null,
          "tags": [
            "llm training",
            "continual pre-training",
            "scaling laws",
            "perplexity",
            "data selection",
            "knowledge gap"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d01b1cad6e908309c7e813cb1d98f2c41345aa1e4d78cbdaf163996c5d111b4_w640_q70.webp",
          "contributions": "1. Proposes a novel perplexity-aware data scaling law that predicts model test loss from the perplexity landscape of domain data, moving beyond dataset size. 2. Introduces the concept of \"perplexity landscapes\" to quantify the informational value and knowledge gap of candidate training samples. 3. Enables adaptive selection of high-utility data subsets for Continual Pre-training, improving efficiency and performance by prioritizing informative content and reducing redundancy.",
          "summary": "This paper addresses the inefficiency of scaling data for Continual Pre-training (CPT) of LLMs, where simply adding more data yields diminishing returns. The authors propose a new scaling law that uses the model's perplexity on domain data as a proxy for the knowledge gap, allowing for the predictive selection of optimal training subsets. Experiments show this method consistently identifies high-utility data, leading to superior performance on domain-specific benchmarks.",
          "mindmap": "graph TB\n        A[Perplexity-Aware Data Scaling Law<br>困惑度感知数据缩放定律] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[CPT中单纯增加数据收益递减<br>Diminishing returns from scaling data in CPT]\n        C --> C1[提出基于困惑度景观的缩放定律<br>Propose perplexity-landscape-based scaling law]\n        C1 --> C2[利用困惑度量化知识差距<br>Use perplexity to quantify knowledge gap]\n        C2 --> C3[自适应选择高价值数据子集<br>Adaptively select high-utility data subsets]\n        D --> D1[识别接近最优的训练子集<br>Identifies near-optimal training subsets]\n        D1 --> D2[在领域基准上取得优越性能<br>Achieves superior performance on domain benchmarks]"
        },
        {
          "title": "Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data",
          "authors": "Hongqing He, Jie Xu, Wenyuan Yang, Yonghua Zhu, Guoqiu Wen, Xiaofeng Zhu",
          "institution": "Guangxi Normal University, University of Electronic Science and Technology of China, Singapore University of Technology and Design, Hainan University",
          "link": "https://arxiv.org/pdf/2512.21516",
          "code": null,
          "tags": [
            "multi-view clustering",
            "contrastive learning",
            "incomplete multi-view data",
            "noise-robust clustering",
            "graph-guided learning",
            "imputation-free"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp",
          "contributions": "1. Proposes a global-graph guided contrastive learning strategy to address the rare-paired sample issue in incomplete multi-view data by constructing a global affinity graph to form new sample pairs. 2. Introduces a local-graph weighted contrastive learning mechanism to mitigate the mis-paired sample issue caused by noise, using local neighbors to generate adaptive weights for pair-wise contrast. 3. Integrates these strategies into a unified, imputation-free framework for effective clustering on both incomplete and noisy multi-view data.",
          "summary": "This paper addresses the challenges of rare-paired and mis-paired samples in contrastive learning for multi-view clustering on incomplete and noisy data. It proposes a unified framework combining global-graph guided contrastive learning to explore complementary information and local-graph weighted contrastive learning to adaptively handle noisy pairs. Experiments show the method outperforms state-of-the-art approaches on both incomplete and noisy data settings.",
          "mindmap": "graph TB\n        A[Global-Graph Guided and Local-Graph Weighted Contrastive Learning<br/>全局-局部图引导对比学习] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>Rare-paired & Mis-paired Samples<br/>样本配对稀少与错误] --> B1[Incomplete & Noise Multi-View Data<br/>不完整与噪声多视图数据]\n        C[主要方法/Method<br/>Unified Contrastive Learning Framework<br/>统一对比学习框架] --> C1[Global-Graph Guided CL<br/>全局图引导对比学习]\n        C --> C2[Local-Graph Weighted CL<br/>局部图加权对比学习]\n        C1 --> C1a[Construct Global Affinity Graph<br/>构建全局亲和力图]\n        C2 --> C2a[Generate Adaptive Weights<br/>生成自适应权重]\n        D[关键结果/Results<br/>Superior Clustering Performance<br/>优越的聚类性能] --> D1[Outperforms SOTA Methods<br/>超越现有最佳方法]\n        D --> D2[Effective on Incomplete & Noise Data<br/>在不完整与噪声数据上有效]"
        },
        {
          "title": "Generative Actor Critic",
          "authors": "Aoyang Qin, Deqian Kong, Wei Wang, Ying Nian Wu, Song-Chun Zhu, Sirui Xie",
          "institution": "Tsinghua University, Beijing Institute of General Artificial Intelligence (BIGAI), UCLA, Peking University",
          "link": "https://arxiv.org/pdf/2512.21527",
          "code": "github.com/qayqaq/Generative-Actor-Critic",
          "tags": [
            "reinforcement learning",
            "generative modeling",
            "policy evaluation",
            "latent plan",
            "offline-to-online",
            "actor-critic"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62b697a3a1c4a1b559c19af7d65fd19d2eed0bb097eccecdd9008a509a0456c0_w640_q70.webp",
          "contributions": "1. Proposes the Generative Actor Critic (GAC) framework that reframes policy evaluation as learning a generative model of the joint distribution over trajectories and returns, decoupling decision-making. 2. Introduces a specific instantiation using a latent variable model with continuous latent plan vectors and novel inference strategies for exploitation and exploration. 3. Demonstrates strong offline performance and significantly enhanced offline-to-online improvement on benchmarks, even without step-wise rewards.",
          "summary": "This paper introduces Generative Actor Critic (GAC), a novel reinforcement learning framework that decouples sequential decision-making by learning a generative model of trajectories and returns and then performing inference on it. It shows strong performance in offline learning and significantly improves when fine-tuned online, even in sparse-reward environments.",
          "mindmap": "graph TB\n        A[Generative Actor Critic] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统RL在线改进离线预训练模型存在挑战/Challenges in refining offline models online]\n        C --> C1[将策略评估重构为学习轨迹与回报的联合生成模型/Reframe policy evaluation as learning p(τ, y)]\n        C --> C2[将策略改进重构为在模型上进行多样化推理/Reframe policy improvement as versatile inference]\n        C --> C3[基于潜变量模型的实例化与新颖推理策略/Instantiation with latent plans & novel inference]\n        D --> D1[离线性能强大/Strong offline performance]\n        D --> D2[离线到在线改进显著/Enhanced offline-to-online improvement]"
        },
        {
          "title": "AVP-Fusion: Adaptive Multi-Modal Fusion and Contrastive Learning for Two-Stage Antiviral Peptide Identification",
          "authors": "Xinru Wen, Weizhong Lin, Xuan Xiao",
          "institution": "JCI (inferred from email domain `jci.edu.cn`)",
          "link": "https://arxiv.org/pdf/2512.21544",
          "code": null,
          "tags": [
            "bioinformatics",
            "adaptive gating mechanism",
            "contrastive learning",
            "transfer learning"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72c58b1c8ae5a53950bfc6d9e8fcca6dc27fc849f514d47d4a2cdff795cb10b_w640_q70.webp",
          "contributions": "1. Proposes a two-stage deep learning framework (AVP-Fusion) for antiviral peptide identification and subclass prediction. 2. Introduces an Adaptive Gating Mechanism to dynamically fuse local (CNN) and global (BiLSTM) sequence features. 3. Employs a contrastive learning strategy with OHEM and BLOSUM62-based data augmentation to sharpen decision boundaries and handle hard samples.",
          "summary": "This paper proposes AVP-Fusion, a two-stage deep learning framework that integrates adaptive feature fusion and contrastive learning for identifying antiviral peptides (AVPs). The method dynamically fuses multi-modal sequence features and uses contrastive learning to improve classification, achieving state-of-the-art accuracy and enabling precise prediction of antiviral activity against specific viral families.",
          "mindmap": "graph TB\n        Root[AVP-Fusion: 抗病毒肽识别 / Antiviral Peptide Identification] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题 / Problem] --> P1[现有方法难以捕捉复杂序列依赖 / Current methods struggle with sequence dependencies]\n        Problem --> P2[难以处理模糊样本 / Hard to handle ambiguous samples]\n        Method[主要方法 / Method] --> M1[构建全景特征空间 / Construct panoramic feature space]\n        Method --> M2[自适应门控机制融合特征 / Adaptive Gating Mechanism for feature fusion]\n        Method --> M3[对比学习与数据增强 / Contrastive learning & data augmentation]\n        Results[关键结果 / Results] --> R1[准确率0.9531, MCC 0.9064 / Accuracy 0.9531, MCC 0.9064]\n        Results --> R2[优于现有方法 / Outperforms SOTA]\n        Results --> R3[实现病毒家族亚类预测 / Enables viral family subclass prediction]"
        },
        {
          "title": "Discovering Sparse Recovery Algorithms Using Neural Architecture Search",
          "authors": "Patrick Yubeaton, Sarthak Gupta, M. Salman Asif, Chinmay Hegde",
          "institution": "New York University, University of California, Riverside",
          "link": "https://arxiv.org/pdf/2512.21563",
          "code": null,
          "tags": [
            "sparse recovery",
            "neural architecture search",
            "meta-learning",
            "iterative shrinkage thresholding algorithm",
            "sparse optimization",
            "algorithm discovery"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49f1d5d0a89c3d52b36f1e443675494175442f0930725fc8a763e525ca05243a_w640_q70.webp",
          "contributions": "1. Proposes a meta-learning framework using Neural Architecture Search (NAS) for automated discovery of sparse recovery algorithms. 2. Demonstrates the framework's capability to rediscover key elements of ISTA and FISTA from a search space of over 50,000 variables. 3. Shows the framework's applicability to various data distributions and algorithms beyond ISTA/FISTA.",
          "summary": "This paper introduces a meta-learning framework that uses Neural Architecture Search (NAS) to automatically discover sparse recovery algorithms. It successfully rediscovers components of ISTA and FISTA from a large search space and demonstrates generalizability to other algorithms and data distributions.",
          "mindmap": "graph TB\n    A[Discovering Sparse Recovery Algorithms Using Neural Architecture Search] --> B[核心问题/Problem: Automated discovery of sparse optimization algorithms is difficult and heuristic-driven]\n    A --> C[主要方法/Method: Meta-learning framework using Neural Architecture Search (NAS) for algorithm rediscovery]\n    A --> D[关键结果/Results: Rediscovered ISTA/FISTA elements; framework applies to various data and algorithms]"
        },
        {
          "title": "AnchorGK: Anchor-based Incremental and Stratified Graph Learning Framework for Inductive Spatio-Temporal Kriging",
          "authors": "Xiaobin Ren, Kaiqi Zhao, Katerina Taškova, Patricia Riddle",
          "institution": "University of Auckland, Harbin Institute of Technology, Shenzhen",
          "link": "https://arxiv.org/pdf/2512.21569",
          "code": "https://github.com/xren451/Spatial-interpolation",
          "tags": [
            "spatio-temporal kriging",
            "graph neural networks",
            "incremental learning",
            "data stratification",
            "anchor locations",
            "incomplete features"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c757748db9cf59d7295a4cdf698c5e194dc4ae79ec767aa6f7c71c0e78243768_w640_q70.webp",
          "contributions": "1. Introduces an anchor-based stratification framework to handle sparse spatial distributions and heterogeneous feature availability in sensor networks. 2. Proposes a dual-view graph learning layer that jointly aggregates feature-relevant and location-relevant information to learn stratum-specific representations. 3. Designs an incremental representation mechanism that systematically utilizes all available features across different strata to mitigate feature incompleteness.",
          "summary": "The paper proposes AnchorGK, a graph learning framework for inductive spatio-temporal kriging that addresses sparse sensor deployment and incomplete features. It uses anchor-based stratification and a dual-view graph learning layer to model correlations and incrementally integrate features. Experiments show it outperforms existing state-of-the-art methods.",
          "mindmap": "graph TB\n        A[AnchorGK: Anchor-based Incremental and Stratified Graph Learning Framework for Inductive Spatio-Temporal Kriging] --> B(核心问题/Problem: Sparse sensor distribution and incomplete features hinder accurate spatio-temporal kriging)\n        A --> C(主要方法/Method: Anchor-based stratification and dual-view graph learning for incremental feature integration)\n        A --> D(关键结果/Results: Outperforms state-of-the-art baselines on multiple benchmark datasets)"
        },
        {
          "title": "nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures",
          "authors": "Hui Guo, Qihang Zheng, Chenghai Huo, Dongliang Guo, Haoqi Yang, Yang Zhang",
          "institution": "Canaan Inc.",
          "link": "https://arxiv.org/pdf/2512.21571",
          "code": "https://github.com/kendryte/nncase",
          "tags": [
            "compiler & ir",
            "e-graph",
            "term rewriting",
            "phase ordering",
            "NUMA abstraction",
            "auto vectorize"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a555b38ecd80e8c11606362e5402405bbf0053e11754e06f720d50ce213ee0d_w640_q70.webp",
          "contributions": "1. Proposes an end-to-end compilation framework (nncase) that unifies LLM deployment across heterogeneous memory architectures using a NUMA abstraction for a \"compile once, adapt everywhere\" capability. 2. Introduces an e-graph-based term rewriting engine with equality saturation to mitigate the phase ordering problem and enable global optimization of computation and data movement. 3. Integrates three key automated optimization modules: Auto Vectorize for heterogeneous computing units, Auto Distribution for parallel strategies with communication optimization, and Auto Schedule for on-chip cache locality.",
          "summary": "The paper presents nncase, an end-to-end compiler framework designed to tackle the challenge of efficiently deploying large language models on heterogeneous memory architectures. Its core innovation is an e-graph-based rewriting engine that avoids the phase ordering problem, enabling unified optimization across diverse hardware targets. Evaluations show nncase outperforms frameworks like MLC LLM and Intel IPEX, achieving performance close to hand-optimized llama.cpp, demonstrating the viability of automated compilation for high-performance LLM deployment.",
          "mindmap": "graph TB\n        A[nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures] --> B[核心问题/Problem: LLM部署受限于内存架构异构性，传统编译器流程碎片化/Memory architecture heterogeneity hinders efficient LLM deployment, traditional compilers have fragmented workflows.]\n        A --> C[主要方法/Method: 基于e-graph的项重写引擎，统一NUMA抽象，集成自动向量化、分布、调度模块/E-graph-based term rewriting engine, unified NUMA abstraction, integrates Auto Vectorize, Distribution, Schedule modules.]\n        A --> D[关键结果/Results: 性能超越MLC LLM和Intel IPEX，接近手工优化的llama.cpp/Outperforms MLC LLM & Intel IPEX, achieves performance comparable to hand-optimized llama.cpp.]"
        },
        {
          "title": "A Unified Definition of Hallucination, Or: It's the World Model, Stupid",
          "authors": "Emmy Liu, Varun Gangal, Chelsea Zou, Xiaoqi Huang, Michael Yu, Alex Chang, Zhuofu Tao, Sachin Kumar, Steven Y. Feng",
          "institution": "Carnegie Mellon University, Stanford University, The Ohio State University, Patronus AI, DegenAI Labs, Independent Researchers",
          "link": "https://arxiv.org/pdf/2512.21577",
          "code": null,
          "tags": [
            "hallucination detection & evaluation",
            "hallucination",
            "world modeling",
            "knowledge conflict",
            "benchmark",
            "language model evaluation"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e2cc31121f769cca7464239d4aa27b26c8c4bc903970a4833fbebac56dc9b85_w640_q70.webp",
          "contributions": "1. Proposes a unified definition of hallucination as inaccurate internal world modeling that is observable to the user, synthesizing prior definitions. 2. Provides a framework for analyzing hallucinations by varying the reference world model and knowledge conflict policy, clarifying what constitutes a hallucination versus other error types. 3. Outlines plans for a family of benchmarks based on synthetic, fully-specified world models to stress-test and improve the world modeling components of language models.",
          "summary": "This paper argues that the persistent problem of hallucination in language models stems from inaccurate internal world modeling. It unifies various historical definitions under this core concept and proposes a framework for clearer evaluation. The authors conclude by sketching plans for new benchmarks to rigorously test and improve language models' world modeling capabilities.",
          "mindmap": "graph TB\n        Root[”A Unified Definition of Hallucination / 幻觉的统一定义”] --> Problem[”核心问题/Problem”]\n        Root[”A Unified Definition of Hallucination / 幻觉的统一定义”] --> Method[”主要方法/Method”]\n        Root[”A Unified Definition of Hallucination / 幻觉的统一定义”] --> Results[”关键结果/Results”]\n        Problem --> P1[”Hallucination persists in LLMs / 幻觉在LLM中持续存在”]\n        Method --> M1[”Unified definition: inaccurate world modeling / 统一定义：不准确的世界建模”]\n        Method --> M2[”Framework: reference world & conflict policy / 框架：参考世界与冲突策略”]\n        Results --> R1[”Clarifies evaluation & terminology / 澄清评估与术语”]\n        Results --> R2[”Proposes new benchmark plans / 提出新基准计划”]"
        },
        {
          "title": "RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models",
          "authors": "Anthony Bolton, Wuyang Zhou, Zehua Chen, Giorgos Iacovides, Danilo Mandic",
          "institution": "Imperial College London",
          "link": "https://arxiv.org/pdf/2512.21572",
          "code": null,
          "tags": [
            "time series forecasting",
            "Schrödinger Bridge",
            "Low-rank Adaptation",
            "Time Series Foundation Models",
            "Financial Forecasting",
            "Generative Refinement"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d984f91104f06a2169d1d04626078f4bd0698ade16094c7642b5b47c5a1a6198_w640_q70.webp",
          "contributions": "1. Proposes RefineBridge, a novel post-processing module built on a tractable Schrödinger Bridge framework to refine TSFM forecasts. 2. Introduces a paradigm that treats TSFM predictions as a generative prior and learns context-conditioned stochastic transport maps to iteratively improve them. 3. Demonstrates consistent performance improvements over state-of-the-art TSFMs on multiple financial benchmarks, addressing challenges like non-stationarity and noise.",
          "summary": "The paper addresses the challenge of applying Time Series Foundation Models (TSFMs) to financial forecasting, where data properties like non-stationarity degrade performance. It proposes RefineBridge, a generative refinement module based on Schrödinger Bridges that iteratively transports TSFM predictions toward ground truths. Experiments show RefineBridge consistently enhances TSFM forecasts across various financial benchmarks.",
          "mindmap": "graph TB\n        A[RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[TSFMs在金融预测中表现不佳<br/>TSFMs underperform in financial forecasting]\n        B --> B2[LoRA等适应方法存在局限<br/>Limitations of adaptation methods like LoRA]\n        C --> C1[提出基于薛定谔桥的RefineBridge模块<br/>Propose RefineBridge based on Schrödinger Bridge]\n        C --> C2[将TSFM预测作为先验进行迭代优化<br/>Iteratively refine TSFM predictions as prior]\n        D --> D1[在多个基准上提升TSFM性能<br/>Improves TSFM performance on multiple benchmarks]\n        D --> D2[对不同预测范围均有效<br/>Effective across different prediction horizons]"
        },
        {
          "title": "Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations",
          "authors": "Xin Liu, Haoran Li, Dongbin Zhao",
          "institution": "Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences",
          "link": "https://arxiv.org/pdf/2512.21586",
          "code": null,
          "tags": [
            "imitation learning",
            "behavior cloning",
            "latent representation",
            "self-supervised learning",
            "sample efficiency"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c96d71be701998ebf55ef6ed2a0c0a001a306b44f3555239559ced527b17559_w640_q70.webp",
          "contributions": "1. Proposes a novel, unsupervised framework (BCV-LR) for imitation learning from videos (ILV) that learns latent actions from visual inputs. 2. Introduces an iterative policy improvement loop that aligns pre-trained latent actions with the real action space online, enabling highly sample-efficient learning. 3. Demonstrates state-of-the-art sample efficiency, outperforming existing ILV and RL methods on a wide range of visual control tasks.",
          "summary": "This paper proposes BCV-LR, a framework for learning policies from videos without action labels. It uses self-supervised learning to extract latent actions and an iterative alignment process for sample-efficient behavior cloning. The method achieves expert-level performance on many tasks with minimal interaction, showing videos can be highly effective supervision for policy learning.",
          "mindmap": "graph TB\n        A[Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[”从视频模仿学习的挑战 / Challenges of Imitation Learning from Videos”]\n        C --> C1[”BCV-LR框架 / BCV-LR Framework”]\n        C1 --> C2[”自监督提取潜在特征 / Self-supervised Latent Feature Extraction”]\n        C1 --> C3[”基于动态的潜在动作预测 / Dynamics-based Latent Action Prediction”]\n        C1 --> C4[”在线对齐与迭代策略改进 / Online Alignment & Iterative Policy Improvement”]\n        D --> D1[”高样本效率 / High Sample Efficiency”]\n        D --> D2[”超越SOTA方法 / Outperforms SOTA Baselines”]\n        D --> D3[”首次证明视频可作为高效监督 / First to Show Videos as Efficient Supervision”]"
        },
        {
          "title": "Quantitative Verification of Omega-regular Properties in Probabilistic Programming",
          "authors": "Peixin Wang, Jianhao Bai, Min Zhang, C.-H. Luke Ong",
          "institution": "East China Normal University, Nanyang Technological University",
          "link": "https://arxiv.org/pdf/2512.21596",
          "code": null,
          "tags": [
            "probabilistic programming and verification",
            "temporal posterior inference",
            "omega-regular properties",
            "stochastic barrier certificates",
            "Rabin automata",
            "quantitative verification"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c111a01f9d5e96a85d9b5c62645dae0f5bb40053d723e34cb57dc7f31554dcda_w640_q70.webp",
          "contributions": "1. Introduces Temporal Posterior Inference (TPI), a new framework unifying probabilistic programming with temporal logic to compute posterior distributions over execution traces satisfying omega-regular properties. 2. Develops a novel method for computing rigorous upper and lower bounds on satisfaction probabilities by decomposing Rabin acceptance conditions and constructing sound stochastic barrier certificates. 3. Implements the approach in a prototype tool named TPInfer and demonstrates its effectiveness and efficiency on a suite of benchmarks.",
          "summary": "This paper addresses the limitation of standard probabilistic program inference, which fails to capture temporal behavior, by proposing Temporal Posterior Inference (TPI). TPI computes posterior distributions over program traces that satisfy omega-regular temporal specifications, using a method based on stochastic barrier certificates to provide quantitative verification bounds. The approach is implemented in the TPInfer tool and shown to be effective for inference over rich temporal properties.",
          "mindmap": "graph TB\n        Root(”Quantitative Verification of Omega-regular Properties in Probabilistic Programming”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”标准后验推断的局限/Limitation of Standard Posterior Inference”)\n        P1 --> P2(”无法捕捉程序执行的时间演化/Fails to capture temporal evolution”)\n        Method --> M1(”提出时间后验推断框架/Propose Temporal Posterior Inference (TPI)”)\n        M1 --> M2(”统一概率编程与时序逻辑/Unifies Probabilistic Programming & Temporal Logic”)\n        M2 --> M3(”基于随机屏障证书的定量验证方法/Quantitative Verification via Stochastic Barrier Certificates”)\n        Results --> R1(”实现原型工具 TPInfer/Implement Prototype Tool TPInfer”)\n        Results --> R2(”在基准测试中展示有效性与效率/Demonstrates Effectiveness & Efficiency on Benchmarks”)"
        },
        {
          "title": "Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data in Emergency and Critical Care",
          "authors": "Yusuf Brima, Marcellin Atemkeng",
          "institution": "Osnabrück University, Rhodes University, National Institute for Theoretical and Computational Sciences (NITheCS)",
          "link": "https://arxiv.org/pdf/2512.21602",
          "code": null,
          "tags": [
            "imbalanced classification",
            "XGBoost",
            "TabNet",
            "TabResNet",
            "Bayesian hyperparameter search",
            "class imbalance"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp",
          "contributions": "1. Systematic evaluation of robustness and scalability for classical ML and deep learning models on imbalanced clinical tabular data. 2. Introduction of TabResNet, a custom lightweight residual network designed as a computationally efficient alternative to TabNet for real-time clinical use. 3. Evidence-based finding that tree-based ensemble methods (especially XGBoost) offer the most stable and scalable performance for high-stakes, time-sensitive clinical environments.",
          "summary": "This paper evaluates the robustness and scalability of machine learning models on imbalanced clinical data from emergency and critical care settings. It proposes TabResNet, a lightweight deep learning alternative to TabNet, and compares it against tree-based methods like XGBoost. The main conclusion is that tree-based ensemble models, particularly XGBoost, provide the most stable performance and computational scalability for practical clinical deployment, outperforming more complex deep learning architectures in these environments.",
          "mindmap": "graph TB\n        A[Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data<br>不平衡临床数据中机器学习的鲁棒性与可扩展性] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Imbalanced clinical data in emergency/critical care<br>急诊/重症监护中的不平衡临床数据]\n        C[主要方法/Method<br>Systematic evaluation of tree-based models, TabNet, and proposed TabResNet<br>系统评估树模型、TabNet及提出的TabResNet]\n        D[关键结果/Results<br>XGBoost most robust & scalable; deep models degrade under imbalance<br>XGBoost最鲁棒且可扩展；深度学习模型在不平衡下性能下降]"
        },
        {
          "title": "A Data-Driven Multi-Objective Approach for Predicting Mechanical Performance, Flowability, and Porosity in Ultra-High-Performance Concrete (UHPC)",
          "authors": "Jagaran Chakma, Zhiguang Zhou, Jyoti Chakma, Cao YuSen",
          "institution": "Tongji University, University of Chittagong",
          "link": "https://arxiv.org/pdf/2512.21610",
          "code": null,
          "tags": [
            "predictive modeling",
            "XGBoost",
            "SHAP analysis",
            "K-Fold Cross-Validation",
            "Isolation Forest",
            "hyperparameter tuning"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/988c9f68c148fb1ef37ff8e292bf9a2cab1878ea08a2f5baee1a1a47f095be23_w640_q70.webp",
          "contributions": "1. Developed a two-stage data-driven framework for UHPC property prediction, integrating model selection, data cleaning (multicollinearity removal, outlier detection), and feature importance analysis. 2. Demonstrated the superior performance of the XGBoost model after rigorous hyperparameter tuning and validation, achieving high accuracy across multiple objectives (mechanical performance, flowability, porosity). 3. Created a practical graphical user interface (GUI) to facilitate the application of the predictive model by material designers, reducing reliance on extensive experimental testing.",
          "summary": "This paper proposes a data-driven, multi-objective machine learning framework to predict the mechanical performance, flowability, and porosity of Ultra-High-Performance Concrete (UHPC). The method involves testing 21 algorithms, selecting and tuning XGBoost, and refining the dataset through multicollinearity removal, outlier detection, and SHAP-based feature selection. The final model achieves high prediction accuracy, and a supporting GUI is developed to aid in UHPC mix design, reducing the need for experimental tests.",
          "mindmap": "graph TB\n        A[”A Data-Driven Multi-Objective Approach for Predicting UHPC Properties<br>预测UHPC性能的数据驱动多目标方法”] --> B[”Problem: Predicting UHPC mechanical performance, flowability, porosity<br>核心问题: 预测UHPC的力学性能、流动性和孔隙率”]\n        A --> C[”Method: Two-stage ML framework with XGBoost, data cleaning, SHAP<br>主要方法: 两阶段ML框架，使用XGBoost、数据清洗和SHAP”]\n        A --> D[”Results: High prediction accuracy, developed GUI for designers<br>关键结果: 高预测精度，为设计师开发了GUI”]"
        },
        {
          "title": "MAD-NG: Meta-Auto-Decoder Neural Galerkin Method for Solving Parametric Partial Differential Equations",
          "authors": "Qiuqi Li, Yiting Liu, Jin Zhao, Wencan Zhu",
          "institution": "Hunan University, Capital Normal University",
          "link": "https://arxiv.org/pdf/2512.21633",
          "code": null,
          "tags": [
            "scientific machine learning",
            "Neural Galerkin Method",
            "meta-learning",
            "parametric PDEs",
            "space-time decoupling",
            "randomized sparse updates"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8574d601c121a7a53ef19693b53d019c5139f87d5ae2dd641aec204463aa59c_w640_q70.webp",
          "contributions": "1. Proposes a novel framework that enhances the Neural Galerkin Method by integrating the Meta-Auto-Decoder paradigm for solving parametric PDEs. 2. Introduces space-time decoupling to enable more stable and efficient time integration compared to full space-time approximations. 3. Employs meta-learning for rapid adaptation to new parameters and randomized sparse updates to reduce computational cost without sacrificing accuracy.",
          "summary": "The paper addresses the challenges of generalization and efficiency in neural network-based solvers for parametric PDEs. It proposes MAD-NG, a framework that combines the Neural Galerkin Method with meta-learning and space-time decoupling to achieve stable, efficient, and accurate long-term predictions. Numerical experiments show the method performs well in accuracy, robustness, and adaptability.",
          "mindmap": "graph TB\n        A[MAD-NG: Meta-Auto-Decoder Neural Galerkin Method<br>MAD-NG: 元自解码器神经伽辽金方法] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[传统神经求解器泛化与效率挑战<br>Traditional Neural Solvers' Generalization & Efficiency Challenges]\n        C --> C1[空间-时间解耦与元学习<br>Space-Time Decoupling & Meta-Learning]\n        C --> C2[随机稀疏更新<br>Randomized Sparse Updates]\n        D --> D1[物理一致的长时预测<br>Physically Consistent Long-Horizon Predictions]\n        D --> D2[较低计算开销<br>Lower Computational Overhead]"
        },
        {
          "title": "Mechanical Strength Prediction of Steel-Polypropylene Fiber-based High-Performance Concrete Using Hybrid Machine Learning Algorithms",
          "authors": "Jagaran Chakma, Zhiguang Zhou, Badhan Chakma",
          "institution": "Tongji University, Chongqing Jiaotong University",
          "link": "https://arxiv.org/pdf/2512.21638",
          "code": null,
          "tags": [
            "materials informatics",
            "hybrid machine learning",
            "SHAP analysis",
            "uncertainty quantification",
            "strength prediction",
            "high-performance concrete"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76eefc8193c0d98df6f997d7ae3134d34a448f8aba255f28a39044233c1f3bb8_w640_q70.webp",
          "contributions": "1. Developed and evaluated three novel hybrid machine learning model families (ET-XGB, RF-LGBM, Transformer-XGB) for predicting the mechanical properties of fiber-reinforced concrete. 2. Conducted a comprehensive evaluation including k-fold cross-validation, hyperparameter optimization, SHAP analysis for interpretability, and uncertainty quantification for robustness assessment. 3. Identified key influential material parameters (fiber aspect ratios, silica fume, steel fiber content) and negative factors (water content, water-binder ratio) on concrete strength through SHAP analysis.",
          "summary": "This research develops hybrid machine learning models to predict the compressive, flexural, and tensile strength of steel-polypropylene fiber-reinforced high-performance concrete. The study compares three hybrid models (ET-XGB, RF-LGBM, Transformer-XGB) and finds that the ET-XGB model achieved the highest overall accuracy, while SHAP analysis reveals the most influential material factors. The findings confirm that these ML models provide accurate and interpretable tools for optimizing concrete mix design in engineering applications.",
          "mindmap": "graph TB\n        A[Mechanical Strength Prediction of Steel-Polypropylene Fiber-based High-Performance Concrete Using Hybrid Machine Learning Algorithms] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Predict mechanical properties (CS, FS, TS) of fiber-reinforced HPC]\n        C[主要方法/Method<br>Develop & evaluate hybrid ML models (ET-XGB, RF-LGBM, Transformer-XGB) with SHAP & uncertainty analysis]\n        D[关键结果/Results<br>ET-XGB most accurate, RF-LGBM most stable for FS, key influential factors identified via SHAP]"
        },
        {
          "title": "Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search",
          "authors": "Maximilian Weichart",
          "institution": "University of Regensburg",
          "link": "https://arxiv.org/pdf/2512.21648",
          "code": "https://github.com/Max-We/inverse-rpo",
          "tags": [
            "reinforcement learning",
            "Monte Carlo Tree Search",
            "Upper Confidence Bound",
            "Variance-Aware",
            "Prior-Based Tree Policy",
            "Inverse-RPO"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c9098504a8a9013ab805fffa4a23f04af76e28f5d8b8a0353e1e7d5583f589_w640_q70.webp",
          "contributions": "1. Introduces Inverse-RPO, a general methodology to systematically derive prior-based UCTs from any prior-free UCB., 2. Applies Inverse-RPO to UCB-V to create two new variance-aware prior-based tree policies., 3. Provides an extension to the mctx library for variance-aware UCTs, showing minimal code changes and improved performance over PUCT in benchmarks.",
          "summary": "This paper addresses the challenge of extending prior-based tree policies in Monte Carlo Tree Search beyond the empirically derived PUCT. The authors propose Inverse-RPO, a principled method to derive prior-based UCTs from any prior-free UCB, and apply it to create variance-aware policies. Their new policies outperform the standard PUCT across multiple benchmarks without added computational cost.",
          "mindmap": "graph TB\n        A[Variance-Aware Prior-Based Tree Policies for MCTS] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Extending prior-based UCTs from other UCBs is challenging]\n        C[主要方法/Method: Propose Inverse-RPO to derive prior-based UCTs; apply to UCB-V]\n        D[关键结果/Results: New policies outperform PUCT without extra cost]"
        },
        {
          "title": "Causal-HM: Restoring Physical Generative Logic in Multimodal Anomaly Detection via Hierarchical Modulation",
          "authors": "Xiao Liu, Junchen Jin, Yanjie Zhao, Zhixuan Xing",
          "institution": "Chongqing University",
          "link": "https://arxiv.org/pdf/2512.21650",
          "code": null,
          "tags": [
            "anomaly detection",
            "multimodal fusion",
            "causal modeling",
            "hierarchical modulation",
            "sensor guidance",
            "unsupervised learning"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f0b7760ac76038dd08b0ccd2890cb2628906dcf233282246d08ee584093193_w640_q70.webp",
          "contributions": "1. Proposes a unified multimodal UAD framework (Causal-HM) that explicitly models the physical Process→Result dependency to address causal blindness. 2. Introduces a Sensor-Guided CHM Modulation mechanism that uses low-dimensional sensor signals as context to guide high-dimensional audio-visual feature extraction. 3. Designs a Causal-Hierarchical Architecture that enforces a unidirectional generative mapping to identify anomalies violating physical consistency.",
          "summary": "The paper addresses the problem of causal blindness and the heterogeneity gap in multimodal anomaly detection for industrial processes like welding. It proposes the Causal-HM framework, which models the physical generative logic from process to result modalities using sensor-guided modulation and a causal-hierarchical architecture. The method achieves state-of-the-art performance on a new Weld-4M benchmark, demonstrating its effectiveness.",
          "mindmap": "graph TB\n        A[Causal-HM: Restoring Physical Generative Logic in Multimodal Anomaly Detection via Hierarchical Modulation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[因果盲区/Causal Blindness]\n        B --> B2[模态异质性/Modality Heterogeneity]\n        C --> C1[传感器引导调制/Sensor-Guided CHM Modulation]\n        C --> C2[因果分层架构/Causal-Hierarchical Architecture]\n        D --> D1[新基准/Weld-4M Benchmark]\n        D --> D2[SOTA性能/SOTA I-AUROC 90.7%]"
        },
        {
          "title": "Semantic Codebooks as Effective Priors for Neural Speech Compression",
          "authors": "Liuyang Bai, Weiyi Lu, Li Guo",
          "institution": "NYU Shanghai",
          "link": "https://arxiv.org/pdf/2512.21653",
          "code": null,
          "tags": [
            "speech compression",
            "semantic codebooks",
            "residual vector quantization (RVQ)",
            "HuBERT",
            "FiLM-conditioned decoder",
            "neural audio codec"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a14c953c6c23139c4473b8d6e59b36c7615f79f4316119e168deb19de30eced_w640_q70.webp",
          "contributions": "1. Proposes SemDAC, a semantic-aware neural audio codec that uses semantic codebooks as priors for compression., 2. Introduces a design where the first RVQ quantizer is distilled from HuBERT to capture phonetic content, and a FiLM-conditioned decoder uses these semantic tokens., 3. Demonstrates superior performance over baseline DAC in perceptual metrics and ASR (Whisper) WER at significantly lower bitrates.",
          "summary": "The paper proposes SemDAC, a neural speech codec that uses semantic codebooks distilled from HuBERT as priors within an RVQ framework to separate phonetic from acoustic information. This method achieves better perceptual quality and lower word error rates for speech recognition at much lower bitrates compared to traditional neural codecs. The results show that semantic priors provide an effective inductive bias for efficient, recognition-friendly speech compression.",
          "mindmap": "graph TB\n        Root[”Semantic Codebooks as Effective Priors for Neural Speech Compression”] --> Problem[”核心问题/Problem: Traditional codecs inefficiently allocate bits for acoustic detail, neglecting linguistic structure.”]\n        Root --> Method[”主要方法/Method: Propose SemDAC, using HuBERT-distilled semantic codebooks in RVQ and a FiLM-conditioned decoder.”]\n        Root --> Results[”关键结果/Results: Outperforms DAC in perceptual metrics & ASR WER at lower bitrates (e.g., 0.95 vs 2.5 kbps).”]"
        },
        {
          "title": "Rethinking Output Alignment For 1-bit Post-Training Quantization of Large Language Models",
          "authors": "Dung Anh Hoang, Cuong Pham, Cuong Nguyen, Trung le, Jianfei Cai, Thanh-Toan Do",
          "institution": "Monash University, University of Surrey",
          "link": "https://arxiv.org/pdf/2512.21651",
          "code": null,
          "tags": [
            "model compression (quantization/pruning)",
            "1-bit quantization",
            "post-training quantization",
            "output alignment",
            "activation error",
            "large language models"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0357717d29d733787933b581fbbb292b187b9fbc651fbc0f15bb04ef03e619eb_w640_q70.webp",
          "contributions": "1. Investigates the failure conditions of naive output-matching in 1-bit LLM quantization, 2. Proposes a novel data-aware PTQ approach that explicitly accounts for activation error accumulation, 3. Demonstrates consistent performance improvements over existing 1-bit PTQ methods with minimal overhead",
          "summary": "This paper addresses the significant performance degradation in 1-bit post-training quantization (PTQ) of Large Language Models. The authors propose a new data-aware PTQ method that efficiently accounts for activation error accumulation. Their solution is shown to consistently outperform existing 1-bit PTQ approaches.",
          "mindmap": "graph TB\n        Root[”Rethinking Output Alignment For 1-bit PTQ of LLMs<br>重新思考大语言模型1比特训练后量化的输出对齐”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”1-bit PTQ causes significant performance drop<br>1比特训练后量化导致性能显著下降”] --> P1[”Focus on weight alignment, not output<br>关注权重对齐而非输出”]\n        Problem --> P2[”Naive output-matching fails<br>简单的输出匹配方法失败”]\n        Method[”Propose a data-aware PTQ approach<br>提出一种数据感知的训练后量化方法”] --> M1[”Accounts for activation error accumulation<br>考虑激活误差累积”]\n        Method --> M2[”Keeps optimization efficient<br>保持优化高效”]\n        Results[”Consistently outperforms existing 1-bit PTQ methods<br>持续优于现有1比特训练后量化方法”] --> R1[”Minimal overhead<br>开销极小”]"
        },
        {
          "title": "The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds",
          "authors": "Subramanyam Sahoo, Jared Junkin",
          "institution": "University of California, Berkeley, Johns Hopkins University",
          "link": "https://arxiv.org/pdf/2512.21670",
          "code": "https://github.com/SubramanyamSahoo/The-Deepfake-Detective",
          "tags": [
            "deepfake detection",
            "mechanistic interpretability",
            "sparse autoencoder",
            "forensic manifold analysis",
            "feature selectivity",
            "vision-language model"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp",
          "contributions": "1. Introduces a novel mechanistic interpretability framework specifically for deepfake detection, combining sparse autoencoder analysis with forensic manifold analysis. 2. Demonstrates that only a small fraction of latent features are actively used in each layer of the model for detection. 3. Shows that geometric properties of the model's feature manifold (e.g., intrinsic dimensionality, curvature) vary systematically with different types of deepfake artifacts, linking learned features to specific forensic cues.",
          "summary": "This paper addresses the \"black box\" nature of deepfake detectors by proposing a mechanistic interpretability framework that analyzes a vision-language model's internal representations. The method uses sparse autoencoders and a novel forensic manifold analysis to probe how the model's features respond to forensic artifacts. The key findings are that detection relies on a sparse set of features and that the geometry of the feature manifold correlates with specific artifact types, providing a step towards more interpretable and robust detectors.",
          "mindmap": "graph TB\n        A[The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[深度伪造检测器是黑盒模型/Deepfake detectors are black boxes]\n        C --> C1[稀疏自编码器分析/Sparse Autoencoder (SAE) Analysis]\n        C --> C2[法证流形分析/Forensic Manifold Analysis]\n        D --> D1[潜在特征稀疏使用/Latent features are sparsely used]\n        D --> D2[流形几何特性揭示伪影/Manifold geometry reveals artifacts]"
        },
        {
          "title": "Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles",
          "authors": "Jalal Khan",
          "institution": "United Arab Emirates University",
          "link": "https://arxiv.org/pdf/2512.21673",
          "code": null,
          "tags": [
            "object detection",
            "YOLO-NAS",
            "YOLOv8",
            "perception",
            "autonomous vehicles",
            "custom dataset"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp",
          "contributions": "1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS.",
          "summary": "This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model.",
          "mindmap": "graph TB\n    A[Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[评估自动驾驶感知中深度学习模型的性能/Evaluate DL model performance for AV perception]\n    C --> C1[使用自定义数据集比较YOLO-NAS与YOLOv8/Compare YOLO-NAS and YOLOv8 using a custom dataset]\n    D --> D1[YOLOv8s训练时间减少75%/YOLOv8s saves 75% training time]\n    D --> D2[YOLOv8s准确率更高(83% vs 81%)/YOLOv8s has higher accuracy (83% vs 81%)]"
        },
        {
          "title": "Dictionary-Transform Generative Adversarial Networks",
          "authors": "Angshul Majumdar",
          "institution": "Indraprastha Institute of Information Technology Delhi (IIIT-D)",
          "link": "https://arxiv.org/pdf/2512.21677",
          "code": null,
          "tags": [
            "generative adversarial networks",
            "dictionary learning",
            "transform learning",
            "sparse modeling",
            "adversarial learning",
            "nash equilibrium"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e368966cab749cb3ef0799abfb9a31d801a73291e7a5d2b4a5b81fd185a9d25_w640_q70.webp",
          "contributions": "1. Introduces DT-GAN, a fully model-based adversarial framework with a sparse synthesis dictionary generator and an analysis transform discriminator, enabling rigorous theoretical analysis. 2. Proves the adversarial game is well-posed, admits at least one Nash equilibrium, and that equilibrium solutions are identifiable under a sparse generative model. 3. Establishes finite-sample stability and consistency of empirical equilibria, demonstrating reliable convergence and robustness, validated on synthetic data.",
          "summary": "This paper addresses the theoretical fragility and instability of classical GANs by proposing DT-GAN, a model-based adversarial framework where the generator and discriminator are constrained linear operators (a sparse synthesis dictionary and an analysis transform). The authors prove the framework is well-posed, has identifiable equilibria, and converges reliably, demonstrating that adversarial learning can be made interpretable and provably correct for data with sparse structure.",
          "mindmap": "graph TB\n        A[Dictionary-Transform GANs] --> B[核心问题/Problem: Classical GANs are theoretically fragile, with ill-posed objectives and unstable training.]\n        A --> C[主要方法/Method: Propose DT-GAN, a model-based framework with a sparse synthesis dictionary generator and an analysis transform discriminator.]\n        A --> D[关键结果/Results: Game is well-posed with Nash equilibrium; solutions are identifiable and stable; framework is interpretable and provably correct.]"
        },
        {
          "title": "RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting",
          "authors": "Haochen Lv, Yan Lin, Shengnan Guo, Xiaowei Mao, Hong Nie, Letian Gong, Youfang Lin, Huaiyu Wan",
          "institution": "Beijing Jiaotong University, Aalborg University",
          "link": "https://arxiv.org/pdf/2512.21685",
          "code": null,
          "tags": [
            "spatiotemporal forecasting",
            "probabilistic forecasting",
            "uncertainty estimation",
            "principal component analysis",
            "road impedance",
            "traffic flow"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f835b2051524d67481fbf9f4479086a541b29307c2876046f2c3ee4eec60f92_w640_q70.webp",
          "contributions": "1. Proposes a dynamic impedance evolution network to model directional traffic transfer patterns driven by congestion and flow variability, revealing causes of uncertainty and enhancing reliability and interpretability. 2. Designs a principal component network to forecast the dominant eigenvectors of future flow covariance, enabling the capture of spatiotemporal uncertainty correlations. 3. Integrates domain-specific transportation theory with spatiotemporal principal component learning for probabilistic traffic flow forecasting, achieving superior performance over existing methods.",
          "summary": "The paper proposes RIPCN, a Road Impedance Principal Component Network for probabilistic traffic flow forecasting. It integrates transportation theory with principal component learning to model the causes of uncertainty and capture spatiotemporal uncertainty correlations. Experimental results show it outperforms existing probabilistic forecasting methods.",
          "mindmap": "graph TB\n        A[RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(如何建模交通流不确定性的成因? / How to model the causes of traffic flow uncertainty?)\n        B --> B2(如何捕捉不确定性的时空相关性? / How to capture spatiotemporal correlations of uncertainty?)\n        C --> C1(动态阻抗演化网络 / Dynamic Impedance Evolution Network)\n        C --> C2(主成分网络 / Principal Component Network)\n        D --> D1(超越现有概率预测方法 / Outperforms existing probabilistic forecasting methods)"
        },
        {
          "title": "Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities",
          "authors": "Abd Ullah Khan, Adnan Shahid, Haejoon Jung, Hyundong Shin",
          "institution": "Kyung Hee University, Ghent University",
          "link": "https://arxiv.org/pdf/2512.21717",
          "code": null,
          "tags": [
            "communication & networking",
            "multiconnectivity",
            "SAGIN",
            "resource allocation",
            "agentic reinforcement learning",
            "heterogeneous networks"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31a130dffac74abb8ad0616817d555bf857333050b690554853596eda30c2fa7_w640_q70.webp",
          "contributions": "1. Provides a comprehensive review of current developments and key implementation challenges in SAGIN-enabled multiconnectivity. 2. Highlights the transformative potential of AI-driven approaches, particularly agentic reinforcement learning, for resource optimization in heterogeneous SAGIN environments. 3. Presents a case study demonstrating that learning-based methods can effectively enhance network performance (latency, capacity) with a moderate trade-off in power consumption.",
          "summary": "This paper reviews the challenges of implementing multiconnectivity in heterogeneous Space-Air-Ground Integrated Networks (SAGIN) and proposes AI-driven solutions, specifically agentic reinforcement learning, for optimal resource allocation. A case study shows these methods significantly improve latency and capacity, albeit with a moderate increase in power consumption as a trade-off. The work concludes by outlining open research problems for realizing efficient SAGIN-enabled multiconnectivity.",
          "mindmap": "graph TB\n        Root[”Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem: Heterogeneous SAGIN complicates multiconnectivity and resource allocation”]\n        Method[”主要方法/Method: Use AI-driven approaches, specifically agentic reinforcement learning”]\n        Results[”关键结果/Results: Enhanced network performance (latency, capacity) with moderate power trade-off”]"
        },
        {
          "title": "An Information Theoretic Perspective on Agentic System Design",
          "authors": "Shizhe He, Avanika Narayan, Ishan S. Khare, Scott W. Linderman, Christopher Ré, Dan Biderman",
          "institution": "Stanford University",
          "link": "https://arxiv.org/pdf/2512.21720",
          "code": null,
          "tags": [
            "agent system",
            "mutual information",
            "noisy channel",
            "compressor-predictor",
            "on-device AI",
            "information-theoretic"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124535642e159e8f7a123525ffbf3cb5f163a7ae4a7876a4d1e71e7e6c885ace_w640_q70.webp",
          "contributions": "1. Proposes an information-theoretic framework for analyzing agentic LM systems, viewing the compressor as a noisy channel. 2. Introduces a task-independent estimator of mutual information between context and compression to quantify compression quality. 3. Empirically demonstrates that scaling compressor models is more effective than scaling predictors for performance and cost, enabling efficient on-device compression.",
          "summary": "The paper addresses the ad-hoc design of agentic LM systems that use a compressor LM to summarize context for a predictor LM. It proposes an information-theoretic framework using mutual information to evaluate compressors, finding that larger compressors are more accurate, concise, and information-dense, making scaling compressors more effective than scaling predictors for cost-efficient performance.",
          "mindmap": "graph TB\n        A[An Information Theoretic Perspective on Agentic System Design] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(”Agentic系统设计缺乏理论指导<br/>Agentic system design lacks theoretical guidance”)\n        C --> C1(”提出信息论框架与互信息估计器<br/>Propose information-theoretic framework & mutual information estimator”)\n        D --> D1(”更大压缩器更高效、更准确<br/>Larger compressors are more efficient and accurate”)\n        D --> D2(”扩展压缩器优于扩展预测器<br/>Scaling compressors outperforms scaling predictors”)"
        },
        {
          "title": "HELP: Hierarchical Embodied Language Planner for Household Tasks",
          "authors": "Alexandr V. Korchemnyi, Anatoly O. Onishchenko, Eva A. Bakaeva, Alexey K. Kovalev, Aleksandr I. Panov",
          "institution": "MIRAI, Cognitive AI Systems Lab",
          "link": "https://arxiv.org/pdf/2512.21723",
          "code": null,
          "tags": [
            "agent system",
            "embodied agent",
            "hierarchical planning",
            "large language model",
            "household tasks",
            "open source LLM"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp",
          "contributions": "1. Proposes a Hierarchical Embodied Language Planner (HELP) architecture using multiple LLM-based agents for decomposing and grounding natural language instructions. 2. Demonstrates the approach on a real-world household task using an embodied agent. 3. Focuses on the use of relatively small, open-source LLMs to enable autonomous deployment.",
          "summary": "The paper addresses the challenge of planning for embodied agents following ambiguous natural language instructions in complex environments. It proposes HELP, a hierarchical planner using multiple LLM-based agents to decompose high-level instructions into grounded, executable subtasks. The method is evaluated on a household task with a real robot, showing the feasibility of using smaller, open-source LLMs for autonomous operation.",
          "mindmap": "graph TB\n        A[HELP: Hierarchical Embodied Language Planner for Household Tasks] --> B[核心问题/Problem: Embodied agents need robust planning for ambiguous natural language instructions in complex environments.]\n        A --> C[主要方法/Method: Hierarchical planner with multiple LLM-based agents to decompose and ground instructions into executable steps.]\n        A --> D[关键结果/Results: Evaluated on real-world household task; demonstrates use of smaller open-source LLMs for autonomous deployment.]"
        },
        {
          "title": "A Model of Causal Explanation on Neural Networks for Tabular Data",
          "authors": "Takashi Isozaki, Masahiro Yamamoto, Atsushi Noda",
          "institution": "Sony Computer Science Laboratories, Inc., Sony Corporation of America",
          "link": "https://arxiv.org/pdf/2512.21746",
          "code": null,
          "tags": [
            "explainable ai",
            "CENNET",
            "structural causal models",
            "entropy",
            "causal explanation",
            "tabular data"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02e8ba8968728eba560984773ed8ba2b124e42c46e3c839dec8a1ca2a5975ce1_w640_q70.webp",
          "contributions": "1. Proposes CENNET, a novel causal explanation method for neural network predictions on tabular data. 2. Introduces a new explanation power index based on entropy for evaluating the proposed method. 3. Demonstrates the method's effectiveness by combining structural causal models with neural networks for causal explanations, validated on synthetic and quasi-real data.",
          "summary": "This paper addresses the challenge of providing causal explanations for neural network predictions on tabular data, where pseudo-correlations can mislead. The authors propose a method called CENNET, which integrates structural causal models with neural networks to generate causal explanations and introduces an entropy-based index to measure explanation power. Experiments on synthetic and quasi-real data show that CENNET effectively provides causal explanations compared to existing methods.",
          "mindmap": "graph TB\n        A[A Model of Causal Explanation on Neural Networks for Tabular Data] --> B[核心问题/Problem: Explaining NN predictions on tabular data, addressing pseudo-correlation and causality]\n        A --> C[主要方法/Method: Propose CENNET, a causal explanation method using SCMs and an entropy-based index]\n        A --> D[关键结果/Results: CENNET provides causal explanations, validated via comparative experiments]"
        },
        {
          "title": "Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning",
          "authors": "Hengyi Wu, Zhenyi Wang, Heng Huang",
          "institution": "University of Maryland, College Park, University of Central Florida",
          "link": "https://arxiv.org/pdf/2512.21743",
          "code": null,
          "tags": [
            "continual learning",
            "entropy scaling",
            "catastrophic forgetting",
            "stability-plasticity dilemma",
            "dynamic feedback",
            "layer-wise control"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp",
          "contributions": "1. Proposes a novel entropy-aware continual learning method that uses a dynamic feedback mechanism to regulate each layer based on its entropy, addressing underfitting and overfitting. 2. Introduces Self-Adaptive Entropy Scaling, which adaptively adjusts regularization strength per layer to encourage convergence to wider local minima for better generalization. 3. Presents a complementary adaptive training mechanism that modulates layer plasticity based on performance, preserving knowledge in high-performing layers while amplifying updates in underperforming ones.",
          "summary": "The paper addresses catastrophic forgetting in continual learning by proposing a dynamic feedback mechanism that regulates each neural network layer based on its entropy. This entropy-aware method reduces entropy in high-entropy layers to prevent underfitting and increases it in low-entropy layers to prevent overfitting, promoting convergence to wider minima for improved generalization. The approach is general and integrates with existing replay- and regularization-based methods, showing substantial performance gains over state-of-the-art baselines.",
          "mindmap": "graph TB\n        Root[Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Catastrophic forgetting in continual learning due to uniform layer treatment] --> P1[高熵层欠拟合/High-entropy layers underfit]\n        Problem --> P2[低熵层过拟合/Low-entropy layers overfit]\n        Method[主要方法/Method: Entropy-aware dynamic feedback for layer-wise control] --> M1[减少高熵层熵值/Reduce entropy in high-entropy layers]\n        Method --> M2[增加低熵层熵值/Increase entropy in low-entropy layers]\n        Results[关键结果/Results: Improved generalization and performance] --> R1[收敛到更宽的局部极小值/Converge to wider local minima]\n        Results --> R2[超越现有基线方法/Outperforms state-of-the-art baselines]"
        },
        {
          "title": "Approximation Capabilities of Feedforward Neural Networks with GELU Activations",
          "authors": "Konstantin Yakovlev, Nikita Puchkin",
          "institution": "HSE University",
          "link": "https://arxiv.org/pdf/2512.21749",
          "code": null,
          "tags": [
            "neural network approximation theory",
            "GELU activation",
            "feedforward neural networks",
            "approximation error bounds",
            "derivative approximation",
            "constructive approximation"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af63acd2ab9f3e42763901f23d1762f6658fd25b72daf9e1f1f73e7b30ce1173_w640_q70.webp",
          "contributions": "1. Derivation of simultaneous approximation error bounds for a function and all its derivatives up to any prescribed order using GELU networks. 2. Constructive approximation guarantees for elementary operations (multiplication, division, exponential) with control over network size, weight magnitudes, and behavior at infinity. 3. Extension of approximation results to multivariate polynomials and other elementary functions, ensuring global boundedness of higher-order derivatives of the approximators.",
          "summary": "This paper analyzes the approximation capabilities of feedforward neural networks using the GELU activation function. The authors provide constructive methods to approximate elementary functions and their derivatives, proving simultaneous error bounds for the function and all its higher-order derivatives. The main conclusion is that GELU networks can effectively approximate key operations like multiplication, division, and exponentiation with controlled network complexity and globally bounded derivatives.",
          "mindmap": "graph TB\n    Root(”Approximation Capabilities of Feedforward Neural Networks with GELU Activations<br>GELU激活前馈神经网络的逼近能力”) --> Problem(”核心问题/Problem”)\n    Root --> Method(”主要方法/Method”)\n    Root --> Results(”关键结果/Results”)\n    Problem --> P1(”逼近函数及其导数<br>Approximating functions and their derivatives”)\n    Method --> M1(”构造性乘法逼近<br>Constructive multiplication approximation”)\n    Method --> M2(”扩展到除法和指数<br>Extension to division and exponent”)\n    Results --> R1(”同时误差界<br>Simultaneous error bounds”)\n    Results --> R2(”全局有界导数<br>Globally bounded derivatives”)\n    Results --> R3(”网络规模控制<br>Network size control”)"
        },
        {
          "title": "Assessing the Effectiveness of Membership Inference on Generative Music",
          "authors": "Kurtis Chow, Omar Samiullah, Vinesh Sridhar, Hewen Zhang",
          "institution": "University of California, Irvine",
          "link": "https://arxiv.org/pdf/2512.21762",
          "code": null,
          "tags": [
            "membership inference attacks",
            "membership inference attack (MIA)",
            "generative music",
            "MuseGAN",
            "privacy",
            "copyright"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09b7176fccbb69c4f0a15bfa6bcbb6ff59b09cf6ac02e0f524334f306ce4041f_w640_q70.webp",
          "contributions": "1. Conducts the first preliminary study on the effectiveness of membership inference attacks (MIAs) specifically on generative music models., 2. Evaluates several existing MIA techniques on the popular MuseGAN model to assess their performance in this domain., 3. Provides empirical evidence suggesting that generative music data is relatively resilient to known membership inference techniques, aligning with prior findings in generative audio.",
          "summary": "This paper investigates whether membership inference attacks (MIAs) are effective against generative music models. The authors conduct a preliminary study by applying several existing MIA techniques to the MuseGAN model. Their findings indicate that generative music data is fairly resilient to these known attacks.",
          "mindmap": "graph TB\n        Root[”Assessing the Effectiveness of Membership Inference on Generative Music”] --> Problem[”核心问题/Problem: Lack of MIA study on generative music, privacy & copyright concerns”]\n        Root --> Method[”主要方法/Method: Apply existing MIAs to MuseGAN model”]\n        Root --> Results[”关键结果/Results: Music data is resilient to known MIAs”]"
        },
        {
          "title": "BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization",
          "authors": "Evgeny Alves Limarenko, Anastasiia Studenikina",
          "institution": "Moscow Institute of Physics and Technology",
          "link": "https://arxiv.org/pdf/2512.21769",
          "code": null,
          "tags": [
            "3d medical image analysis",
            "Masked Autoencoder",
            "Swin Transformer",
            "Self-Supervised Learning",
            "3D Vision Transformer",
            "Structural Priority Loss"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp",
          "contributions": "1. Proposed BertsWin, a hybrid architecture combining full BERT-style token masking with Swin Transformer windows to preserve 3D spatial topology during SSL pre-training. 2. Introduced a structural priority loss function to enhance learning. 3. Demonstrated significant acceleration in semantic convergence (5.8x) and a 15-fold reduction in training epochs to reach SOTA fidelity when combined with the GradientConductor optimizer, without increasing computational FLOPs.",
          "summary": "The paper addresses the difficulty of applying standard Masked Autoencoders to 3D medical images, which lose spatial context. It proposes BertsWin, a hybrid architecture that maintains a full 3D token grid using Swin Transformer windows and a structural loss. The method achieves much faster convergence and state-of-the-art reconstruction fidelity for 3D CBCT scans without extra computational cost.",
          "mindmap": "graph TB\n        Root(”BertsWin: 3D MAE优化”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”3D MAE拓扑稀疏性/Topological Sparsity in 3D MAE”)\n        Problem --> P2(”破坏空间关系/Destroys Spatial Context”)\n        Method --> M1(”BertsWin混合架构/BertsWin Hybrid Architecture”)\n        Method --> M2(”完整3D令牌网格/Full 3D Token Grid”)\n        Method --> M3(”Swin窗口 & 结构损失/Swin Windows & Structural Loss”)\n        Results --> R1(”5.8x语义收敛加速/5.8x Faster Convergence”)\n        Results --> R2(”15倍训练轮次减少/15x Fewer Epochs”)\n        Results --> R3(”FLOPs持平，总资源减少/FLOP Parity, Net Resource Reduction”)"
        },
        {
          "title": "Accelerating Scientific Discovery with Autonomous Goal-evolving Agents",
          "authors": "Yuanqi Du, Botao Yu, Tianyu Liu, Tony Shen, Junwu Chen, Jan G. Rittig, Kunyang Sun, Yikun Zhang, Zhangde Song, Bo Zhou, Cassandra Masschelein, Yingze Wang, Haorui Wang, Haojun Jia, Chao Zhang, Hongyu Zhao, Martin Ester, Teresa Head-Gordon, Carla P. Gomes, Huan Sun, Chenru Duan, Philippe Schwaller, Wengong Jin",
          "institution": "Cornell University, The Ohio State University, Yale University, Simon Fraser University, École Polytechnique Fédérale de Lausanne, University of California Berkeley, Northeastern University, Deep Principle, University of Illinois Chicago, Georgia Institute of Technology, Broad Institute of MIT and Harvard",
          "link": "https://arxiv.org/pdf/2512.21782",
          "code": null,
          "tags": [
            "scientific discovery agents",
            "autonomous goal evolution",
            "bi-level optimization",
            "LLM agents",
            "objective function design",
            "scientific discovery"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9035dcf8fd16c34c235e39c8960c63fa826c45df283663a01722181f7ab419d8_w640_q70.webp",
          "contributions": "1. Identifies and addresses the unmet requirement of automating objective function design for scientific discovery agents, moving beyond fixed, imperfect proxies. 2. Proposes the SAGA framework, a novel bi-level architecture where an outer loop of LLM agents evolves objectives and an inner loop optimizes solutions under them. 3. Demonstrates the framework's effectiveness across diverse scientific domains (antibiotic, materials, DNA, chemical process design), showing improved discovery outcomes.",
          "summary": "This paper introduces SAGA, a framework for scientific discovery where LLM agents autonomously evolve and refine the objective functions used to guide optimization, rather than relying on fixed human-specified goals. This bi-level architecture enables systematic exploration of objective spaces and their trade-offs. The method is shown to substantially improve the effectiveness of discovery agents across multiple application domains.",
          "mindmap": "graph TB\n        Root[Accelerating Scientific Discovery with Autonomous Goal-evolving Agents] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[Fixed objectives are imperfect proxies for grand scientific challenges / 固定的目标函数是科学重大挑战的不完美代理]\n        Method[主要方法/Method] --> M1[Proposes SAGA: Scientific Autonomous Goal-evolving Agent / 提出SAGA: 科学自主目标演化智能体]\n        M1 --> M2[Bi-level architecture: LLM outer loop evolves objectives, inner loop optimizes solutions / 双层架构: LLM外循环演化目标，内循环优化解]\n        Results[关键结果/Results] --> R1[Applied to antibiotic, materials, DNA, chemical process design / 应用于抗生素、材料、DNA、化工过程设计]\n        R1 --> R2[Automating objective formulation improves discovery effectiveness / 自动化目标制定提升了发现效能]"
        },
        {
          "title": "Synthetic Financial Data Generation for Enhanced Financial Modelling",
          "authors": "Christophe D. Hounwanou, Yae Ulrich Gaba, Pierre Ntakirutimana",
          "institution": "AIMS Rwanda, Carnegie Mellon University, Sefako Makgatho Health Sciences University",
          "link": "https://arxiv.org/pdf/2512.21791",
          "code": null,
          "tags": [
            "synthetic data generation",
            "synthetic financial data",
            "TimeGAN",
            "ARIMA-GARCH",
            "VAE",
            "Maximum Mean Discrepancy"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3f9e4bbcf426a2c32c05089e0964e9937c0becca521cac8a83b48c652d0d86c_w640_q70.webp",
          "contributions": "1. Proposes a unified multi-criteria evaluation framework for synthetic financial data. 2. Empirically compares three generative paradigms (ARIMA-GARCH, VAE, TimeGAN) on fidelity, temporal structure, and downstream utility. 3. Provides practical guidelines for model selection and releases a reproducible codebase to standardize benchmarking.",
          "summary": "This paper addresses data scarcity in finance by proposing a unified evaluation framework for synthetic financial data generation. It compares ARIMA-GARCH, VAE, and TimeGAN models using S&P 500 data, finding that TimeGAN offers the best trade-off between realism and temporal coherence. The work concludes with practical guidelines and aims to standardize benchmarking in the field.",
          "mindmap": "graph TB\n        Root[”Synthetic Financial Data Generation for Enhanced Financial Modelling”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”数据稀缺与保密性<br/>Data Scarcity & Confidentiality”]\n        Method[”统一评估框架与三种生成模型<br/>Unified Evaluation Framework & Three Generative Models”]\n        Results[”TimeGAN最佳权衡与实用指南<br/>TimeGAN Best Trade-off & Practical Guidelines”]"
        },
        {
          "title": "Multi-agent Adaptive Mechanism Design",
          "authors": "Qiushi Han, David Simchi-Levi, Renfei Tan, Zishuo Zhao",
          "institution": "Massachusetts Institute of Technology, University of Illinois Urbana-Champaign",
          "link": "https://arxiv.org/pdf/2512.21794",
          "code": null,
          "tags": [
            "mechanism design",
            "distributionally robust optimization",
            "online learning",
            "incentive compatibility",
            "adaptive mechanism",
            "regret analysis"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ece7d8b60855735e1eee48c51256eacf5f3997d56ced3396434f12a30ad44_w640_q70.webp",
          "contributions": "1. Introduces DRAM, a novel framework combining mechanism design and online learning to handle unknown agent beliefs. 2. Provides theoretical guarantees of high-probability truthfulness and achieves optimal $\\tilde\\{O\\}(\\sqrt\\{T\\})$ cumulative regret with a matching lower bound. 3. Generalizes the framework (DRAM+) to support plug-in estimators, structured priors, and delayed feedback.",
          "summary": "This paper addresses the problem of designing a truthful mechanism when the principal has no prior knowledge of agents' beliefs. It proposes the Distributionally Robust Adaptive Mechanism (DRAM), which iteratively learns beliefs and updates a robust optimization problem to minimize cost while ensuring truthfulness. The mechanism is proven to achieve optimal regret, and the framework is the first to maintain truthfulness under these general learning conditions.",
          "mindmap": "graph TB\n        A[Multi-agent Adaptive Mechanism Design] --> B[核心问题/Problem: Elicit truthful reports with no prior knowledge of agent beliefs]\n        A --> C[主要方法/Method: Distributionally Robust Adaptive Mechanism (DRAM)]\n        A --> D[关键结果/Results: Guaranteed truthfulness & optimal $\\tilde{O}(\\sqrt{T})$ regret]"
        },
        {
          "title": "VAMP-Net: An Interpretable Multi-Path Framework of Genomic Permutation-Invariant Set Attention and Quality-Aware 1D-CNN for MTB Drug Resistance",
          "authors": "Aicha Boutorh, Kamar Hibatallah Baghdadi, Anais Daoud",
          "institution": "National School of Artificial Intelligence (ENSIA)",
          "link": "https://arxiv.org/pdf/2512.21786",
          "code": null,
          "tags": [
            "bioinformatics",
            "Set Attention Transformer",
            "1D-CNN",
            "Multi-Path Network",
            "Interpretable Machine Learning",
            "Genomic Variant Analysis"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf587b5249dfee9f3a9866a887e3d791367588c15a447d0c86619ea9c8df8a45_w640_q70.webp",
          "contributions": "1. Proposes a novel multi-path deep learning architecture (VAMP-Net) that combines a permutation-invariant Set Attention Transformer for capturing epistatic interactions and a 1D-CNN for analyzing sequencing quality metrics. 2. Introduces a comparative evaluation of unmasked vs. padding-masked Set Attention Blocks within the architecture. 3. Provides dual-layer interpretability through attention weight analysis for epistatic networks and gradient-based methods for feature importance on both genetic variants and quality metrics.",
          "summary": "This paper introduces VAMP-Net, a multi-path deep learning framework designed to predict drug resistance in Mycobacterium tuberculosis. It combines a Set Attention Transformer to model genetic interactions and a 1D-CNN to assess data quality, achieving high accuracy and AUC. The work concludes that this approach offers superior predictive performance and dual-layer interpretability for clinical genomics.",
          "mindmap": "graph TB\n        Root[VAMP-Net: Interpretable Multi-Path Framework] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[核心问题/Problem<br>Challenges in MTB Drug Resistance Prediction] --> P1[复杂的上位性相互作用/Complex Epistatic Interactions]\n        Problem --> P2[测序数据质量多变/Variable Sequencing Data Quality]\n    \n        Method[主要方法/Method<br>VAMP-Net Multi-Path Architecture] --> M1[路径1: 集合注意力变换器/Path-1: Set Attention Transformer]\n        Method --> M2[路径2: 质量感知1D-CNN/Path-2: Quality-Aware 1D-CNN]\n        Method --> M3[融合模块/Fusion Module]\n        M1 --> M1_Detail[处理变异集合/Processes Variant Sets]\n        M2 --> M2_Detail[分析质量指标/Analyzes Quality Metrics]\n    \n        Results[关键结果/Results<br>Superior Performance & Interpretability] --> R1[性能: >95% 准确率, ~97% AUC/Performance: >95% Acc, ~97% AUC]\n        Results --> R2[可解释性: 双层面分析/Interpretability: Dual-Layer Analysis]\n        R2 --> R2_1[注意力权重揭示上位网络/Attention Weights Reveal Epistatic Networks]\n        R2 --> R2_2[梯度分析识别关键位点与质量指标/Gradient Analysis Identifies Key Loci & Quality Metrics]"
        },
        {
          "title": "Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers",
          "authors": "Krishna Chaitanya Sunkara, Rambabu Konakanchi",
          "institution": "Oracle, Charles Schwab",
          "link": "https://arxiv.org/pdf/2512.21801",
          "code": null,
          "tags": [
            "cluster infrastructure",
            "LSTM",
            "Random Forest",
            "MQTT",
            "InfluxDB",
            "Streamlit"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa113d59b2dc6ec7a3bf00f9eebc8541689a6235867cf59ce376cdcfa30a2a5c_w640_q70.webp",
          "contributions": "1. Probabilistic LSTM forecasting validated within ±30-minute windows for coolant leaks, 2. 96.5% F1-score Random Forest detection for immediate leak identification, 3. Integrated smart IoT architecture design with MQTT streaming, InfluxDB storage, and Streamlit dashboards for energy-efficient data center operations.",
          "summary": "The paper proposes a smart IoT monitoring system combining LSTM neural networks for probabilistic leak forecasting and Random Forest classifiers for instant detection in liquid-cooled AI data centers. The system, tested on synthetic data, achieves 96.5% detection accuracy and 87% forecasting accuracy, potentially preventing significant energy waste through proactive maintenance.",
          "mindmap": "graph TB\n        A[Smart IoT-Based Leak Forecasting and Detection] --> B[核心问题/Problem: Coolant leaks cause energy loss in AI data centers]\n        A --> C[主要方法/Method: LSTM for forecasting + Random Forest for detection with IoT sensors]\n        A --> D[关键结果/Results: 96.5% detection accuracy, 87% forecasting accuracy, 1,500 kWh energy saved]"
        },
        {
          "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
          "authors": "Mengqi He, Xinyu Tian, Xin Shen, Jinhong Ni, Shu Zou, Zhaoyuan Yang, Jing Zhang",
          "institution": "Australian National University, The University of Queensland, GE Research",
          "link": "https://arxiv.org/pdf/2512.21815",
          "code": null,
          "tags": [
            "adversarial attacks",
            "entropy-guided attacks",
            "vision-language models",
            "adversarial robustness",
            "harmful content generation",
            "transferability"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp",
          "contributions": "1. Identifies that only a small fraction (~20%) of high-entropy tokens (critical decision points) disproportionately govern output trajectories in autoregressive VLMs, challenging the prior assumption of equal token importance. 2. Proposes a selective attack strategy that concentrates adversarial perturbations on these high-entropy positions, achieving strong semantic degradation with a smaller perturbation budget and inducing 35-49% of benign outputs to become harmful. 3. Demonstrates that vulnerable high-entropy forks recur across diverse VLMs, enabling feasible transferable attacks (17-26% harmful rates), and proposes the Entropy-bank Guided Adversarial attack (EGA) method which achieves high attack success rates (93-95%) while exposing new safety weaknesses.",
          "summary": "This paper reveals that adversarial attacks on Vision-Language Models (VLMs) can be made more efficient and dangerous by targeting only the critical high-entropy tokens during autoregressive generation, rather than all tokens. The proposed Entropy-bank Guided Adversarial attack (EGA) concentrates perturbations on these positions, achieving high attack success and converting many benign outputs into harmful ones, while also showing significant transferability across models. This exposes a critical and previously overlooked vulnerability in current VLM safety mechanisms.",
          "mindmap": "graph TB\n        Root[Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[VLMs易受对抗攻击/VLMs are vulnerable to adversarial attacks]\n        Problem --> P2[先验攻击假设所有token同等重要/Prior attacks assume all tokens are equally important]\n        Method[主要方法/Method] --> M1[识别高熵关键决策点/Identify high-entropy critical decision points]\n        Method --> M2[提出熵库引导对抗攻击(EGA)/Propose Entropy-bank Guided Adversarial attack (EGA)]\n        Results[关键结果/Results] --> R1[高效攻击:小预算实现强语义退化/Efficient attack: strong degradation with small budget]\n        Results --> R2[高有害转化率:35-49%/High harmful conversion: 35-49%]\n        Results --> R3[可行迁移性:17-26%/Feasible transferability: 17-26%]"
        },
        {
          "title": "Scalable Class-Incremental Learning Based on Parametric Neural Collapse",
          "authors": "Chuangxin Zhang, Guangfeng Lin, Enhui Zhao, Kaiyang Liao, Yajun Chen",
          "institution": "Not explicitly stated in the provided content. Affiliation information is not included.",
          "link": "https://arxiv.org/pdf/2512.21845",
          "code": "this",
          "tags": [
            "class-incremental learning",
            "parametric neural collapse",
            "equiangular tight frame",
            "knowledge distillation",
            "adaptive expansion",
            "feature alignment"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp",
          "contributions": "1. Proposes a scalable class-incremental learning method (SCL-PNC) based on parametric neural collapse, enabling demand-driven, minimal-cost backbone expansion via an adapt-layer. 2. Introduces a dynamic parametric Equiangular Tight Frame (ETF) classifier to address class misalignment caused by evolving class distributions. 3. Presents a parallel expansion framework with a knowledge distillation algorithm to counteract feature drift and ensure feature consistency across expansion modules.",
          "summary": "The paper addresses challenges in class-incremental learning, such as overfitting and catastrophic forgetting, by proposing SCL-PNC. This method uses a parametric neural collapse framework with an adaptive backbone expansion and a dynamic ETF classifier to efficiently handle growing categories while maintaining feature consistency via distillation. Experiments show the method is effective and efficient.",
          "mindmap": "graph TB\n        Root[”Scalable Class-Incremental Learning Based on Parametric Neural Collapse”] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[”核心问题 / Problem”] --> P1[”过拟合新数据 / Overfitting to new data”]\n        Problem --> P2[”灾难性遗忘旧数据 / Catastrophic forgetting of old data”]\n        Problem --> P3[”特征差异与类别错位 / Feature difference & Class misalignment”]\n    \n        Method[”主要方法 / Method”] --> M1[”SCL-PNC方法 / SCL-PNC Method”]\n        M1 --> M1_1[”自适应层扩展主干 / Adapt-layer for backbone expansion”]\n        M1 --> M1_2[”动态参数化ETF分类器 / Dynamic Parametric ETF Classifier”]\n        M1 --> M1_3[”并行扩展与知识蒸馏 / Parallel expansion & Knowledge distillation”]\n    \n        Results[”关键结果 / Results”] --> R1[”高效处理类别增长 / Efficiently handles increasing categories”]\n        Results --> R2[”解决类别错位 / Addresses class misalignment”]\n        Results --> R3[”确保特征一致性 / Ensures feature consistency”]"
        },
        {
          "title": "A Comedy of Estimators: On KL Regularization in RL Training of LLMs",
          "authors": "Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro, Yoshua Bengio, Nikolay Malkin, Moksh Jain, Siddarth Venkatraman, Aaron Courville",
          "institution": "Mila – Quebec AI Institute, Université de Montréal, McGill University, LLNL, University of Edinburgh, CIFAR",
          "link": "https://arxiv.org/pdf/2512.21852",
          "code": null,
          "tags": [
            "reinforcement learning",
            "KL divergence",
            "policy gradient",
            "on-policy sampling",
            "off-policy training",
            "gradient bias"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96999f081bc421143202d98f560b5d13a6fb0c09613b8c160b121158bce3811a_w640_q70.webp",
          "contributions": "1. Systematic analysis of KL divergence estimator configurations in RL for LLMs, revealing how design choices introduce gradient bias. 2. Empirical demonstration that estimator configurations with unbiased gradients lead to better and more stable performance on both in-domain and out-of-domain tasks. 3. Investigation showing KL regularization can stabilize off-policy RL training in asynchronous setups.",
          "summary": "This paper analyzes the use of various estimators for the KL divergence regularization term in RL fine-tuning of LLMs, finding that common practices introduce biased gradients. Through experiments on models like Qwen2.5-7B, the study shows that using estimator configurations with unbiased gradients improves training stability and downstream task performance. The work also finds that KL regularization helps stabilize off-policy RL training.",
          "mindmap": "graph TB\n        Root[”A Comedy of Estimators: On KL Regularization in RL Training of LLMs<br>论文标题”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>KL正则化估计器配置缺乏系统研究，梯度存在偏差”] --> P1[”实践问题/Practical Issue<br>广泛使用但实现与目标不一致”]\n        Problem --> P2[”理论问题/Theoretical Issue<br>梯度偏差影响训练稳定性”]\n        Method[”主要方法/Method<br>分析梯度偏差并进行实证验证”] --> M1[”分析/Analysis<br>研究多种估计器配置的梯度”]\n        Method --> M2[”实验/Experiments<br>RL微调多个LLM并评估性能”]\n        Results[”关键结果/Results<br>无偏梯度配置带来更好性能”] --> R1[”在线策略/On-Policy<br>无偏梯度配置提升稳定性和性能”]\n        Results --> R2[”离线策略/Off-Policy<br>KL正则化有助于稳定异步训练”]"
        },
        {
          "title": "Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening",
          "authors": "Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan",
          "institution": "North South University, Korea Institute of Oriental Medicine, American International University–Bangladesh",
          "link": "https://arxiv.org/pdf/2512.21861",
          "code": null,
          "tags": [
            "medical image classification",
            "feature-level fusion",
            "convolutional neural networks",
            "diabetic retinopathy screening",
            "EfficientNet",
            "DenseNet"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp",
          "contributions": "1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency.",
          "summary": "This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening.",
          "mindmap": "graph TB\n        A[Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Large-scale DR screening is constrained by limited specialists and variable image quality.]\n        C[主要方法/Method<br>Feature-level fusion of complementary CNN backbones (e.g., EfficientNet, DenseNet) on pooled fundus images.]\n        D[关键结果/Results<br>Fusion outperforms single models. Eff+Den fusion offers best accuracy-latency balance.]"
        },
        {
          "title": "Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation",
          "authors": "Yiming Qian, Thorsten Neumann, Xueyining Huang, David Hardoon, Fei Gao, Yong Liu, Siow Mong Rick Goh",
          "institution": "Institute of High Performance Computing (A*STAR), Standard Chartered Bank, Xi’an Jiaotong–Liverpool University",
          "link": "https://arxiv.org/pdf/2512.21866",
          "code": null,
          "tags": [
            "Privacy-preserving machine learning",
            "dataset distillation",
            "random forest",
            "synthetic data generation",
            "explainable AI",
            "membership-inference attack"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6994d363f1e76d7cc78ab02d48685c11199b353aab61b3eb5297064b1df9b722_w640_q70.webp",
          "contributions": "1. Proposes a privacy-preserving dataset distillation framework that converts a trained random forest into transparent rule regions and generates synthetic data by uniform sampling within these regions, creating a compact, auditable surrogate dataset. 2. Enables explainable AI by providing both global pattern summaries (e.g., support, lift) from aggregated rules and local, human-readable rationales with calibrated uncertainty for individual cases based on tree-vote disagreement. 3. Demonstrates strong utility-privacy trade-offs, showing the distilled data maintains competitive model performance (e.g., precision, F1) with significant data reduction (85-93%), improves cross-institution learning, and resists membership-inference attacks (AUC ~0.5), indicating low memorization risk.",
          "summary": "This paper proposes a dataset distillation method for financial fraud detection that converts a random forest model into interpretable rule regions to generate synthetic data, preserving privacy and model utility. The approach produces a compact, explainable dataset that supports collaborative learning across institutions while resisting privacy attacks. Experiments show it reduces data volume by over 85% with minimal performance loss and enhances cross-cluster fraud detection.",
          "mindmap": "graph TB\n        Root(”Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”需要隐私保护的协作式欺诈检测/Need for privacy-preserving collaborative fraud detection”)\n        Problem --> P2(”模型需要可解释性/Model needs explainability”)\n        Method --> M1(”将随机森林转换为规则区域/Convert random forest to rule regions”)\n        Method --> M2(”在区域内均匀采样生成合成数据/Uniformly sample within regions to generate synthetic data”)\n        Results --> R1(”数据量减少85-93%/Data volume reduced by 85-93%”)\n        Results --> R2(”保持竞争性性能/Maintains competitive performance”)\n        Results --> R3(”抵抗成员推理攻击/Resists membership-inference attacks”)"
        },
        {
          "title": "MMCTOP: A Multimodal Textualization and Mixture-of-Experts Framework for Clinical Trial Outcome Prediction",
          "authors": "Carolina Aparício, Qi Shi, Bo Wen, Tesfaye Yadete, Qiwei Han",
          "institution": "Nova School of Business and Economics, Hogarthian Technologies, IBM Research, Cleveland Clinic, Oregon Health & Science University",
          "link": "https://arxiv.org/pdf/2512.21897",
          "code": null,
          "tags": [
            "multi-modal training",
            "multimodal fusion",
            "sparse mixture-of-experts",
            "schema-guided textualization",
            "clinical trial prediction",
            "temperature scaling"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb552211ea905d5cbf8e190ead9078caf65c5d200327a02c7a6dab156a030645_w640_q70.webp",
          "contributions": "1. A multimodal framework (MMCTOP) that integrates molecular structures, protocol metadata, eligibility narratives, and disease ontologies for clinical trial outcome prediction. 2. A novel architecture combining schema-guided textualization for data normalization and a drug-disease-conditioned sparse Mixture-of-Experts (SMoE) for context-aware, specialized multimodal fusion. 3. Demonstrates improved prediction performance (precision, F1, AUC) over baselines and incorporates operational safeguards like temperature scaling for calibrated probabilities to enhance auditability and reproducibility.",
          "summary": "The paper proposes MMCTOP, a multimodal framework for predicting clinical trial outcomes. It uses schema-guided textualization to normalize heterogeneous data and a sparse Mixture-of-Experts model for specialized fusion, achieving better performance than existing methods and providing calibrated risk estimates.",
          "mindmap": "graph TB\n        Root[MMCTOP: 多模态文本化与专家混合框架<br>MMCTOP: Multimodal Textualization and Mixture-of-Experts Framework] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>多模态数据融合挑战<br>Multimodal Data Fusion Challenge] --> P1[高维生物医学信息学<br>High-Dim Biomedical Informatics]\n        Method[主要方法/Method<br>多模态框架<br>Multimodal Framework] --> M1[模式感知表征学习<br>Modality-Aware Representation Learning]\n        Method --> M2[架构设计/Architecture Design]\n        M1 --> M1_1[领域特定编码器<br>Domain-Specific Encoders]\n        M2 --> M2_1[模式感知表征学习<br>Modality-Aware Representation Learning]\n        M2 --> M2_2[稀疏专家混合<br>Sparse Mixture-of-Experts (SMoE)]\n        M2 --> M2_3[模式感知表征学习<br>Modality-Aware Representation Learning]\n        Results[关键结果/Results<br>性能提升与校准<br>Performance & Calibration] --> R1[指标改进<br>Metric Improvements]\n        Results --> R2[消融研究<br>Ablation Studies]\n        Results --> R3[概率校准<br>Probability Calibration]"
        },
        {
          "title": "GQ-VAE: A gated quantized VAE for learning variable length tokens",
          "authors": "Theo Datta, Kayla Huang, Sham Kakade, David Brandfonbrener",
          "institution": "Kempner Institute, Harvard University",
          "link": "https://arxiv.org/pdf/2512.21913",
          "code": "https://github.com/Theo-Datta-115/gq-vae",
          "tags": [
            "tokenization",
            "GQ-VAE",
            "variable-length tokens",
            "VQ-VAE",
            "neural tokenizer",
            "byte-pair encoding"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49412452efda1a18023f64f968f3406b217d747381b47a09694b7d37c61c918a_w640_q70.webp",
          "contributions": "1. Proposes GQ-VAE, a novel gated quantized variational autoencoder architecture for learning discrete, variable-length tokens. 2. Demonstrates the model can serve as a standalone, pre-trained drop-in replacement for existing tokenizers, improving over fixed-chunk baselines. 3. Shows that with equivalent compression, GQ-VAE improves downstream language model learning compared to BPE.",
          "summary": "The paper addresses the limitations of traditional tokenizers like BPE and complex neural tokenizers by proposing GQ-VAE, a novel architecture that learns variable-length discrete tokens. GQ-VAE can be independently pre-trained as a drop-in replacement, approaching BPE's performance and, under equivalent compression, improving downstream language model learning.",
          "mindmap": "graph TB\n        Root[GQ-VAE: A gated quantized VAE for learning variable length tokens] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[传统分词器如BPE是确定性的/Traditional tokenizers like BPE are deterministic]\n        Problem --> P2[神经分词器复杂且难集成/Neural tokenizers are complex and hard to integrate]\n        Method[主要方法/Method] --> M1[提出GQ-VAE架构/Propose GQ-VAE architecture]\n        Method --> M2[学习变长离散令牌/Learn variable-length discrete tokens]\n        Method --> M3[可独立预训练/Can be independently pre-trained]\n        Results[关键结果/Results] --> R1[压缩和语言建模性能接近BPE/Compression & LM performance approaches BPE]\n        Results --> R2[在同等压缩下提升下游LM学习/Improves downstream LM learning at equivalent compression]"
        },
        {
          "title": "Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs",
          "authors": "Yafeng Tang, Xiaoou Ding, Jianzhuo Du, Zishuo Yan, Zhuang Ma, Zheng Liang, Zekai Qian, Hongzhi Wang",
          "institution": "Harbin Institute of Technology",
          "link": "https://arxiv.org/pdf/2512.21915",
          "code": "https://github.com/windblow32/DATE",
          "tags": [
            "others",
            "Tabular Data Generation",
            "Large Language Models",
            "Multi-Arm Bandit",
            "Data Diversity",
            "In-context Learning"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d8fa8039f8a5fa0717fc5e4a9c7ba2cf1fd158e44821059f4a767bc88790eaf_w640_q70.webp",
          "contributions": "1. Introduces DATE, a framework that partitions heterogeneous tabular data into diverse subsets to prepare high-quality examples for in-context learning. 2. Proves the selection problem in heterogeneous data generation lacks the greedy-choice property and designs a Multi-Arm Bandit-based sampling algorithm to balance diversity and quality. 3. Demonstrates that DATE-generated data improves downstream tasks like Direct Preference Optimization (DPO) and enhances LLM reasoning on target data.",
          "summary": "This paper addresses the challenge of generating high-quality synthetic tabular data from heterogeneous distributions. It proposes DATE, a framework that uses LLMs with decision tree feedback for subset-specific generation and a Multi-Arm Bandit algorithm for data selection. Experiments show DATE outperforms existing methods, reducing error rates and improving downstream model performance.",
          "mindmap": "graph TB\n        Root[”Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs”] --> Problem[”核心问题/Problem: Real-world tabular data is heterogeneous, making universal generation models challenging”]\n        Root --> Method[”主要方法/Method: DATE framework partitions data, uses LLMs with decision tree feedback, and applies Multi-Arm Bandit for selection”]\n        Root --> Results[”关键结果/Results: Outperforms SOTA methods, reduces error rate by 23.75%, improves DPO and LLM reasoning”]"
        },
        {
          "title": "Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model",
          "authors": "Nathan Kallus",
          "institution": "Netflix, Cornell University",
          "link": "https://arxiv.org/pdf/2512.21917",
          "code": null,
          "tags": [
            "reinforcement learning from human feedback (rlhf)",
            "preference optimization",
            "single-index model",
            "semiparametric",
            "link function",
            "policy learning"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/378875cc0ef0eb142c184d960e479724ea3df83c84cf81e185efea5469a4387e_w640_q70.webp",
          "contributions": "1. Formulates policy alignment as a semiparametric single-index model problem, relaxing the need for a known link function between preferences and rewards. 2. Develops novel policy learners based on profiling, orthogonalizing, and link-agnostic ranking objectives, providing theoretical error bounds. 3. Proposes practical first-order optimization implementations that are robust to unknown preference noise and scale, enabling direct policy optimization without explicit reward fitting.",
          "summary": "The paper addresses the problem of bias in aligning language models when the assumed link function between human preferences and latent rewards is misspecified. It proposes a semiparametric framework that treats the link function as unknown, develops several robust policy learning algorithms, and provides theoretical guarantees. The main conclusion is that this approach enables more reliable policy alignment without needing to correctly specify the preference noise distribution.",
          "mindmap": "graph TB\n        Root[”Semiparametric Preference Optimization<br>你的语言模型是一个单指标模型”] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[”核心问题/Problem<br>已知链接函数错误导致策略偏差<br>Misspecified link function causes policy misalignment”]\n        Method[”主要方法/Method<br>将链接函数视为未知的半参数单指标模型<br>Treat link as unknown semiparametric single-index model”]\n        Results[”关键结果/Results<br>开发鲁棒的策略学习器并提供理论保证<br>Develop robust policy learners with theoretical guarantees”]"
        },
        {
          "title": "AutoPP: Towards Automated Product Poster Generation and Optimization",
          "authors": "Jiahao Fan, Yuxin Qin, Wei Feng, Yanyin Chen, Yaoyu Li, Ao Ma, Yixiu Li, Li Zhuang, Haoyi Bian, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law",
          "institution": "JD.COM",
          "link": "https://arxiv.org/pdf/2512.21921",
          "code": "https://github.com/JD-GenX/AutoPP",
          "tags": [
            "image generation",
            "product poster generation",
            "click-through rate optimization",
            "isolated direct preference optimization",
            "AutoPP1M dataset",
            "unified design module"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp",
          "contributions": "1. An automated pipeline (AutoPP) for end-to-end product poster generation and optimization, requiring only basic product information as input. 2. A novel optimization method that uses systematic element replacement and Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to specific poster elements. 3. The creation and release of AutoPP1M, the largest dataset for product poster generation and optimization, containing one million posters and user feedback.",
          "summary": "The paper introduces AutoPP, an automated pipeline that generates product posters from basic product information and then optimizes them for higher Click-Through Rate (CTR) using online feedback and a novel Isolated Direct Preference Optimization technique. It is supported by a large-scale dataset, AutoPP1M. Experiments show that AutoPP achieves state-of-the-art performance in both offline and online evaluations.",
          "mindmap": "graph TB\n        A[AutoPP: Towards Automated Product Poster Generation and Optimization] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[人工制作与优化海报耗时耗力/Manual poster creation and optimization is laborious]\n        C --> C1[自动化生成与优化管道/Automated generation and optimization pipeline]\n        C1 --> C1_1[生成器: 统一设计模块与元素渲染/Generator: Unified design & element rendering]\n        C1 --> C1_2[优化器: 元素替换与IDPO/Optimizer: Element replacement & IDPO]\n        C --> C2[数据集: AutoPP1M/Dataset: AutoPP1M]\n        D --> D1[离线和在线SOTA结果/Offline and online SOTA results]\n        D --> D2[代码与数据集公开/Code & dataset released]"
        },
        {
          "title": "Data relativistic uncertainty framework for low-illumination anime scenery image enhancement",
          "authors": "Yiquan Gao, John See",
          "institution": "Heriot-Watt University",
          "link": "https://arxiv.org/pdf/2512.21944",
          "code": null,
          "tags": [
            "low-light image enhancement",
            "Data Relativistic Uncertainty",
            "Unsupervised Learning",
            "EnlightenGAN",
            "Anime Scenery",
            "Domain Gap"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp",
          "contributions": "1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods.",
          "summary": "This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality.",
          "mindmap": "graph TB\n        Root(”Data relativistic uncertainty framework for low-illumination anime scenery image enhancement”) --> Problem\n        Root --> Method\n        Root --> Results\n        Problem(”核心问题/Problem: Low-illumination quality degradation in anime scenery images, an underexplored task with a domain gap from natural images.”)\n        Method(”主要方法/Method: Propose a Data Relativistic Uncertainty (DRU) framework to quantify illumination uncertainty and dynamically adjust objective functions for model recalibration.”)\n        Results(”关键结果/Results: DRU framework yields superior perceptual and aesthetic qualities beyond state-of-the-art methods.”)"
        },
        {
          "title": "Hybrid Combinatorial Multi-armed Bandits with Probabilistically Triggered Arms",
          "authors": "Kongchang Zhou, Tingyu Zhang, Wei Chen, Fang Kong",
          "institution": "Southern University of Science and Technology, Microsoft Research",
          "link": "https://arxiv.org/pdf/2512.21925",
          "code": null,
          "tags": [
            "multi-armed bandits",
            "combinatorial multi-armed bandits",
            "probabilistically triggered arms",
            "hybrid learning",
            "offline data",
            "online interaction"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e97b8cf09a0691d48238a8272fecd32791ddc20b9ba00115a757d778b1e2017f_w640_q70.webp",
          "contributions": "1. Proposes a new hybrid CMAB-T framework that integrates offline data with online interaction to address the complementary weaknesses of purely online or offline methods. 2. Introduces the hybrid CUCB algorithm, which leverages offline data to guide exploration and strategically uses online interactions to correct dataset bias. 3. Provides theoretical regret guarantees and empirical results demonstrating the algorithm's consistent advantage over purely online or offline baselines.",
          "summary": "This paper proposes a hybrid framework for combinatorial multi-armed bandits with probabilistically triggered arms (CMAB-T) that combines offline data with online interaction. The core method is the hybrid CUCB algorithm, which uses offline data to accelerate learning and online interaction to correct for dataset limitations. Theoretical and empirical results show this hybrid approach outperforms purely online or offline methods.",
          "mindmap": "graph TB\n        Root(”Hybrid CMAB-T<br>混合组合多臂老虎机”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”在线方法成本高、适应慢<br>Online: High Cost, Slow”)\n        Problem --> P2(”离线方法受数据质量限制<br>Offline: Data Quality Limits”)\n        Method --> M1(”提出混合CMAB-T框架<br>Propose Hybrid CMAB-T Framework”)\n        Method --> M2(”设计混合CUCB算法<br>Design Hybrid CUCB Algorithm”)\n        M2 --> M2a(”利用离线数据引导探索<br>Use Offline Data to Guide”)\n        M2 --> M2b(”结合在线交互纠正偏差<br>Use Online to Correct Bias”)\n        Results --> R1(”理论悔恨界保证<br>Theoretical Regret Guarantee”)\n        Results --> R2(”实验显示一致优势<br>Empirical Consistent Advantage”)"
        },
        {
          "title": "Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs",
          "authors": "Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji, Zhongshi He",
          "institution": "Chongqing University, Xinjiang University",
          "link": "https://arxiv.org/pdf/2512.21999",
          "code": "https://github.com/hujiayu1223/ALEAHallu",
          "tags": [
            "multi-modal training",
            "hallucination mitigation",
            "adversarial parametric editing",
            "parameter clustering",
            "visual-language models",
            "activation dataset"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp",
          "contributions": "1. Proposes a novel adversarial parametric editing framework (ALEAHallu) following an Activate-Locate-Edit Adversarially paradigm for mitigating hallucinations in VLMs. 2. Introduces a method to construct an activation dataset with grounded and hallucinatory response pairs and identifies critical hallucination-prone parameter clusters via differential hidden state analysis. 3. Demonstrates the framework's effectiveness by fine-tuning identified parameter clusters with adversarially tuned prefixes to force the model to prioritize visual evidence over linguistic priors.",
          "summary": "This paper addresses the hallucination problem in Vision-Language Models (VLMs), where models generate content misaligned with visual inputs due to over-reliance on linguistic priors. The authors propose ALEAHallu, an adversarial parametric editing framework that identifies and fine-tunes hallucination-prone parameter clusters using adversarially optimized prompts to enhance visual grounding. Evaluations show the method significantly reduces hallucinations in both generative and discriminative VLM tasks.",
          "mindmap": "graph TB\n        A[Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: VLM幻觉问题/VLM Hallucination Issue]\n        C[主要方法/Method: ALEAHallu框架/ALEAHallu Framework]\n        D[关键结果/Results: 有效缓解幻觉/Effectively Mitigates Hallucinations]\n        C --> C1[激活数据集/Activation Dataset]\n        C --> C2[定位关键参数/Locate Critical Parameters]\n        C --> C3[对抗性编辑/Adversarial Editing]"
        },
        {
          "title": "DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction",
          "authors": "Aicha Boutorh, Soumia Bouyahiaoui, Sara Belhadj, Nour El Yakine Guendouz, Manel Kara Laouar",
          "institution": "National School of Artificial Intelligence (ENSIA)",
          "link": "https://arxiv.org/pdf/2512.22007",
          "code": null,
          "tags": [
            "computational biology",
            "protein language model",
            "ESM-2",
            "dual-stream architecture",
            "1D CNN",
            "transformer encoder"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1985304be62407b10b5e5be53aea80fe4ff1468be03e261847b49774ddd937a8_w640_q70.webp",
          "contributions": "1. Proposes DuaDeep-SeqAffinity, a novel sequence-only deep learning framework for antigen-antibody affinity prediction using a dual-stream hybrid architecture. 2. Integrates pre-trained ESM-2 embeddings with 1D CNNs for local motifs and Transformer encoders for global context, followed by a fusion module. 3. Demonstrates superior performance over single-branch models and existing SOTA methods, even surpassing some structure-sequence hybrid models, proving the efficacy of sequence-only high-fidelity embeddings.",
          "summary": "This paper introduces DuaDeep-SeqAffinity, a deep learning framework that predicts antigen-antibody binding affinity using only amino acid sequences. It combines ESM-2 embeddings with a dual-stream architecture of 1D CNNs and Transformers to capture local and global features. The model outperforms existing methods, showing that sequence-only models can effectively capture binding patterns and accelerate therapeutic discovery.",
          "mindmap": "graph TB\n        A[DuaDeep-SeqAffinity: 序列抗原-抗体亲和力预测 / Sequence-Only Antigen-Antibody Affinity Prediction] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[传统方法依赖稀缺的3D结构 / Traditional methods rely on scarce 3D structures]\n        C --> C1[双流混合架构 / Dual-Stream Hybrid Architecture]\n        C1 --> C2[使用ESM-2嵌入 / Uses ESM-2 Embeddings]\n        C1 --> C3[1D CNN检测局部模式 / 1D CNN for Local Motifs]\n        C1 --> C4[Transformer编码全局上下文 / Transformer for Global Context]\n        C1 --> C5[融合模块整合特征 / Fusion Module Integrates Features]\n        D --> D1[性能超越SOTA / Outperforms SOTA]\n        D --> D2[皮尔逊相关: 0.688 / Pearson: 0.688]\n        D --> D3[AUC: 0.890]\n        D --> D4[证明序列嵌入的有效性 / Proves Efficacy of Sequence Embeddings]"
        },
        {
          "title": "HWL-HIN: A Hypergraph-Level Hypergraph Isomorphism Network as Powerful as the Hypergraph Weisfeiler-Lehman Test with Application to Higher-Order Network Robustness",
          "authors": "Chengyu Tian, Wenbin Pei",
          "institution": "Not explicitly stated in the provided content.",
          "link": "https://arxiv.org/pdf/2512.22014",
          "code": null,
          "tags": [
            "graph neural networks",
            "hypergraph isomorphism network",
            "hypergraph weisfeiler-lehman test",
            "higher-order network robustness",
            "hypergraph neural networks"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3b228ae57b3d7937d8586b0c60419df83b49466ba7ca3b946109dd8186f827c_w640_q70.webp",
          "contributions": "1. Proposes a hypergraph-level Hypergraph Isomorphism Network (HWL-HIN) framework for hypergraph learning. 2. Theoretically proves the model's expressive power is strictly equivalent to the Hypergraph Weisfeiler-Lehman test. 3. Successfully applies the model to predict hypergraph robustness, demonstrating superior performance and efficiency over existing graph-based and conventional HGNN models.",
          "summary": "This paper addresses the computational overhead of traditional robustness assessment and the limited expressive power of existing hypergraph neural networks by proposing a hypergraph-level Hypergraph Isomorphism Network (HWL-HIN). The method is proven theoretically to be as powerful as the Hypergraph Weisfeiler-Lehman test and is applied to predict hypergraph robustness. Experiments show it outperforms existing graph-based models and conventional HGNNs in tasks prioritizing topological structure while maintaining high efficiency.",
          "mindmap": "graph TB\n        A[HWL-HIN: Hypergraph-Level Hypergraph Isomorphism Network] --> B(核心问题/Problem: High computational cost of robustness assessment; Limited expressive power of HGNNs)\n        A --> C(主要方法/Method: Propose HWL-HIN framework inspired by GIN; Prove expressive power equivalent to Hypergraph WL test)\n        A --> D(关键结果/Results: Outperforms graph-based models and HGNNs; Maintains superior efficiency)"
        },
        {
          "title": "From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation",
          "authors": "Nagham Osman, Vittorio Lembo, Giovanni Bottegoni, Laura Toni",
          "institution": "University College London, University of Urbino Carlo Bo",
          "link": "https://arxiv.org/pdf/2512.22031",
          "code": null,
          "tags": [
            "generative models for drug discovery",
            "hit-like molecule generation",
            "autoregressive models",
            "diffusion models",
            "docking scores",
            "multi-stage filtering"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0d0d188f2e13e0f8561de5950d5637586c871a0ee36935ebfbd5bb2bad540_w640_q70.webp",
          "contributions": "1. Framing hit-like molecule generation as a standalone task for generative models. 2. Proposing a tailored evaluation framework integrating physicochemical, structural, and bioactivity criteria. 3. Benchmarking autoregressive and diffusion models, with synthesized GSK-3β hits confirmed active in vitro.",
          "summary": "This paper investigates whether generative models can replace the hit identification step in drug discovery. It proposes a multi-stage evaluation framework and benchmarks autoregressive and diffusion models, showing they can generate valid, diverse, and biologically relevant compounds, with some synthesized hits confirmed active in vitro.",
          "mindmap": "graph TB\n        Root(”From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”Hit identification is resource-intensive/命中识别资源密集”)\n        Method --> M1(”Propose tailored evaluation framework/提出定制评估框架”)\n        Method --> M2(”Benchmark autoregressive & diffusion models/基准测试自回归和扩散模型”)\n        Results --> R1(”Models generate valid, diverse, bioactive compounds/模型生成有效、多样、有生物活性的化合物”)\n        Results --> R2(”Selected hits synthesized & confirmed active/选定命中物被合成并确认有效”)"
        },
        {
          "title": "Direction Finding with Sparse Arrays Based on Variable Window Size Spatial Smoothing",
          "authors": "Wesley S. Leite, Rodrigo C. de Lamare, Yuriy Zakharov, Wei Liu, Martin Haardt",
          "institution": "University of York, Pontifical Catholic University of Rio de Janeiro, University of York, University of York, Ilmenau University of Technology",
          "link": "https://arxiv.org/pdf/2512.22024",
          "code": null,
          "tags": [
            "signal processing",
            "DOA estimation",
            "sparse arrays",
            "coarrays",
            "spatial smoothing",
            "MUSIC"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/20bb39a95f66255cc62bbe7ee6e002de101d9a346c703b72e27fc3d3b08e1d30_w640_q70.webp",
          "contributions": "1. Introduces a variable window size (VWS) spatial smoothing framework for coarray-based DOA estimation with sparse linear arrays. 2. Proposes the VWS-CA-MUSIC and VWS-CA-rMUSIC algorithms that replace perturbed data terms with unperturbed ones to improve signal-noise subspace separation. 3. Derives theoretical bounds for the compression parameter to guarantee identifiability and demonstrates performance improvements and complexity savings over fixed-window methods.",
          "summary": "This paper addresses the problem of direction-of-arrival (DOA) estimation for correlated sources using sparse linear arrays. It proposes a variable window size spatial smoothing framework and new VWS-CA-MUSIC algorithms that enhance estimation accuracy by compressing the smoothing aperture. Simulations show the method provides significant performance gains and reduced computational complexity compared to standard fixed-window coarray MUSIC.",
          "mindmap": "graph TB\n        A[Direction Finding with Sparse Arrays Based on Variable Window Size Spatial Smoothing] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[DOA估计精度下降/DOA estimation accuracy degrades for correlated/coherent sources]\n        C --> C1[可变窗口空间平滑/Variable Window Size Spatial Smoothing]\n        C --> C2[压缩平滑孔径/Compressing the smoothing aperture]\n        C --> C3[VWS-CA-MUSIC算法/VWS-CA-MUSIC algorithm]\n        D --> D1[提高信噪子空间分离/Increased signal-noise subspace separation]\n        D --> D2[性能提升与复杂度降低/Performance improvements and complexity savings]"
        },
        {
          "title": "LibContinual: A Comprehensive Library towards Realistic Continual Learning",
          "authors": "Wenbin Li, Shangge Liu, Borui Kang, Yiyang Chen, KaXuan Lew, Yang Chen, Yinghuan Shi, Lei Wang, Yang Gao, Jiebo Luo",
          "institution": "Nanjing University, University of Wollongong, University of Rochester",
          "link": "https://arxiv.org/pdf/2512.22029",
          "code": "https://github.com/RL-VIG/LibContinual",
          "tags": [
            "others",
            "catastrophic forgetting",
            "stability-plasticity dilemma",
            "modular architecture",
            "memory budget",
            "online continual learning"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a733a967f663a216bbd5fb0cee657ed8bb70a4e6f0205d669f983cbf9bb6fd_w640_q70.webp",
          "contributions": "1. Proposed LibContinual, a unified, modular, and reproducible library for Continual Learning (CL) that integrates 19 representative algorithms across five methodological categories. 2. Systematically identified and investigated three unrealistic implicit assumptions (offline data accessibility, unregulated memory, intra-task semantic homogeneity) prevalent in mainstream CL evaluation. 3. Conducted a comprehensive analysis under stricter, more realistic settings (strict online CL, unified memory budget, category-randomized tasks), revealing significant performance drops in many existing methods and highlighting the need for resource-aware and semantically robust CL strategies.",
          "summary": "This paper introduces LibContinual, a comprehensive library designed to unify and standardize research in Continual Learning (CL). By using this framework to evaluate existing methods under more realistic constraints, the study shows that many current CL algorithms suffer significant performance drops, underscoring the gap between common evaluation practices and real-world applicability.",
          "mindmap": "graph TB\n        A[LibContinual: A Comprehensive Library towards Realistic Continual Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[研究碎片化，缺乏统一框架/Fragmented research landscape, lack of unified framework]\n        B --> B2[评估存在不现实的隐含假设/Unrealistic implicit assumptions in evaluation]\n        C --> C1[构建模块化、可复现的库/Build a modular, reproducible library]\n        C --> C2[集成19种代表性算法/Integrate 19 representative algorithms]\n        C --> C3[在更现实的设定下系统评估/Systematically evaluate under more realistic settings]\n        D --> D1[现有方法在现实约束下性能显著下降/Existing methods show significant performance drop under realistic constraints]\n        D --> D2[强调资源感知和语义鲁棒策略的必要性/Highlight the necessity of resource-aware and semantically robust strategies]"
        },
        {
          "title": "Why Smooth Stability Assumptions Fail for ReLU Learning",
          "authors": "Ronald Katende",
          "institution": "Kabale University",
          "link": "https://arxiv.org/pdf/2512.22055",
          "code": null,
          "tags": [
            "optimization theory",
            "ReLU networks",
            "nonsmooth optimization",
            "stability analysis",
            "generalized derivatives",
            "learning dynamics"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3150186f6ab289f5e6db87d8ca89851af409290b0b4c37667d493ee4b7e894b_w640_q70.webp",
          "contributions": "1. Demonstrates that no uniform smoothness-based stability proxy (e.g., gradient Lipschitzness) can hold globally for ReLU networks, even in simple, empirically stable settings. 2. Provides a concrete counterexample showing the failure of classical stability bounds for ReLU learning dynamics. 3. Identifies a minimal generalized derivative condition under which stability statements can be meaningfully restored for nonsmooth learning systems.",
          "summary": "The paper shows that smoothness assumptions like gradient Lipschitzness, commonly used in stability analyses, fail globally for ReLU networks due to their inherent nonsmoothness. It provides a counterexample to classical bounds and proposes a minimal condition based on generalized derivatives to restore meaningful stability analysis. This clarifies the limitations of smooth approximations and motivates nonsmooth-aware frameworks.",
          "mindmap": "graph TB\n        A[Why Smooth Stability Assumptions Fail for ReLU Learning] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Smooth stability assumptions are violated by ReLU networks.]\n        C[主要方法/Method<br>Provide counterexample and identify minimal generalized derivative condition.]\n        D[关键结果/Results<br>Classical bounds fail; stability can be restored under nonsmooth-aware condition.]"
        },
        {
          "title": "Scaling Adversarial Training via Data Selection",
          "authors": "Youran Ye, Dejin Wang, Ajinkya Bhandare",
          "institution": "Northeastern University",
          "link": "https://arxiv.org/pdf/2512.22069",
          "code": "https://github.com/youranye/Selective-Adversarial-Training",
          "tags": [
            "adversarial robustness",
            "adversarial training",
            "PGD",
            "sample selection",
            "gradient matching",
            "margin-based sampling"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad822d1db3ef050d2caa84f51828c7b48f93eee442f9a9f954f4f36b07929f44_w640_q70.webp",
          "contributions": "1. Proposes Selective Adversarial Training, which perturbs only a critical subset of samples per minibatch to reduce computational cost., 2. Introduces two novel sample selection criteria: margin-based sampling and gradient-matching sampling., 3. Demonstrates that the method achieves comparable or superior robustness to full PGD adversarial training while reducing adversarial computation by up to 50%.",
          "summary": "This paper addresses the high computational cost of Projected Gradient Descent (PGD) in adversarial training. It proposes Selective Adversarial Training, which uses principled criteria to select and perturb only critical samples in each batch. Experiments show the method maintains robustness while cutting adversarial computation by up to 50%.",
          "mindmap": "graph TB\n        A[Scaling Adversarial Training via Data Selection] --> B[核心问题/Problem: PGD计算成本高/High PGD Computational Cost]\n        A --> C[主要方法/Method: 选择性对抗训练/Selective Adversarial Training]\n        C --> D[选择标准1: 基于边界的采样/Margin-based Sampling]\n        C --> E[选择标准2: 梯度匹配采样/Gradient-matching Sampling]\n        A --> F[关键结果/Results: 鲁棒性相当，计算减少50%/Comparable Robustness, 50% Computation Reduction]"
        },
        {
          "title": "Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling",
          "authors": "Hannah Atmer, Yuan Yao, Thiemo Voigt, Stefanos Kaxiras",
          "institution": "Uppsala University",
          "link": "https://arxiv.org/pdf/2512.22066",
          "code": null,
          "tags": [
            "llm inference",
            "SRAM",
            "frequency scaling",
            "energy-delay product",
            "systolic array",
            "memory bandwidth"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52c2db009cb44a92a388e1684ea0d1bdb9ae27c0c4a4e683651eb7bd091bbe93_w640_q70.webp",
          "contributions": "1. Quantified the distinct energy and performance impacts of SRAM size and operating frequency on the compute-bound prefill and memory-bound decode phases of LLM inference. 2. Demonstrated a counter-intuitive result: high compute frequency can reduce total energy by shortening execution time and reducing static energy more than the dynamic power increase. 3. Identified an optimal hardware configuration (high frequency, small SRAM buffer) that minimizes the energy-delay product, providing concrete design insights for energy-efficient accelerators.",
          "summary": "This paper investigates how on-chip SRAM size and operating frequency affect the energy efficiency of LLM inference. Using a simulation methodology combining OpenRAM, LLMCompass, and ScaleSIM, it finds that a high-frequency, small-SRAM configuration optimizes the energy-delay product, as memory bandwidth caps decode phase improvements. The analysis provides architectural guidance for designing efficient LLM accelerators.",
          "mindmap": "graph TB\n        Root[”Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>LLM推理能耗高，Prefill与Decode阶段瓶颈不同”] --> Problem_Sub1[”SRAM大小与频率如何影响能效？”]\n        Problem --> Problem_Sub2[”内存带宽如何限制性能？”]\n        Method[”主要方法/Method<br>结合OpenRAM, LLMCompass, ScaleSIM的模拟方法”] --> Method_Sub1[”能耗建模/Energy Modeling”]\n        Method --> Method_Sub2[”延迟模拟/Latency Simulation”]\n        Method --> Method_Sub3[”操作强度分析/Operational Intensity”]\n        Results[”关键结果/Results”] --> Results_Sub1[”总能耗主要由SRAM大小决定<br>大缓存增加静态能耗”]\n        Results --> Results_Sub2[”高频可降低总能耗<br>（减少静态能耗）”]\n        Results --> Results_Sub3[”最优配置：高频(1200-1400MHz) + 小缓存(32-64KB)”]"
        },
        {
          "title": "Unifying Learning Dynamics and Generalization in Transformers Scaling Law",
          "authors": "Chiwun Yang",
          "institution": "Sun Yat-sen University",
          "link": "https://arxiv.org/pdf/2512.22088",
          "code": null,
          "tags": [
            "learning theory",
            "scaling law",
            "learning dynamics",
            "generalization error",
            "transformer",
            "stochastic gradient descent"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9b067b56202cd4607e684058e78ac331373bf12bf6848ca444276e2dcafe9f9_w640_q70.webp",
          "contributions": "1. Formalizes the learning dynamics of transformers as an ODE system and approximates it to kernel behaviors, moving beyond toy models to analyze SGD on multi-layer transformers with arbitrary data distributions. 2. Establishes a theoretical upper bound on excess risk with a distinct phase transition: exponential decay in the optimization phase and a power-law decay of Θ(C^\\{-1/6\\}) in the statistical phase. 3. Derives isolated scaling laws for model size, training time, and dataset size, explaining how each variable independently governs generalization bounds.",
          "summary": "This paper provides a theoretical foundation for the empirical scaling laws of large language models. It models transformer learning dynamics as an ODE system and analyzes SGD training on realistic data. The main result is a unified theory showing a phase transition in generalization error, from exponential to power-law decay, as computational resources scale.",
          "mindmap": "graph TB\n        A[Unifying Learning Dynamics and Generalization in Transformers Scaling Law] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Scaling Law理论原理不清 / Poorly understood theoretical underpinnings of scaling laws]\n        C --> C1[形式化学习动态为ODE系统 / Formalize learning dynamics as ODE system]\n        C --> C2[近似为核行为 / Approximate to kernel behaviors]\n        C --> C3[分析SGD训练真实Transformer / Analyze SGD training for real transformers]\n        D --> D1[泛化误差上界与相变 / Upper bound on excess risk with phase transition]\n        D --> D2[优化相:指数衰减 / Optimization phase: Exponential decay]\n        D --> D3[统计相:幂律衰减 Θ(C^{-1/6}) / Statistical phase: Power-law decay Θ(C^{-1/6})]\n        D --> D4[分离的规模定律 / Isolated scaling laws for model size, time, data]"
        },
        {
          "title": "A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting",
          "authors": "Shuyu Gan, Renxiang Wang, James Mooney, Dongyeop Kang",
          "institution": "University of Minnesota",
          "link": "https://arxiv.org/pdf/2512.22101",
          "code": null,
          "tags": [
            "agent system",
            "multi-agent pipeline",
            "automated data analysis",
            "insight generation",
            "report synthesis",
            "visual analytics"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92651ded84480402816f8db1df902e28dd62cce1b0958cece60e0f518bdd7e1c_w640_q70.webp",
          "contributions": "1. A Data Analyzer agent that orchestrates data profiling, generates diverse visualizations, filters low-quality charts, and automatically scores candidate insights for depth, correctness, and actionability. 2. A Presenter agent that sequences topics, composes chart-grounded narratives from top insights, writes transitions, and revises the document to produce a coherent, publication-ready report. 3. An end-to-end Analyzer-to-Presenter (A2P) pipeline that operationalizes co-analysis by coupling quality-assured analysis with narrative synthesis, improving the real-world usefulness of automated data analysis.",
          "summary": "This paper presents A2P-Vis, a two-part multi-agent pipeline designed to automate the generation of data visualization reports. The system uses a Data Analyzer to create and vet visual insights and a Presenter to assemble them into a coherent narrative. The authors claim this end-to-end approach improves the practical utility of automated data analysis for practitioners.",
          "mindmap": "graph TB\n        A[A2P-Vis] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[自动化数据科学流程的瓶颈/Gaps in automating data science]\n        B1 --> B2[生成有洞察力的可视化/Generating insightful visual evidence]\n        B1 --> B3[组装成专业报告/Assembling coherent professional report]\n        C --> C1[两部分多智能体管道/Two-part multi-agent pipeline]\n        C1 --> C2[数据分析器/Data Analyzer]\n        C2 --> C3[生成并评估图表与洞察/Generates & evaluates charts & insights]\n        C1 --> C4[报告呈现器/Presenter]\n        C4 --> C5[编排主题并撰写叙述/Orders topics & composes narrative]\n        D --> D1[端到端协同分析/End-to-end co-analysis]\n        D1 --> D2[提高自动化数据分析的实用性/Improves usefulness of automated analysis]"
        },
        {
          "title": "Explainable Multimodal Regression via Information Decomposition",
          "authors": "Zhaozhao Ma, Shujian Yu",
          "institution": "Zhejiang University, Georgia Institute of Technology, Vrije Universiteit Amsterdam, UiT - The Arctic University of Norway",
          "link": "https://arxiv.org/pdf/2512.22102",
          "code": "https://github.com/xxx/PIDReg",
          "tags": [
            "multimodal machine learning",
            "Partial Information Decomposition (PID)",
            "multimodal regression",
            "interpretability",
            "Gaussianity assumption",
            "conditional independence regularizer"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a07cab8b6bb765060f8a015e46e79e686a9acb69bac3aa8045cf96acec9f61e6_w640_q70.webp",
          "contributions": "1. A novel multimodal regression framework based on Partial Information Decomposition (PID) to quantify unique, redundant, and synergistic information from modalities. 2. Introduction of a Gaussianity assumption to resolve the underdetermined nature of PID, enabling analytical computation of its terms. 3. Derivation of a closed-form conditional independence regularizer to isolate unique information within each modality.",
          "summary": "This paper addresses the lack of interpretability in multimodal regression by proposing a new framework based on Partial Information Decomposition (PID). The method resolves PID's underdetermination by enforcing Gaussianity and uses a conditional independence regularizer to isolate unique modality information. Experiments on six datasets, including brain age prediction, show the framework outperforms state-of-the-art methods in both accuracy and interpretability, while aiding modality selection.",
          "mindmap": "graph TB\n        A[Explainable Multimodal Regression via Information Decomposition<br/>可解释多模态回归与信息分解] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法缺乏量化模态贡献与交互的工具<br/>Existing methods lack tools to quantify modality contributions & interactions]\n        C --> C1[基于PID分解模态信息<br/>Decompose modality info via PID]\n        C --> C2[引入高斯性假设与正则化<br/>Introduce Gaussianity & regularizer]\n        D --> D1[在6个数据集上超越SOTA<br/>Outperforms SOTA on 6 datasets]\n        D --> D2[提升预测精度与可解释性<br/>Improves predictive accuracy & interpretability]"
        },
        {
          "title": "Sensitivity Analysis of the Consistency Assumption",
          "authors": "Brian Knaeble, Qinyun Lin, Erich Kummerfeld, Kenneth A. Frank",
          "institution": "Utah Valley University, University of Gothenburg, University of Minnesota, Michigan State University",
          "link": "https://arxiv.org/pdf/2512.21379",
          "code": null,
          "tags": [
            "causal inference",
            "consistency assumption",
            "sensitivity analysis",
            "hidden versions of treatment",
            "partial identification",
            "stable unit treatment value assumption (SUTVA)"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0816c25e8631081977ffb91ecadbb3f527cfd99c7daf15f81c3f18a332125fcc_w640_q70.webp",
          "contributions": "1. A formal mathematical framework for analyzing violations of the consistency assumption. 2. A novel sensitivity parameter to quantify bias induced by hidden versions of treatment. 3. Methods for specifying bounds on this parameter to enable partial identification of causal effects.",
          "summary": "This paper addresses violations of the consistency assumption in causal inference, which posits no hidden versions of treatment. It proposes a new sensitivity analysis method focused on confounding by hidden treatment versions, introducing a formal framework and a novel sensitivity parameter. The work enables researchers to assess and bound the bias from consistency violations when interpreting causal estimates.",
          "mindmap": "graph TB\n        A[Sensitivity Analysis of the Consistency Assumption] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[一致性假设可能被违反/Consistency Assumption May Be Violated]\n        B1 --> B2[存在隐藏的治疗版本/Hidden Versions of Treatment Exist]\n        C --> C1[新颖的敏感性分析方法/Novel Sensitivity Analysis Method]\n        C1 --> C2[专注于隐藏版本导致的混杂/Focus on Confounding by Hidden Versions]\n        C2 --> C3[引入新的数学符号/Introduces New Mathematical Notation]\n        D --> D1[提出新的敏感性参数/Proposes New Sensitivity Parameter]\n        D1 --> D2[便于部分识别因果估计量/Facilitates Partial Identification of Causal Estimands]"
        },
        {
          "title": "Dynamic Attention (DynAttn): Interpretable High-Dimensional Spatio-Temporal Forecasting (with Application to Conflict Fatalities)",
          "authors": "Stefano M. Iacus, Haodong Qi, Marcello Carammia, Thomas Juneau",
          "institution": "Harvard University, Stockholm University, Malmö University, University of Catania, University of Toronto",
          "link": "https://arxiv.org/pdf/2512.21435",
          "code": null,
          "tags": [
            "spatio-temporal forecasting",
            "dynamic attention",
            "zero-inflated negative binomial",
            "elastic-net gating"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad96b82177be36f56ddfe0b4e4d6be51396aa49723cdabef2c85f538301f2e0e_w640_q70.webp",
          "contributions": "1. Introduces DynAttn, an interpretable dynamic-attention framework for high-dimensional spatio-temporal count processes. 2. Combines rolling-window estimation, shared elastic-net feature gating, a compact self-attention encoder, and a ZINB likelihood for calibrated forecasts. 3. Demonstrates superior predictive accuracy and enables structured interpretation of regional conflict dynamics, showing the roles of persistence, diffusion, and climate stress.",
          "summary": "The paper introduces DynAttn, an interpretable forecasting framework for sparse, high-dimensional spatio-temporal data like conflict fatalities. It combines dynamic attention, feature gating, and a specialized likelihood to produce accurate, multi-horizon forecasts and probabilities. The method outperforms benchmarks, particularly in sparse settings, and provides interpretable insights into conflict drivers such as persistence, spatial diffusion, and conditional climate effects.",
          "mindmap": "graph TB\n        A[DynAttn: Interpretable Spatio-Temporal Forecasting] --> B(核心问题/Problem: Forecasting sparse, bursty conflict fatalities)\n        A --> C(主要方法/Method: Dynamic attention, elastic-net gating, ZINB likelihood)\n        A --> D(关键结果/Results: Higher accuracy, interpretable regional dynamics)"
        },
        {
          "title": "Atomistic Simulation Guided Convolutional Neural Networks for Thermal Modeling of Friction Stir Welding",
          "authors": "Akshansh Mishra",
          "institution": "Politecnico di Milano, AI Fab Lab",
          "link": "https://arxiv.org/pdf/2512.21344",
          "code": null,
          "tags": [
            "physics-informed machine learning",
            "molecular dynamics",
            "convolutional neural network",
            "friction stir welding",
            "explainable AI",
            "LAMMPS"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34d6f71500319f52aecac1e95e1951a537fdc504134a8ba43851f31acc2e41c6_w640_q70.webp",
          "contributions": "1. Developed a novel method to transform atomistic simulation data (atomic positions and velocities) into physics-based 2D spatial grids for deep learning input. 2. Created and optimized a 2D CNN model to directly predict temperature evolution from spatially resolved atomistic data, achieving high accuracy (R²=0.94). 3. Used Class Activation Map analysis to provide explainability, showing the model's focus aligns with physical mechanisms (e.g., tool-material interface).",
          "summary": "This paper presents a method that combines molecular dynamics simulations with convolutional neural networks for thermal modeling in friction stir welding. The method transforms atomic-scale simulation data into spatial grids and uses a CNN to accurately predict temperature, with results validated against physical mechanisms. The approach demonstrates that deep learning can effectively learn from atomistic data to model complex thermomechanical processes.",
          "mindmap": "graph TB\n        Root[”Atomistic Simulation Guided CNNs for Thermal Modeling of FSW / 原子模拟引导的CNN用于搅拌摩擦焊热建模”]\n        Root --> Problem[”准确预测温度演化对于理解搅拌摩擦焊的热机械行为至关重要 / Accurate prediction of temperature evolution is essential for understanding thermomechanical behavior in FSW”]\n        Root --> Method[”使用LAMMPS进行分子动力学模拟，将原子数据转换为物理二维空间网格，并开发2D CNN进行预测 / Use LAMMPS for MD simulations, transform atomic data into physics-based 2D spatial grids, and develop a 2D CNN for prediction”]\n        Root --> Results[”模型预测精度高（R²=0.94），CAM分析表明模型关注与剧烈变形和生热相关的区域 / Model achieves high predictive accuracy (R²=0.94), CAM analysis shows model focuses on regions associated with intense deformation and heat generation”]"
        },
        {
          "title": "An approach to Fisher-Rao metric for infinite dimensional non-parametric information geometry",
          "authors": "Bing Cheng, Howell Tong",
          "institution": "Not explicitly stated in provided content. Affiliation likely inferred from author names or arXiv metadata, but not present in the given text.",
          "link": "https://arxiv.org/pdf/2512.21451",
          "code": null,
          "tags": [
            "information geometry",
            "Fisher-Rao metric",
            "non-parametric",
            "G-entropy",
            "Covariate Fisher Information Matrix (cFIM)",
            "intrinsic dimensionality"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a8f179358a2d0d118621b506d45d051ad949b1cc0acfe723d3c3268c4390e5a_w640_q70.webp",
          "contributions": "1. Introduces an orthogonal decomposition of the tangent space to derive a finite-dimensional, computable Covariate Fisher Information Matrix (cFIM) from the infinite-dimensional Fisher-Rao metric. 2. Establishes the geometric foundation of G-entropy via a Trace Theorem, linking it to total explainable statistical information. 3. Connects the cFIM to the Cramér-Rao Lower Bound and the Manifold Hypothesis, providing a method for estimating intrinsic dimensionality.",
          "summary": "This paper tackles the intractability of the infinite-dimensional Fisher-Rao metric in non-parametric information geometry by proposing an orthogonal decomposition of the tangent space. This decomposition yields a finite-dimensional Covariate Fisher Information Matrix (cFIM), which serves as a computable geometric object for analyzing statistical information and model efficiency. The framework rigorously grounds G-entropy, links to fundamental statistical bounds, and provides a testable condition for the Manifold Hypothesis, bridging abstract geometry with explainable AI.",
          "mindmap": "graph TB\n        A[An approach to Fisher-Rao metric for infinite dimensional non-parametric information geometry] --> B(核心问题/Problem: Intractability of infinite-dimensional Fisher-Rao metric)\n        A --> C(主要方法/Method: Orthogonal decomposition of tangent space to derive Covariate Fisher Information Matrix (cFIM))\n        A --> D(关键结果/Results: Trace Theorem for G-entropy, link to CRLB, testable Manifold Hypothesis via cFIM rank)"
        },
        {
          "title": "Deep learning-enhanced dual-mode multiplexed optical sensor for point-of-care diagnostics of cardiovascular diseases",
          "authors": "Gyeo-Re Han, Merve Eryilmaz, Artem Goncharov, Yuzhu Li, Shun Ye, Aoi Tomoeda, Emily Ngo, Margherita Scussat, Xiao Wang, Zixiang Ji, Max Zhang, Jeffrey J. Hsu, Omai B. Garner, Dino Di Carlo, Aydogan Ozcan",
          "institution": "University of California, Los Angeles (UCLA)",
          "link": "https://arxiv.org/pdf/2512.21389",
          "code": null,
          "tags": [
            "biomedical sensing and diagnostics",
            "vertical flow assay",
            "dual-mode detection",
            "neural network-based quantification",
            "multiplexed optical sensor",
            "point-of-care diagnostics"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6eb23e46dc346744879ce56ec8c667b82f8cab434c54e4a4922a2eb842bf77d_w640_q70.webp",
          "contributions": "1. Developed a dual-mode (colorimetric and chemiluminescent) multiplexed vertical flow assay (xVFA) for simultaneous detection of three cardiac biomarkers, achieving a wide dynamic range of ~6 orders of magnitude. 2. Integrated a deep learning-based quantification pipeline with a portable optical reader to automate analysis and enhance accuracy, achieving high correlation (Pearson's r > 0.96) with reference assays. 3. Created a compact, cost-effective, and rapid (23 min) point-of-care diagnostic system for cardiovascular diseases using only 50 µL of serum.",
          "summary": "This paper presents a deep learning-enhanced, dual-mode optical sensor for point-of-care cardiovascular diagnostics. The system uses a multiplexed vertical flow assay with colorimetric and chemiluminescent detection to simultaneously measure three key biomarkers with high sensitivity across a wide dynamic range, and a neural network pipeline for accurate quantification. The result is a rapid, portable, and automated diagnostic tool validated on patient samples.",
          "mindmap": "graph TB\n        A[Deep learning-enhanced dual-mode multiplexed optical sensor<br>深度学习增强的双模式多重光学传感器] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Current POC tests: long turnaround, narrow range, single-analyte<br>当前POC测试：耗时长、范围窄、单分析物]\n        C[主要方法/Method<br>Dual-mode (colorimetric+chemiluminescent) xVFA + Neural Network<br>双模式(比色+化学发光)xVFA + 神经网络]\n        D[关键结果/Results<br>Simultaneous 3-analyte quant in 23 min, wide dynamic range, r>0.96<br>23分钟同步3分析物定量，宽动态范围，r>0.96]"
        },
        {
          "title": "Quantum Nondecimated Wavelet Transform: Theory, Circuits, and Applications",
          "authors": "Brani Vidakovic",
          "institution": "Texas A&M University",
          "link": "https://arxiv.org/pdf/2512.21478",
          "code": null,
          "tags": [
            "quantum signal processing",
            "quantum nondecimated wavelet transform",
            "epsilon decimation",
            "Hadamard test",
            "quantum wavelet shrinkage",
            "shift invariance"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01489d687d6b4e38b42c82e3054e553beab66883e073aec7483d40c87388a47a_w640_q70.webp",
          "contributions": "1. Developed a quantum formulation of the Nondecimated Wavelet Transform (NDWT) based on epsilon-decimation, using a quantum register for shift indices and controlled circular shifts to realize all shifted transforms simultaneously. 2. Proposed an alternative quantum NDWT formulation using the Hadamard test and diagonal phase operators to directly access shift-invariant energy scalograms without full coefficient reconstruction. 3. Demonstrated that quantum redundancy and translation invariance can be exploited for applications like denoising and feature extraction, providing a foundation for multiscale quantum signal processing.",
          "summary": "This paper introduces two quantum formulations of the classical Nondecimated Wavelet Transform (NDWT) to achieve shift invariance and redundancy in quantum computation. The first uses controlled shifts and a wavelet analysis unitary, while the second employs the Hadamard test with phase operators for direct energy measurement. The work shows these quantum NDWTs enable coherent multiscale signal processing tasks like denoising and feature extraction.",
          "mindmap": "graph TB\n        A[Quantum Nondecimated Wavelet Transform<br/>量子非抽取小波变换] --> B(核心问题/Problem: How to embed classical NDWT's redundancy and shift invariance into quantum computation?<br/>如何将经典NDWT的冗余性和平移不变性嵌入量子计算？)\n        A --> C(主要方法/Method: Two complementary quantum formulations.<br/>两种互补的量子形式。)\n        C --> C1(Formulation 1: Epsilon-decimated, uses controlled circular shifts & wavelet unitary.<br/>方法一：基于ε抽取，使用受控循环移位和小波酉变换。)\n        C --> C2(Formulation 2: Hadamard test, uses diagonal phase operators for interference.<br/>方法二：基于Hadamard测试，使用对角相位算子进行干涉。)\n        A --> D(关键结果/Results: Quantum NDWTs enable coherent postprocessing (e.g., shrinkage) and direct access to scalograms/spectra for applications like denoising.<br/>量子NDWT支持相干后处理并可直接获取尺度图/频谱，用于去噪等应用。)"
        },
        {
          "title": "Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models",
          "authors": "Takuro Kutsuna",
          "institution": "Toyota Central R&D Labs., Inc.",
          "link": "https://arxiv.org/pdf/2512.21593",
          "code": null,
          "tags": [
            "diffusion models",
            "diffusion models",
            "generative modeling",
            "evidence lower bound",
            "residual learning",
            "two-stage framework"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp",
          "contributions": "1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps.",
          "summary": "The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps.",
          "mindmap": "graph TB\n        Root[”Residual Prior Diffusion (RPD) / 残差先验扩散模型”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”单一扩散模型难以同时捕捉全局结构和局部细节 / Single diffusion model struggles with global structure and local details”]\n        Method --> M1[”两阶段框架: 粗粒度先验 + 残差扩散模型 / Two-stage framework: coarse prior + residual diffusion model”]\n        Method --> M2[”概率模型与可处理ELBO / Probabilistic model with tractable ELBO”]\n        Results --> R1[”在合成数据上准确捕捉细节 / Accurately captures details on synthetic data”]\n        Results --> R2[”自然图像生成匹配或超越基线 / Natural image generation matches or exceeds baselines”]\n        Results --> R3[”少步推理保持性能 / Maintains performance with few inference steps”]"
        },
        {
          "title": "Incorporating rank-free coupling and external field via an amplitude-only modulated spatial photonic Ising machine",
          "authors": "Ze Zheng, Yuegang Li, Hang Xu, Jingzheng Huang, Tailong Xiao, Guihua Zeng",
          "institution": "Shanghai Jiao Tong University, Hefei National Laboratory, Shanghai Research Center for Quantum Sciences",
          "link": "https://arxiv.org/pdf/2512.21587",
          "code": null,
          "tags": [
            "photonic computing",
            "spatial photonic Ising machine",
            "Hadamard product",
            "amplitude-only modulation",
            "rank-free coupling",
            "incoherent light field"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fe8a02df2acd50923ce3480e4b3fb28b68540ae3331e12dd83853a5b046a5c_w640_q70.webp",
          "contributions": "1. Proposes an amplitude-only modulated, rank-free spatial photonic Ising machine (AR-SPIM) that eliminates the need for repeated/auxiliary operations by reformulating the Ising Hamiltonian as a sum of Hadamard products. 2. Demonstrates direct mapping of a 797-spin Ising model with external fields into an incoherent light field using aligned amplitude modulators, achieving high encoding accuracy (correlation >0.98). 3. Shows the system's capability for ground-state search with &lt;0.3% error, complex phase transition observation, and scalable spin counts for sparse problems by removing zero-valued terms.",
          "summary": "This paper introduces a new spatial photonic Ising machine (AR-SPIM) that uses an amplitude-only modulation scheme to encode arbitrary-rank spin-spin couplings and external fields directly into an incoherent light field. The method reformulates the Ising Hamiltonian as a sum of Hadamard products, enabling efficient and accurate computation. The demonstrated system achieves high-speed iterations, low error rates for optimization problems, and offers scalability for practical applications in optimization and quantum simulations.",
          "mindmap": "graph TB\n        Root[”Incorporating rank-free coupling and external field via an amplitude-only modulated spatial photonic Ising machine<br>振幅调制空间光子伊辛机”]\n        Root --> Problem[”核心问题/Problem<br>Existing SPIMs sacrifice efficiency or scale to encode high-rank coupling and external fields.<br>现有SPIM编码高秩耦合和外场时牺牲效率或规模。”]\n        Root --> Method[”主要方法/Method<br>Reformulate Hamiltonian as sum of Hadamard products; map to incoherent light via amplitude modulators.<br>将哈密顿量重写为哈达玛积之和；通过振幅调制器映射到非相干光场。”]\n        Root --> Results[”关键结果/Results<br>797 spins, >0.98 correlation, <0.3% error rate, enables phase transition observation.<br>797个自旋，>0.98相关性，<0.3%错误率，支持相变观测。”]"
        },
        {
          "title": "Tilt Matching for Scalable Sampling and Fine-Tuning",
          "authors": "Peter Potaptchik, Cheuk-Kit Lee, Michael S. Albergo",
          "institution": "Harvard University, University of Oxford, Kempner Institute, IAIFI",
          "link": "https://arxiv.org/pdf/2512.21829",
          "code": null,
          "tags": [
            "diffusion models",
            "Tilt Matching",
            "stochastic interpolants",
            "flow matching",
            "unnormalized densities",
            "fine-tuning"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2cc2b096f1ad1f69d17a5fbdbac71d5ccac470d3cc86a47d74fed9dba5202c88_w640_q70.webp",
          "contributions": "1. Proposes Tilt Matching, a simple, scalable algorithm for sampling and fine-tuning based on stochastic interpolants and an implicit stochastic optimal control solution., 2. Demonstrates that the method has strictly lower variance than standard flow matching and does not require gradients of the reward or backpropagation through trajectories., 3. Empirically shows state-of-the-art results on sampling under Lennard-Jones potentials and competitive performance on fine-tuning Stable Diffusion without reward multipliers.",
          "summary": "The paper introduces Tilt Matching, a new algorithm for sampling from complex distributions and fine-tuning generative models. It works by deriving a velocity field from stochastic interpolants to target a \"tilted\" distribution, offering lower variance than flow matching and avoiding gradient computations. The method is shown to be scalable and effective, achieving strong results on molecular sampling and image generation tasks.",
          "mindmap": "graph TB\n        Root[Tilt Matching for Scalable Sampling and Fine-Tuning] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Sampling from unnormalized densities and fine-tuning generative models] --> Problem_Detail[挑战/Challenges: Requires scalable, low-variance methods without reward gradients]\n        Method[主要方法/Method: Tilt Matching] --> Method_Detail1[基础/Basis: Dynamical equation relating flow matching velocity to tilted distribution]\n        Method_Detail1 --> Method_Detail2[特性/Properties: Implicitly solves stochastic optimal control, lower variance, no reward gradients needed]\n        Results[关键结果/Results: Empirical Verification] --> Results_Detail1[应用/Applications: State-of-the-art on Lennard-Jones potentials, competitive on Stable Diffusion fine-tuning]"
        },
        {
          "title": "Modeling high dimensional point clouds with the spherical cluster model",
          "authors": "Frédéric Cazals, Antoine Commaret, Louis Goldenberg",
          "institution": "Université Côte d'Azur, Inria, Ecole Polytechnique",
          "link": "https://arxiv.org/pdf/2512.21960",
          "code": null,
          "tags": [
            "clustering",
            "spherical cluster model",
            "high-dimensional median",
            "non-smooth optimization",
            "Clarke gradient",
            "stratified cell complex"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c9618adf1a45723b5b136e8c21eaa91dc645be7664e2fae293ae569f2adde20_w640_q70.webp",
          "contributions": "1. Showed that fitting a spherical cluster model is a strictly convex but non-smooth combinatorial optimization problem. 2. Presented an exact solver using the Clarke gradient on a stratified cell complex derived from an arrangement of hyperspheres. 3. Demonstrated through experiments that the exact solver is significantly faster than BFGS heuristics for many datasets and that the model's center behaves as a parameterized high-dimensional median.",
          "summary": "This paper introduces the Spherical Cluster (SC) model for approximating high-dimensional point clouds with a sphere. It proposes an exact solver for this non-smooth optimization problem, which is shown to be much faster than heuristic methods for many datasets, especially in high dimensions. The model's center is found to act as a robust, parameterized median.",
          "mindmap": "graph TB\n        A[Modeling high dimensional point clouds with the spherical cluster model] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[为高维点云建模/Modeling high-dimensional point clouds]\n        C --> C1[球形聚类模型/Spherical Cluster Model]\n        C --> C2[精确求解器使用Clarke梯度/Exact solver using Clarke gradient]\n        D --> D1[精确算法比BFGS快得多/Exact algorithm much faster than BFGS]\n        D --> D2[中心表现为参数化高维中位数/Center acts as parameterized high-dimensional median]"
        },
        {
          "title": "A Frobenius-Optimal Projection for Enforcing Linear Conservation in Learned Dynamical Models",
          "authors": "John M. Mango, Ronald Katende",
          "institution": "Makerere University, Kabale University",
          "link": "https://arxiv.org/pdf/2512.22084",
          "code": null,
          "tags": [
            "dynamical systems",
            "numerical linear algebra",
            "linear conservation laws",
            "Frobenius norm",
            "orthogonal projection",
            "matrix correction",
            "data-driven models"
          ],
          "day": "2025-12-29",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff37179b43efd7a7b64ff062f4e49c8aeaa1dc0b65febe24067b7ddd46dba12c_w640_q70.webp",
          "contributions": "1. Provides a closed-form, Frobenius-norm optimal projection to enforce linear conservation laws on any learned linear dynamical operator. 2. Proves that the correction is uniquely defined, low-rank, and fully determined by the constraint violation, reducing to a rank-one update for a single invariant. 3. Demonstrates that the method enforces exact conservation while minimally perturbing the learned dynamics, verified with a numerical example.",
          "summary": "This paper addresses the problem of learned linear dynamical models violating known linear conservation laws. It proposes a closed-form orthogonal projection that minimally perturbs a learned operator in the Frobenius norm to exactly satisfy the constraints. The method provides a general, optimal post-processing step for embedding invariants into data-driven linear models.",
          "mindmap": "graph TB\n        Root[”A Frobenius-Optimal Projection for Enforcing Linear Conservation in Learned Dynamical Models”] --> Problem[”核心问题/Problem: Learned linear models violate known linear conservation laws.”]\n        Root --> Method[”主要方法/Method: Apply orthogonal projection A* = Â - C(CᵀC)⁻¹CᵀÂ to enforce CᵀA=0.”]\n        Root --> Results[”关键结果/Results: Enforces exact conservation with minimal perturbation; correction is unique and low-rank.”]"
        },
        {
          "title": "Parameter-Efficient Neural CDEs via Implicit Function Jacobians",
          "authors": "Ilya Kuleshov, Alexey Zaytsev",
          "institution": "Applied AI Institute",
          "link": "https://arxiv.org/pdf/2512.20625",
          "code": null,
          "tags": [
            "time series analysis",
            "Neural Controlled Differential Equations",
            "parameter efficiency",
            "implicit function Jacobians",
            "continuous RNN"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f69d35dc890877df610e96a1c984c641596f7fcac2c4ff1dbf30f641c90d5d77_w640_q70.webp",
          "contributions": "1. Proposes a novel, parameter-efficient formulation of Neural Controlled Differential Equations (NCDEs) that drastically reduces the number of required parameters. 2. Introduces a logical interpretation of the method as a \"Continuous RNN,\" aligning with the original inspiration of NCDEs. 3. Presents a method leveraging implicit function Jacobians to achieve this efficiency.",
          "summary": "This paper addresses the high parameter cost of Neural Controlled Differential Equations (NCDEs) for temporal sequence analysis. It proposes a new, parameter-efficient formulation that reinterprets NCDEs as a \"Continuous RNN\" and uses implicit function Jacobians to reduce the parameter count. The main conclusion is that this approach maintains the modeling power of NCDEs while being significantly more parameter-efficient.",
          "mindmap": "graph LR\n    A[Parameter-Efficient Neural CDEs via Implicit Function Jacobians] --> B[核心问题/Problem: NCDEs require many parameters]\n    A --> C[主要方法/Method: Parameter-efficient formulation via implicit Jacobians, ”Continuous RNN” analogy]\n    A --> D[关键结果/Results: Achieves similar performance with far fewer parameters]"
        },
        {
          "title": "Uncovering Patterns of Brain Activity from EEG Data Consistently Associated with Cybersickness Using Neural Network Interpretability Maps",
          "authors": "Jacqueline Yau, Katherine J. Mimnaugh, Evan G. Center, Timo Ojala, Steven M. LaValle, Wenzhen Yuan, Nancy Amato, Minje Kim, Kara Federmeier",
          "institution": "University of Illinois Urbana-Champaign, University of Oulu",
          "link": "https://arxiv.org/pdf/2512.20620",
          "code": null,
          "tags": [
            "brain-computer interface",
            "EEG",
            "cybersickness",
            "interpretability maps",
            "convolutional neural networks",
            "event-related potentials"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5601623e3cdfb620242f065ddf726ad49d48b3d131d2e3cc1f6fa48032be9946_w640_q70.webp",
          "contributions": "1. Introduced a method using CNNs and transformers with interpretability maps (integrated gradients and class activation) to identify EEG features for cybersickness classification. 2. Identified a consistent and surprising pattern: amplitudes near the left prefrontal cortex electrode are important for cybersickness classification. 3. Proposed using the identified scalp location as a tagged feature for better real-time cybersickness classification with EEG.",
          "summary": "This paper addresses the challenge of detecting cybersickness from EEG data by using event-related potentials to isolate sickness-related brain activity from visual stimulus confounds. The authors employ trained convolutional neural networks and transformer models with interpretability maps to identify key EEG features. The main finding is that amplitudes recorded near the left prefrontal cortex are consistently important for classification, suggesting this location as a valuable feature for real-time detection.",
          "mindmap": "graph LR\n        A[Uncovering Patterns of Brain Activity from EEG Data Consistently Associated with Cybersickness Using Neural Network Interpretability Maps] --> B(核心问题/Problem: Cybersickness detection in VR using EEG is confounded by visual stimulus processing.)\n        A --> C(主要方法/Method: Use ERPs, CNNs/Transformers, and interpretability maps (integrated gradients/class activation) to analyze EEG data.)\n        A --> D(关键结果/Results: Left prefrontal cortex electrode amplitudes are consistently important for cybersickness classification.)"
        },
        {
          "title": "Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning",
          "authors": "Wenlong Tang",
          "institution": "Independent Researcher (No institutional affiliation inferred from provided content)",
          "link": "https://arxiv.org/pdf/2512.20629",
          "code": "https://github.com/wltang-dev/Latent-Strategy-RL-Agent",
          "tags": [
            "agent system",
            "multi-agent language systems",
            "latent strategy evolution",
            "reinforcement feedback",
            "external latent vectors",
            "dual-loop architecture"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c15e8361c02b5e6e0c755d3089af5adddafaad00ffda95b887b8eca526280761_w640_q70.webp",
          "contributions": "1. Proposes a novel multi-agent language framework that enables continual strategy evolution without fine-tuning the underlying language model's parameters. 2. Introduces a dual-loop architecture (behavior loop and language loop) that updates external latent vectors through environmental interaction and semantic reflection on generated text. 3. Demonstrates that this approach allows agents to develop stable, disentangled strategic styles and shows emergent adaptation capabilities, providing a low-cost, scalable, and interpretable form of abstract strategic representation.",
          "summary": "This paper addresses the limitation of static semantic representations in language models by proposing a framework where agents evolve strategies without model fine-tuning. The core method uses a dual-loop architecture to update external latent vectors through environmental rewards and reflection on generated text. The results show that this enables agents to develop adaptable and interpretable strategic behaviors, offering a scalable alternative to parameter tuning.",
          "mindmap": "graph LR\n    A[Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning] --> B[核心问题/Problem: Static semantic representations in LLMs cannot evolve with experience.]\n    A --> C[主要方法/Method: Dual-loop architecture (Behavior Loop & Language Loop) updates external latent vectors via reinforcement and reflection.]\n    A --> D[关键结果/Results: Agents develop stable, disentangled strategies; latent spaces show convergence and emergent adaptation.]"
        },
        {
          "title": "Zero-Training Temporal Drift Detection for Transformer Sentiment Models: A Comprehensive Analysis on Authentic Social Media Streams",
          "authors": "Aayam Bansal, Ishaan Gangwani",
          "institution": "IEEE",
          "link": "https://arxiv.org/pdf/2512.20631",
          "code": null,
          "tags": [
            "sentiment analysis",
            "temporal drift",
            "zero-training detection",
            "transformer models",
            "social media streams",
            "model instability"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8e40c0a23df847aa858c5e0ce28602f612024103d66ccaea9f9da99a1dded46_w640_q70.webp",
          "contributions": "1. Demonstrated significant temporal drift in transformer sentiment models during real-world events, with accuracy drops up to 23.4% on authentic social media data. 2. Introduced four novel zero-training drift detection metrics that outperform embedding-based baselines and are suitable for production deployment. 3. Provided comprehensive statistical validation on 12,279 authentic social media posts from major events, establishing practical significance exceeding industry monitoring thresholds.",
          "summary": "This paper addresses the problem of temporal drift in transformer-based sentiment models during real-world events without requiring model retraining. It proposes a zero-training detection framework using novel inference-time metrics, validated on authentic social media data. The main conclusion is that this method effectively detects significant model instability and enables immediate deployment for real-time monitoring systems.",
          "mindmap": "graph LR\n    A[Zero-Training Temporal Drift Detection for Transformer Sentiment Models] --> B[核心问题/Problem: Transformer模型在动态事件期间的行为不稳定/Transformer model instability during dynamic events]\n    A --> C[主要方法/Method: 零训练检测框架与四个新指标/Zero-training detection framework with four novel metrics]\n    A --> D[关键结果/Results: 在真实数据上验证，准确率下降达23.4%，检测能力强/Validated on authentic data, 23.4% accuracy drop, strong detection capability]"
        },
        {
          "title": "Enhancing Lung Cancer Treatment Outcome Prediction through Semantic Feature Engineering Using Large Language Models",
          "authors": "MunHwan Lee, Shaika Chowdhury, Xiaodi Li, Sivaraman Rajaganapathy, Eric W Klee, Ping Yang, Terence Sio, Liewei Wang, James Cerhan, Nansu NA Zong",
          "institution": "Mayo Clinic",
          "link": "https://arxiv.org/pdf/2512.20633",
          "code": null,
          "tags": [
            "clinical prediction",
            "large language models",
            "semantic feature engineering",
            "multi-modal data integration",
            "goal-oriented knowledge curator",
            "treatment outcome prediction"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c8925ebbf05c9b81fd60fa118034a660d2673702659d23d8b6cf7c1d976903_w640_q70.webp",
          "contributions": "1. Proposes a novel framework using Large Language Models (LLMs) as Goal-oriented Knowledge Curators (GKC) to generate task-aligned semantic features from raw clinical data, 2. Demonstrates that GKC, as an offline preprocessing step, outperforms expert-engineered features, direct embeddings, and end-to-end transformers in predicting lung cancer treatment outcomes, 3. Shows the complementary value of integrating laboratory, genomic, and medication modalities through ablation studies, highlighting semantic representation quality as key for accuracy in sparse data.",
          "summary": "The paper addresses the challenge of predicting lung cancer treatment outcomes from sparse, heterogeneous clinical data by introducing a framework that uses Large Language Models as Goal-oriented Knowledge Curators to engineer semantic, task-specific features. This method outperforms traditional baselines, achieving a mean AUROC of 0.803, and demonstrates that high-quality semantic representation is crucial for predictive accuracy in clinical settings.",
          "mindmap": "graph LR\n    A[Enhancing Lung Cancer Treatment Outcome Prediction<br>增强肺癌治疗结果预测] --> B(核心问题/Problem: Sparse, heterogeneous clinical data<br>稀疏、异构的临床数据)\n    A --> C(主要方法/Method: LLMs as Goal-oriented Knowledge Curators<br>LLMs作为目标导向知识策展器)\n    A --> D(关键结果/Results: Superior AUROC 0.803, outperforms baselines<br>优异的AUROC 0.803，超越基线)"
        },
        {
          "title": "Real Time Detection and Quantitative Analysis of Spurious Forgetting in Continual Learning",
          "authors": "Weiwei Wang",
          "institution": "Shenzhen Sunline Tech Co., Ltd.",
          "link": "https://arxiv.org/pdf/2512.20634",
          "code": null,
          "tags": [
            "llm training",
            "catastrophic forgetting",
            "spurious forgetting",
            "shallow alignment",
            "deep alignment",
            "task alignment depth"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2920732eeec32977638a62ffbcf4b4b075dbdd77ebc58fa777ff8a10e117219_w640_q70.webp",
          "contributions": "1. Introduced a quantitative framework (shallow vs. deep alignment) to measure task alignment depth across token positions. 2. Developed real-time detection methods and analysis tools for identifying shallow alignment and spurious forgetting during training. 3. Proposed adaptive mitigation strategies that automatically distinguish forgetting types and promote deep alignment to improve model robustness.",
          "summary": "This paper addresses catastrophic forgetting in continual learning for LLMs by identifying that performance drops are often due to \"spurious forgetting\" from shallow task alignment. The authors propose a framework to quantitatively measure alignment depth, detect shallow alignment in real-time, and apply mitigation strategies to promote deep alignment. Experiments show their method accurately identifies spurious forgetting and improves model robustness against forgetting by 3.3-7.1% over baselines.",
          "mindmap": "graph LR\n    A[Real-Time Detection and Quantitative Analysis of Spurious Forgetting<br/>虚假遗忘的实时检测与定量分析] --> B[核心问题/Problem: Catastrophic forgetting from shallow task alignment<br/>由浅层任务对齐导致的灾难性遗忘]\n    A --> C[主要方法/Method: Quantitative metrics & real-time detection for alignment depth<br/>对齐深度的量化指标与实时检测]\n    A --> D[关键结果/Results: High identification accuracy & improved robustness<br/>高识别准确率与提升的鲁棒性]"
        },
        {
          "title": "SHRP: Specialized Head Routing and Pruning for Efficient Encoder Compression",
          "authors": "Zeli Su, Ziyin Zhang, Wenzheng Zhang, Zhou Liu, Guixian Xu, Wentao Zhang",
          "institution": "Minzu University of China, Shanghai Jiao Tong University, Peking University",
          "link": "https://arxiv.org/pdf/2512.20635",
          "code": null,
          "tags": [
            "model compression (quantization/pruning)",
            "structured pruning",
            "attention head",
            "expert attention",
            "dynamic routing",
            "inference latency"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eee1b632660555a4b466586c0f3f7e064778c9087cde3160efa73cb5e0bf7723_w640_q70.webp",
          "contributions": "1. Proposes SHRP, a novel structured pruning framework that treats attention heads as independent experts and uses a unified Top-1 usage-driven mechanism for dynamic routing and deterministic pruning. 2. Introduces Expert Attention, a modular design with a lightweight shared expander feed-forward network to refine outputs after head selection. 3. Demonstrates significant compression on BERT-base, achieving high parameter/FLOP reduction with minimal accuracy loss, enabling practical deployment for latency-sensitive services.",
          "summary": "This paper addresses the high inference latency and memory consumption of Transformer encoders by proposing SHRP, a structured pruning framework that identifies and removes redundant attention heads. The method uses an Expert Attention module and a unified routing mechanism to compress the model while preserving accuracy. Experiments show SHRP can reduce BERT-base's parameters by 48% with 93% accuracy retained, and achieve a 4.2x throughput gain under extreme compression.",
          "mindmap": "graph LR\n    A[SHRP: Specialized Head Routing and Pruning] --> B(核心问题/Problem: Transformer编码器推理延迟高、内存消耗大/High inference latency & memory consumption of Transformer encoders)\n    A --> C(主要方法/Method: 专家注意力与动态路由/Expert Attention & dynamic routing for structured pruning)\n    A --> D(关键结果/Results: 高压缩比与精度保持/High compression ratio & accuracy preservation)"
        },
        {
          "title": "Data-Free Pruning of Self-Attention Layers in LLMs",
          "authors": "Dhananjay Saikumar, Blesson Varghese",
          "institution": "University of St Andrews",
          "link": "https://arxiv.org/pdf/2512.20636",
          "code": null,
          "tags": [
            "model compression (quantization/pruning)",
            "attention pruning",
            "data-free pruning",
            "Gate-Norm",
            "inference acceleration",
            "attention suppression hypothesis"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9338cbf768451f7709aa625ae03202bc7b84fcaa758ea1d75a6f5eaa4aa228c_w640_q70.webp",
          "contributions": "1. Proposes the Attention Suppression Hypothesis to explain the redundancy of deep self-attention layers in LLMs. 2. Introduces Gate-Norm, a one-shot, weight-only criterion for ranking and pruning attention sublayers without requiring data, forward passes, or fine-tuning. 3. Demonstrates that pruning 8-16 attention layers with Gate-Norm yields up to 1.30x higher inference throughput while maintaining accuracy within 2% of the baseline, matching data-driven methods but being ~1000x faster.",
          "summary": "The paper addresses the high inference cost of LLMs by proposing a data-free method to prune redundant self-attention layers. It introduces Gate-Norm, a fast weight-only criterion based on query-key coupling, which removes layers without needing calibration data or fine-tuning. The method significantly speeds up inference while preserving model accuracy, enabling practical LLM compression.",
          "mindmap": "graph LR\n    A[Data-Free Pruning of Self-Attention Layers in LLMs] --> B[核心问题/Problem: LLM推理成本高，注意力层是瓶颈/High LLM inference cost, attention layers are bottleneck]\n    A --> C[主要方法/Method: 提出Gate-Norm，基于权重无数据剪枝/Propose Gate-Norm, weight-only data-free pruning]\n    A --> D[关键结果/Results: 推理速度提升1.30倍，精度损失<2%，速度快1000倍/1.30x faster inference, <2% accuracy drop, 1000x faster scoring]"
        },
        {
          "title": "Uncovering Competency Gaps in Large Language Models and Their Benchmarks",
          "authors": "Matyas Bohacek, Nino Scherrer, Nicholas Dufour, Thomas Leung, Christoph Bregler, Stephanie C. Y. Chan",
          "institution": "Stanford University, Google DeepMind",
          "link": "https://arxiv.org/pdf/2512.20638",
          "code": "competency-gaps.github.io",
          "tags": [
            "llm evaluation",
            "sparse autoencoders",
            "benchmark gaps",
            "model gaps",
            "concept activations",
            "competency gaps"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6d5ab9e8ede467e65cbc2079e58ecf9b8d8ade8145f7e9e90f1b6f8382e288b_w640_q70.webp",
          "contributions": "1. Proposes a novel method using sparse autoencoders (SAEs) to automatically uncover fine-grained competency gaps in LLMs and benchmarks. 2. Introduces a representation-grounded evaluation approach that computes saliency-weighted performance scores based on model-internal concept activations. 3. Demonstrates the method's ability to identify specific model weaknesses (e.g., non-sycophantic behaviors) and benchmark coverage imbalances (e.g., over-representation of obedience concepts) without manual supervision.",
          "summary": "This paper addresses the problem that aggregated benchmark scores can hide specific weaknesses in LLMs and imbalances in benchmark coverage. The authors propose an automated method using sparse autoencoders to decompose benchmark performance into fine-grained concepts based on the model's internal representations. Their analysis of two models and ten benchmarks revealed model gaps in areas like non-sycophancy and safety, and benchmark gaps such as an over-representation of obedience-related concepts.",
          "mindmap": "graph LR\n        A[Uncovering Competency Gaps<br/>揭示能力差距] --> B[Problem: Aggregated metrics obscure model/benchmark gaps<br/>问题：聚合指标掩盖模型/基准差距]\n        A --> C[Method: Use Sparse Autoencoders (SAEs) for concept-level decomposition<br/>方法：使用稀疏自编码器进行概念级分解]\n        A --> D[Results: Found gaps in non-sycophancy, safety; benchmark over-represents obedience<br/>结果：发现非谄媚、安全方面的差距；基准过度代表服从性]"
        },
        {
          "title": "Forecasting N-Body Dynamics: A Comparative Study of Neural Ordinary Differential Equations and Universal Differential Equations",
          "authors": "Suriya R S, Prathamesh Dinesh Joshi, Rajat Dandekar, Raj Dandekar, Sreedath Panat",
          "institution": "Vizuara AI Labs",
          "link": "https://arxiv.org/pdf/2512.20643",
          "code": null,
          "tags": [
            "scientific machine learning",
            "Neural Ordinary Differential Equations",
            "Universal Differential Equations",
            "forecasting breakdown point",
            "n-body problem",
            "Julia"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06b852f384c602c478fd1ea2166cf9ac3e8442f63a46b1d479333a1e00699c6b_w640_q70.webp",
          "contributions": "1. Conducted a comparative study of Neural ODEs and Universal Differential Equations (UDEs) for forecasting n-body dynamics, a fundamental astrophysics problem. 2. Introduced and determined the \"forecasting breakdown point\" to quantify the minimal training data required for accurate future predictions. 3. Demonstrated that the UDE model, which blends known physics with neural networks, is significantly more data-efficient, requiring only 20% of data for a correct forecast compared to 90% for a Neural ODE.",
          "summary": "This paper compares two Scientific Machine Learning frameworks, Neural ODEs and Universal Differential Equations (UDEs), for forecasting the dynamics of the n-body problem. The study introduces the concept of a \"forecasting breakdown point\" to measure data efficiency and finds that the UDE model, which incorporates known physical laws, is far more efficient, requiring only 20% of the training data that a Neural ODE needs for accurate predictions.",
          "mindmap": "graph LR\n    A[Forecasting N-Body Dynamics<br/>N体动力学预测] --> B[核心问题/Problem<br/>传统黑盒模型忽略物理定律<br/>Traditional black-box models ignore physics]\n    A --> C[主要方法/Method<br/>使用科学机器学习框架<br/>Use Scientific ML frameworks (NODEs, UDEs)]\n    A --> D[关键结果/Results<br/>UDE数据效率更高<br/>UDE is more data-efficient]"
        },
        {
          "title": "MaskOpt: A Large-Scale Mask Optimization Dataset to Advance AI in Integrated Circuit Manufacturing",
          "authors": "Yuting Hu, Lei Zhuang, Hua Xiang, Jinjun Xiong, Gi-Joon Nam",
          "institution": "University at Buffalo, IBM Research",
          "link": "https://arxiv.org/pdf/2512.20655",
          "code": null,
          "tags": [
            "others",
            "mask optimization",
            "optical proximity correction",
            "inverse lithography technique",
            "deep learning",
            "benchmark dataset"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce9bc1ac552258257c450a9bc4d4c4ae0264c958efae291f56fc44af367bf07a_w640_q70.webp",
          "contributions": "1. Introduces MaskOpt, a large-scale benchmark dataset for AI-driven mask optimization constructed from real 45nm IC designs, addressing limitations of synthetic data. 2. The dataset preserves standard-cell hierarchy and includes varying context window sizes to enable cell- and context-aware model training. 3. Provides comprehensive benchmarks by evaluating state-of-the-art deep learning models, revealing performance trade-offs and validating the importance of contextual and cell-aware inputs.",
          "summary": "The paper presents MaskOpt, a large-scale dataset built from real integrated circuit designs to advance deep learning for mask optimization in semiconductor manufacturing. It addresses the limitations of existing synthetic datasets by including cell hierarchy and surrounding context. Benchmarking results demonstrate the dataset's utility and highlight the critical role of context and cell information for accurate mask generation.",
          "mindmap": "graph LR\n    A[MaskOpt Dataset<br/>MaskOpt数据集] --> B[核心问题/Problem<br/>Existing datasets are synthetic, lack cell hierarchy & context<br/>现有数据集为合成数据，缺乏单元层次和上下文];\n    A --> C[主要方法/Method<br/>Build large-scale dataset from real 45nm IC designs with cell-aware tiles & context windows<br/>基于真实45nm设计构建大规模数据集，包含单元感知切片和上下文窗口];\n    A --> D[关键结果/Results<br/>Benchmarks show model trade-offs, context & cell info are crucial<br/>基准测试显示模型权衡，上下文和单元信息至关重要];"
        },
        {
          "title": "Q-RUN: Quantum-Inspired Data Re-uploading Networks",
          "authors": "Wenbo Qiao, Shuaixian Wang, Peng Zhang, Yan Ming, Jiaming Zhao",
          "institution": "Tianjin University",
          "link": "https://arxiv.org/pdf/2512.20654",
          "code": null,
          "tags": [
            "quantum machine learning",
            "data re-uploading",
            "Fourier-expressive",
            "quantum-inspired",
            "neural network layer",
            "parameter efficiency"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0762ab538e009e0b634d9be449e9107606ff0182095b377b357ae3a8796d291b_w640_q70.webp",
          "contributions": "1. Proposes Q-RUN, a novel quantum-inspired neural network layer that translates the mathematical paradigm of Data Re-uploading Quantum Circuits (DRQC) into a classical model, retaining their Fourier-expressive power without requiring quantum hardware. 2. Demonstrates that Q-RUN significantly outperforms standard fully connected layers and other state-of-the-art layers, reducing error by 1-3 orders of magnitude on certain tasks while using fewer parameters. 3. Shows that Q-RUN can serve as a versatile, drop-in replacement for fully connected layers, improving performance across a wide range of neural architectures and illustrating how quantum ML principles can enhance classical AI design.",
          "summary": "This paper introduces Q-RUN, a quantum-inspired neural network layer based on data re-uploading principles, designed to capture high-frequency functions efficiently without quantum hardware. It demonstrates superior performance and parameter efficiency compared to standard layers across various modeling tasks. The work shows how quantum machine learning concepts can guide the development of more expressive classical AI models.",
          "mindmap": "graph LR\n    A[Q-RUN: Quantum-Inspired Data Re-uploading Networks] --> B[核心问题/Problem: DRQC量子模型受限于硬件可扩展性<br>DRQC quantum models are limited by hardware scalability]\n    A --> C[主要方法/Method: 提出经典量子启发式数据重上传网络<br>Propose classical quantum-inspired data re-uploading network (Q-RUN)]\n    A --> D[关键结果/Results: 性能更优，参数更少，可作为全连接层替代<br>Superior performance, fewer parameters, drop-in replacement for FC layers]"
        },
        {
          "title": "Managing the Stochastic: Foundations of Learning in Neuro-Symbolic Systems for Software Engineering",
          "authors": "Matthew Thompson",
          "institution": "Independent Researcher",
          "link": "https://arxiv.org/pdf/2512.20660",
          "code": null,
          "tags": [
            "agent system",
            "dual-state architecture",
            "atomic action pairs",
            "guard functions",
            "neuro-symbolic systems",
            "code generation"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a73fceac46d6997904de43696e8db407d645c6e4388012a9e24a3b9565e06fb_w640_q70.webp",
          "contributions": "1. Proposes a control boundary that treats the LLM as a stochastic environment component, not the decision-making agent, to manage its unpredictability. 2. Formalizes a Dual-State Architecture separating deterministic workflow state from stochastic environment state. 3. Introduces Atomic Action Pairs and Guard Functions to couple generation with verification as indivisible transactions, projecting probabilistic outputs onto observable workflow state.",
          "summary": "This paper addresses the problem of stochastic failures in AI coding agents by proposing a neuro-symbolic architectural framework that treats the LLM as part of the environment. The method uses a Dual-State Architecture with Atomic Action Pairs and Guard Functions to separate deterministic control from stochastic generation. The main conclusion is that such architectural constraints can significantly improve task success rates for qualified models, potentially substituting for parameter scale in achieving reliable code generation.",
          "mindmap": "graph LR\n    A[Managing the Stochastic<br>管理随机性] --> B[Problem: LLM-based agents prone to stochastic failures<br>问题: 基于LLM的智能体易受随机性故障影响]\n    A --> C[Method: Dual-State Architecture, Atomic Action Pairs, Guard Functions<br>方法: 双态架构, 原子动作对, 守卫函数]\n    A --> D[Results: Improved success rates, architectural constraints can substitute for scale<br>结果: 成功率提升, 架构约束可替代模型规模]"
        },
        {
          "title": "Graph Neural Networks for Source Detection: A Review and Benchmark Study",
          "authors": "Martin Sterchi, Nathan Brack, Lorenz Hilfiker",
          "institution": "University of Applied Sciences and Arts Northwestern Switzerland FHNW, University of Zürich",
          "link": "https://arxiv.org/pdf/2512.20657",
          "code": null,
          "tags": [
            "graph neural networks",
            "Graph Neural Networks",
            "Epidemic Source Detection",
            "Rumor Centrality",
            "Benchmark Study",
            "Single-Source Problems"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3b943e72216650945e413f1e6dcd8c0978e12972df42eff52dc2deb20fcbe59_w640_q70.webp",
          "contributions": "1. A comprehensive review of existing GNN-based methods for source detection, clarifying their settings and models. 2. Proposal of a principled GNN architecture specifically tailored for the source detection task. 3. A systematic benchmark study demonstrating that GNNs substantially outperform traditional source detection methods across various network types, and advocating for epidemic source detection as a benchmark task for evaluating GNN architectures.",
          "summary": "This paper reviews Graph Neural Network (GNN) approaches for detecting the source of an epidemic in a network, proposes a new GNN architecture for the task, and conducts a benchmark study. The experiments show that GNNs significantly outperform traditional methods like rumor centrality, establishing them as highly effective for source detection and suggesting the task as a standard benchmark for GNN evaluation.",
          "mindmap": "graph LR\n        A[Graph Neural Networks for Source Detection: A Review and Benchmark Study] --> B[核心问题/Problem: 识别网络中流行病传播的源头/Identify the source of an epidemic in a network]\n        A --> C[主要方法/Method: 回顾GNN方法并提出新的GNN架构/Review GNN methods and propose a new GNN architecture]\n        A --> D[关键结果/Results: GNN显著优于传统方法，提议作为基准任务/GNNs substantially outperform traditional methods, propose as a benchmark task]"
        },
        {
          "title": "Dominating vs. Dominated: Generative Collapse in Diffusion Models",
          "authors": "Hayeon Jeong, Jong-Seok Lee",
          "institution": "Yonsei University",
          "link": "https://arxiv.org/pdf/2512.20666",
          "code": null,
          "tags": [
            "text-to-image generation",
            "diffusion models",
            "cross-attention",
            "generative collapse",
            "multi-concept generation",
            "attention dynamics"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e3797020eb871e661d9cda59fdbe8a7bdf314a2935eff1ccf97c35a90d39ee_w640_q70.webp",
          "contributions": "1. Identifies and defines the Dominant-vs-Dominated (DvD) phenomenon in multi-concept text-to-image generation, 2. Introduces DominanceBench for systematic analysis of the DvD imbalance, 3. Provides causal analysis from data (limited instance diversity) and architecture (cross-attention saturation & distributed head mechanisms) perspectives.",
          "summary": "This paper investigates the \"Dominant-vs-Dominated\" (DvD) imbalance in diffusion models, where one concept token suppresses others in multi-concept prompts. The authors analyze this using a new benchmark and find causes in limited training data diversity and cross-attention dynamics. Their findings offer insights into generative collapse for more reliable text-to-image generation.",
          "mindmap": "graph LR\n        A[Dominating vs. Dominated<br/>支配 vs. 被支配] --> B[核心问题/Problem<br/>Multi-concept prompt generation imbalance<br/>多概念提示生成失衡];\n        A --> C[主要方法/Method<br/>Introduce DominanceBench & analyze causes<br/>引入DominanceBench并分析原因];\n        A --> D[关键结果/Results<br/>Data diversity & attention dynamics cause DvD<br/>数据多样性和注意力动态导致DvD];"
        },
        {
          "title": "Forward Only Learning for Orthogonal Neural Networks of any Depth",
          "authors": "Paul Caillon, Alex Colagrande, Erwan Fagnou, Blaise Delattre, Alexandre Allauzen",
          "institution": "Université Paris-Dauphine - PSL, ESPCI PSL",
          "link": "https://arxiv.org/pdf/2512.20668",
          "code": "https://github.com/",
          "tags": [
            "neural network training algorithms",
            "forward-only learning",
            "orthogonal neural networks",
            "backpropagation alternative",
            "FOTON",
            "PEPITA"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14ce0cf521f1d147eae7293cef8a5a53d6a0ca2fda6723bc707498635d351a0b_w640_q70.webp",
          "contributions": "1. Theoretical analysis of limitations in existing forward-only frameworks like PEPITA, 2. Design of a forward-only algorithm equivalent to backpropagation under linear/orthogonal assumptions, 3. Introduction of FOTON, a practical forward-only training method for orthogonal networks that scales to any depth and works on CNNs>",
          "summary": "This paper addresses the computational burden of backpropagation by proposing a forward-only training algorithm called FOTON for orthogonal neural networks. The method replaces the backward pass with a modulated forward pass, enabling training of deep networks without backpropagation. Experiments show FOTON outperforms prior forward-only methods and scales to networks of any depth, including convolutional architectures.",
          "mindmap": "graph LR\n    A[Forward Only Learning for Orthogonal Neural Networks<br>前向传播学习用于正交神经网络] --> B[Problem: Backpropagation is computationally expensive<br>问题: 反向传播计算成本高]\n    A --> C[Method: FOTON - Forward-Only Training with modulated forward pass<br>方法: FOTON - 使用调制前向传播的前向训练]\n    A --> D[Results: Trains networks of any depth, outperforms PEPITA<br>结果: 可训练任意深度网络，性能优于PEPITA]"
        },
        {
          "title": "Improving Cardiac Risk Prediction Using Data Generation Techniques",
          "authors": "Alexandre Cabodevila, Pedro Gamallo-Fernandez, Juan C. Vidal, Manuel Lama",
          "institution": "Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS), Universidade de Santiago de Compostela",
          "link": "https://arxiv.org/pdf/2512.20669",
          "code": null,
          "tags": [
            "generative models",
            "Conditional Variational Autoencoder",
            "synthetic data generation",
            "cardiac risk prediction",
            "data augmentation",
            "clinical records"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0661f3f558f42310471788ea2bad662661692287e3137248998355eb91c8470b_w640_q70.webp",
          "contributions": "1. Proposes a novel architecture based on a Conditional Variational Autoencoder (CVAE) for generating realistic and coherent synthetic clinical records. 2. Addresses key limitations in medical data analysis such as data scarcity, unsuitability, and high prevalence of missing values. 3. Demonstrates that using the generated synthetic data improves the accuracy of cardiac risk prediction classifiers, outperforming other deep learning data generation approaches.",
          "summary": "This paper addresses the challenges of scarce and incomplete real-world medical data for cardiac risk prediction by proposing a Conditional Variational Autoencoder (CVAE) architecture to generate realistic synthetic clinical records. The generated data is used to augment datasets, which in turn enhances the performance of cardiac risk prediction models. The results show that the proposed method successfully generates coherent data and improves classifier accuracy compared to state-of-the-art alternatives.",
          "mindmap": "graph LR\n    A[Improving Cardiac Risk Prediction Using Data Generation Techniques] --> B(核心问题/Problem: 真实医疗数据稀缺、不完整且存在缺失值/Real-world medical data is scarce, incomplete, and has missing values)\n    A --> C(主要方法/Method: 基于条件变分自编码器的架构生成合成临床记录/CVAE-based architecture for synthetic clinical record generation)\n    A --> D(关键结果/Results: 生成的数据提高了心脏风险预测分类器的准确性/Generated data improves cardiac risk prediction classifier accuracy)"
        },
        {
          "title": "Disentangling Fact from Sentiment: A Dynamic Conflict-Consensus Framework for Multimodal Fake News Detection",
          "authors": "Weilin Zhou, Zonghao Ying, Junjie Mu, Shengwei Tian, Quanchen Zou, Deyue Zhang, Dongdong Yang, Xiangzheng Zhang",
          "institution": "Xinjiang University, 360 AI Security Lab, Beihang University, Politecnico di Milano",
          "link": "https://arxiv.org/pdf/2512.20670",
          "code": null,
          "tags": [
            "multimodal fake news detection",
            "inconsistency detection",
            "feature disentanglement",
            "conflict-consensus mechanism",
            "physics-inspired dynamics",
            "cross-modal discrepancy"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c06ff160d4943f1ae5c4648f6e6cd042a0c1232af6212adf0021ba7f0b7c2ab_w640_q70.webp",
          "contributions": "1. Proposes a paradigm shift from consistency-seeking to inconsistency-seeking for multimodal fake news detection, explicitly amplifying cross-modal contradictions as evidence. 2. Introduces a novel framework (DCCF) that disentangles inputs into independent Fact and Sentiment spaces to separate objective mismatches from emotional dissonance. 3. Employs physics-inspired feature dynamics and a conflict-consensus mechanism to actively polarize and standardize local discrepancies against a global context for robust judgment.",
          "summary": "The paper identifies a flaw in mainstream multimodal fake news detection, which treats cross-modal discrepancies as noise, and proposes a new Dynamic Conflict-Consensus Framework (DCCF) designed to actively seek and amplify these inconsistencies as evidence of fabrication. The method disentangles fact from sentiment and uses physics-inspired dynamics to extract conflicts. Experiments show DCCF outperforms state-of-the-art baselines with an average accuracy improvement of 3.52%.",
          "mindmap": "graph LR\n    A[Disentangling Fact from Sentiment: A Dynamic Conflict-Consensus Framework for Multimodal Fake News Detection] --> B[核心问题/Problem: 主流一致性融合将关键跨模态差异误判为噪声，稀释了伪造证据]\n    A --> C[主要方法/Method: 提出DCCF框架，解耦事实与情感，利用物理启发的动力学主动放大矛盾]\n    A --> D[关键结果/Results: 在三个真实数据集上超越SOTA，平均准确率提升3.52%]"
        },
        {
          "title": "Revisiting the Learning Objectives of Vision-Language Reward Models",
          "authors": "Simon Roy, Samuel Barbeau, Giovanni Beltrame, Christian Desrosiers, Nicolas Thome",
          "institution": "Polytechnique Montréal, École de Technologie Supérieure, Sorbonne Université",
          "link": "https://arxiv.org/pdf/2512.20675",
          "code": null,
          "tags": [
            "reinforcement learning",
            "reward modeling",
            "vision-language models",
            "triplet loss",
            "Meta-World",
            "contrastive learning"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca907344b4dbef770bf1367dee07e4ba6b6f7a2b525ad28c7bf8d0ff11f62075_w640_q70.webp",
          "contributions": "1. Proposes a unified framework to isolate and evaluate the impact of learning objectives in vision-language reward models, controlling for backbone, data, and evaluation environments. 2. Demonstrates that a simple triplet loss objective can outperform more complex state-of-the-art methods for reward modeling. 3. Suggests that improvements in recent approaches may be attributed more to differences in training data and model architectures rather than the complexity of their learning objectives.",
          "summary": "This paper investigates the impact of different learning objectives for adapting vision-language models into reward functions for embodied intelligence. By comparing methods under a unified framework, the authors find that a simple triplet loss outperforms more complex state-of-the-art objectives. The results suggest that recent improvements in reward modeling may stem from data and architecture differences rather than objective complexity.",
          "mindmap": "graph LR\n        A[Revisiting VLM Reward Models] --> B(核心问题/Problem: 难以比较不同奖励模型目标/Difficulty in comparing reward model objectives)\n        A --> C(主要方法/Method: 统一框架评估/Unified framework evaluation)\n        A --> D(关键结果/Results: 三元组损失更优/Triplet loss outperforms SOTA)"
        },
        {
          "title": "HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model",
          "authors": "Yuanhao Xi, Xiaohuan Bing, Ramin Yahyapour",
          "institution": "Liaoning Technical University, University of Göttingen, Gesellschaft für Wissenschaftliche Datenverarbeitung mbH Göttingen",
          "link": "https://arxiv.org/pdf/2512.20674",
          "code": null,
          "tags": [
            "multi-modal training",
            "Low-Rank Adaptation (LoRA)",
            "parameter-efficient fine-tuning",
            "rank adaptation",
            "mobile vision language model",
            "dynamic scheduling"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9589d36616e78e4826e61d2dbafa4ccc718075f3659352d4d01c1b6f4795a02a_w640_q70.webp",
          "contributions": "1. Proposes HyDRA, a parameter-efficient fine-tuning framework for mobile VLMs that implements hierarchical and dynamic rank scheduling. 2. Introduces hierarchical optimization with coarse-grained (layer-level) and fine-grained (intra-layer) rank assignment. 3. Employs dynamic adjustment via an end-to-end automatic optimization using a lightweight performance model to determine ranks during fine-tuning.",
          "summary": "This paper introduces HyDRA, a parameter-efficient fine-tuning framework for mobile Vision Language Models (VLMs) that addresses the computational inefficiency of standard LoRA by implementing hierarchical and dynamic rank scheduling. The method uses a two-pronged optimization strategy and a lightweight performance model to adjust ranks automatically. Experiments show HyDRA outperforms baselines, achieving a 4.7% average improvement without extra parameters and sometimes surpassing full fine-tuning.",
          "mindmap": "graph LR\n    A[HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model] --> B[核心问题/Problem: Standard LoRA with fixed rank is insufficient for training mobile VLMs]\n    A --> C[主要方法/Method: HyDRA framework with hierarchical & dynamic rank scheduling]\n    A --> D[关键结果/Results: Outperforms baseline by 4.7%, no extra parameters, sometimes beats full fine-tuning]"
        },
        {
          "title": "Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems",
          "authors": "Stefano Grassi",
          "institution": "None (No affiliation or email domain provided in the given content)",
          "link": "https://arxiv.org/pdf/2512.20688",
          "code": null,
          "tags": [
            "multi-agent systems",
            "Differentiable Price Mechanism",
            "Dominant Strategy Incentive Compatibility",
            "VCG-equivalent incentive",
            "Dec-POMDPs",
            "Bayesian Incentive Compatibility"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfdd6bb51cf65ab558a1d13cbaf0fbdda25f9c06c58be3e201a62b231f808da4_w640_q70.webp",
          "contributions": "1. Proposes Mechanism-Based Intelligence (MBI), a new paradigm framing intelligence as emergent from the coordination of multiple agents. 2. Introduces the Differentiable Price Mechanism (DPM), which computes exact loss gradients as incentive signals to guarantee Dominant Strategy Incentive Compatibility and convergence. 3. Demonstrates a framework that scales linearly with the number of agents, bypassing Dec-POMDP complexity and showing significant empirical speedup over model-free RL.",
          "summary": "The paper addresses the fragility of multi-agent systems in coordinating private information and aligning incentives. It proposes Mechanism-Based Intelligence (MBI) and its core Differentiable Price Mechanism (DPM), which uses differentiable incentives to align agent actions with global objectives. The method guarantees incentive compatibility, scales efficiently, and is shown to be much faster than standard reinforcement learning approaches.",
          "mindmap": "graph LR\n    A[Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems] --> B[核心问题/Problem: Hayekian Information Problem & Hurwiczian Incentive Problem]\n    A --> C[主要方法/Method: Differentiable Price Mechanism (DPM) & Bayesian Extension]\n    A --> D[关键结果/Results: DSIC/BIC Guarantee, Linear Scaling, 50x Faster than Model-Free RL]"
        },
        {
          "title": "PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation",
          "authors": "Yuma Ichikawa, Naoya Takagi, Takumi Nakagawa, Yuzi Kanazawa, Akira Sakai",
          "institution": "Fujitsu Limited, RIKEN Center for AIP, Institute of Science Tokyo, Tokai University",
          "link": "https://arxiv.org/pdf/2512.20687",
          "code": null,
          "tags": [
            "llm inference",
            "hierarchical autoregressive model",
            "KV-cache optimization",
            "memory-bound inference",
            "multi-resolution context",
            "throughput-quality trade-off"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6824d7ad660d8d52e1568c90187924380b5fd436a69942bfac67084af3298d40_w640_q70.webp",
          "contributions": "1. Proposes PHOTON, a hierarchical autoregressive model that replaces the Transformer's flat token-by-token scanning with a vertical, multi-resolution context access pattern. 2. Introduces a persistent hierarchy of latent streams, with a bottom-up encoder compressing tokens and lightweight top-down decoders reconstructing token representations, reducing decode-time KV-cache traffic. 3. Demonstrates significant improvements in throughput per unit memory (up to 10^3x) and advantages in long-context and multi-query tasks compared to Transformer-based models.",
          "summary": "The paper identifies that Transformer inference becomes memory-bound due to ever-growing KV-cache reads/writes during autoregressive decoding. To solve this, it proposes PHOTON, a hierarchical model that accesses context vertically at multiple resolutions instead of scanning tokens horizontally. This architectural change drastically reduces memory traffic, yielding orders-of-magnitude higher throughput per unit memory while maintaining quality.",
          "mindmap": "graph LR\n    A[PHOTON: Hierarchical Autoregressive Modeling] --> B[核心问题/Problem: Transformer水平扫描导致KV缓存读写成为内存瓶颈/Horizontal scanning causes memory-bound KV-cache bottleneck]\n    A --> C[主要方法/Method: 用垂直多分辨率层次模型替代/Replace with vertical multi-resolution hierarchical model]\n    A --> D[关键结果/Results: 内存效率与吞吐量大幅提升/Significant improvement in memory efficiency & throughput]"
        },
        {
          "title": "Real-World Adversarial Attacks on RF-Based Drone Detectors",
          "authors": "Omer Gazit, Yael Itzhakev, Yuval Elovici, Asaf Shabtai",
          "institution": "Ben-Gurion University of the Negev",
          "link": "https://arxiv.org/pdf/2512.20712",
          "code": null,
          "tags": [
            "adversarial machine learning",
            "adversarial attack",
            "drone detection",
            "radio frequency (RF)",
            "over-the-air (OTA)",
            "I/Q perturbation"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/384206c86ac50ef9b9d711e77f0b2e691640ea7831fb12c1517a40575d8c07f3_w640_q70.webp",
          "contributions": "1. The first physical adversarial attack targeting image-based object detection models for RF-based drone detection. 2. A novel method that optimizes adversarial perturbations directly in the complex baseband (I/Q) domain for over-the-air transmission. 3. Demonstration of the attack's effectiveness and hardware compatibility through both digital and physical (OTA) evaluations with multiple drone types.",
          "summary": "This paper proposes the first physical adversarial attack against RF-based drone detectors. Instead of modifying digital spectrogram images, the method generates and transmits optimized I/Q perturbation waveforms alongside legitimate drone signals. The results show these structured perturbations are compatible with standard RF hardware and reliably reduce target drone detection while maintaining detection of legitimate ones.",
          "mindmap": "graph LR\n    A[Real-World Adversarial Attacks on RF-Based Drone Detectors] --> B(核心问题/Problem: Digital RF attacks are hard to implement over-the-air)\n    A --> C(主要方法/Method: Generate & transmit I/Q perturbation waveforms)\n    A --> D(关键结果/Results: Perturbations reduce target detection, preserve others, are hardware-compatible)"
        },
        {
          "title": "FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs",
          "authors": "Saeed Mohammadzadeh, Erfan Hamdi, Joel Shor, Emma Lejeune",
          "institution": "Boston University, Move37 Labs",
          "link": "https://arxiv.org/pdf/2512.20732",
          "code": null,
          "tags": [
            "llm inference",
            "Finite Element Method (FEM)",
            "Code Generation",
            "LLM Benchmark",
            "Computational Mechanics",
            "Scientific Machine Learning"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1933a2d33b13b692f95ee8ddec0a65840af091998d38a7f0154837874636590_w640_q70.webp",
          "contributions": "1. Introduces FEM-Bench, a novel benchmark for evaluating LLMs' ability to generate scientifically valid code for computational mechanics problems. 2. Provides a structured suite of tasks based on finite element methods that enforce physical and numerical constraints for objective evaluation. 3. Presents initial evaluation results showing that state-of-the-art LLMs (e.g., Gemini 3 Pro, GPT-5) still struggle to reliably solve these introductory tasks.",
          "summary": "The paper identifies a lack of benchmarks for evaluating LLMs' scientific reasoning and code generation for physical modeling. It proposes FEM-Bench, a computational mechanics benchmark based on the Finite Element Method, to fill this gap. Initial evaluations show that even advanced LLMs cannot reliably solve all its tasks, establishing a foundation for tracking progress in AI-generated scientific code.",
          "mindmap": "graph LR\n        A[FEM-Bench Paper] --> B[核心问题/Problem: 缺乏评估LLM生成科学物理模型代码能力的基准/Lack of benchmark for evaluating LLMs' ability to generate scientifically valid physical model code]\n        A --> C[主要方法/Method: 提出基于计算力学和有限元法的结构化基准/Proposes a structured benchmark based on computational mechanics and the Finite Element Method]\n        A --> D[关键结果/Results: 先进LLM无法可靠解决所有基准任务，为跟踪进展奠定基础/State-of-the-art LLMs cannot reliably solve all benchmark tasks, establishing a foundation for tracking progress]"
        },
        {
          "title": "AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent",
          "authors": "Haipeng Luo, Huawen Feng, Qingfeng Sun, Can Xu, Kai Zheng, Yufei Wang, Tao Yang, Han Hu, Yansong Tang, Di Wang",
          "institution": "Tsinghua University, Tencent Hunyuan",
          "link": "https://arxiv.org/pdf/2512.20745",
          "code": null,
          "tags": [
            "agent system",
            "tool-augmented agent",
            "agentic reinforcement learning",
            "supervised fine-tuning (SFT)",
            "request-level asynchronous rollout",
            "prefix-aware load balancing"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7211416872630b2f7d460fe4b986a1d141827d69b65487826e3374c5e4cce08d_w640_q70.webp",
          "contributions": "1. An automated method to convert natural language chain-of-thought into structured tool-augmented trajectories for generating high-quality SFT data. 2. A novel agentic reinforcement learning paradigm that dynamically interleaves natural language generation with real-time code execution for learning tool-use strategies. 3. An efficient training system with techniques like asynchronous rollout scheduling and prefix-aware load balancing, achieving 4-5x speedup for RL training on long sequences.",
          "summary": "This paper introduces AgentMath, a framework that combines language model reasoning with code interpreter precision to solve complex math problems. It uses automated SFT data generation, agentic RL for tool-use learning, and an efficient training system, achieving state-of-the-art results on benchmarks like AIME24 and AIME25.",
          "mindmap": "graph LR\n    A[AgentMath] --> B[核心问题/Problem: LRMs are inefficient and inaccurate for complex math]\n    A --> C[主要方法/Method: Tool-augmented agent framework with SFT data generation, agentic RL, and efficient training system]\n    A --> D[关键结果/Results: SOTA performance on AIME24, AIME25, HMMT25 benchmarks]"
        },
        {
          "title": "AI-Driven Green Cognitive Radio Networks for Sustainable 6G Communication",
          "authors": "Anshul Sharma, Shujaatali Badami, Biky Chouhan, Pushpanjali Pandey, Brijeena Rana, Navneet Kaur",
          "institution": "Independent Researcher (USA), Liverpool John Moores University (UK), Chandigarh University (India), Gyancity Research Consultancy (India)",
          "link": "https://arxiv.org/pdf/2512.20739",
          "code": null,
          "tags": [
            "wireless networks",
            "Deep Reinforcement Learning (DRL)",
            "Reconfigurable Intelligent Surfaces (RIS)",
            "Energy Harvesting (EH)"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22a82510c37e7a60d88746806bbece62f0d234e13f225f50ad5635a0bb4ae5ee_w640_q70.webp",
          "contributions": "1. A holistic system model integrating PUs/SUs, energy harvesting, and RIS for sustainable CRN operation. 2. A DRL-based controller enhanced with transfer learning and hybrid metaheuristics for dynamic sensing and resource allocation. 3. EH-aware scheduling and RIS-phase co-adaptation algorithms to reduce SU power consumption.",
          "summary": "This paper proposes an AI-driven framework for green Cognitive Radio Networks (CRNs) in 6G. It integrates Deep Reinforcement Learning (DRL) with transfer learning, energy harvesting, and reconfigurable intelligent surfaces (RIS) to optimize spectrum sensing and resource allocation. The framework demonstrates significant energy savings, high sensing accuracy, and improved packet delivery ratio compared to traditional baselines, offering a sustainable path for 6G IoT and vehicular networks.",
          "mindmap": "graph LR\n    A[AI-Driven Green CRNs for 6G] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[频谱稀缺与高能耗/Spectrum Scarcity & High Energy Consumption]\n    C --> C1[AI驱动框架/AI-Driven Framework]\n    C1 --> C2[集成DRL, TL, EH, RIS/Integrates DRL, TL, EH, RIS]\n    D --> D1[节能25-30%/25-30% Energy Saving]\n    D --> D2[AUC>0.90, PDR提升/AUC>0.90, PDR Improved]"
        },
        {
          "title": "TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection",
          "authors": "Tony Tran, Bin Hu",
          "institution": "University of Houston",
          "link": "https://arxiv.org/pdf/2512.20746",
          "code": null,
          "tags": [
            "on-device ai",
            "neural architecture search",
            "hardware-aware search",
            "edge detection",
            "TinyML",
            "waste detection"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4dbb2f9a6c26aed837d78c4cfa78db76890d85a7fadbb49f8592bc1ae30894e1_w640_q70.webp",
          "contributions": "1. Proposes an iterative hardware-aware neural architecture search (NAS) framework that alternates between backbone and neck/head optimization for efficient object detection under TinyML constraints. 2. Introduces a population passthrough mechanism and an accuracy predictor to reduce search cost and improve the stability of the evolutionary search. 3. Delivers a family of deployment-ready detectors (TrashDets) that significantly improve efficiency (energy, latency, power) and accuracy on microcontroller hardware compared to existing TinyML baselines.",
          "summary": "This paper addresses efficient waste detection for edge/IoT devices under TinyML constraints. It proposes an iterative hardware-aware neural architecture search framework to generate a family of efficient detectors called TrashDets. The resulting models achieve higher accuracy with fewer parameters and significantly reduce energy consumption and latency on resource-constrained microcontrollers.",
          "mindmap": "graph LR\n        A[TrashDet] --> B[核心问题/Problem: 边缘设备垃圾检测<br>TinyML Constraints];\n        A --> C[主要方法/Method: 迭代硬件感知NAS<br>Iterative Hardware-aware NAS];\n        A --> D[关键结果/Results: 高效TrashDet家族<br>Efficient TrashDet Family];\n        B --> D;\n        C --> D;"
        },
        {
          "title": "Stabilizing Multimodal Autoencoders: A Theoretical and Empirical Analysis of Fusion Strategies",
          "authors": "Diyar Altinses, Andreas Schwung",
          "institution": "South Westphalia University of Applied Sciences",
          "link": "https://arxiv.org/pdf/2512.20749",
          "code": null,
          "tags": [
            "multi-modal training",
            "Lipschitz Continuity",
            "Attention Mechanism",
            "Aggregation Methods",
            "Training Stability",
            "Multimodal Autoencoders"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3484b58bc84f22d71a010fca63235d2811ea4f720d1103584b13e220d263f42d_w640_q70.webp",
          "contributions": "1. Derivation of theoretical Lipschitz constants for aggregation methods in multimodal autoencoders. 2. Introduction of a novel regularized attention-based fusion method designed from the theoretical analysis to improve training stability. 3. Empirical validation of the theoretical findings and demonstration of the proposed method's superior performance in consistency, convergence speed, and accuracy.",
          "summary": "This paper analyzes the stability of multimodal autoencoders by theoretically deriving Lipschitz constants for fusion strategies and proposes a new regularized attention-based fusion method. The method is empirically validated and shown to outperform existing strategies, providing a more stable and performant training process for multimodal models.",
          "mindmap": "graph LR\n    A[Stabilizing Multimodal Autoencoders<br/>稳定多模态自编码器] --> B(核心问题/Problem: Training Stability & Robustness<br/>训练稳定性与鲁棒性)\n    A --> C(主要方法/Method: Theoretical Lipschitz Analysis & Regularized Attention Fusion<br/>理论Lipschitz分析与正则化注意力融合)\n    A --> D(关键结果/Results: Improved Consistency, Convergence, Accuracy<br/>提升的一致性、收敛速度与精度)"
        },
        {
          "title": "Bridging Efficiency and Safety: Formal Verification of Neural Networks with Early Exits",
          "authors": "Yizhak Yisrael Elboher, Avraham Raviv, Amihay Elboher, Zhouxing Shi, Omri Azencot, Hillel Kugler, Guy Katz",
          "institution": "The Hebrew University of Jerusalem, Bar Ilan University, Ben-Gurion University of the Negev, University of California, Riverside",
          "link": "https://arxiv.org/pdf/2512.20755",
          "code": null,
          "tags": [
            "others",
            "formal verification",
            "neural network robustness",
            "early exits",
            "adversarial perturbations",
            "off-the-shelf solvers"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cb8be1b40cc4cc73a6f6a75d4c206b456d4189c26838e784e46e437ea5a87b_w640_q70.webp",
          "contributions": "1. Defined a formal robustness property specifically tailored for neural network architectures with early exits. 2. Presented a baseline verification algorithm for such networks, enhanced with an early stopping strategy and heuristic optimizations that maintain soundness and completeness. 3. Demonstrated empirically that early exits not only accelerate inference but also enhance verifiability, solving more queries in less time compared to standard networks.",
          "summary": "This paper addresses the challenge of formally verifying the robustness of neural networks that use early exits for efficiency. The authors propose a tailored robustness property and an enhanced verification algorithm using off-the-shelf solvers. Their experiments show that early exits can improve both inference speed and verifiability, helping navigate the trade-off between accuracy and efficiency.",
          "mindmap": "graph LR\n    A[论文标题 / Paper Title<br>Bridging Efficiency and Safety] --> B(核心问题 / Problem<br>Verifying Early Exit Networks);\n    A --> C(主要方法 / Method<br>Tailored Robustness Property & Enhanced Algorithm);\n    A --> D(关键结果 / Results<br>Improved Verifiability & Efficiency);"
        },
        {
          "title": "TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior",
          "authors": "Gül Sena Altıntaş, Malikeh Ehghaghi, Brian Lester, Fengyuan Liu, Wanru Zhao, Marco Ciccone, Colin Raffel",
          "institution": "University of Toronto, Vector Institute, Google DeepMind, McGill University, Mila - Quebec AI Institute, University of Cambridge, Hugging Face",
          "link": "https://arxiv.org/pdf/2512.20757",
          "code": "https://github.com/r-three/Tokenizers",
          "tags": [
            "tokenization",
            "tokenizer",
            "language models",
            "benchmark",
            "subword segmentation",
            "BPE"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48b6cd3c25dd384b02a8b1601f98df302b0d84a5c6e3b043841ea275f5ffdcbd_w640_q70.webp",
          "contributions": "1. Introduces TokSuite, a collection of fourteen language models that are identical except for their tokenizers, enabling isolated study of tokenizer impact. 2. Curates and releases a new benchmark designed to measure model performance under real-world text perturbations that affect tokenization. 3. Provides a robust framework that supports novel findings on the benefits and shortcomings of various popular tokenizers.",
          "summary": "This paper addresses the challenge of isolating the impact of tokenizer choice on language model behavior. It proposes TokSuite, a suite of models with different tokenizers but identical other components, along with a specialized benchmark. The work enables systematic analysis and reveals new insights into how different tokenizers affect model performance.",
          "mindmap": "graph LR\n        A[TokSuite: Measuring Tokenizer Impact] --> B[核心问题/Problem: Tokenization's role in LM performance is poorly understood]\n        A --> C[主要方法/Method: TokSuite - Identical models with different tokenizers + new benchmark]\n        A --> D[关键结果/Results: Novel findings on tokenizer benefits and shortcomings]"
        },
        {
          "title": "Generalization of RLVR Using Causal Reasoning as a Testbed",
          "authors": "Brian Lu, Hongyu Zhao, Shuo Sun, Hao Peng, Rui Ding, Hongyuan Mei",
          "institution": "Johns Hopkins University, University of Maryland, College Park, National University of Singapore, University of Illinois at Urbana-Champaign, Microsoft Research Asia, Toyota Technological Institute at Chicago",
          "link": "https://arxiv.org/pdf/2512.20760",
          "code": null,
          "tags": [
            "reinforcement learning",
            "RLVR",
            "causal reasoning",
            "generalization",
            "supervised fine-tuning",
            "large language models"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/472c557c79b64b352421bedd952ba76d099165d613e99323a1beb8845a24cf4c_w640_q70.webp",
          "contributions": "1. Provides an empirical study of RLVR generalization using causal inference as a structured testbed, examining generalization across query levels and structural complexity. 2. Identifies that RLVR's benefits over SFT for generalization are contingent on specific combinations of model size and training query level, and depend on the model's initial reasoning competence. 3. Shows that RLVR improves specific causal reasoning subskills, such as marginalization strategy and intermediate probability calculation, leading to accuracy gains on complex queries.",
          "summary": "This paper studies the generalization of Reinforcement Learning with Verifiable Rewards (RLVR) for large language models on causal reasoning tasks. It finds that RLVR can outperform supervised fine-tuning in generalization, but its effectiveness depends on model size, training data, and the model's initial competence. The results indicate RLVR improves specific reasoning sub-skills when the model has a sufficient foundational ability.",
          "mindmap": "graph LR\n    A[”Generalization of RLVR Using Causal Reasoning as a Testbed<br>以因果推理为测试平台的RLVR泛化研究”] --> B[”核心问题/Problem<br>RLVR何时能实现鲁棒泛化？<br>When does RLVR yield robust generalization?”]\n    A --> C[”主要方法/Method<br>在因果图模型上实证研究RLVR与SFT<br>Empirical study of RLVR vs SFT on causal graphical models”]\n    A --> D[”关键结果/Results<br>RLVR泛化更强，但依赖模型规模与初始能力<br>RLVR yields stronger generalization but depends on model size & initial competence”]"
        },
        {
          "title": "TS-Arena Technical Report -- A Pre-registered Live Forecasting Platform",
          "authors": "Marcel Meyer, Sascha Kaltenpoth, Kevin Zalipski, Henrik Albers, Oliver Müller",
          "institution": "Paderborn University",
          "link": "https://arxiv.org/pdf/2512.20761",
          "code": "https://huggingface.co/spaces/DAG-UPB/TS-Arena",
          "tags": [
            "others",
            "time series foundation models",
            "live forecasting",
            "pre-registration",
            "information leakage",
            "temporal split"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea84e3460e7e5ece43727d2db2e7515fe17057e90ce083ef73f979343188043f_w640_q70.webp",
          "contributions": "1. Introduces TS-Arena, a platform that uses live data streams and a pre-registration mechanism to create a strict global temporal split for evaluation, preventing historical data contamination. 2. Proposes a methodology that treats the genuinely unknown future as the definitive test environment, establishing a moving temporal frontier for authentic assessment of model generalization. 3. Provides a sustainable infrastructure initially applied in the energy sector for comparing Time Series Foundation Models (TSFMs) under real-world constraints, addressing the evaluation crisis caused by data reuse and leakage.",
          "summary": "The paper identifies an evaluation crisis in Time Series Foundation Models (TSFMs) caused by information leakage from overlapping training/test data. To solve this, it proposes TS-Arena, a live forecasting platform that enforces evaluation on future, unseen data via pre-registration, ensuring a valid temporal split. The platform provides a fair and realistic infrastructure for benchmarking TSFMs, with an initial application in the energy sector.",
          "mindmap": "graph LR\n    A[TS-Arena Technical Report] --> B[核心问题/Problem: TSFM评估危机 / TSFM Evaluation Crisis]\n    A --> C[主要方法/Method: 预注册实时预测平台 / Pre-registered Live Forecasting Platform]\n    A --> D[关键结果/Results: 防止历史污染，真实评估泛化 / Prevents Historical Contamination, Authentic Generalization Assessment]\n    B --> E[信息泄露与数据重用 / Information Leakage & Data Reuse]\n    C --> F[实时数据流与严格时间分割 / Live Data Streams & Strict Temporal Split]\n    D --> G[可持续的基准测试基础设施 / Sustainable Benchmarking Infrastructure]"
        },
        {
          "title": "Subgroup Discovery with the Cox Model",
          "authors": "Zachary Izzo, Iain Melvin",
          "institution": "NEC Labs America",
          "link": "https://arxiv.org/pdf/2512.20762",
          "code": null,
          "tags": [
            "survival analysis",
            "subgroup discovery",
            "Cox proportional hazards model",
            "expected prediction entropy",
            "conditional rank statistics",
            "interpretable machine learning"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa075e21de650db93aab9430f1fc0b8df1921ae52ac2d9c0e9c3dadc8ba3c5f7_w640_q70.webp",
          "contributions": "1. Introduced two novel metrics for evaluating survival models in the context of subgroup discovery: the Expected Prediction Entropy (EPE) and the Conditional Rank Statistics (CRS). 2. Proposed eight algorithms for the Cox subgroup discovery problem, with a main algorithm that leverages both EPE and CRS and has theoretical correctness guarantees. 3. Demonstrated the effectiveness of the methods through empirical evaluation on synthetic and real data, including a case study on NASA jet engine simulation data.",
          "summary": "This paper addresses the problem of subgroup discovery for survival analysis, aiming to find interpretable data subsets where the Cox model is highly accurate. The authors propose two new evaluation metrics (EPE and CRS) and eight algorithms to solve this problem. Their methods successfully recover ground-truth subgroups and improve model fit compared to fitting a Cox model on the entire dataset, as validated on synthetic, real, and NASA case study data.",
          "mindmap": "graph LR\n    A[Subgroup Discovery with the Cox Model] --> B(核心问题/Problem: Find interpretable subsets where Cox model is accurate)\n    A --> C(主要方法/Method: Introduce EPE & CRS metrics; Propose 8 algorithms)\n    A --> D(关键结果/Results: Recovers ground-truth subgroups; Better model fit; Validated on NASA data)"
        },
        {
          "title": "Improving Matrix Exponential for Generative AI Flows: A Taylor-Based Approach Beyond Paterson--Stockmeyer",
          "authors": "Jorge Sastre, Daniel Faronbi, José Miguel Alonso, Peter Traver, Javier Ibáñez, Nuria Lloret",
          "institution": "Universitat Politècnica de València, New York University",
          "link": "https://arxiv.org/pdf/2512.20777",
          "code": null,
          "tags": [
            "gpu kernels",
            "matrix exponential",
            "scaling and squaring",
            "Taylor series",
            "generative AI",
            "numerical stability"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf8f618b421ab20009271fba403be914454dbd115182614b0f5256ce16218271_w640_q70.webp",
          "contributions": "1. An optimized Taylor-based algorithm for matrix exponential designed for high-throughput generative AI flows. 2. A rigorous error analysis and a dynamic selection strategy for Taylor order and scaling factor to minimize computation under error tolerance. 3. Extensive numerical experiments demonstrating significant acceleration and high numerical stability compared to state-of-the-art methods.",
          "summary": "This paper proposes an optimized Taylor-based algorithm for computing the matrix exponential, a key operation in generative AI. The method improves upon classical techniques like Paterson-Stockmeyer and includes a dynamic strategy to balance accuracy and speed. Experiments show it offers significant acceleration while maintaining high numerical stability for large-scale generative modeling.",
          "mindmap": "graph LR\n        A[Improving Matrix Exponential for Generative AI Flows<br/>生成式AI流程的矩阵指数改进] --> B(Problem: Standard methods (Padé) may be inefficient for high-throughput AI<br/>问题: 标准方法对高吞吐AI效率不足)\n        A --> C(Method: Optimized Taylor-based algorithm with dynamic parameter selection<br/>方法: 优化的基于泰勒级数的算法与动态参数选择)\n        A --> D(Results: Significant acceleration & high numerical stability demonstrated<br/>结果: 显著加速并保持高数值稳定性)"
        },
        {
          "title": "NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts",
          "authors": "Raja Mallina, Bryar Shareef",
          "institution": "University of Nevada, Las Vegas",
          "link": "https://arxiv.org/pdf/2512.20783",
          "code": null,
          "tags": [
            "medical image segmentation",
            "nullable prompts",
            "mixed-supervision",
            "vision-language models",
            "breast ultrasound segmentation"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9c68929834a07c86ac43fe9856efa137aa11c30a231a02ea87272f2b689e4f7f_w640_q70.webp",
          "contributions": "1. Proposes NullBUS, a multimodal mixed-supervision framework for BUS segmentation that can learn from images both with and without prompts in a single model. 2. Introduces nullable prompts, implemented as learnable null embeddings with presence masks, to handle missing text metadata by enabling fallback to image-only evidence. 3. Demonstrates state-of-the-art performance on a unified pool of three public BUS datasets under mixed prompt availability.",
          "summary": "The paper addresses the problem that many public breast ultrasound datasets lack reliable text or spatial prompts, which limits the training of promptable segmentation models. It proposes NullBUS, a framework that uses nullable global-local prompts to learn from both prompted and prompt-free images. The method achieves state-of-the-art segmentation performance on a unified evaluation of three public datasets, showing robustness under mixed prompt availability.",
          "mindmap": "graph LR\n    A[NULLBUS] --> B[核心问题/Problem: BUS数据集缺乏可靠提示词]\n    A --> C[主要方法/Method: 可空全局-局部提示的混合监督框架]\n    A --> D[关键结果/Results: 在混合提示下达到SOTA性能]"
        },
        {
          "title": "Symbolic regression for defect interactions in 2D materials",
          "authors": "Mikhail Lazarev, Andrey Ustyuzhanin",
          "institution": "HSE University, Institute for Functional Intelligent Materials (National University of Singapore), Constructor University",
          "link": "https://arxiv.org/pdf/2512.20785",
          "code": null,
          "tags": [
            "symbolic regression",
            "Symbolic Regression",
            "SEGVAE",
            "Graph Neural Networks",
            "2D Materials",
            "Interpretability"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e61aacf1f171adb632fccf66c94fbeaf1d24724489a5e5c0d3c2a23d60bf7d16_w640_q70.webp",
          "contributions": "1. Applied the deep symbolic regression algorithm SEGVAE to model defect interactions in 2D materials. 2. Demonstrated that symbolic regression can achieve performance comparable to state-of-the-art graph neural network methods. 3. Discussed the broader applicability and advantages (e.g., interpretability) of symbolic regression methods in natural sciences.",
          "summary": "This paper applies the SEGVAE deep symbolic regression algorithm to discover analytical equations describing defect interactions in 2D materials. The results show that this interpretable method achieves performance comparable to state-of-the-art graph neural networks, highlighting its potential for scientific discovery.",
          "mindmap": "graph LR\n    A[Symbolic regression for defect interactions in 2D materials] --> B(核心问题/Problem: ML models lack interpretability for scientific data)\n    A --> C(主要方法/Method: Apply SEGVAE for symbolic regression)\n    A --> D(关键结果/Results: Comparable performance to GNNs, offers interpretability)"
        },
        {
          "title": "FedMPDD: Communication-Efficient Federated Learning with Privacy Preservation Attributes via Projected Directional Derivative",
          "authors": "Mohammadreza Rostami, Solmaz S. Kia",
          "institution": "University of California Irvine",
          "link": "https://arxiv.org/pdf/2512.20814",
          "code": null,
          "tags": [
            "federated learning",
            "gradient compression",
            "directional derivative",
            "privacy preservation",
            "communication efficiency",
            "low-rank projection"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/026756c87a289335823e9350b3b3cf6b10435dec5873ca0fe832db069efa00b9_w640_q70.webp",
          "contributions": "1. Proposes FedMPDD, a novel FL algorithm that compresses high-dimensional gradients into low-dimensional messages via multi-projected directional derivatives, reducing uplink cost from O(d) to O(m)., 2. Provides theoretical convergence analysis showing FedMPDD achieves an O(1/√K) convergence rate, matching FedSGD, by averaging multiple projections to overcome single-projection limitations., 3. Demonstrates the method offers inherent privacy against gradient inversion attacks due to geometric properties of low-rank projections, providing a tunable privacy-utility trade-off.",
          "summary": "This paper addresses the high communication cost and privacy risks in Federated Learning. It proposes FedMPDD, which compresses client gradients by computing their directional derivatives along multiple random vectors, significantly reducing bandwidth usage while providing inherent privacy. Theoretical and experimental results show the method maintains convergence performance comparable to FedSGD and offers a tunable privacy-utility trade-off.",
          "mindmap": "graph LR\n    A[FedMPDD] --> B[核心问题/Problem: High Communication Cost & Privacy Risk in FL]\n    A --> C[主要方法/Method: Multi-Projected Directional Derivative for Gradient Compression]\n    A --> D[关键结果/Results: O(1/√K) Convergence, O(m) Communication, Inherent Privacy]"
        },
        {
          "title": "GraphFire-X: Physics-Informed Graph Attention Networks and Structural Gradient Boosting for Building-Scale Wildfire Preparedness at the Wildland-Urban Interface",
          "authors": "Miguel Esparza, Vamshi Battal, Ali Mostafavi",
          "institution": "Texas A&M University",
          "link": "https://arxiv.org/pdf/2512.20813",
          "code": null,
          "tags": [
            "physics-informed machine learning",
            "graph neural networks",
            "XGBoost",
            "AlphaEarth embeddings",
            "contagion dynamics",
            "ensemble learning"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2b3a0fb230e974a7c6219e056bcb5e5d527990414b399512769ab5745bdc847_w640_q70.webp",
          "contributions": "1. A novel dual-specialist ensemble framework that disentangles wildfire vulnerability into distinct environmental contagion and structural fragility vectors. 2. Integration of a physics-informed Graph Neural Network (GNN) for neighborhood-scale contagion modeling with an XGBoost model for asset-level structural resilience. 3. Generation of a diagnostic risk topology enabling targeted mitigation strategies, such as vegetation management for high-connectivity clusters and structural hardening for vulnerable nodes.",
          "summary": "This paper proposes GraphFire-X, a dual-specialist ensemble framework combining a physics-informed Graph Neural Network and XGBoost to model building-scale wildfire risk at the Wildland-Urban Interface. The method disentangles environmental contagion from structural fragility, revealing that neighborhood-scale environmental pressure dominates propagation pathways while eaves are a key micro-scale vulnerability. The ensemble provides a diagnostic risk map to guide precise, proactive mitigation strategies.",
          "mindmap": "graph LR\n    A[GraphFire-X] --> B[核心问题/Problem: 传统模型无法捕捉WUI非线性蔓延动态/Traditional models fail to capture non-linear contagion dynamics at WUI]\n    A --> C[主要方法/Method: 双专家集成框架/Dual-Specialist Ensemble: 物理信息GNN(环境) + XGBoost(结构)/Physics-informed GNN (Environment) + XGBoost (Structure)]\n    A --> D[关键结果/Results: 环境压力主导蔓延路径，屋檐是主要入侵点，生成诊断风险拓扑/Environmental pressure dominates pathways, eaves are key ingress, diagnostic risk topology generated]"
        },
        {
          "title": "Defending against adversarial attacks using mixture of experts",
          "authors": "Mohammad Meymani, Roozbeh Razavi-Far",
          "institution": "University of New Brunswick",
          "link": "https://arxiv.org/pdf/2512.20821",
          "code": null,
          "tags": [
            "Adversarial Machine Learning",
            "Mixture of Experts",
            "Adversarial Training",
            "Robustness",
            "Ensemble Learning",
            "Evasion Attacks"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0952838b3d0745a7f0396318a91c52e0ef0a7e273730cc1cf1a311f7bfecf079_w640_q70.webp",
          "contributions": "1. Proposes a defense system that integrates an adversarial training module within a Mixture of Experts (MoE) architecture. 2. Employs nine pre-trained experts with ResNet-18 backbones and jointly optimizes both the expert parameters and the gating mechanism during end-to-end training. 3. Demonstrates that the proposed system outperforms state-of-the-art defense systems and plain classifiers, even those with more complex architectures.",
          "summary": "This paper addresses the vulnerability of machine learning models to adversarial attacks. It proposes a defense system that combines adversarial training with a Mixture of Experts architecture, using nine pre-trained ResNet-18 models. The system is shown to be more robust than existing defenses and standard classifiers.",
          "mindmap": "graph LR\n    A[Defending against adversarial attacks using mixture of experts] --> B(核心问题/Problem: ML模型易受对抗性攻击/ML Models Vulnerable to Adversarial Attacks)\n    A --> C(主要方法/Method: 对抗训练与专家混合体结合/Adversarial Training within Mixture-of-Experts)\n    A --> D(关键结果/Results: 优于现有防御与分类器/Outperforms State-of-the-art Defenses & Classifiers)"
        },
        {
          "title": "Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions",
          "authors": "Rashmeet Kaur Nayyar, Naman Shah, Siddharth Srivastava",
          "institution": "Arizona State University, Brown University",
          "link": "https://arxiv.org/pdf/2512.20831",
          "code": "https://github.com/AAIR-lab/PEARL.git",
          "tags": [
            "reinforcement learning",
            "parameterized actions",
            "state abstraction",
            "action abstraction",
            "TD(λ)",
            "sample efficiency"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c74b22dc11c38dfc5a75f48c2cd94c57d0eecaffdac79525f6362b0b32c448b0_w640_q70.webp",
          "contributions": "1. Enables agents to autonomously learn both state and action abstractions online for RL with parameterized actions., 2. Introduces algorithms that progressively refine these abstractions during learning, focusing detail on critical regions., 3. Extends RL to long-horizon, sparse-reward settings with parameterized actions, achieving higher sample efficiency than baselines.",
          "summary": "This paper addresses the challenge of reinforcement learning in environments with parameterized actions, which combine discrete choices with continuous parameters. It proposes a method where agents autonomously learn and progressively refine state and action abstractions online. The approach enables TD(λ) to achieve significantly higher sample efficiency in continuous-state, parameterized-action domains compared to state-of-the-art methods.",
          "mindmap": "graph LR\n    A[Context-Sensitive Abstractions for RL with Parameterized Actions] --> B(核心问题/Problem: RL for Parameterized Actions)\n    A --> C(主要方法/Method: Learn & Refine State/Action Abstractions)\n    A --> D(关键结果/Results: Higher Sample Efficiency for TD(λ))"
        },
        {
          "title": "CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images",
          "authors": "Vidit Agrawal, John Peters, Tyler N. Thompson, Mohammad Vali Sanian, Chau Pham, Nikita Moshkov, Arshad Kazi, Aditya Pillai, Jack Freeman, Byunguk Kang, Samouil L. Farhi, Ernest Fraenkel, Ron Stewart, Lassi Paavolainen, Bryan A. Plummer, Juan C. Caicedo",
          "institution": "Morgridge Institute for Research, University of Wisconsin-Madison, Institute for Molecular Medicine Finland (FIMM), University of Helsinki, Boston University, Institute of Computational Biology (Helmholtz Munich), Massachusetts Institute of Technology, Broad Institute of MIT and Harvard",
          "link": "https://arxiv.org/pdf/2512.20833",
          "code": null,
          "tags": [
            "bioimage analysis",
            "multi-channel microscopy",
            "cellular morphology",
            "pre-training dataset",
            "channel-adaptive models",
            "heterogeneous data"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3554315333d73be844495c47c136c201cae29e405327a25f6e11fc1488c5f4f9_w640_q70.webp",
          "contributions": "1. Introduces CHAMMI-75, a novel open-access dataset of heterogeneous, multi-channel microscopy images curated from 75 diverse biological studies. 2. Proposes the use of this dataset to investigate and develop channel-adaptive models for cellular morphology that can process any microscopy image type. 3. Demonstrates experimentally that pre-training with the diverse CHAMMI-75 dataset improves performance on multi-channel bioimaging tasks.",
          "summary": "This paper addresses the problem that models for quantifying cell morphology are typically specialized to single microscopy types, limiting their reuse. It proposes CHAMMI-75, a diverse, multi-channel microscopy image dataset, and shows that pre-training with it improves model performance by enabling channel-adaptability and handling heterogeneous modalities. The work aims to pave the way for more generalizable cellular morphology models.",
          "mindmap": "graph LR\n        A[CHAMMI-75: Pre-training Multi-channel Models] --> B[核心问题/Problem: Specialized models cannot be reused across studies]\n        A --> C[主要方法/Method: Curate CHAMMI-75, a heterogeneous multi-channel dataset]\n        A --> D[关键结果/Results: Training with CHAMMI-75 improves performance via diversity]"
        },
        {
          "title": "Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning",
          "authors": "NVIDIA, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Faisal Ladhak, Fay Wang, Fei Jia, Felipe Soares, Feng Chen, Ferenc Galko, Frankie Siino, Gal Hubara Agam, Ganesh Ajjanagadde, Gantavya Bhatt",
          "institution": "NVIDIA",
          "link": "https://arxiv.org/pdf/2512.20848",
          "code": null,
          "tags": [
            "llm inference",
            "Mixture-of-Experts",
            "Mamba-Transformer",
            "agentic reasoning",
            "sparse activation",
            "long context"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96a4b5c012acd8208519dcb9669276bd8c3c3709f26e7290e2fce500151c1ccc_w640_q70.webp",
          "contributions": "1. Introduces Nemotron 3 Nano, a hybrid MoE Mamba-Transformer model that sparsely activates only 3.2B out of 31.6B parameters per forward pass for efficiency. 2. Demonstrates superior inference throughput (up to 3.3x faster) compared to similarly-sized open models while maintaining or improving accuracy on benchmarks. 3. Supports an extended context length of up to 1 million tokens and shows enhanced agentic and reasoning capabilities through post-training.",
          "summary": "This paper presents Nemotron 3 Nano, an efficient 30B-parameter language model that combines Mixture-of-Experts with a Mamba-Transformer architecture to achieve sparse activation. It was pre-trained on 25 trillion tokens and post-trained for agentic reasoning, resulting in higher inference throughput and accuracy compared to similar models while supporting up to 1M token contexts.",
          "mindmap": "graph LR\n    A[Nemotron 3 Nano<br>论文标题/Paper Title] --> B[构建高效、能进行智能体推理的大模型<br>核心问题/Problem];\n    A --> C[混合MoE与Mamba-Transformer架构，稀疏激活参数<br>主要方法/Method];\n    A --> D[更高推理吞吐与精度，支持100万令牌上下文<br>关键结果/Results];"
        },
        {
          "title": "NVIDIA Nemotron 3: Efficient and Open Intelligence",
          "authors": "NVIDIA, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Anjulie Agrusa, Ankur Verma, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asit Mishra, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Cyril Meurillon, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Lo, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elad Segal, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Evgeny Tsykunov, Faisal Ladhak, Fay Wang, Fei Jia",
          "institution": "NVIDIA",
          "link": "https://arxiv.org/pdf/2512.20856",
          "code": null,
          "tags": [
            "llm inference",
            "Mixture-of-Experts",
            "Mamba-Transformer",
            "LatentMoE",
            "NVFP4",
            "multi-environment reinforcement learning"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5203ecd520d6e99bc9f0034f05e8945272d4a34a746eeeae01be1cc728049b5_w640_q70.webp",
          "contributions": "1. Introduces the Nemotron 3 family of models (Nano, Super, Ultra) built on a Mixture-of-Experts hybrid Mamba-Transformer architecture for high throughput and long context (up to 1M tokens). 2. Proposes novel techniques including LatentMoE for improved model quality and MTP layers for faster text generation in the larger models. 3. Employs multi-environment reinforcement learning for post-training, enabling advanced capabilities like reasoning, multi-step tool use, and granular reasoning budget control.",
          "summary": "This paper introduces the Nemotron 3 family of open models designed for efficient and intelligent agentic applications. The models use a novel hybrid Mamba-Transformer architecture and are trained with techniques like LatentMoE and multi-environment RL to achieve strong reasoning, conversational, and tool-use capabilities with high throughput. The conclusion is that these models provide state-of-the-art accuracy and efficiency, with plans for open release of weights, software, and data.",
          "mindmap": "graph LR\n    A[NVIDIA Nemotron 3] --> B[核心问题/Problem: Efficient and open intelligence for agentic applications]\n    A --> C[主要方法/Method: Mixture-of-Experts hybrid Mamba-Transformer, LatentMoE, multi-environment RL]\n    A --> D[关键结果/Results: High throughput, 1M context, strong agentic/reasoning capabilities, open release]"
        },
        {
          "title": "Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs",
          "authors": "Pierre Abillama, Changwoo Lee, Juechu Dong, David Blaauw, Dennis Sylvester, Hun-Seok Kim",
          "institution": "University of Michigan",
          "link": "https://arxiv.org/pdf/2512.20861",
          "code": "https://github.com/pabillam/mem-efficient-blr",
          "tags": [
            "llm inference",
            "block low-rank (BLR)",
            "Triton kernels",
            "memory-bound optimization",
            "Jetson Orin Nano",
            "roofline analysis"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5f95e7768493ecd557f85d3dd08d75532f4bfee4218e02d377351eaf02b4c20_w640_q70.webp",
          "contributions": "1. Identified through roofline analysis that multi-token inference for BLR-compressed models becomes memory-bound, limiting speedups despite compiler optimizations. 2. Introduced custom Triton kernels with partial fusion and memory layout optimizations specifically for Monarch and BLR-AST (BLAST) methods. 3. Demonstrated significant speedups (up to 3.76x) and model compression (3x) on memory-constrained GPUs (e.g., Jetson Orin Nano, A40) across various foundation models.",
          "summary": "This paper addresses the memory bottleneck in multi-token inference for block low-rank (BLR) compressed foundation models. The authors propose custom Triton kernels with fusion and layout optimizations for BLR methods like Monarch and BLAST. Their solution achieves up to 3.76x speedup and 3x model compression on resource-constrained GPUs compared to optimized PyTorch baselines.",
          "mindmap": "graph LR\n    A[Memory-Efficient Acceleration of Block Low-Rank Foundation Models] --> B[核心问题/Problem: BLR模型多token推理存在内存墙/Multi-token inference for BLR models is memory-bound]\n    A --> C[主要方法/Method: 定制Triton内核与内存优化/Custom Triton kernels with memory optimizations]\n    A --> D[关键结果/Results: 显著加速与模型压缩/Significant speedup & model compression]"
        },
        {
          "title": "Robustness Certificates for Neural Networks against Adversarial Attacks",
          "authors": "Sara Taheri, Mahalakshmi Sabanayagam, Debarghya Ghoshdastidar, Majid Zamani",
          "institution": "LMU Munich, TUM Munich, University of Colorado Boulder",
          "link": "https://arxiv.org/pdf/2512.20865",
          "code": null,
          "tags": [
            "adversarial robustness",
            "barrier certificates",
            "data poisoning",
            "formal verification",
            "scenario convex program",
            "robustness certification"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5a85af42ea34173637ac88fc6859bba1d4a68d3161f2fcc2567276ca7d37b5b_w640_q70.webp",
          "contributions": "1. Introduces a formal robustness certification framework by modeling gradient-based training as a discrete-time dynamical system and formulating poisoning robustness as a safety verification problem., 2. Adapts barrier certificates from control theory to derive sufficient conditions for certifying a robust radius against worst-case ℓp-norm poisoning, and makes it practical by parameterizing BCs as neural networks., 3. Derives PAC bounds via a scenario convex program to provide a confidence lower bound on the certified robustness radius, and extends the unified framework to also certify against test-time attacks.",
          "summary": "This paper addresses the lack of formal guarantees in defending neural networks against data poisoning attacks. It proposes a certification framework that models training as a dynamical system, uses barrier certificates for verification, and provides PAC-bounded robustness radii. Experiments show the approach certifies non-trivial perturbation budgets without needing prior attack knowledge.",
          "mindmap": "graph LR\n    A[Robustness Certificates for Neural Networks against Adversarial Attacks] --> B[核心问题/Problem: Lack of formal guarantees for neural networks against data poisoning attacks]\n    A --> C[主要方法/Method: Model training as dt-DS, use Barrier Certificates & Scenario Convex Program for verification]\n    A --> D[关键结果/Results: Certifies non-trivial robustness radii on MNIST/SVHN/CIFAR-10, model-agnostic, no prior attack knowledge needed]"
        },
        {
          "title": "Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification",
          "authors": "Jakir Hossain, Gurvinder Singh, Lukasz Ziarek, Ahmet Erdem Sarıyüce",
          "institution": "University at Buffalo",
          "link": "https://arxiv.org/pdf/2512.20872",
          "code": "https://erdemub.github.io/BCG-dataset",
          "tags": [
            "malware detection",
            "function call graphs",
            "Android malware",
            "graph-based classification",
            "APK",
            "dataset"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87c9a8db7055364694bd0257a161b2a24a77226da1ab49504cd20679d8699222_w640_q70.webp",
          "contributions": "1. Introduces the Better Call Graphs (BCG) dataset, a new large-scale collection of unique and recent Android FCGs for malware classification., 2. Addresses limitations of existing datasets (outdated, redundant, small graphs) to prevent overfitting and enable reliable evaluation., 3. Demonstrates the necessity and value of the BCG dataset through extensive experiments with baseline classifiers.",
          "summary": "This paper addresses the lack of high-quality datasets for Android malware classification using function call graphs (FCGs). It introduces the Better Call Graphs (BCG) dataset, which contains large, unique, and recent FCGs from Android APKs. Experiments show BCG is necessary for reliable evaluation and helps overcome overfitting issues present with older datasets.",
          "mindmap": "graph LR\n    A[Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification] --> B(核心问题/Problem: Lack of large-scale, high-quality Android FCG datasets hinders malware classification research)\n    A --> C(主要方法/Method: Introduce BCG dataset with large, unique, recent Android APK FCGs)\n    A --> D(关键结果/Results: BCG enables more reliable evaluation and addresses overfitting compared to existing datasets)"
        },
        {
          "title": "Architectural Trade-offs in Small Language Models Under Compute Constraints",
          "authors": "Shivraj Singh Bhatti",
          "institution": "University of Massachusetts Amherst",
          "link": "https://arxiv.org/pdf/2512.20877",
          "code": null,
          "tags": [
            "language modeling",
            "small language models",
            "compute constraints",
            "architectural trade-offs",
            "rotary positional embeddings",
            "transformer"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2b54dee144c67bdb877818e8171163f9556734c09b3a1d7d29fe8464aee558b_w640_q70.webp",
          "contributions": "1. A systematic empirical study of architectural choices (from linear predictors to transformers) for small language models under strict compute constraints. 2. An analysis showing attention-based models are more FLOP-efficient than MLPs even at small scale, and that increasing depth/context without sufficient optimization can hurt performance. 3. An investigation revealing that techniques like Rotary Positional Embeddings (RoPE), successful in large models, do not necessarily transfer effectively to the small-model regime.",
          "summary": "This paper systematically studies how architectural choices affect small language model performance under limited compute. The method involves progressively building from linear predictors to multi-layer transformers and evaluating them on character and word-level datasets. The main conclusion is that attention is more efficient than MLPs per FLOP at small scales, but scaling depth or applying large-model techniques like RoPE can be detrimental without careful optimization.",
          "mindmap": "graph LR\n    A[Architectural Trade-offs in Small Language Models<br>小型语言模型的架构权衡] --> B[核心问题/Problem<br>How do architectural choices affect performance under compute constraints?<br>计算约束下架构选择如何影响性能？]\n    A --> C[主要方法/Method<br>Progressive architectural study from linear to transformer models<br>从线性到Transformer模型的渐进式架构研究]\n    A --> D[关键结果/Results<br>Attention > MLPs in per-FLOP efficiency; RoPE may not transfer<br>注意力机制单位FLOP效率优于MLP；RoPE可能不适用于小模型]"
        },
        {
          "title": "Time-Efficient Evaluation and Enhancement of Adversarial Robustness in Deep Neural Networks",
          "authors": "Runqi Lin",
          "institution": "The University of Sydney",
          "link": "https://arxiv.org/pdf/2512.20893",
          "code": null,
          "tags": [
            "adversarial robustness",
            "adversarial robustness",
            "deep neural networks",
            "time-efficient",
            "red-blue adversarial framework",
            "adversarial evaluation"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c027984dda35366287528822de7264568bdef6dac0b744de35e5554d0dc77b9_w640_q70.webp",
          "contributions": "1. Proposes time-efficient methods for evaluating adversarial robustness in DNNs, 2. Proposes time-efficient methods for enhancing adversarial robustness in DNNs, 3. Aims to overcome the computational intensity limitation of existing approaches for large-scale models.",
          "summary": "This thesis addresses the computational inefficiency of existing methods for evaluating and improving the adversarial robustness of deep neural networks. It proposes new time-efficient techniques for both identifying vulnerabilities (red team) and mitigating them (blue team). The main goal is to make adversarial robustness assessment and enhancement more applicable to large-scale models.",
          "mindmap": "graph LR\n    A[Time-Efficient Evaluation and Enhancement of Adversarial Robustness in Deep Neural Networks] --> B[核心问题/Problem: 现有对抗鲁棒性评估与增强方法计算成本高 / Existing adversarial robustness evaluation and enhancement methods are computationally intensive.]\n    A --> C[主要方法/Method: 为红队（评估）和蓝队（增强）提供时间高效的方法 / Provides time-efficient methods for the red team (evaluation) and blue team (enhancement).]\n    A --> D[关键结果/Results: 旨在克服大规模模型的应用限制 / Aims to overcome the applicability limitation for large-scale models.]"
        },
        {
          "title": "From GNNs to Symbolic Surrogates via Kolmogorov-Arnold Networks for Delay Prediction",
          "authors": "Sami Marouani, Kamal Singh, Baptiste Jeudy, Amaury Habrard",
          "institution": "Université Jean Monnet Saint-Étienne, CNRS, Institut d'Optique Graduate School, Laboratoire Hubert Curien, Institut Universitaire de France (IUF), Inria",
          "link": "https://arxiv.org/pdf/2512.20885",
          "code": null,
          "tags": [
            "communication & networking",
            "Graph Neural Networks",
            "Kolmogorov-Arnold Networks",
            "Symbolic Regression",
            "Attention",
            "Message Passing"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c96bffa46ff987475631fa980227e1a967ebbe523f541ff36441600d02001b3_w640_q70.webp",
          "contributions": "1. A heterogeneous Graph Neural Network with attention-based message passing as a strong baseline for flow delay prediction. 2. FlowKANet, a fully KAN-based GNN architecture that integrates KAN operators into message-passing and attention computation for efficiency and interpretability. 3. A symbolic distillation of FlowKANet via block-wise regression to produce lightweight, transparent closed-form surrogate models.",
          "summary": "This paper tackles flow delay prediction in communication networks by proposing a progression of models. It introduces FlowKANet, a GNN that replaces standard MLP layers with Kolmogorov-Arnold Networks (KANs) for better efficiency and interpretability, and further distills it into symbolic surrogate models. The results show that KANs offer a good efficiency-accuracy trade-off, and the symbolic surrogates enable lightweight, transparent deployment.",
          "mindmap": "graph LR\n    A[From GNNs to Symbolic Surrogates via KANs for Delay Prediction] --> B[核心问题/Problem: Accurate flow delay prediction for network optimization]\n    A --> C[主要方法/Method: Propose FlowKANet (KAN-based GNN) and distill it into symbolic surrogates]\n    A --> D[关键结果/Results: KANs balance efficiency & accuracy; surrogates enable lightweight, transparent deployment]"
        },
        {
          "title": "DiEC: Diffusion Embedded Clustering",
          "authors": "Haidong Hu",
          "institution": "Not explicitly provided in the given content.",
          "link": "https://arxiv.org/pdf/2512.20905",
          "code": null,
          "tags": [
            "deep clustering",
            "diffusion models",
            "representation selection",
            "self-training",
            "graph regularization",
            "denoising consistency"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66551d88d8940c7a650dca6264246e32d115d3596bb088334602fd1943ca8558_w640_q70.webp",
          "contributions": "1. Proposes DiEC, a novel deep clustering method that directly leverages the internal representation trajectory (across layers and timesteps) of a pretrained diffusion U-Net instead of a single fixed embedding. 2. Introduces a two-stage search strategy (CML and OTS) to efficiently identify the most cluster-friendly representation from the diffusion model's internal activations. 3. Enhances the clustering training with a DEC-style objective augmented by adaptive graph regularization, entropy regularization, and a denoising-consistency branch to strengthen and stabilize cluster structures.",
          "summary": "The paper addresses the problem of finding cluster-friendly representations in deep clustering by proposing DiEC, which extracts and optimizes features from the internal activations of a pretrained diffusion model. The method uses a two-stage search to select optimal representations and employs a regularized self-training objective with a consistency branch. Experiments show that DiEC achieves competitive clustering performance on standard benchmarks.",
          "mindmap": "graph LR\n    A[DiEC: Diffusion Embedded Clustering] --> B[核心问题/Problem: Single fixed embedding ignores varying clusterability in diffusion model's internal trajectory];\n    A --> C[主要方法/Method: Two-stage search (CML & OTS) on layer×timestep, regularized self-training with denoising-consistency];\n    A --> D[关键结果/Results: Achieves competitive clustering performance on multiple benchmarks];"
        },
        {
          "title": "RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks",
          "authors": "Ningyuan Liu, Jing Yang, Kaitong Cai, Keze Wang",
          "institution": "Sun Yat-sen University",
          "link": "https://arxiv.org/pdf/2512.20920",
          "code": null,
          "tags": [
            "llm training",
            "reversible networks",
            "memory-efficient fine-tuning",
            "mixture-of-experts",
            "full-parameter fine-tuning",
            "activation recomputation"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5502accb933d07822fb8b8c8802a3eda6d016d84580bfda3089eae32cc0ea597_w640_q70.webp",
          "contributions": "1. Proposes RevFFN, a novel memory-efficient fine-tuning paradigm for Mixture-of-Experts (MoE) LLMs. 2. Designs reversible Transformer blocks that reconstruct layer inputs from outputs during backpropagation, eliminating the need to store most intermediate activations. 3. Enables efficient full-parameter fine-tuning on a single GPU by drastically reducing peak memory consumption while preserving model capacity.",
          "summary": "The paper addresses the high memory overhead of full-parameter fine-tuning for large language models (LLMs), especially Mixture-of-Experts (MoE) models, caused by caching intermediate activations. It introduces RevFFN, a method using reversible Transformer blocks to recompute activations during backpropagation, significantly reducing memory usage. This allows for efficient full fine-tuning on a single GPU without compromising the model's expressive power.",
          "mindmap": "graph LR\n    A[RevFFN: Memory-Efficient Fine-Tuning] --> B[核心问题/Problem: Full fine-tuning memory overhead高]\n    A --> C[主要方法/Method: 使用可逆Transformer块/Use reversible Transformer blocks]\n    A --> D[关键结果/Results: 单GPU高效全参数微调/Efficient full fine-tuning on single GPU]"
        },
        {
          "title": "Towards a General Framework for Predicting and Explaining the Hardness of Graph-based Combinatorial Optimization Problems using Machine Learning and Association Rule Mining",
          "authors": "Bharat Sharman, Elkafi Hassini",
          "institution": "(Inferred from author names and arXiv submission; specific institution not provided in abstract. Could be a university research group.)",
          "link": "https://arxiv.org/pdf/2512.20915",
          "code": null,
          "tags": [
            "combinatorial optimization",
            "graph features",
            "hardness prediction",
            "association rule mining",
            "maximum clique problem",
            "machine learning classification"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d6637eddd49fc539b03c9e2248f2656c238346a34b80db77df78bdd41cc252f_w640_q70.webp",
          "contributions": "1. Proposes GCO-HPIF, a general two-stage ML framework for predicting and explaining the hardness of graph-based combinatorial optimization problems. 2. Demonstrates the framework's effectiveness by applying it to the maximum clique problem using diverse algorithms (exact and GNN-based) and achieving high prediction accuracy with few features. 3. Introduces the use of association rule mining (FP-Growth) to generate human-interpretable explanations for the hardness predictions.",
          "summary": "This paper introduces GCO-HPIF, a machine learning framework that predicts and explains the computational hardness of graph-based combinatorial optimization problems by first classifying instances using graph features and then explaining the predictions via association rule mining. The framework was validated on a large dataset of maximum clique problem instances, achieving excellent prediction performance and generating interpretable rules. The results show the framework's potential for both accurate hardness forecasting and providing insights into problem difficulty.",
          "mindmap": "graph LR\n    A[Towards a General Framework...<br/>预测与解释图组合优化问题难度的通用框架] --> B(核心问题/Problem: Predicting computational hardness of graph-based combinatorial optimization problems<br/>预测图组合优化问题的计算难度)\n    A --> C(主要方法/Method: Two-stage ML framework (GCO-HPIF)<br/>两阶段机器学习框架)\n    C --> C1(Stage 1: Classification using graph features<br/>阶段1: 使用图特征的分类)\n    C --> C2(Stage 2: Explanation using Association Rule Mining (FP-Growth)<br/>阶段2: 使用关联规则挖掘进行解释)\n    A --> D(关键结果/Results: High prediction accuracy (F1=0.9921), interpretable rules, low error for time prediction<br/>高预测精度, 可解释规则, 低时间预测误差)"
        },
        {
          "title": "Guardrailed Elasticity Pricing: A Churn-Aware Forecasting Playbook for Subscription Strategy",
          "authors": "Deepit Sapru",
          "institution": "University of Illinois Urbana-Champaign",
          "link": "https://arxiv.org/pdf/2512.20932",
          "code": null,
          "tags": [
            "revenue management",
            "constrained optimization",
            "Bayesian hierarchical modeling",
            "Monte Carlo simulation",
            "price elasticity",
            "churn prediction"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0bf97f0a7c7df406f95ecaff29da9d37a9c9851d8013ef82f3f2669083a60ae7_w640_q70.webp",
          "contributions": "1. A novel framework integrating demand forecasting, segment-level price elasticity, and churn propensity into a single constrained optimization system for subscription pricing. 2. A methodology blending seasonal time-series models with tree-based learners and using Monte Carlo scenario tests to map risk envelopes for pricing decisions. 3. A modular, API-driven system designed for real-time recalibration with model explainability for governance, functioning as a managerial strategy playbook.",
          "summary": "This paper proposes a dynamic pricing framework for subscription services that combines forecasting, elasticity modeling, and churn prediction within a constrained optimization system to balance revenue and retention. The method uses Monte Carlo simulations and enforces business guardrails on margins and churn. It outperforms static pricing by targeting price changes to high willingness-to-pay segments while protecting sensitive customers.",
          "mindmap": "graph LR\n    A[Guardrailed Elasticity Pricing] --> B[核心问题/Problem: Static pricing fails to balance revenue & retention];\n    A --> C[主要方法/Method: Forecast + Elasticity + Churn model with constrained optimization];\n    A --> D[关键结果/Results: Outperforms static pricing, protects customers, enables durable growth];"
        },
        {
          "title": "A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws for GNN-based Aerodynamic Field Surrogate",
          "authors": "Yiren Shen, Juan J. Alonso",
          "institution": "Stanford University",
          "link": "https://arxiv.org/pdf/2512.20941",
          "code": null,
          "tags": [
            "others",
            "graph neural network",
            "surrogate model",
            "multi-fidelity dataset",
            "scaling laws",
            "aerodynamic field prediction"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bb0e5406bfc3665bfcafc6d32aeeb524e6e21ffb9ceb2ac785fe0ddd6b60b3_w640_q70.webp",
          "contributions": "1. Release of an open-source, multi-fidelity aerodynamic dataset for double-delta wings, generated using a nested Saltelli sampling scheme. 2. Conducted an empirical scaling study linking training data size and model size to prediction accuracy for a GNN-based surrogate, revealing a power-law relationship. 3. Derived practical guidelines, estimating an optimal sampling density of approximately eight samples per dimension in a design space.",
          "summary": "This paper investigates the relationship between dataset size and model performance for a Graph Neural Network (GNN) surrogate used in aerodynamic field prediction. The authors release a new multi-fidelity dataset for double-delta wings and conduct a scaling study, finding that test error decreases with data size following a power law, which indicates efficient data utilization and informs optimal sampling strategies.",
          "mindmap": "graph LR\n        A[论文标题 / Paper Title<br>A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws] --> B(核心问题 / Problem<br>缺乏开源多保真数据集与数据规模对模型性能影响的实证指导 / Lack of open-source multi-fidelity datasets and empirical guidelines on data scaling)\n        A --> C(主要方法 / Method<br>发布数据集并进行缩放研究 / Release dataset and conduct scaling study)\n        B --> D(关键结果 / Results<br>误差随数据量呈幂律下降 / Test error decreases with data size via power law)\n        C --> D"
        },
        {
          "title": "AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences",
          "authors": "Zhe Wang, Jinghang Li, Yifei Zhu",
          "institution": "Shanghai Jiao Tong University",
          "link": "https://arxiv.org/pdf/2512.20943",
          "code": null,
          "tags": [
            "communication & networking",
            "4D Gaussian Splatting",
            "video streaming",
            "integer linear programming",
            "pruning",
            "keyframe selection"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67fcf9423d5e160d4a5d4e949518213bf3a4f37a910a1c4fe209190f990922dc_w640_q70.webp",
          "contributions": "1. Proposes a streaming-optimized 4DGS framework that converts Gaussian streams into multi-channel 2D formats and uses intelligent keyframe identification to enhance reconstruction quality and reduce training time. 2. Models the 4DGS delivery problem as an integer linear programming problem and designs a lightweight pruning algorithm to adaptively prune Gaussian updates for bandwidth-efficient transmission. 3. Demonstrates significant improvements in quality stability (reducing PSNR deviation by >20%), training speed (6x acceleration), and transmission efficiency (50% size reduction) compared to state-of-the-art methods.",
          "summary": "The paper presents AirGS, a framework that optimizes the training and delivery pipeline for 4D Gaussian Splatting to enable real-time free-viewpoint video streaming. It addresses quality degradation and high bandwidth overhead by introducing a 2D representation format, keyframe selection, and an adaptive pruning algorithm for transmission. Experiments show AirGS significantly improves quality stability, accelerates training, and reduces transmission size.",
          "mindmap": "graph LR\n    A[AirGS: Real-Time 4D Gaussian Streaming] --> B[核心问题/Problem: 4DGS质量下降与高带宽开销/4DGS Quality Degradation & High Bandwidth Overhead]\n    A --> C[主要方法/Method: 流优化框架与自适应剪枝/Streaming-Optimized Framework & Adaptive Pruning]\n    A --> D[关键结果/Results: 质量稳定、训练加速、传输减小/Quality Stable, Training Faster, Transmission Smaller]"
        },
        {
          "title": "MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment",
          "authors": "Mohammad Mahdi Abootorabi, Alireza Ghahramani Kure, Mohammadali Mohammadkhani, Sina Elahimanesh, Mohammad Ali Ali Panah",
          "institution": "Based on the provided email domains (gmail.com), no specific institution can be reliably inferred. The team name is \"MultiMind\".",
          "link": "https://arxiv.org/pdf/2512.20950",
          "code": null,
          "tags": [
            "crosslingual information retrieval",
            "dual-encoder",
            "contrastive learning",
            "hard negative sampling",
            "data augmentation",
            "multi-source alignment"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccb6e1762a9617f640573a86ea65e0e68afff48d53006aa74213e0a557970889_w640_q70.webp",
          "contributions": "1. Introduces TriAligner, a novel dual-encoder architecture with contrastive learning for crosslingual claim retrieval. 2. Proposes a method to learn the relative importance of different information sources (e.g., native text, English translations) for alignment. 3. Enhances robustness through LLM-based data preprocessing/augmentation and hard negative sampling strategies.",
          "summary": "This paper addresses the challenge of retrieving fact-checked claims across multiple languages to combat misinformation. The proposed TriAligner system uses a dual-encoder with contrastive learning and multi-source alignment, enhanced by LLM-based data processing. The method shows significant improvements in retrieval accuracy on monolingual and crosslingual benchmarks.",
          "mindmap": "graph LR\n        A[MultiMind at SemEval-2025 Task 7<br>Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment] --> B(核心问题/Problem: Rapid spread of multilingual misinformation);\n        A --> C(主要方法/Method: TriAligner - dual-encoder with contrastive learning & multi-source alignment);\n        A --> D(关键结果/Results: Improved retrieval accuracy on benchmarks);"
        },
        {
          "title": "Solving Functional PDEs with Gaussian Processes and Applications to Functional Renormalization Group Equations",
          "authors": "Xianjin Yang, Matthieu Darcy, Matthew Hudes, Francis J. Alexander, Gregory Eyink, Houman Owhadi",
          "institution": "Caltech",
          "link": "https://arxiv.org/pdf/2512.20956",
          "code": null,
          "tags": [
            "operator learning",
            "Gaussian processes",
            "functional renormalization group",
            "functional PDEs",
            "operator learning",
            "non-perturbative methods"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18c810a016d418e9edb4adddfcb5d19e76d457adf7e311d4c5ca1b064e1fa145_w640_q70.webp",
          "contributions": "1. Proposes a Gaussian process-based operator learning framework for solving functional PDEs directly in function space, independent of specific discretizations. 2. Demonstrates the method's application to non-perturbative functional renormalization group equations (e.g., Wetterich, Wilson-Polchinski), achieving performance equal to or better than traditional approximations like the local-potential approximation. 3. Shows the framework's flexibility in handling non-constant fields and incorporating physical priors, making it promising for studying complex field configurations like instantons.",
          "summary": "This paper introduces a Gaussian process operator learning framework to solve functional partial differential equations, specifically targeting non-perturbative functional renormalization group equations. The method directly represents functionals in function space, offering flexibility and independence from equation-specific discretizations. The results demonstrate that it matches or outperforms traditional approximations like the local-potential approximation and can handle complex, non-constant field configurations.",
          "mindmap": "graph LR\n    A[Solving Functional PDEs with Gaussian Processes] --> B(核心问题/Problem: Solving functional renormalization group equations)\n    A --> C(主要方法/Method: Gaussian process operator learning in function space)\n    A --> D(关键结果/Results: Matches or beats traditional approximations, handles non-constant fields)"
        },
        {
          "title": "ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design",
          "authors": "R Yadunandan, Nimisha Ghosh",
          "institution": "Department of Computer Science and Engineering, Shiv Nadar University Chennai",
          "link": "https://arxiv.org/pdf/2512.20958",
          "code": "https://github.com/YadunandanRaman/ReACT-Drug/",
          "tags": [
            "reinforcement learning",
            "Proximal Policy Optimization (PPO)",
            "ChemBERTa",
            "ESM-2",
            "reaction-template",
            "de novo drug design"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/021dd36ada6572d99e5bbd07493acfe6d2766f9ca2c6bf611ba28e410f041efc_w640_q70.webp",
          "contributions": "1. A target-agnostic RL framework (ReACT-Drug) that uses protein embeddings to find similar proteins and initialize a biologically relevant fragment search space. 2. A PPO agent that guides molecular generation through a dynamic action space defined by chemically valid, reaction-template-based transformations. 3. Ensures 100% chemical validity and novelty while generating candidates with competitive binding affinity and high synthetic accessibility.",
          "summary": "This paper introduces ReACT-Drug, a reinforcement learning framework for de novo drug design. It uses ESM-2 protein embeddings to find similar proteins and their ligands, decomposes them into fragments to guide a PPO agent, which then builds new molecules using reaction-template-based actions encoded by ChemBERTa. The method generates novel, synthetically accessible drug candidates with high binding affinity and guaranteed chemical validity.",
          "mindmap": "graph LR\n        A[ReACT-Drug] --> B[核心问题/Problem: Navigating vast chemical space for synthesizable, high-affinity drugs];\n        A --> C[主要方法/Method: RL + Protein Embeddings + Reaction-Template Actions];\n        A --> D[关键结果/Results: Novel, valid, synthetically accessible candidates];"
        },
        {
          "title": "Can Agentic AI Match the Performance of Human Data Scientists?",
          "authors": "An Luo, Jin Du, Fangqiao Tian, Xun Xian, Robert Specht, Ganghua Wang, Xuan Bi, Charles Fleming, Jayanth Srinivasa, Ashish Kundu, Mingyi Hong, Jie Ding",
          "institution": "University of Minnesota, University of Chicago, Cisco Research",
          "link": "https://arxiv.org/pdf/2512.20959",
          "code": null,
          "tags": [
            "automated data science",
            "agentic AI",
            "domain knowledge",
            "synthetic data",
            "large language models",
            "human-AI teaming"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79fb0613736763ea339bd898e4056c45f61f97d7ed163ac22b97fef63af1c01a_w640_q70.webp",
          "contributions": "1. Designed a novel prediction task where a critical latent variable is hidden in image data to test the limitations of generic agentic AI workflows. 2. Demonstrated through experiments that current agentic AI systems, which rely on generic code generation, fail to match human data scientists who can leverage domain-specific insights. 3. Highlighted a key limitation of current LLM-driven data science automation and underscored the need for future research to develop AI systems that can better incorporate domain knowledge.",
          "summary": "The paper investigates whether agentic AI can match human data scientists by designing a property insurance prediction task where a crucial variable is hidden in image data. Experiments show that AI relying on generic workflows performs poorly compared to methods using domain-specific insights. The study concludes that current agentic AI has a key limitation in incorporating domain knowledge, highlighting a need for future research in this direction.",
          "mindmap": "graph LR\n    A[Can Agentic AI Match Human Data Scientists?] --> B[核心问题/Problem: Can agentic AI match human performance using domain knowledge?];\n    A --> C[主要方法/Method: Design task with latent variable in images, use synthetic insurance data];\n    A --> D[关键结果/Results: Agentic AI with generic workflow falls short; highlights need for domain-aware AI];"
        },
        {
          "title": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
          "authors": "Zekai Zhang, Xiao Li, Xiang Li, Lianghe Shi, Meng Wu, Molei Tao, Qing Qu",
          "institution": "University of Michigan, Georgia Institute of Technology",
          "link": "https://arxiv.org/pdf/2512.20963",
          "code": "https://deepthink-umich.github.io",
          "tags": [
            "diffusion models",
            "representation learning",
            "denoising autoencoder",
            "memorization detection",
            "representation steering",
            "generalization"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58c26c58c94d92dfa7924dfd1c702b8c3239c0a63a4e6ef6f0eaaf42d7dc410a_w640_q70.webp",
          "contributions": "1. Provides a theoretical analysis linking memorization and generalization in diffusion models to specific representation structures (\"spiky\" vs. \"balanced\") using a two-layer ReLU DAE model. 2. Validates the theoretical findings on real-world unconditional and text-to-image diffusion models, showing the practical emergence of these representation regimes. 3. Proposes a representation-based method for detecting memorization and a training-free editing technique for precise control via representation steering.",
          "summary": "This paper analyzes the distinction between memorization and generalization in diffusion models through representation learning. It theoretically proves that memorization leads to \"spiky\" representations while generalization yields \"balanced\" ones, and validates this on real models. Based on these insights, the authors propose methods for memorization detection and training-free image editing, highlighting the central role of good representations for meaningful generation.",
          "mindmap": "graph LR\n    A[Generalization of Diffusion Models Arises with a Balanced Representation Space] --> B(核心问题/Problem: Diffusion models risk memorizing training data)\n    A --> C(主要方法/Method: Analyze via representation learning using a two-layer ReLU DAE)\n    A --> D(关键结果/Results: Memorization yields spiky representations, generalization yields balanced ones; Enables detection and editing techniques)"
        },
        {
          "title": "Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions",
          "authors": "Linggao Kong, Yuedong Xu, Lei Jiao, Chuan Xu",
          "institution": "Fudan University, University of Oregon, Inria",
          "link": "https://arxiv.org/pdf/2512.20967",
          "code": null,
          "tags": [
            "llm training",
            "spot instance",
            "online scheduling",
            "deadline-aware",
            "LoRA",
            "integer programming"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6addc088c50bc798ccb2da0947c0b05adb32b7751a390fde44b4c5be3c5b85f3_w640_q70.webp",
          "contributions": "1. Formulated an integer programming problem for deadline-aware LLM fine-tuning using a mix of volatile spot and reliable on-demand GPU instances. 2. Proposed a prediction-based online allocation algorithm and a complementary algorithm without predictions, with a policy selection algorithm that learns the best policy from a parameterized pool. 3. Provided theoretical analysis showing the prediction-based algorithm's performance improves with prediction accuracy and that the policy selection algorithm has a sublinear regret bound, with experiments showing up to 54.8% utility improvement.",
          "summary": "This paper tackles the challenge of cost-effective, deadline-aware scheduling for fine-tuning large language models (LLMs) on volatile GPU spot instances. It proposes an online framework that uses a mix of spot and on-demand instances, featuring a prediction-based algorithm, a non-prediction algorithm, and a policy selection mechanism. The framework adapts to market dynamics, is theoretically grounded, and significantly outperforms baselines in experiments.",
          "mindmap": "graph LR\n    A[Deadline-Aware Online Scheduling for LLM Fine-Tuning] --> B(核心问题/Problem: Expensive LLM fine-tuning with volatile spot instances)\n    A --> C(主要方法/Method: Mixed instance scheduling with prediction & online policy selection)\n    A --> D(关键结果/Results: O(√T) regret, up to 54.8% utility gain)"
        },
        {
          "title": "Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions",
          "authors": "Jingyang You, Hanna Kurniawati",
          "institution": "Australian National University",
          "link": "https://arxiv.org/pdf/2512.20974",
          "code": null,
          "tags": [
            "reinforcement learning",
            "Bayesian Reinforcement Learning",
            "Meta-Reinforcement Learning",
            "Generalised Linear Models",
            "Learnable Basis Functions",
            "Variational Inference"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d35dd58d9d51b22dbce9eb7fc7a54a60d532c573648a3a29596e023ac63db13_w640_q70.webp",
          "contributions": "1. Proposes GLiBRL, a novel deep Bayesian RL method using Generalised Linear Models with learnable basis functions for efficient and accurate model learning. 2. Enables fully tractable marginal likelihood and Bayesian inference on task parameters and model noises, avoiding the need to optimize the difficult Evidence Lower Bound (ELBO). 3. Demonstrates significant performance improvements on MetaWorld benchmarks, outperforming state-of-the-art methods like VariBAD and showing low-variance, consistent results.",
          "summary": "This paper addresses the problem of inefficient and unstable model learning in deep Bayesian Reinforcement Learning (BRL), which traditionally relies on optimizing the difficult Evidence Lower Bound (ELBO). The authors propose a new method called GLiBRL, which uses Generalised Linear Models with learnable basis functions to enable tractable marginal likelihood and Bayesian inference. The method significantly improves success rates on challenging MetaWorld benchmarks compared to existing deep BRL and meta-RL approaches.",
          "mindmap": "graph LR\n    A[GLiBRL] --> B[核心问题/Problem: Classical BRL assumes known models, Deep BRL with ELBO is hard to optimize]\n    A --> C[主要方法/Method: Use GLMs with learnable basis for tractable likelihood & inference]\n    A --> D[关键结果/Results: Improves success rate vs. VariBAD (2.7x), low-variance performance]"
        },
        {
          "title": "Automatic Replication of LLM Mistakes in Medical Conversations",
          "authors": "Oleksii Proniakin, Diego Fajardo, Ruslan Nazarenko, Razvan Marinescu",
          "institution": "Lumos AI",
          "link": "https://arxiv.org/pdf/2512.20983",
          "code": null,
          "tags": [
            "llm evaluation",
            "medical conversation",
            "mistake replication",
            "benchmark creation",
            "llm judges",
            "single-shot qa"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7799ccba99ce08a1cee1bd87ba7b9e986a373df0f78a25479ad9fec7478ca0e9_w640_q70.webp",
          "contributions": "1. Introduces MedMistake, an automatic pipeline for extracting and replicating LLM mistakes from complex medical conversations into a benchmark format. 2. Releases MedMistake-All, a dataset of 3,390 single-shot QA pairs derived from identified mistakes, and a validated subset, MedMistake-Bench. 3. Provides a comprehensive evaluation of 12 frontier LLMs using the validated benchmark, revealing performance trends among top models.",
          "summary": "The paper addresses the difficulty of replicating specific mistakes made by LLMs in clinical conversations. It proposes MedMistake, an automated pipeline that generates conversational data, uses LLM judges to identify errors, and distills them into single-shot QA pairs to create a benchmark. The resulting benchmark was used to evaluate 12 LLMs, finding that GPT, Claude, and Grok models performed best.",
          "mindmap": "graph LR\n    A[Automatic Replication of LLM Mistakes in Medical Conversations] --> B(核心问题/Problem: LLM错误难以在其他模型中复现/Mistakes hard to replicate across LLMs)\n    A --> C(主要方法/Method: MedMistake自动管道/MedMistake automatic pipeline)\n    A --> D(关键结果/Results: 发布基准并评估12个LLM/Released benchmark & evaluated 12 LLMs)\n    C --> C1(生成对话/Generate conversations)\n    C --> C2(LLM委员会评估/LLM committee evaluation)\n    C --> C3(创建单轮QA对/Create single-shot QA pairs)\n    D --> D1(MedMistake-All数据集/MedMistake-All dataset)\n    D --> D2(MedMistake-Bench验证子集/MedMistake-Bench validated subset)\n    D --> D3(GPT/Claude/Grok表现最佳/GPT/Claude/Grok performed best)"
        },
        {
          "title": "CoSeNet: A Novel Approach for Optimal Segmentation of Correlation Matrices",
          "authors": "Alberto. Palomo-Alonso, David Casillas-Perez, Silvia Jimenez-Fernandez, Antonio Portilla-Figueras, Sancho Salcedo-Sanz",
          "institution": "Department of Signal Processing and Communications, Universidad de Alcalá (Spain)",
          "link": "https://arxiv.org/pdf/2512.21000",
          "code": null,
          "tags": [
            "correlation matrix segmentation",
            "correlation matrices",
            "segmentation algorithms",
            "metaheuristic optimization",
            "overlapping technique",
            "window difference metric"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a9a31a49fc5fbd3e09151d607fb89a66d2e15d7c89ecd0f67b8be73bebac208_w640_q70.webp",
          "contributions": "1. Proposes CoSeNet, a novel four-layer algorithmic architecture for optimal segmentation of noisy correlation matrices. 2. Introduces a method using a heuristic algorithm to optimize the re-scaling layer parameters based on a Window Difference-based fitness metric. 3. Utilizes an overlapping technique and pre-trained ML algorithms to enhance robustness and generalizability.",
          "summary": "This paper introduces CoSeNet, a novel four-layer model for optimally segmenting correlated groups within noisy correlation matrices. The method uses an overlapping technique, pre-trained ML algorithms, and a heuristic to optimize its re-scaling parameters. The output is a clean, binary segmentation matrix, offering a balanced solution in terms of efficiency, memory, and speed.",
          "mindmap": "graph LR\n    A[CoSeNet: 相关矩阵分割] --> B[核心问题: 噪声相关矩阵中的相关段识别 / Problem: Identifying correlated segments in noisy correlation matrices]\n    A --> C[主要方法: 四层架构与启发式优化 / Method: Four-layer architecture with heuristic optimization]\n    A --> D[关键结果: 生成无噪声二值分割矩阵 / Results: Generates noise-free binary segmentation matrix]\n    C --> C1[输入层 / Input Layer]\n    C --> C2[格式化层 / Formatting Layer]\n    C --> C3[重缩放层 / Re-scaling Layer]\n    C --> C4[分割层 / Segmentation Layer]\n    C3 --> C3a[启发式参数优化 / Heuristic Parameter Optimization]\n    C4 --> C4a[重叠技术与预训练ML / Overlapping Technique & Pre-trained ML]"
        },
        {
          "title": "LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics",
          "authors": "Jiashuo Liu, Jiayun Wu, Chunjie Wu, Jingkai Liu, Zaiyuan Wang, Huan Zhou, Wenhao Huang, Hongseok Namkoong",
          "institution": "ByteDance Seed, Carnegie Mellon University, Columbia University",
          "link": "https://arxiv.org/pdf/2512.21010",
          "code": null,
          "tags": [
            "llm evaluation",
            "Competitive Swiss-System Dynamics",
            "Expected Win Score",
            "Failure Sensitivity Analysis",
            "Monte Carlo Simulation",
            "risk appetite"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c6a4e260d9e6100733ae95995348a1b30295905871b863cf4afa821492e22eb_w640_q70.webp",
          "contributions": "1. Introduces the Competitive Swiss-System Dynamics (CSD) framework, a novel sequential contest simulation for holistic LLM ranking across multiple benchmarks, 2. Proposes the Expected Win Score via Monte Carlo Simulation to provide a statistically robust ranking that reduces noise from random pairing, 3. Implements Failure Sensitivity Analysis to profile models by risk appetite, distinguishing between robust generalists and aggressive specialists.",
          "summary": "The paper addresses the limitations of static, fragmented LLM evaluation by proposing the Competitive Swiss-System Dynamics (CSD) framework, which simulates a multi-round sequential contest to aggregate performance across benchmarks dynamically. It uses Monte Carlo simulation to compute a robust Expected Win Score and includes a Failure Sensitivity Analysis to assess model risk profiles. The authors demonstrate that CSD provides a more nuanced and context-aware ranking than traditional methods, advancing risk-informed LLM evaluation.",
          "mindmap": "graph LR\n    A[LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics] --> B[核心问题/Problem: Fragmented benchmarks and static scoring fail to capture dynamic competitive fitness and risk]\n    A --> C[主要方法/Method: Competitive Swiss-System Dynamics (CSD) with Monte Carlo Simulation and Failure Sensitivity Analysis]\n    A --> D[关键结果/Results: More nuanced, context-aware ranking distinguishing robust generalists vs. aggressive specialists]"
        },
        {
          "title": "Towards Better Search with Domain-Aware Text Embeddings for C2C Marketplaces",
          "authors": "Andre Rusli, Miao Cao, Shoma Ishimoto, Sho Akiyama, Max Frenzel",
          "institution": "Mercari, Inc.",
          "link": "https://arxiv.org/pdf/2512.21021",
          "code": null,
          "tags": [
            "text embeddings",
            "Matryoshka Representation Learning",
            "role-specific prefixes",
            "purchase-driven fine-tuning"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f78c022541a8ea7125744d1404efda5d54f335700045da22b53146a30bf9632_w640_q70.webp",
          "contributions": "1. Proposed a domain-aware Japanese text-embedding model fine-tuned on purchase-driven query-title pairs for C2C marketplace search. 2. Introduced the use of role-specific prefixes to model the query-item asymmetry inherent in search tasks. 3. Applied Matryoshka Representation Learning to create compact, truncation-robust embeddings that meet production latency and throughput constraints.",
          "summary": "This paper addresses the challenge of improving search relevance in noisy, user-generated C2C marketplaces by fine-tuning a Japanese text-embedding model with role-specific prefixes and Matryoshka Representation Learning. The method produces compact embeddings that are robust to truncation for efficiency. Offline and online evaluations show significant improvements in retrieval quality and business metrics, providing a practical foundation for enhanced search experiences.",
          "mindmap": "graph LR\n    A[Towards Better Search with Domain-Aware Text Embeddings for C2C Marketplaces] --> B[核心问题/Problem: C2C搜索挑战<br>Short queries, Noisy listings, Production constraints]\n    A --> C[主要方法/Method: 领域感知嵌入<br>Domain-aware embeddings via fine-tuning, Role prefixes, Matryoshka Learning]\n    A --> D[关键结果/Results: 离线与在线提升<br>Offline gains, Online revenue & efficiency improvements]"
        },
        {
          "title": "Agentic Multi-Persona Framework for Evidence-Aware Fake News Detection",
          "authors": "Roopa Bukke, Soumya Pandey, Suraj Kumar, Soumi Chattopadhyay, Chandranath Adak",
          "institution": "Indian Institute of Technology (Indore, Patna)",
          "link": "https://arxiv.org/pdf/2512.21039",
          "code": null,
          "tags": [
            "misinformation detection",
            "multi-persona agent",
            "LLM-SLM synergy",
            "evidence-grounded",
            "multimodal fusion",
            "credibility fusion"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e8f4708c11ed1d6f5c25e0cab8a4e68e02e81ca73057ce067c85265f0213b46_w640_q70.webp",
          "contributions": "1. Proposed AMPEND-LS, an agentic multi-persona framework that integrates textual, visual, and contextual evidence through a structured LLM reasoning pipeline. 2. Introduced a credibility fusion mechanism combining semantic similarity, domain trustworthiness, and temporal context to improve reliability. 3. Designed a complementary SLM classifier to mitigate LLM uncertainty and hallucinations, enhancing robustness and explainability.",
          "summary": "The paper addresses the challenge of multimodal fake news detection by proposing AMPEND-LS, a framework that synergizes LLMs and SLMs within a multi-persona agent structure to reason over diverse evidence. It demonstrates superior performance over state-of-the-art baselines in accuracy and robustness across multiple datasets. The work contributes to developing more adaptive and explainable systems for combating online misinformation.",
          "mindmap": "graph LR\n    A[AMPEND-LS] --> B[核心问题/Problem: 虚假新闻检测的挑战/Fake News Detection Challenges];\n    A --> C[主要方法/Method: 多角色智能体与证据融合/Agentic Multi-Persona & Evidence Fusion];\n    A --> D[关键结果/Results: 性能超越基线/Outperforms SOTA Baselines];\n    B --> B1[多模态内容/Multimodal Content];\n    B --> B2[领域泛化/Domain Generalization];\n    B --> B3[可解释性/Explainability];\n    C --> C1[LLM-SLM协同/LLM-SLM Synergy];\n    C --> C2[可信度融合/Credibility Fusion];\n    C --> C3[结构化推理/Structured Reasoning];\n    D --> D1[高准确率与F1/High Accuracy & F1];\n    D --> D2[强鲁棒性/Robustness];\n    D --> D3[透明推理/Transparent Reasoning];"
        },
        {
          "title": "zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy",
          "authors": "Savvy Sharma, George Petrovic, Sarthak Kaushik",
          "institution": "George Brown Polytechnic",
          "link": "https://arxiv.org/pdf/2512.21048",
          "code": null,
          "tags": [
            "federated learning",
            "zero-knowledge proofs",
            "trusted execution environments",
            "blockchain",
            "medical AI",
            "verifiable aggregation"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaf3da543259b835ceee63f63b6675f8ca4fdd248035da93b3582ae546b60093_w640_q70.webp",
          "contributions": "1. Proposes a novel architecture (zkFL-Health) that integrates Federated Learning with Zero-Knowledge Proofs and Trusted Execution Environments to ensure privacy and verifiable correctness in medical AI training. 2. Introduces a protocol where the aggregator, operating within a TEE, generates a succinct ZK proof to attest it used the correct inputs and aggregation rule, without revealing client updates. 3. Leverages a blockchain to provide an immutable audit trail of cryptographic commitments and proof verification, removing the need to trust a single party and enhancing regulatory compliance.",
          "summary": "The paper addresses privacy leakage and trust issues in federated learning for healthcare by proposing zkFL-Health, a framework that combines FL with zero-knowledge proofs and TEEs to enable verifiable and private model aggregation. The method ensures the aggregator's computations are provably correct and recorded on a blockchain for auditability. The conclusion is that this approach provides strong confidentiality, integrity, and auditability, which are crucial for clinical adoption.",
          "mindmap": "graph LR\n    A[zkFL-Health] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[隐私泄露与聚合器信任/Privacy Leakage & Aggregator Trust]\n    C --> C1[FL+ZKP+TEE/FL+ZKP+TEE]\n    C --> C2[链上验证/On-chain Verification]\n    D --> D1[可验证的隐私保护/Verifiable Privacy]\n    D --> D2[审计与合规/Auditability & Compliance]"
        },
        {
          "title": "DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors",
          "authors": "Kaustubh Kundu, Hrishav Bakul Barua, Lucy Robertson-Bell, Zhixi Cai, Kalin Stefanov",
          "institution": "Monash University, TCS Research",
          "link": "https://arxiv.org/pdf/2512.21054",
          "code": "https://github.com/kaustesseract/DexAvatar",
          "tags": [
            "3D human pose estimation",
            "3D sign language reconstruction",
            "biomechanical accuracy",
            "hand and body pose priors",
            "monocular video",
            "SMPL-X"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/594bef871fe9a00d58a9f3f12c9a0b4bf4f66d3d738bd3f02dedcbad04bdcd25_w640_q70.webp",
          "contributions": "1. A novel framework (DexAvatar) for reconstructing biomechanically accurate 3D hand and body poses from monocular sign language videos. 2. The use of learned 3D hand and body pose priors to guide the reconstruction and overcome challenges like self-occlusion and motion blur. 3. Demonstrating strong performance on the SGNify benchmark, achieving a 35.11% improvement over the state-of-the-art.",
          "summary": "The paper introduces DexAvatar, a framework that uses learned 3D hand and body pose priors to reconstruct accurate 3D sign language poses from monocular videos. It addresses the limitations of existing 2D datasets and noisy 3D estimations. The method significantly outperforms prior work on the SGNify motion capture benchmark.",
          "mindmap": "graph LR\n        A[DexAvatar] --> B[核心问题/Problem: 手语视频缺乏准确3D数据，现有3D姿态估计质量差]\n        A --> C[主要方法/Method: 利用学习到的3D手部和身体姿态先验，从单目视频重建]\n        A --> D[关键结果/Results: 在SGNify数据集上性能提升35.11%]"
        },
        {
          "title": "Understanding Scaling Laws in Deep Neural Networks via Feature Learning Dynamics",
          "authors": "Zihan Yao, Ruoyu Wu, Tianxiang Gao",
          "institution": "DePaul University, Iowa State University",
          "link": "https://arxiv.org/pdf/2512.21075",
          "code": null,
          "tags": [
            "deep learning theory",
            "scaling laws",
            "feature learning",
            "infinite-depth limit",
            "ResNets",
            "hyperparameter transfer"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06065072ce8d20b2298a45760b95c3f905a6aff3d726e09d4ddf1ecd2e9cc359_w640_q70.webp",
          "contributions": "1. Derives Neural Feature Dynamics (NFD), a theoretical framework characterizing feature learning in ResNets in the joint infinite-width and infinite-depth limit. 2. Identifies a vanishing mechanism induced by 1/√depth scaling that explains feature-learning collapse in deep networks and the failure of depth-µP. 3. Proposes a practical depth-aware learning-rate correction to counteract the collapse and restore depth-wise hyperparameter transfer for improved performance.",
          "summary": "This paper addresses the lack of theoretical understanding behind scaling laws in deep learning by analyzing feature learning dynamics in deep ResNets. It proposes the Neural Feature Dynamics (NFD) framework in the infinite-width and depth limit, which explains when scaling succeeds and identifies a cause of feature collapse. Based on this insight, the authors propose a simple learning-rate correction that improves training stability and performance in deeper networks.",
          "mindmap": "graph LR\n    A[Understanding Scaling Laws via Feature Learning Dynamics] --> B[核心问题/Problem: Scaling laws describe success but not when/why scaling fails]\n    A --> C[主要方法/Method: Derive Neural Feature Dynamics (NFD) in infinite-width & depth limit]\n    A --> D[关键结果/Results: Explains diminishing returns, proposes depth-aware LR correction]"
        },
        {
          "title": "Blurb-Refined Inference from Crowdsourced Book Reviews using Hierarchical Genre Mining with Dual-Path Graph Convolutions",
          "authors": "Suraj Kumar, Utsav Kumar Nareti, Soumi Chattopadhyay, Chandranath Adak, Prolay Mallick",
          "institution": "Indian Institute of Technology Indore, Indian Institute of Technology Patna",
          "link": "https://arxiv.org/pdf/2512.21076",
          "code": null,
          "tags": [
            "text classification",
            "hierarchical genre classification",
            "zero-shot semantic alignment",
            "dual-path graph convolution",
            "label co-occurrence graph",
            "blurb-refined inference"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3afc3f4ebea9058118426cd2d72f37e4c09ae5bcc05f191e73c452f78fc501af_w640_q70.webp",
          "contributions": "1. Proposes a two-phase hierarchical genre mining framework (HiGeMine) that integrates noisy user reviews with authoritative blurbs using a zero-shot semantic alignment strategy to filter reviews. 2. Introduces a dual-path, two-level graph-based classification architecture that models inter-genre dependencies via a label co-occurrence graph for coarse and fine-grained prediction. 3. Curates a new hierarchical book genre dataset to facilitate systematic evaluation of the proposed method.",
          "summary": "This paper addresses the problem of noisy and unreliable book genre classification by proposing HiGeMine, a framework that filters user reviews using semantic alignment with book blurbs and then performs hierarchical classification using a dual-path graph-based model. The method outperforms baselines on a newly curated dataset, demonstrating an effective solution for leveraging structured and unstructured text in hierarchical genre analysis.",
          "mindmap": "graph LR\n    A[HiGeMine: Blurb-Refined Inference from Crowdsourced Book Reviews] --> B[核心问题/Problem: Noisy reviews & flat genre classification degrade reliability]\n    A --> C[主要方法/Method: Two-phase framework: 1. Zero-shot review filtering 2. Dual-path graph classification]\n    A --> D[关键结果/Results: Outperforms baselines on new hierarchical dataset]"
        },
        {
          "title": "LLM Personas as a Substitute for Field Experiments in Method Benchmarking",
          "authors": "Enoch Hyunwook Kang",
          "institution": "University of Washington",
          "link": "https://arxiv.org/pdf/2512.21080",
          "code": null,
          "tags": [
            "algorithmic fairness & evaluation",
            "field experiments",
            "A/B testing",
            "LLM personas",
            "algorithmic benchmarking",
            "information-theoretic bounds"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f1e74907f157cdb694f3120aed988d1affab03b63adb6bf73297f43733c1b8ba_w640_q70.webp",
          "contributions": "1. Provides a formal, if-and-only-if characterization of the conditions (aggregate-only observation, algorithm-blind evaluation) under which swapping humans for LLM personas is a valid benchmark substitution, equivalent to changing the evaluation panel. 2. Moves from validity to usefulness by defining an information-theoretic measure of discriminability for the aggregate channel induced by persona simulation. 3. Derives explicit sample-size bounds on the number of independent persona evaluations required to make persona benchmarking as decision-relevant as a field experiment for distinguishing between methods.",
          "summary": "The paper addresses the high cost and latency of field experiments (A/B tests) for benchmarking methods in societal systems by proposing LLM-based persona simulation as a synthetic alternative. It formally proves the conditions under which this substitution is valid and provides information-theoretic bounds on the required number of persona evaluations to make the benchmark useful. The main conclusion is that persona benchmarking can be a viable, efficient substitute for field experiments under specific, well-defined conditions.",
          "mindmap": "graph LR\n    A[LLM Personas as a Substitute for Field Experiments] --> B[核心问题/Problem: Field experiments are costly and slow, hindering iterative method development.]\n    A --> C[主要方法/Method: Use LLM-based persona simulation as a cheap synthetic benchmark under specific conditions.]\n    A --> D[关键结果/Results: Formal validity conditions proven; sample-size bounds derived for decision-relevance.]"
        },
        {
          "title": "Hierarchical Modeling Approach to Fast and Accurate Table Recognition",
          "authors": "Takaya Kawakatsu",
          "institution": "Preferred Networks, Inc.",
          "link": "https://arxiv.org/pdf/2512.21083",
          "code": null,
          "tags": [
            "document analysis",
            "table recognition",
            "multi-task learning",
            "non-causal attention",
            "parallel inference",
            "hierarchical modeling"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd3e215f24ffc72ff43528e39e86c65289bbfb5de75955d48f0284f277f0089e_w640_q70.webp",
          "contributions": "1. A novel multi-task model utilizing non-causal attention to capture the entire table structure. 2. A parallel inference algorithm for cell content recognition that significantly speeds up inference. 3. A hierarchical modeling approach that extends the performance of multi-task models while addressing their speed and explainability limitations.",
          "summary": "This paper addresses the slow inference speed and lack of explainability in existing table recognition models. It proposes a new multi-task model with non-causal attention for global structure understanding and a parallel inference algorithm to decode cell contents simultaneously, achieving both superior accuracy and a 10x+ speedup on large public datasets.",
          "mindmap": "graph LR\n    A[Hierarchical Modeling Approach to Fast and Accurate Table Recognition] --> B(核心问题/Problem: 现有表格识别模型推理慢且有效性未充分解释/Existing models are slow and their effectiveness is not fully explained)\n    A --> C(主要方法/Method: 使用非因果注意力的多任务模型与并行推理算法/Novel multi-task model with non-causal attention & parallel inference algorithm)\n    A --> D(关键结果/Results: 在大型公开数据集上实现视觉与统计上的优越性/Superiority demonstrated visually and statistically on two large public datasets)"
        },
        {
          "title": "Dyna-Style Reinforcement Learning Modeling and Control of Non-linear Dynamics",
          "authors": "Karim Abdelsalam, Zeyad Gamal, Ayman El-Badawy",
          "institution": "German University in Cairo",
          "link": "https://arxiv.org/pdf/2512.21081",
          "code": null,
          "tags": [
            "reinforcement learning",
            "Dyna-Style RL",
            "SINDy",
            "TD3",
            "Model-based RL",
            "Bi-rotor Control"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/508875b78acef042533866808253222b6799135bb65f28295e2b374106b10052_w640_q70.webp",
          "contributions": "1. Proposes a Dyna-Style RL framework that integrates SINDy for data-driven dynamics modeling with TD3 for policy learning., 2. Introduces a method to periodically inject synthetic rollouts from the learned SINDy model into the RL replay buffer to improve sample efficiency., 3. Demonstrates the framework's effectiveness on a bi-rotor system, showing superior accuracy and robustness in stabilization and trajectory tracking compared to direct model-free RL.",
          "summary": "This paper proposes a hybrid control framework combining Sparse Identification of Nonlinear Dynamics (SINDy) and Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning to efficiently control nonlinear systems. The SINDy model generates synthetic data to augment real-world training, improving sample efficiency. The method is validated on a bi-rotor system, showing better performance than direct model-free RL.",
          "mindmap": "graph LR\n    A[Dyna-Style RL Modeling and Control of Non-linear Dynamics] --> B[核心问题/Problem<br>控制复杂非线性系统<br>Controlling Complex Nonlinear Systems]\n    A --> C[主要方法/Method<br>SINDy + TD3 混合框架<br>SINDy + TD3 Hybrid Framework]\n    A --> D[关键结果/Results<br>在双旋翼系统上性能更优<br>Superior Performance on Bi-rotor]\n    B --> E[样本效率低<br>Sample Inefficiency]\n    C --> F[SINDy识别模型<br>SINDy Identifies Model]\n    C --> G[生成合成数据<br>Generates Synthetic Rollouts]\n    C --> H[TD3学习策略<br>TD3 Learns Policy]\n    D --> I[精度与鲁棒性提升<br>Improved Accuracy & Robustness]"
        },
        {
          "title": "Shared Representation Learning for High-Dimensional Multi-Task Forecasting under Resource Contention in Cloud-Native Backends",
          "authors": "Zixiao Huang, Jixiao Yang, Sijia Li, Chi Zhang, Jinyu Chen, Chengda Xu",
          "institution": "University of Washington, Westcliff University, University of Michigan, Northeastern University, University of Virginia",
          "link": "https://arxiv.org/pdf/2512.21102",
          "code": null,
          "tags": [
            "others",
            "multi-task time series forecasting",
            "shared representation learning",
            "cloud-native systems",
            "resource contention",
            "dynamic adjustment mechanism"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9838b10e22d58d1ccca2c68a210133b154701b0c5ff14deadba418ccdbdb92f2_w640_q70.webp",
          "contributions": "1. A unified forecasting framework with a shared encoding structure for high-dimensional, multi-task time series in cloud-native backends. 2. A cross-task structural propagation module to model complex dependencies among nodes caused by resource contention and service topology changes. 3. A dynamic adjustment mechanism that regulates internal feature flows to adapt to non-stationary behaviors like sudden load shifts.",
          "summary": "This paper proposes a unified forecasting framework for high-dimensional multi-task time series in cloud-native backend systems. The method uses shared representation learning, a structural propagation module, and a dynamic adjustment mechanism to model complex dependencies and adapt to non-stationary behaviors. Experimental results show the framework achieves superior performance and provides reliable predictive capability for dynamic cloud environments.",
          "mindmap": "graph LR\n    A[Shared Representation Learning for High-Dimensional Multi-Task Forecasting under Resource Contention in Cloud-Native Backends] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[高维多任务时序预测/High-Dimensional Multi-Task Forecasting]\n    B --> B2[云原生后端动态环境/Cloud-Native Backend Dynamics]\n    C --> C1[共享编码结构/Shared Encoding Structure]\n    C --> C2[跨任务结构传播/Cross-Task Structural Propagation]\n    C --> C3[动态调整机制/Dynamic Adjustment Mechanism]\n    D --> D1[性能优越/Superior Performance]\n    D --> D2[可靠预测能力/Reliable Predictive Capability]"
        },
        {
          "title": "Semi-Supervised Learning for Large Language Models Safety and Content Moderation",
          "authors": "Eduard Stefan Dinuta, Iustin Sirbu, Traian Rebedea",
          "institution": "National University of Science and Technology Politehnica Bucharest, Renius Technologies, NVIDIA",
          "link": "https://arxiv.org/pdf/2512.21107",
          "code": null,
          "tags": [
            "content moderation",
            "semi-supervised learning",
            "data augmentation",
            "safety classifiers",
            "LLM safety",
            "prompt harmfulness"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/035c08c88d89969ce37594942a40aa577a3c0c7c7743cd71bdf84366a9dfa5f2_w640_q70.webp",
          "contributions": "1. Analysis of state-of-the-art semi-supervised learning algorithms for LLM safety, focusing on both prompt and response harmfulness. 2. Introduction of a new, task-specific augmentation technique for safety tasks. 3. Demonstration that task-specific augmentations significantly outperform general-purpose methods like backtranslation.",
          "summary": "This paper addresses the challenge of acquiring high-quality labeled data for training safety classifiers for Large Language Models. It proposes using semi-supervised learning techniques that leverage both labeled and unlabeled data, and introduces a task-specific data augmentation method. The key finding is that this approach, particularly with custom augmentations, significantly improves performance on safety tasks compared to using general-purpose techniques.",
          "mindmap": "graph LR\n    A[论文标题 / Paper Title<br>Semi-Supervised Learning for LLM Safety] --> B[核心问题 / Problem<br>依赖大量标注数据 / Reliance on large labeled data]\n    A --> C[主要方法 / Method<br>半监督学习与任务特定增强 / SSL & Task-Specific Augmentation]\n    A --> D[关键结果 / Results<br>性能显著提升 / Significant Performance Improvement]"
        },
        {
          "title": "Semantic Refinement with LLMs for Graph Representations",
          "authors": "Safal Thapaliya, Zehong Wang, Jiazheng Li, Ziming Li, Yanfang Ye, Chuxu Zhang",
          "institution": "University of Connecticut, University of Notre Dame",
          "link": "https://arxiv.org/pdf/2512.21106",
          "code": null,
          "tags": [
            "graph representation learning",
            "graph neural network",
            "large language model",
            "semantic refinement",
            "structure-semantics heterogeneity",
            "data-centric adaptation"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfae73c72759ca834d898f3c6ca5f0824bada06285918fca678e0f809fce9afd_w640_q70.webp",
          "contributions": "1. Proposes a data-centric perspective to address structure-semantics heterogeneity in graphs by treating node semantics as a task-adaptive variable, shifting focus from model-centric inductive bias injection. 2. Introduces the Data-Adaptive Semantic Refinement (DAS) framework, which couples a fixed GNN and an LLM in a closed feedback loop for iterative semantic refinement and graph learning. 3. Demonstrates the framework's effectiveness on diverse graphs, showing consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs.",
          "summary": "This paper addresses the challenge of structure-semantics heterogeneity in graph data, where predictive signals vary across domains. It proposes a Data-Adaptive Semantic Refinement (DAS) framework that uses a closed feedback loop between a GNN and an LLM to iteratively refine node semantics for the learning task. The method shows strong performance on structure-dominated graphs and remains competitive on semantics-rich graphs, validating the data-centric adaptation approach.",
          "mindmap": "graph LR\n    A[Semantic Refinement with LLMs for Graph Representations] --> B(核心问题/Problem: Graph structure-semantics heterogeneity 图的结构-语义异质性)\n    A --> C(主要方法/Method: Data-Adaptive Semantic Refinement (DAS) framework 数据自适应语义精炼框架)\n    A --> D(关键结果/Results: Improves structure-dominated graphs, competitive on semantics-rich graphs 提升结构主导图性能，在语义丰富图上保持竞争力)"
        },
        {
          "title": "STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting",
          "authors": "Shi Quan Foo, Chi-Ho Wong, Zhihan Gao, Dit-Yan Yeung, Ka-Hing Wong, Wai-Kin Wong",
          "institution": "The Hong Kong University of Science and Technology, Hong Kong Observatory",
          "link": "https://arxiv.org/pdf/2512.21118",
          "code": "https://github.com/sqfoo/stldm_official",
          "tags": [
            "diffusion models",
            "precipitation nowcasting",
            "latent diffusion model",
            "spatio-temporal prediction",
            "variational autoencoder",
            "conditioning network"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3f04a94a2a07676690c2f108f7a602a6b052c1463701d217a319cd81e05ecce_w640_q70.webp",
          "contributions": "1. Proposes STLDM, a novel two-stage diffusion-based architecture for precipitation nowcasting that combines deterministic forecasting with generative enhancement. 2. Introduces an end-to-end learning framework that jointly trains a Variational Autoencoder, a conditioning network, and a latent diffusion model. 3. Demonstrates superior performance and improved inference efficiency compared to state-of-the-art methods on multiple radar datasets.",
          "summary": "The paper addresses the challenge of precipitation nowcasting, where deterministic models produce blurry predictions and generative models suffer from poor accuracy. It proposes STLDM, a spatio-temporal latent diffusion model that decomposes the task into a deterministic forecasting stage and a generative enhancement stage. Experiments show STLDM outperforms state-of-the-art methods while being more efficient.",
          "mindmap": "graph LR\n    A[STLDM: 降水临近预报模型] --> B[核心问题/Problem: 确定性模型模糊，生成模型精度差]\n    A --> C[主要方法/Method: 两阶段潜扩散模型]\n    A --> D[关键结果/Results: 性能优越，推理高效]"
        },
        {
          "title": "A Mechanistic Analysis of Transformers for Dynamical Systems",
          "authors": "Gregory Duthé, Nikolaos Evangelou, Wei Liu, Ioannis G. Kevrekidis, Eleni Chatzi",
          "institution": "ETH Zürich, Johns Hopkins University, Singapore-ETH Centre",
          "link": "https://arxiv.org/pdf/2512.21113",
          "code": null,
          "tags": [
            "dynamical systems modeling",
            "transformers",
            "attention mechanism",
            "dynamical systems",
            "representational analysis",
            "delay embedding"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ad3a459d499a20015cb02bd1e25bf788476d675d70043dd137b07a4b0f35b55_w640_q70.webp",
          "contributions": "1. Provides a mechanistic interpretation of causal self-attention in Transformers as a linear, history-dependent recurrence from a dynamical systems perspective. 2. Demonstrates that the softmax attention's convexity constraint fundamentally limits the representation of certain linear dynamics, leading to oversmoothing. 3. Shows that for nonlinear partially observable systems, attention acts as an adaptive delay-embedding mechanism for state reconstruction.",
          "summary": "This paper investigates the representational capabilities of single-layer Transformers for modeling dynamical systems, interpreting attention as a history-dependent recurrence. Through linear and nonlinear case studies, it finds that softmax attention restricts representable linear dynamics but can enable state reconstruction in nonlinear systems via adaptive delay embedding. The work bridges empirical Transformer performance with classical dynamical systems theory.",
          "mindmap": "graph LR\n    A[论文标题/Paper Title: A Mechanistic Analysis of Transformers for Dynamical Systems] --> B[核心问题/Problem: Transformers as black boxes for time-series, lack of dynamical systems theory understanding];\n    A --> C[主要方法/Method: Interpret causal self-attention as linear recurrence, analyze via linear/nonlinear case studies];\n    A --> D[关键结果/Results: Softmax restricts linear dynamics (oversmoothing), attention enables nonlinear state reconstruction via delay embedding];"
        },
        {
          "title": "AutoBaxBuilder: Bootstrapping Code Security Benchmarking",
          "authors": "Tobias von Arx, Niels Mündler, Mark Vero, Maximilian Baader, Martin Vechev",
          "institution": "ETH Zurich, Snyk, INSAIT (Sofia University \"St. Kliment Ohridski\")",
          "link": "https://arxiv.org/pdf/2512.21132",
          "code": "https://github.com/eth-sri/autobaxbuilder",
          "tags": [
            "code security evaluation",
            "automated benchmarking",
            "LLM-generated code",
            "security vulnerabilities",
            "exploit generation",
            "plausibility checks"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385abd6729afb970eba2217cc3d408efe70ab80f9d7aa0cbe7c2e0254f48b74c_w640_q70.webp",
          "contributions": "1. Introduces AutoBaxBuilder, a framework for generating code security benchmarking tasks and tests from scratch, addressing the limitations of manual benchmarks. 2. Proposes a robust pipeline with fine-grained plausibility checks that leverages LLMs to construct functionality tests and end-to-end security exploits. 3. Releases AutoBaxBench, a new benchmark of generated tasks, and demonstrates the framework's efficiency (under 2 hours and $10 per task) and quality through comparison with human-crafted tasks.",
          "summary": "The paper presents AutoBaxBuilder, a framework that automatically generates tasks and tests for benchmarking the security of code produced by large language models (LLMs). It uses an LLM-powered pipeline to create functional tests and security exploits, ensuring benchmark quality and scalability. The authors show the method is efficient and release a new benchmark, AutoBaxBench, to evaluate LLM security capabilities.",
          "mindmap": "graph LR\n    A[AutoBaxBuilder] --> B[核心问题/Problem: Manual security benchmarks are insufficient]\n    A --> C[主要方法/Method: Auto-generate tasks & tests with LLM pipeline]\n    A --> D[关键结果/Results: New benchmark, low cost, under 2 hours/task]"
        },
        {
          "title": "MODE: Multi-Objective Adaptive Coreset Selection",
          "authors": "Tanmoy Mukherjee, Pierre Marquis, Zied Bouraoui",
          "institution": "CRIL, Université d'Artois",
          "link": "https://arxiv.org/pdf/2512.21152",
          "code": null,
          "tags": [
            "others",
            "coreset selection",
            "submodular maximization",
            "data efficiency",
            "adaptive weighting",
            "multi-objective optimization"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8cefebbb0215d55d5050eb92783788b0acf7386684074cdf20b76685dfef159_w640_q70.webp",
          "contributions": "1. Proposes MODE, a dynamic framework that adaptively combines multiple coreset selection strategies based on their real-time contribution to model performance across different training phases. 2. Provides theoretical guarantees, achieving a (1-1/e)-approximation for the coreset selection problem with O(n log n) complexity and convergence bounds for strategy weights. 3. Demonstrates practical benefits including reduced memory requirements and provides interpretable insights into the evolution of data utility during training.",
          "summary": "The paper addresses the challenge of selecting small, representative data subsets (coresets) for efficient deep learning by proposing MODE, a framework that dynamically adapts selection criteria (like class balance, diversity, and uncertainty) to different training phases. It shows that MODE achieves strong theoretical approximation guarantees and competitive model accuracy while reducing computational and memory costs.",
          "mindmap": "graph LR\n    A[MODE: Multi-Objective Adaptive Coreset Selection] --> B[核心问题/Problem: Static coreset selection methods cannot adapt to changing data utility during training.]\n    A --> C[主要方法/Method: Dynamic, multi-objective framework that adaptively weights selection strategies based on training phase.]\n    A --> D[关键结果/Results: (1-1/e)-approximation, O(n log n) complexity, reduced memory, interpretable insights.]"
        },
        {
          "title": "ElfCore: A 28nm Neural Processor Enabling Dynamic Structured Sparse Training and Online Self-Supervised Learning with Activity-Dependent Weight Update",
          "authors": "Zhe Su, Giacomo Indiveri",
          "institution": "Institute of Neuroinformatics, University of Zurich and ETH Zurich",
          "link": "https://arxiv.org/pdf/2512.21153",
          "code": null,
          "tags": [
            "on-device ai",
            "spiking neural network",
            "self-supervised learning",
            "structured sparsity",
            "activity-dependent update",
            "event-driven processing"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d2912d0f31a3824ddb49ba116a841b201d285560ef2177125f8dac188572270_w640_q70.webp",
          "contributions": "1. A local online self-supervised learning engine enabling multi-layer temporal learning without labeled data. 2. A dynamic structured sparse training engine supporting high-accuracy sparse-to-sparse learning. 3. An activity-dependent sparse weight update mechanism that gates updates based on input activity and network dynamics.",
          "summary": "This paper introduces ElfCore, a 28nm digital spiking neural network processor designed for event-driven sensory processing. It uniquely integrates online self-supervised learning, dynamic structured sparse training, and activity-dependent weight updates. The chip demonstrates significant improvements in power consumption, memory efficiency, and network capacity across various tasks like gesture and speech recognition.",
          "mindmap": "graph LR\n    A[ElfCore] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[边缘设备需要高效、自适应的稀疏事件处理/Edge devices need efficient, adaptive sparse event processing]\n    C --> C1[集成在线自监督学习/Integrate Online Self-Supervised Learning]\n    C --> C2[动态结构化稀疏训练/Dynamic Structured Sparse Training]\n    C --> C3[活动依赖权重更新/Activity-Dependent Weight Update]\n    D --> D1[功耗降低16倍/16x Lower Power]\n    D --> D2[片上内存减少3.8倍/3.8x Less On-Chip Memory]\n    D --> D3[网络容量效率提升5.9倍/5.9x Greater Network Capacity]"
        },
        {
          "title": "BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft",
          "authors": "Qizhi Wang",
          "institution": "PingCAP, Data & AI-Innovation Lab",
          "link": "https://arxiv.org/pdf/2512.21165",
          "code": null,
          "tags": [
            "distributed consensus",
            "Raft",
            "election timeout",
            "contextual bandits",
            "LinUCB",
            "fault tolerance"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75bff5f2f039d6eaeb1861fcd564ebda78e1b3bd0f91a85b3fc977c335d273e6_w640_q70.webp",
          "contributions": "1. BALLAST, a lightweight contextual-bandit framework for Raft election timeouts with safe exploration and non-stationary adaptation. 2. A reproducible evaluation methodology (discrete-event simulation, fault injection, protocol-level logging, CI-based aggregation) to study election stability under tail latency and recovery turbulence. 3. Demonstration that BALLAST substantially reduces recovery time and unwritable time compared to standard heuristics in challenging WAN regimes.",
          "summary": "This paper addresses the problem of leader-election instability in the Raft consensus protocol under variable network conditions like long-tail latency. It proposes BALLAST, a method that uses online linear contextual bandits to adaptively select election timeouts, augmented with safe exploration. The evaluation shows that BALLAST significantly improves recovery performance in unstable WAN environments while remaining competitive in stable settings.",
          "mindmap": "graph LR\n    A[BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft] --> B(核心问题/Problem: Brittle randomized timeouts under long-tail latency & jitter)\n    A --> C(主要方法/Method: Lightweight online adaptation with contextual bandits & safe exploration)\n    A --> D(关键结果/Results: Reduces recovery/unwritable time in WAN, competitive in stable settings)"
        },
        {
          "title": "A Community-Enhanced Graph Representation Model for Link Prediction",
          "authors": "Lei Wang, Darong Lai",
          "institution": "Southeast University",
          "link": "https://arxiv.org/pdf/2512.21166",
          "code": null,
          "tags": [
            "graph representation learning",
            "Graph Neural Networks",
            "Link Prediction",
            "Community Detection",
            "Structure Enhancement"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/154686865f18f6411728ca7ec41c4f9a507cd6a00a6b336c6fbfca553149d9b1_w640_q70.webp",
          "contributions": "1. Proposes a Community-Enhanced Link Prediction (CELP) framework that integrates community structure to jointly model local and global graph topology. 2. Introduces a graph enhancement technique via community-aware, confidence-guided edge completion and pruning. 3. Integrates multi-scale structural features to improve link prediction accuracy, with experimental validation showing superior performance.",
          "summary": "The paper addresses the limitation of Graph Neural Networks (GNNs) in link prediction, where they often underperform traditional heuristic methods due to over-reliance on local information. It proposes the CELP framework, which incorporates community structure to enhance the graph and capture multi-scale features. Experimental results show CELP achieves superior performance, validating the importance of community structure for accurate link prediction.",
          "mindmap": "graph LR\n        A[A Community-Enhanced Graph Representation Model for Link Prediction] --> B[核心问题/Problem: GNNs for link prediction underperform traditional heuristics due to local focus and over-smoothing.]\n        A --> C[主要方法/Method: Proposes CELP framework integrating community structure for graph enhancement and multi-scale feature learning.]\n        A --> D[关键结果/Results: Superior performance on benchmarks, validating the role of community structure.]"
        },
        {
          "title": "A Unified Framework for EEG Seizure Detection Using Universum-Integrated Generalized Eigenvalues Proximal Support Vector Machine",
          "authors": "Yogesh Kumar, Vrushank Ahire, M. A. Ganaie",
          "institution": "Indian Institute of Technology Ropar",
          "link": "https://arxiv.org/pdf/2512.21170",
          "code": null,
          "tags": [
            "machine learning for healthcare",
            "Universum learning",
            "GEPSVM",
            "EEG classification"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ebfba0923a4428be5bd05b82efd0f6ff72c162a8ee710189c4ccb146c2374f3_w640_q70.webp",
          "contributions": "1. Proposes U-GEPSVM, a novel classifier integrating Universum constraints into the GEPSVM framework via a ratio-based objective function. 2. Introduces IU-GEPSVM, an improved variant with a weighted difference-based formulation for enhanced stability and independent control over class separation and Universum alignment. 3. Demonstrates the effectiveness of the proposed models for EEG seizure detection, achieving superior accuracy on the Bonn University dataset and validating improvements with rigorous statistical tests.",
          "summary": "This paper proposes two novel classifiers, U-GEPSVM and IU-GEPSVM, which combine the computational efficiency of generalized eigenvalue decomposition with Universum learning to improve EEG seizure detection. The models are designed to handle challenges like non-stationarity and limited labeled data in EEG signals. Evaluation on the Bonn University dataset shows that IU-GEPSVM outperforms baseline methods, providing an efficient and reliable solution for neurological diagnosis.",
          "mindmap": "graph LR\n    A[论文标题 / Paper Title: A Unified Framework for EEG Seizure Detection] --> B(核心问题 / Problem: EEG信号分类挑战 / EEG Signal Classification Challenges)\n    A --> C(主要方法 / Method: 提出U-GEPSVM和IU-GEPSVM / Propose U-GEPSVM and IU-GEPSVM)\n    A --> D(关键结果 / Results: IU-GEPSVM取得更高准确率 / IU-GEPSVM Achieves Higher Accuracy)\n    B --> B1(非平稳性, 低信噪比, 标记数据有限 / Non-stationarity, Low SNR, Limited Labeled Data)\n    C --> C1(结合通用集学习和广义特征值分解 / Integrates Universum Learning & GEPSVM)\n    D --> D1(O vs S: 85%峰值准确率 / O vs S: 85% Peak Accuracy)\n    D --> D2(Z vs S: 80%峰值准确率 / Z vs S: 80% Peak Accuracy)"
        },
        {
          "title": "Analytic and Variational Stability of Deep Learning Systems",
          "authors": "Ronald Katende",
          "institution": "Kabale University",
          "link": "https://arxiv.org/pdf/2512.21208",
          "code": null,
          "tags": [
            "learning theory",
            "Learning Stability Profile",
            "Lyapunov methods",
            "Clarke generalized derivatives",
            "stability exponents",
            "energy-dissipative systems"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43ea5a1a8d6262f5e2056565c57706a69bbe8f558d9c82e401fa498ff92d5daa_w640_q70.webp",
          "contributions": "1. Proposes a unified analytic and variational framework for studying stability in deep learning systems via a central object called the Learning Stability Profile (LSP). 2. Proves a Fundamental Analytic Stability Theorem linking uniform boundedness of stability signatures to the existence of a dissipative Lyapunov-type energy. 3. Extends the theory to non-smooth learning systems (e.g., ReLU networks, proximal updates) using Clarke generalized derivatives and variational Lyapunov functionals.",
          "summary": "This paper proposes a unified framework to analyze the stability of deep learning systems by viewing them as coupled representation-parameter dynamics. The core method introduces a Learning Stability Profile and proves a theorem linking bounded stability signatures to a dissipative Lyapunov energy, which is extended to non-smooth cases using generalized derivatives. The main conclusion is that this provides a single, foundational dynamical description of stability that unifies and explains robustness across various architectures and optimization methods.",
          "mindmap": "graph LR\n    A[Analytic and Variational Stability of Deep Learning Systems] --> B[核心问题/Problem: 缺乏统一的深度学习稳定性数学描述 / Lack of unified mathematical description of stability in deep learning]\n    A --> C[主要方法/Method: 提出学习稳定性剖面与解析-变分框架 / Proposes Learning Stability Profile and analytic-variational framework]\n    A --> D[关键结果/Results: 证明基本稳定性定理，统一平滑与非平滑系统的稳定性分析 / Proves Fundamental Stability Theorem, unifying stability analysis for smooth and non-smooth systems]"
        },
        {
          "title": "MiST: Understanding the Role of Mid-Stage Scientific Training in Developing Chemical Reasoning Models",
          "authors": "Andres M Bran, Tong Xie, Shai Pranesh, Jeffrey Meng, Xuan Vu Nguyen, Jeremy Goumaz, David Ming Segura, Ruizhi Xu, Dongzhan Zhou, Wenjie Zhang, Bram Hoex, Philippe Schwaller",
          "institution": "École Polytechnique Fédérale de Lausanne (EPFL), University of New South Wales (UNSW), Green Dynamics, Shanghai Artificial Intelligence Laboratory",
          "link": "https://arxiv.org/pdf/2512.21231",
          "code": null,
          "tags": [
            "reinforcement learning",
            "latent solvability",
            "mid-stage scientific training",
            "chemical reasoning",
            "rule-based rewards",
            "symbolic competence"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/734a7266913201c1c5abeaa4d54fa794bfab81e86b7cffa89c8c4dc72702a8fa_w640_q70.webp",
          "contributions": "1. Identifies two necessary conditions for RL-based chemical reasoning: symbolic competence and latent chemical knowledge. 2. Proposes MiST (mid-stage scientific training), a set of techniques including data-mixing with SMILES/CIF-aware pre-processing and continued pre-training. 3. Demonstrates that MiST significantly improves latent solvability and enables RL to achieve large accuracy gains on challenging chemical tasks.",
          "summary": "This paper addresses the problem that reinforcement learning for chemical reasoning fails unless the base model already has some latent solvability. The authors propose MiST, a mid-stage training pipeline involving data pre-processing and continued pre-training, to build the necessary prerequisites. Their method substantially boosts model performance on tasks like organic reaction naming and inorganic material generation, establishing clear prerequisites for training chemical reasoning models.",
          "mindmap": "graph LR\n    A[MiST: 化学推理模型中的中期科学训练 / MiST: Mid-Stage Scientific Training for Chemical Reasoning] --> B[核心问题/Problem: RL需要潜在可解性 / RL requires latent solvability]\n    A --> C[主要方法/Method: 中期科学训练 / Mid-stage scientific training (MiST)]\n    A --> D[关键结果/Results: 准确率大幅提升 / Accuracy significantly improved]\n    B --> B1[条件: 符号能力与潜在知识 / Conditions: Symbolic competence & latent knowledge]\n    C --> C1[技术: 数据混合与持续预训练 / Techniques: Data-mixing & continued pre-training]\n    D --> D1[有机反应命名: 10.9% -> 63.9% / Organic reaction naming: 10.9% -> 63.9%]\n    D --> D2[无机材料生成: 40.6% -> 67.4% / Inorganic material generation: 40.6% -> 67.4%]"
        },
        {
          "title": "Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks",
          "authors": "Xinjie Xu, Shuyu Cheng, Dongwei Xu, Qi Xuan, Chen Ma",
          "institution": "Zhejiang University of Technology, Binjiang Institute of Artificial Intelligence, ZJUT, JQ Investments",
          "link": "https://arxiv.org/pdf/2512.21241",
          "code": "https://github.com/machanic/hard_label_attacks",
          "tags": [
            "adversarial attacks",
            "hard-label black-box attacks",
            "query efficiency",
            "ray search optimization",
            "Nesterov's Accelerated Gradient",
            "momentum-based optimization"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/302b574932ba227c13854e5c9c2cab3d7cf8f1ed29ee145fbc73062632304cd4_w640_q70.webp",
          "contributions": "1. Proposed ARS-OPT, a momentum-based algorithm inspired by Nesterov's Accelerated Gradient to accelerate the convergence of ray search in hard-label attacks. 2. Introduced PARS-OPT, which further enhances ARS-OPT by incorporating surrogate-model priors into the gradient estimation. 3. Provided theoretical convergence guarantees for the proposed methods and demonstrated superior query efficiency over 13 state-of-the-art approaches on ImageNet and CIFAR-10.",
          "summary": "This paper addresses the high query cost of hard-label black-box adversarial attacks by optimizing ray search. The authors propose ARS-OPT, a momentum-based algorithm, and its enhanced version PARS-OPT, which uses surrogate-model priors, to accelerate convergence. Experiments show the methods outperform 13 existing approaches in query efficiency on standard datasets.",
          "mindmap": "graph LR\n    A[Improving the Convergence Rate of Ray Search Optimization<br>改进射线搜索优化的收敛率] --> B[核心问题/Problem<br>Hard-label攻击查询成本高<br>High query cost in hard-label attacks]\n    A --> C[主要方法/Method<br>提出ARS-OPT & PARS-OPT<br>Propose ARS-OPT & PARS-OPT]\n    A --> D[关键结果/Results<br>超越13种SOTA方法<br>Outperforms 13 SOTA methods]"
        },
        {
          "title": "LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation",
          "authors": "Anatoly O. Onishchenko, Alexey K. Kovalev, Aleksandr I. Panov",
          "institution": "MIRAI, Cognitive AI Systems Lab",
          "link": "https://arxiv.org/pdf/2512.21243",
          "code": "https://lookplangraph.github.io/",
          "tags": [
            "embodied ai",
            "scene graph",
            "vision language model",
            "dynamic planning",
            "memory graph",
            "graph augmentation"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39706723670e257f6d0916c7c37badacde760a1f6d3061d011d8c22fa4f29bea_w640_q70.webp",
          "contributions": "1. Proposes LookPlanGraph, a method for embodied instruction following that dynamically updates a scene graph during execution using a Vision Language Model to verify object priors and discover new entities. 2. Introduces the GraSIF (Graph Scenes for Instruction Following) dataset with an automated validation framework, comprising 514 tasks from existing benchmarks. 3. Demonstrates superior performance over static scene graph methods in simulated environments with changed object positions and shows practical applicability in real-world experiments.",
          "summary": "The paper addresses the problem of LLM-based embodied agents failing in dynamic environments due to reliance on pre-built, static scene graphs. It proposes LookPlanGraph, a method that continuously augments a memory graph with real-time visual observations from a VLM to verify and discover objects during plan execution. Experiments show it outperforms static graph methods in simulated and real-world settings, and a new dataset (GraSIF) is introduced for evaluation.",
          "mindmap": "graph LR\n    A[LookPlanGraph] --> B[核心问题/Problem: Static scene graphs fail in dynamic environments];\n    A --> C[主要方法/Method: Dynamic graph update via VLM observation];\n    A --> D[关键结果/Results: Outperforms static methods, new GraSIF dataset];"
        },
        {
          "title": "Assessing the Software Security Comprehension of Large Language Models",
          "authors": "Mohammed Latif Siddiq, Natalie Sekerak, Antonio Karam, Maria Leal, Arvin Islam-Gomes, Joanna C. S. Santos",
          "institution": "University of Notre Dame",
          "link": "https://arxiv.org/pdf/2512.21238",
          "code": null,
          "tags": [
            "software security assessment",
            "Bloom's Taxonomy",
            "knowledge boundary",
            "misconception patterns"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b3809be1ff4cae9ad7acb47676829cd58ca3ea614efa57d9121857f85d97fc7_w640_q70.webp",
          "contributions": "1. Introduced a systematic evaluation framework using Bloom's Taxonomy to assess LLMs' software security comprehension across six cognitive levels. 2. Proposed the concept of a \"software security knowledge boundary\" to identify the highest reliable cognitive performance level for an LLM. 3. Identified and documented 51 recurring misconception patterns made by LLMs in software security tasks.",
          "summary": "This paper systematically evaluates the software security comprehension of five leading LLMs using Bloom's Taxonomy as a framework across diverse datasets. The results show that while LLMs perform well on lower-level cognitive tasks like recalling facts, their performance significantly degrades on higher-order tasks requiring reasoning and secure system creation. The study introduces a knowledge boundary to quantify reliable performance limits and identifies common misconception patterns.",
          "mindmap": "graph LR\n    A[Assessing LLM Software Security Comprehension<br/>评估LLM软件安全理解] --> B{核心问题/Problem};\n    A --> C{主要方法/Method};\n    A --> D{关键结果/Results};\n    B --> B1[LLMs' Security Expertise Unclear<br/>LLM安全专业知识不明];\n    C --> C1[Framework: Bloom's Taxonomy<br/>框架: 布鲁姆分类法];\n    C --> C2[Datasets: MCQs, Code, Courses, Case Studies<br/>数据集: 选择题, 代码, 课程, 案例];\n    D --> D1[Good on Low-Level Tasks<br/>低级任务表现好];\n    D --> D2[Poor on High-Order Reasoning<br/>高阶推理表现差];\n    D --> D3[Knowledge Boundary & Misconceptions<br/>知识边界与误解模式];"
        },
        {
          "title": "Model Merging via Multi-Teacher Knowledge Distillation",
          "authors": "Seyed Arshan Dalili, Mehrdad Mahdavi",
          "institution": "The Pennsylvania State University",
          "link": "https://arxiv.org/pdf/2512.21288",
          "code": "https://github.com/arshandalili/SAMerging",
          "tags": [
            "model merging",
            "model merging",
            "knowledge distillation",
            "PAC-Bayes",
            "sharpness-aware minimization",
            "multi-task learning"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1edfb41aaef41e719d5724fa70ca9022ddcf1e1c683791b3bd1d929286795c62_w640_q70.webp",
          "contributions": "1. Establishes a novel flatness-aware PAC-Bayes generalization bound for model merging, introducing a \"cross-task heterogeneity\" term. 2. Frames model merging as multi-teacher knowledge distillation on scarce unlabeled data, showing minimizing student-teacher KL divergence tightens the risk bound. 3. Proposes SAMerging, a method that operationalizes the objective using Sharpness-Aware Minimization (SAM) to find flat minima.",
          "summary": "This paper addresses the lack of theoretical understanding in model merging by framing it as multi-teacher knowledge distillation and deriving a PAC-Bayes generalization bound. It proposes SAMerging, a method that uses Sharpness-Aware Minimization to optimize the merging process based on this theory. The method achieves state-of-the-art performance on vision and NLP benchmarks with high data efficiency.",
          "mindmap": "graph LR\n    A[Model Merging via Multi-Teacher Knowledge Distillation] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[缺乏理论保证/Lack of Theoretical Guarantees]\n    B --> B2[启发式方法不稳定/Heuristic Methods are Brittle]\n    C --> C1[理论: 平坦性感知PAC-Bayes边界/Theory: Flatness-aware PAC-Bayes Bound]\n    C --> C2[框架: 多教师知识蒸馏/Framework: Multi-Teacher Knowledge Distillation]\n    C --> C3[方法: SAMerging/Method: SAMerging]\n    D --> D1[新SOTA/New SOTA]\n    D --> D2[高数据效率/High Data Efficiency]"
        },
        {
          "title": "Transcriptome-Conditioned Personalized De Novo Drug Generation for AML Using Metaheuristic Assembly and Target-Driven Filtering",
          "authors": "Abdullah G. Elafifi, Basma Mamdouh, Mariam Hanafy, Muhammed Alaa Eldin, Yosef Khaled, Nesma Mohamed El-Gelany, Tarek H.M. Abou-El-Enien",
          "institution": "Faculty of Computers and Artificial Intelligence, Cairo University",
          "link": "https://arxiv.org/pdf/2512.21301",
          "code": null,
          "tags": [
            "computational drug design",
            "metaheuristic assembly",
            "de novo drug generation",
            "transcriptomics",
            "molecular docking",
            "multi-objective optimization"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8733d98ab11f787e5d221faf5b5b5383a6af0c23974a0e3febf13cb0aa80b4f3_w640_q70.webp",
          "contributions": "1. A novel end-to-end computational framework that integrates patient transcriptomics with de novo drug generation for personalized AML therapy. 2. Development of a reaction-first evolutionary metaheuristic algorithm for assembling novel ligands guided by target-specific structural hotspots. 3. Validation of generated drug candidates through in-silico ADMET profiling and molecular docking, demonstrating pharmacologically viable leads.",
          "summary": "This paper proposes a computational framework for personalized drug discovery in Acute Myeloid Leukemia (AML). It uses patient transcriptome data to identify biomarkers, models their 3D structures, and employs a novel metaheuristic algorithm to generate new drug-like molecules targeting these biomarkers. The results show the framework can produce viable drug candidates, offering a scalable approach for precision oncology.",
          "mindmap": "graph LR\n        A[Transcriptome-Conditioned Personalized De Novo Drug Generation] --> B[核心问题/Problem: AML分子异质性与复发率高/AML's molecular heterogeneity & high relapse]\n        A --> C[主要方法/Method: 元启发式组装与靶向过滤/Metaheuristic Assembly & Target-Driven Filtering]\n        A --> D[关键结果/Results: 生成具有药理活性的先导化合物/Generated pharmacologically viable leads]"
        },
        {
          "title": "Learning to Solve PDEs on Neural Shape Representations",
          "authors": "Lilian Welschinger, Yilin Liu, Zican Wang, Niloy Mitra",
          "institution": "University College London, Adobe Research",
          "link": "https://arxiv.org/pdf/2512.21311",
          "code": null,
          "tags": [
            "geometry processing",
            "neural shape representations",
            "mesh-free PDE solver",
            "geometry-conditioned operator",
            "surface PDEs",
            "differentiable pipeline"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed9eac0c574821f5dc10188c61bf1bd40ede7fca32ee8f98accf90b52993315f_w640_q70.webp",
          "contributions": "1. A novel, mesh-free formulation that learns a local update operator conditioned on neural shape attributes to solve surface PDEs directly on neural representations. 2. A method that is trained once on a single shape and generalizes across shape and topology variations, enabling fast inference without per-instance optimization. 3. The first end-to-end pipeline that solves surface PDEs on both neural and classical surface representations, preserving differentiability.",
          "summary": "The paper addresses the mismatch between traditional mesh-based PDE solvers and modern neural shape representations by proposing a learned, mesh-free operator. This operator, trained once, solves surface PDEs directly on neural data, generalizing across shapes and preserving differentiability. The method performs comparably to classical solvers and enables the first end-to-end PDE-solving pipeline for neural surfaces.",
          "mindmap": "graph LR\n    A[Learning to Solve PDEs on Neural Shape Representations] --> B(核心问题/Problem: Mismatch between mesh-based solvers and neural shape data)\n    A --> C(主要方法/Method: Mesh-free, learned local operator conditioned on shape)\n    A --> D(关键结果/Results: Generalizes across shapes, close to FEM, enables end-to-end pipeline)"
        },
        {
          "title": "Does the Data Processing Inequality Reflect Practice? On the Utility of Low-Level Tasks",
          "authors": "Roy Turgeman, Tom Tirer",
          "institution": "Bar-Ilan University",
          "link": "https://arxiv.org/pdf/2512.21315",
          "code": null,
          "tags": [
            "information theory",
            "statistical learning",
            "data processing inequality",
            "Bayes classifier",
            "low-level processing",
            "classification accuracy",
            "finite sample analysis"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59be928ca6eaa4e86b8cd3fb9b4f9e66ff3aed7a604e05c3f63b00a3ca0c56bd_w640_q70.webp",
          "contributions": "1. A theoretical proof that for any finite number of training samples, there exists a pre-classification processing that improves classification accuracy, even for a classifier converging to the optimal Bayes classifier. 2. An analysis of how class separation, training set size, and class balance affect the relative gain from low-level pre-processing. 3. Empirical validation of the theory on a synthetic setup and on practical deep classifiers with denoising/encoding tasks on benchmark datasets, showing consistent trends.",
          "summary": "This paper investigates the practical utility of performing low-level signal processing tasks (like denoising) before classification, which seemingly contradicts the information-theoretic Data Processing Inequality. Through a theoretical binary classification analysis and empirical studies, it demonstrates that for finite training samples, such pre-processing can improve accuracy, with the benefit influenced by dataset characteristics like size and noise level.",
          "mindmap": "graph LR\n    A[Does the Data Processing Inequality Reflect Practice?<br/>数据加工不等式反映实践吗?] --> B(核心问题/Problem: Low-level processing is common in practice but seems to contradict the Data Processing Inequality.<br/>实践中的低级处理似乎与数据加工不等式矛盾)\n    A --> C(主要方法/Method: Theoretical study of a binary classifier + empirical validation on deep networks.<br/>二元分类器的理论研究+深度网络的实证验证)\n    A --> D(关键结果/Results: For finite samples, pre-processing can improve accuracy; gain depends on dataset properties.<br/>对于有限样本，预处理可提高精度；收益取决于数据集属性)"
        },
        {
          "title": "Parallel Token Prediction for Language Models",
          "authors": "Felix Draxler, Justus Will, Farrin Marouf Sofian, Theofanis Karaletsos, Sameer Singh, Stephan Mandt",
          "institution": "University of California, Irvine, Chan-Zuckerberg Initiative, Pyramidal AI",
          "link": "https://arxiv.org/pdf/2512.21323",
          "code": null,
          "tags": [
            "llm inference",
            "parallel token prediction",
            "speculative decoding",
            "autoregressive decoding",
            "transformer inference",
            "latency optimization"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d5681ed12f339fba2f3eb1e012a75a0b58e77b2c9c9aa20352d3b12fcba4f3c_w640_q70.webp",
          "contributions": "1. Proposes Parallel Token Prediction (PTP), a universal framework for parallel sequence generation that jointly predicts multiple dependent tokens in a single transformer call. 2. Proves that PTP can represent arbitrary autoregressive sequence distributions, avoiding the restrictive independence assumptions of prior multi-token prediction methods. 3. Demonstrates state-of-the-art speculative decoding performance, accepting over four tokens per step on Spec-Bench with Vicuna-7B, showing parallel long-sequence generation is feasible without losing modeling power.",
          "summary": "The paper addresses the high latency of autoregressive decoding in large language models by proposing Parallel Token Prediction (PTP), a framework that predicts multiple dependent tokens in parallel within a single transformer call. It proves PTP's universality in representing autoregressive distributions and shows it achieves superior speculative decoding performance, enabling faster text generation without sacrificing quality.",
          "mindmap": "graph LR\n    A[Parallel Token Prediction for Language Models] --> B[核心问题/Problem: Autoregressive decoding latency bottleneck]\n    A --> C[主要方法/Method: Parallel Token Prediction (PTP), joint prediction of dependent tokens]\n    A --> D[关键结果/Results: State-of-the-art speculative decoding, >4 tokens/step, universal framework]"
        },
        {
          "title": "Variationally correct operator learning: Reduced basis neural operator with a posteriori error estimation",
          "authors": "Yuan Qiu, Wolfgang Dahmen, Peng Chen",
          "institution": "Georgia Institute of Technology, University of South Carolina",
          "link": "https://arxiv.org/pdf/2512.21319",
          "code": null,
          "tags": [
            "scientific computing",
            "first-order system least squares",
            "reduced basis neural operator",
            "a posteriori error estimation"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00a56bc0fe5da5167f8e01f84491e1f4cb6dfb3c8d8926f4a22fe5ff594010bf_w640_q70.webp",
          "contributions": "1. Proposes a variationally correct operator learning framework using First-Order System Least-Squares (FOSLS) objectives that are provably equivalent to solution error in PDE-induced norms. 2. Introduces a Reduced Basis Neural Operator (RBNO) that predicts coefficients for a pre-computed, conforming reduced basis to ensure function space conformity and variational stability. 3. Provides a rigorous convergence analysis that decomposes the total error into discretization bias, basis truncation error, neural network approximation error, and statistical estimation errors, with the residual serving as a reliable a posteriori error estimator.",
          "summary": "This paper addresses the issue of \"variational correctness\" in neural operators for solving PDEs, where standard residual losses do not guarantee small solution errors. The authors propose a new framework using a First-Order System Least-Squares (FOSLS) loss and a Reduced Basis Neural Operator (RBNO) to ensure stability and norm equivalence. The method provides a rigorous error bound and demonstrates superior accuracy in PDE-compliant norms, with the residual acting as a reliable error estimator.",
          "mindmap": "graph LR\n    A[Variationally Correct Operator Learning<br>变分正确的算子学习] --> B[核心问题/Problem<br>Standard PDE-residual losses lack variational correctness<br>标准PDE残差损失缺乏变分正确性]\n    A --> C[主要方法/Method<br>FOSLS objective + Reduced Basis Neural Operator (RBNO)<br>FOSLS目标 + 约简基神经算子]\n    A --> D[关键结果/Results<br>Provable error equivalence & reliable a posteriori estimator<br>可证明的误差等价性 & 可靠的后验估计器]"
        },
        {
          "title": "Measuring all the noises of LLM Evals",
          "authors": "Sida Wang",
          "institution": "FAIR at Meta",
          "link": "https://arxiv.org/pdf/2512.21326",
          "code": null,
          "tags": [
            "llm inference",
            "LLM evaluation",
            "statistical noise",
            "paired analysis",
            "prediction variance",
            "data variance"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa15912febac5bc93aec8d1b8870feaf16ae89016c37e24c26550d053d396fec_w640_q70.webp",
          "contributions": "1. Clearly defines and measures three types of noise (prediction, data, total) in LLM evaluations using the law of total variance. 2. Proposes the \"all-pairs paired method\" to apply paired statistical analysis across all model pairs for increased statistical power. 3. Empirically reveals that total noise is predictable per evaluation and that prediction noise typically dominates data noise, enabling more effective significance testing.",
          "summary": "This paper addresses the challenge of statistical noise in Large Language Model (LLM) evaluations. It proposes an \"all-pairs paired method\" to measure prediction, data, and total noise across model pairs. The key findings are that each evaluation benchmark has a characteristic noise level and that reducing prediction noise through averaging can significantly improve the detection of performance differences.",
          "mindmap": "graph LR\n    A[Measuring all the noises of LLM Evals] --> B(核心问题/Problem: LLM评估中的统计噪声/Separating signal from noise in LLM evals)\n    A --> C(主要方法/Method: 全配对分析法/All-pairs paired method)\n    A --> D(关键结果/Results: 可预测的总噪声与主导的预测噪声/Predictable total noise & dominant prediction noise)"
        },
        {
          "title": "Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty",
          "authors": "Ziyu Chen, Xinbei Jiang, Peng Sun, Tao Lin",
          "institution": "Zhejiang University, Westlake University, University of Chicago",
          "link": "https://arxiv.org/pdf/2512.21336",
          "code": "https://github.com/LINs-lab/DenoisingEntropy",
          "tags": [
            "diffusion models",
            "Denoising Entropy",
            "Masked Diffusion Models",
            "decoding path optimization",
            "predictive uncertainty",
            "non-autoregressive generation"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4480eb4fa3d14900373effb4e74dd207b42c650b50de1044a2cad8b4036e465f_w640_q70.webp",
          "contributions": "1. Formalized the problem of decoding path sensitivity in Masked Diffusion Models (MDMs) by introducing the concept of cumulative Path Uncertainty. 2. Proposed Denoising Entropy, a novel, computable metric to quantify predictive uncertainty along a generative path. 3. Developed two entropy-guided algorithms (post-hoc selection and real-time guidance) to optimize the decoding path and improve generation quality.",
          "summary": "The paper identifies that the flexible generation of Masked Diffusion Models (MDMs) leads to variable output quality due to the chosen decoding order. To address this, it introduces Denoising Entropy to measure path uncertainty and proposes two algorithms that use this metric to guide the decoding process. Experiments show these methods significantly improve generation accuracy on reasoning, planning, and code tasks, turning uncertainty into an advantage.",
          "mindmap": "graph LR\n        A[Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty<br/>通过量化不确定性优化掩码扩散模型的解码路径] --> B(核心问题/Problem: MDMs生成质量对解码顺序敏感<br/>MDM output quality is sensitive to decoding order)\n        A --> C(主要方法/Method: 提出去噪熵和路径优化算法<br/>Propose Denoising Entropy & path optimization algorithms)\n        A --> D(关键结果/Results: 熵引导方法提升生成质量<br/>Entropy-guided methods improve generation quality)"
        },
        {
          "title": "Fast and Exact Least Absolute Deviations Line Fitting via Piecewise Affine Lower-Bounding",
          "authors": "Stefan Volz, Martin Storath, Andreas Weinmann",
          "institution": "Technische Hochschule Würzburg-Schweinfurt",
          "link": "https://arxiv.org/pdf/2512.20682",
          "code": null,
          "tags": [
            "optimization",
            "least absolute deviations",
            "piecewise affine lower-bounding",
            "linear programming",
            "robust regression",
            "exact algorithm"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b21c32d0f735b0e43e7203101aa47a6f9a1a7a2ca1eed65c3391f51147bd5e6_w640_q70.webp",
          "contributions": "1. Proposes the Piecewise Affine Lower-Bounding (PALB) method, a novel exact algorithm for LAD line fitting. 2. Provides theoretical proof of correctness and bounds on the number of iterations for the algorithm. 3. Demonstrates empirical log-linear scaling and superior speed compared to existing LP-based and IRLS-based solvers on synthetic and real-world data.",
          "summary": "This paper addresses the computational challenge of exact Least Absolute Deviations (LAD) line fitting, a robust regression method. It proposes the PALB algorithm, which uses piecewise-affine lower bounds and a subdivision scheme to find the exact solution efficiently. The method is proven correct, shows log-linear empirical scaling, and is faster than existing practical solvers like linear programming and iteratively reweighted least squares.",
          "mindmap": "graph LR\n    A[Fast and Exact LAD Line Fitting via PALB] --> B[核心问题/Problem: LAD拟合计算复杂，现有高效算法难实现/LAD fitting is computationally involved, existing efficient algorithms are hard to implement]\n    A --> C[主要方法/Method: 使用分段仿射下界和细分方案/Use piecewise affine lower-bounding and a subdivision scheme]\n    A --> D[关键结果/Results: 算法精确、对数线性扩展、速度快于LP和IRLS/Algorithm is exact, log-linear scaling, faster than LP & IRLS]"
        },
        {
          "title": "Diffusion Models in Simulation-Based Inference: A Tutorial Review",
          "authors": "Jonas Arruda, Niels Bracher, Ullrich Köthe, Jan Hasenauer, Stefan T. Radev",
          "institution": "University of Bonn, Rensselaer Polytechnic Institute, Heidelberg University",
          "link": "https://arxiv.org/pdf/2512.20685",
          "code": null,
          "tags": [
            "diffusion models",
            "simulation-based inference",
            "score-based models",
            "parameter estimation",
            "generative modeling"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/024277637e6ce2f35e35619e320a14456fb6904af22070de1a30be768a5a2475_w640_q70.webp",
          "contributions": "1. Provides a comprehensive tutorial review synthesizing recent developments on diffusion models for Simulation-Based Inference (SBI)., 2. Highlights and discusses key design choices and concepts affecting SBI performance, such as guidance, score composition, flow matching, and noise schedules., 3. Illustrates the application of diffusion models in SBI through case studies and outlines open questions for future research.",
          "summary": "This tutorial review synthesizes how diffusion models, through their score-based formulation, are used for fast and accurate parameter estimation in Simulation-Based Inference (SBI). It covers key design choices in training and inference, discusses factors affecting efficiency and accuracy, and presents case studies to illustrate the concepts.",
          "mindmap": "graph LR\n        A[Diffusion Models in SBI: A Tutorial Review] --> B[核心问题/Problem: 如何从模拟和真实数据中推断潜在参数/How to infer latent parameters from simulated and real data?];\n        A --> C[主要方法/Method: 使用基于分数的扩散模型学习条件或联合分布/Using score-based diffusion models to learn conditional or joint distributions];\n        A --> D[关键结果/Results: 综述了设计选择、概念与应用，并指出未来方向/Reviewed design choices, concepts, applications, and outlined future directions];"
        },
        {
          "title": "Weighted MCC: A Robust Measure of Multiclass Classifier Performance for Observations with Individual Weights",
          "authors": "Rommel Cortez, Bala Krishnamoorthy",
          "institution": "Washington State University",
          "link": "https://arxiv.org/pdf/2512.20811",
          "code": null,
          "tags": [
            "classification evaluation",
            "Matthews Correlation Coefficient",
            "weighted performance measure",
            "multiclass classification",
            "robustness analysis",
            "confusion matrix"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/277843f9180deb3e0b651c6e34535bdaba884d63d88920a3f85e89a8483c52d1_w640_q70.webp",
          "contributions": "1. Proposes a new weighted version of the Pearson-Matthews Correlation Coefficient (MCC) for binary and multiclass classification that is sensitive to individual observation weights. 2. Proves the robustness of the proposed weighted measures, showing that changes in weights by at most ε lead to bounded changes in the measure value (by a factor of ε for binary and ε² for multiclass). 3. Demonstrates empirically that the weighted measures can effectively distinguish classifiers based on their performance on highly weighted observations, unlike standard unweighted measures.",
          "summary": "The paper addresses the lack of performance measures for classification that account for individually weighted observations. It proposes a weighted version of the Matthews Correlation Coefficient (MCC) for binary and multiclass tasks, which is proven to be robust to weight perturbations. The results show that this new measure successfully identifies classifiers that perform well on more important, highly weighted data points.",
          "mindmap": "graph LR\n    A[Weighted MCC] --> B[核心问题/Problem: 现有分类评估指标不适用于带权重的观测值/Existing measures ignore individual observation weights]\n    A --> C[主要方法/Method: 提出加权MCC及其多分类扩展/Propose weighted MCC and multiclass extensions]\n    A --> D[关键结果/Results: 指标对权重变化鲁棒，能识别在重要样本上性能好的分类器/Measures are robust and identify classifiers good on high-weight observations]"
        },
        {
          "title": "A Physics Informed Neural Network For Deriving MHD State Vectors From Global Active Regions Observations",
          "authors": "Subhamoy Chatterjee, Mausumi Dikpati",
          "institution": "Southwest Research Institute, High Altitude Observatory (NSF-NCAR)",
          "link": "https://arxiv.org/pdf/2512.20747",
          "code": null,
          "tags": [
            "astro-physics",
            "solar physics",
            "magnetohydrodynamics",
            "Physics-Informed Neural Network (PINN)",
            "MagnetoHydroDynamic Shallow-Water Tachocline (MHD-SWT)",
            "solar active regions (ARs)",
            "toroidal bands (toroids)",
            "state-vector reconstruction"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64164e9d078622b42b01e54f9efaeb57ab61d047c881403541551b016e24827e_w640_q70.webp",
          "contributions": "1. Proposes PINNBARDS, a novel Physics-Informed Neural Network framework to derive the initial MHD state-vector for solar tachocline models from surface observations of active region distributions. 2. Demonstrates the method's ability to converge to physically consistent solutions that match observed toroidal band patterns, specifically using data from the Feb-14-2024 SDO/HMI synoptic map. 3. Explores the parameter space to constrain key physical properties, finding optimal agreement with observations for toroidal field strengths of 20–30 kG and a bandwidth of ~10 degrees, which is consistent with low-order longitudinal mode excitation.",
          "summary": "This paper addresses the challenge of initializing solar magnetohydrodynamic models for predicting flare-producing active regions, which requires a full state-vector not provided by surface observations. The authors develop PINNBARDS, a Physics-Informed Neural Network that uses observed toroidal band patterns and the governing MHD equations to reconstruct the necessary initial state-vector for the tachocline. Their analysis identifies optimal physical parameters (20-30 kG field strength) that best match observations, providing a novel pathway for weeks-ahead solar activity prediction.",
          "mindmap": "graph LR\n        A[PINNBARDS: 从全球活动区观测推导MHD状态向量 / PINNBARDS: Deriving MHD State Vectors From Global Active Regions Observations] --> B(核心问题/Problem: 表面磁图仅提供活动区分布的几何形状，无法提供初始化MHD模型所需的自洽状态向量 / Problem: Surface magnetograms only provide geometric shape of AR distribution, not the self-consistent state-vector needed to initialize MHD models.)\n        A --> C(主要方法/Method: 开发PINNBARDS，一个基于物理信息神经网络(PINN)的模拟器，使用观测到的环形带和MHD-SWT方程来推导初始状态向量 / Method: Develop PINNBARDS, a PINN-based simulator using observed toroids and MHD-SWT equations to derive the initial state-vector.)\n        A --> D(关键结果/Results: PINN收敛到物理一致解，与观测匹配；最佳参数为20-30 kG环形场和~10度带宽 / Results: PINN converges to physically consistent solutions matching observations; optimal parameters are 20-30 kG toroidal field and ~10 degree bandwidth.)"
        },
        {
          "title": "GenTSE: Enhancing Target Speaker Extraction via a Coarse-to-Fine Generative Language Model",
          "authors": "Haoyang Li, Xuyi Zhuang, Azmat Adnan, Ye Ni, Wei Rao, Shreyas Gopal, Eng Siong Chng",
          "institution": "Nanyang Technological University, Southeast University",
          "link": "https://arxiv.org/pdf/2512.20978",
          "code": null,
          "tags": [
            "speech separation",
            "target speaker extraction",
            "generative language model",
            "coarse-to-fine",
            "exposure bias",
            "direct preference optimization"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2e6714f10d4ca9cf7e6cc6ddc5eea2048c365e1e206f97988dcc13bab4d72ef_w640_q70.webp",
          "contributions": "1. Proposes GenTSE, a fully generative two-stage decoder-only language model architecture for target speaker extraction, separating coarse semantic token prediction from fine acoustic token generation. 2. Introduces a Frozen-LM Conditioning training strategy to mitigate exposure bias by conditioning models on their own past predictions from earlier checkpoints. 3. Employs Direct Preference Optimization to better align the model's outputs with human perceptual preferences.",
          "summary": "This paper introduces GenTSE, a novel generative language model approach for target speaker extraction that uses a two-stage, coarse-to-fine process to generate speech. The method addresses exposure bias with a specific training strategy and aligns outputs with human preferences using DPO. Experiments show it outperforms previous LM-based systems in speech quality, intelligibility, and speaker consistency.",
          "mindmap": "graph LR\n    A[GenTSE] --> B[核心问题/Problem: TSE generalization & fidelity];\n    A --> C[主要方法/Method: Two-stage generative LM, FLC, DPO];\n    A --> D[关键结果/Results: Surpasses prior LM-based systems];"
        },
        {
          "title": "Enhancing diffusion models with Gaussianization preprocessing",
          "authors": "Li Cunzhi, Louis Kang, Hideaki Shimazaki",
          "institution": "Kyoto University, RIKEN Center for Brain Science",
          "link": "https://arxiv.org/pdf/2512.21020",
          "code": null,
          "tags": [
            "diffusion models",
            "Gaussianization",
            "bifurcation",
            "sampling efficiency",
            "generative models",
            "data preprocessing"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1994287172f7f096232171fd21b57b4b1d23f670ef89d86fdbadc32dceabbf46_w640_q70.webp",
          "contributions": "1. Proposes a novel Gaussianization preprocessing step for training data to align the target distribution with the initial Gaussian noise in diffusion models. 2. Aims to mitigate the bifurcation-related sampling inefficiency, particularly improving early-stage reconstruction quality. 3. Enables more stable and efficient sampling, especially beneficial for small-scale network architectures.",
          "summary": "This paper addresses the slow sampling problem in diffusion models, which is caused by a delay before trajectory bifurcation. The authors propose applying Gaussianization preprocessing to the training data to make it more closely resemble the initial Gaussian noise, simplifying the model's learning task. This method improves generation quality in the early reconstruction stages and enables more efficient sampling.",
          "mindmap": "graph LR\n    A[Enhancing diffusion models with Gaussianization preprocessing<br>增强扩散模型的Gaussianization预处理] --> B[Problem: Slow sampling due to bifurcation delay<br>问题: 分岔延迟导致采样慢]\n    A --> C[Method: Gaussianization preprocessing of training data<br>方法: 训练数据的Gaussianization预处理]\n    A --> D[Results: Improved early-stage quality & efficient sampling<br>结果: 提升早期生成质量与高效采样]"
        },
        {
          "title": "Critical Points of Degenerate Metrics on Algebraic Varieties: A Tale of Overparametrization",
          "authors": "Giovanni Luca Marchetti, Erin Connelly, Paul Breiding, Kathlén Kohn",
          "institution": "KTH Royal Institute of Technology, University of Osnabrück, Digital Futures",
          "link": "https://arxiv.org/pdf/2512.21029",
          "code": null,
          "tags": [
            "optimization theory",
            "Euclidean distance degree",
            "overparametrization",
            "algebraic geometry",
            "degenerate metrics",
            "critical points"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a41be3cd2e9b4a085f8c60d76c30d977b51dcb4e0c2ab2a8f0cb584bc2b6a827_w640_q70.webp",
          "contributions": "1. Relates degenerate quadratic optimization problems to nondegenerate ones via a projection, revealing the role of the projection's ramification locus. 2. Provides tools for counting the number of critical points over projective varieties in the degenerate setting. 3. Extends the theory of Euclidean distance degree (EDD) to the overparametrized regime, bridging algebraic geometry with machine learning.",
          "summary": "This paper studies the critical points of a degenerate quadratic objective over an algebraic variety, a scenario arising in overparametrized machine learning. The main method connects the degenerate problem to a nondegenerate one via a projection, highlighting the importance of the projection's ramification locus. The work extends the Euclidean distance degree framework to the degenerate setting and provides tools for analyzing optimization landscapes in overparametrized models.",
          "mindmap": "graph LR\n    A[Critical Points of Degenerate Metrics<br>退化度量临界点] --> B[核心问题/Problem<br>Overparametrized quadratic optimization<br>过参数化二次优化];\n    A --> C[主要方法/Method<br>Projection to nondegenerate problem<br>投影至非退化问题];\n    A --> D[关键结果/Results<br>Ramification locus role & EDD extension<br>分歧轨迹作用与EDD扩展];"
        },
        {
          "title": "Learning from Neighbors with PHIBP: Predicting Infectious Disease Dynamics in Data-Sparse Environments",
          "authors": "Edwin Fong, Lancelot F. James, Juho Lee",
          "institution": "The University of Hong Kong (HKU), The Hong Kong University of Science and Technology (HKUST), Korea Advanced Institute of Science and Technology (KAIST)",
          "link": "https://arxiv.org/pdf/2512.21005",
          "code": null,
          "tags": [
            "Bayesian nonparametric models",
            "Poisson Hierarchical Indian Buffet Process",
            "sparse count data",
            "infectious disease prediction",
            "Bayesian machine learning",
            "microbiome unseen species problems"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71507d2ce74cd2738ff8811f5abfd9a141d92f2c2fbc07f24c76785e9d864c9e_w640_q70.webp",
          "contributions": "1. Introduces the PHIBP, a Bayesian nonparametric model, for robustly modeling sparse count data in epidemiology. 2. Demonstrates the model's ability to borrow statistical strength from related regions to predict outbreaks in areas with zero historical cases. 3. Provides a unified framework that yields accurate predictions and meaningful epidemiological insights (e.g., alpha/beta diversity) in data-sparse environments.",
          "summary": "This paper addresses the challenge of predicting infectious disease outbreaks in regions with historically zero reported cases. It proposes using the Poisson Hierarchical Indian Buffet Process (PHIBP), a Bayesian model that leverages information from neighboring areas to handle sparse count data. The experiments show this approach provides accurate outbreak forecasts and useful epidemiological insights in data-scarce settings.",
          "mindmap": "graph LR\n    A[Learning from Neighbors with PHIBP<br>基于PHIBP向邻居学习] --> B[核心问题/Problem<br>Predicting outbreaks in regions with zero historical cases<br>预测零病例地区的疫情]\n    A --> C[主要方法/Method<br>Poisson Hierarchical Indian Buffet Process (PHIBP)<br>泊松分层印度自助餐过程]\n    A --> D[关键结果/Results<br>Accurate predictions & epidemiological insights in sparse data<br>稀疏数据下的准确预测与流行病学洞见]"
        },
        {
          "title": "Clever Hans in Chemistry: Chemist Style Signals Confound Activity Prediction on Public Benchmarks",
          "authors": "Andrew D. Blevins, Ian K. Quigley",
          "institution": "(Institution not explicitly stated in the provided content. Based on the authors' names and arXiv submission, it cannot be reliably inferred without full paper affiliations.)",
          "link": "https://arxiv.org/pdf/2512.20924",
          "code": null,
          "tags": [
            "cheminformatics",
            "Clever Hans",
            "shortcut learning",
            "activity prediction",
            "author-disjoint splits",
            "CHEMBL"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0714bbaf21a9c4a2f8221248ca78fbc74ff052dffe780f1607a9187bc7047174_w640_q70.webp",
          "contributions": "1. Demonstrates that machine learning models can predict the author of a molecule from its structure alone with high accuracy, revealing distinctive \"chemist style\" signals in public datasets. 2. Shows that an activity prediction model using only inferred author probabilities (and a protein ID) performs comparably to a structure-based baseline, exposing a \"Clever Hans\" failure mode where models exploit chemist intent rather than learning causal chemistry. 3. Analyzes the sources of this data leakage and proposes author-disjoint dataset splits as a mitigation strategy to decouple chemist intent from biological outcomes.",
          "summary": "The paper investigates whether machine learning models for molecular activity prediction exploit \"chemist style\" signals rather than learning causal chemistry. By training a classifier to predict molecule authors from structure and then using only the author probabilities to predict bioactivity, the authors show that models can achieve competitive performance without direct structural input. This reveals a shortcut learning problem in cheminformatics benchmarks, prompting recommendations for author-disjoint data splits to mitigate intent leakage.",
          "mindmap": "graph LR\n    A[Clever Hans in Chemistry<br>化学中的汉斯效应] --> B{核心问题/Problem};\n    A --> C{主要方法/Method};\n    A --> D{关键结果/Results};\n    B --> B1[Can models identify chemist from molecule?<br>模型能从分子识别化学家吗?];\n    B --> B2[Do models exploit intent, not chemistry?<br>模型利用意图而非化学原理吗?];\n    C --> C1[Link CHEMBL assays to authors<br>关联CHEMBL实验与作者];\n    C --> C2[Train author classifier from fingerprints<br>从指纹训练作者分类器];\n    C --> C3[Train activity model on author probabilities<br>基于作者概率训练活性模型];\n    D --> D1[Author prediction is accurate<br>作者预测准确];\n    D --> D2[Author-only model matches baseline<br>仅作者模型媲美基线];\n    D --> D3[Reveals Clever Hans failure<br>揭示汉斯效应失败模式];"
        },
        {
          "title": "Causal-driven attribution (CDA): Estimating channel influence without user-level data",
          "authors": "Georgios Filippou, Boi Mai Quach, Diana Lenghel, Arthur White, Ashish Kumar Jha",
          "institution": "Trinity College Dublin",
          "link": "https://arxiv.org/pdf/2512.21211",
          "code": null,
          "tags": [
            "causal inference",
            "PCMCI",
            "Structural Causal Model",
            "privacy-first analytics",
            "marketing attribution",
            "temporal causal discovery"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/375f379a2c16450357d40be6e36b5fe98344711a4e78be19f7e0ad3d106a71cf_w640_q70.webp",
          "contributions": "1. Proposes a Causal-Driven Attribution (CDA) framework that estimates channel influence using only aggregated impression-level data, eliminating the need for user-level path data. 2. Integrates temporal causal discovery (PCMCI) with causal effect estimation via a Structural Causal Model to recover directional channel relationships and quantify their contributions. 3. Demonstrates the framework's accuracy and robustness on synthetic data, showing it captures cross-channel interdependencies while providing a privacy-preserving, scalable alternative to traditional models.",
          "summary": "This paper introduces Causal-Driven Attribution (CDA), a framework that uses aggregated impression data and causal inference techniques to estimate marketing channel influence without user-level tracking. It combines temporal causal discovery (PCMCI) with structural causal modeling to quantify channel contributions to conversions. The method shows strong accuracy in synthetic experiments and offers a privacy-preserving, scalable solution for attribution modeling.",
          "mindmap": "graph LR\n    A[Causal-driven attribution (CDA): Estimating channel influence without user-level data] --> B[核心问题/Problem: Attribution models rely on inaccessible user-level data due to privacy regulations.]\n    A --> C[主要方法/Method: Integrates PCMCI for temporal causal discovery and Structural Causal Model for effect estimation using aggregated data.]\n    A --> D[关键结果/Results: Achieves low relative RMSE (9.50%-24.23%), captures cross-channel effects, and provides privacy-preserving attribution.]"
        },
        {
          "title": "Autonomous Uncertainty Quantification for Computational Point-of-care Sensors",
          "authors": "Artem Goncharov, Rajesh Ghosh, Hyou-Arm Joung, Dino Di Carlo, Aydogan Ozcan",
          "institution": "University of California, Los Angeles",
          "link": "https://arxiv.org/pdf/2512.21335",
          "code": null,
          "tags": [
            "on-device ai",
            "Monte Carlo dropout",
            "uncertainty quantification",
            "computational point-of-care sensors",
            "vertical flow assays",
            "neural networks"
          ],
          "day": "2025-12-25",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5869aed4bf3e962c5ac24490891076a87e2bc2b992a51fbf1ce76fcce3ce32b_w640_q70.webp",
          "contributions": "1. Proposed an autonomous uncertainty quantification technique for computational point-of-care diagnostics to mitigate erroneous neural network predictions. 2. Integrated a Monte Carlo dropout-based method into a diagnostic pipeline to identify and exclude high-uncertainty predictions without needing ground truth. 3. Demonstrated the method's effectiveness on a Lyme disease diagnostic platform, significantly improving sensitivity from 88.2% to 95.7% in blinded testing.",
          "summary": "This paper addresses the problem of erroneous predictions from neural networks in computational point-of-care diagnostic sensors. The authors propose an autonomous uncertainty quantification method using Monte Carlo dropout to identify and exclude unreliable predictions. Their approach, tested on a Lyme disease diagnostic platform, significantly improved diagnostic sensitivity, demonstrating enhanced robustness for neural network-driven sensing systems.",
          "mindmap": "graph LR\n    A[Autonomous Uncertainty Quantification for Computational Point-of-care Sensors] --> B[核心问题/Problem: Neural network hallucinations cause misdiagnosis in POC sensors]\n    A --> C[主要方法/Method: Integrate Monte Carlo dropout for autonomous uncertainty quantification]\n    A --> D[关键结果/Results: Diagnostic sensitivity increased from 88.2% to 95.7%]"
        },
        {
          "title": "QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning",
          "authors": "Sebastian Racedo, Brigitte Jaumard, Oscar Delgado, Meysam Masoudi",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19696",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b77d355579fa14e02c66f844a9c1cf1fbc3d68fee2d28b38fc709f013395b1a_w640_q70.webp",
          "contributions": "",
          "summary": "QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning",
          "mindmap": ""
        },
        {
          "title": "PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility",
          "authors": "Md Nahid Hasan Shuvo, Moinul Hossain",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19711",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d516fcc6c3de6b0e65c078e5e3f3dda23bfd77fbb9c5f4abfe2c509c2cb6dfe_w640_q70.webp",
          "contributions": "",
          "summary": "PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility",
          "mindmap": ""
        },
        {
          "title": "Large Language Models for EDA Cloud Job Resource and Lifetime Prediction",
          "authors": "Yuxuan Yin, Shengke Zhou, Yunjie Zhang, Ajay Mohindra, Boxun Xu, Peng Li",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19701",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe0c6f8e6d98607a87a162a4a1cada21d732348d823658c9451d5ce5608a7d1_w640_q70.webp",
          "contributions": "",
          "summary": "Large Language Models for EDA Cloud Job Resource and Lifetime Prediction",
          "mindmap": ""
        },
        {
          "title": "Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data",
          "authors": "Behrooz Mamandipoor, Chun-Nan Hsu, Martin Krause, Ulrich H. Schmidt, Rodney A. Gabriel",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19716",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b80fac6c07719f0fdc3b2a60068a2f3820d61d75d5655632ad18fc7fbee5f80_w640_q70.webp",
          "contributions": "",
          "summary": "Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data",
          "mindmap": ""
        },
        {
          "title": "Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance",
          "authors": "James K Ruffle, Samia Mohinta, Guilherme Pombo, Asthik Biswas, Alan Campbell, Indran Davagnanam, David Doig, Ahmed Hamman, Harpreet Hyare, Farrah Jabeen, Emma Lim, Dermot Mallon, Stephanie Owen, Sophie Wilkinson, Sebastian Brandner, Parashkev Nachev",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19707",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e098f2b53a5d94739b784dac1a98f71b53ab4d9f759c65700bc9e1f9500bbafd_w640_q70.webp",
          "contributions": "",
          "summary": "Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance",
          "mindmap": ""
        },
        {
          "title": "Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches",
          "authors": "Taoran Sheng, Manfred Huber",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19713",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8069c49480aa4056d903366d9a07ef262001809b60e89caaaab99b1ab6318bb6_w640_q70.webp",
          "contributions": "",
          "summary": "Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches",
          "mindmap": ""
        },
        {
          "title": "Node-Level Financial Optimization in Demand Forecasting Through Dynamic Cost Asymmetry and Feedback Mechanism",
          "authors": "Alessandro Casadei, Clemens Grupp, Sreyoshi Bhaduri, Lu Guo, Wilson Fung, Rohit Malshe, Raj Ratan, Ankush Pole, Arkajit Rakshit",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19722",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5fc3669c6e3b3729a8ceeab7556b8b190a71237342736f2d91f0e9e99de3dc0_w640_q70.webp",
          "contributions": "",
          "summary": "Node-Level Financial Optimization in Demand Forecasting Through Dynamic Cost Asymmetry and Feedback Mechanism",
          "mindmap": ""
        },
        {
          "title": "Per-Axis Weight Deltas for Frequent Model Updates",
          "authors": "Stefan Kuyumdzhiev, Radostin Cholakov",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19720",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fced4b263f86d7aa4d12f96b832ddc3447334c926ee67499537f6d38e8e740d_w640_q70.webp",
          "contributions": "",
          "summary": "Per-Axis Weight Deltas for Frequent Model Updates",
          "mindmap": ""
        },
        {
          "title": "Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference",
          "authors": "Zhan Zhang",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19717",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ee62945031af7f6dac3a6d51384974eec1f8f5db6dd103388502d84eb58bc6a_w640_q70.webp",
          "contributions": "",
          "summary": "Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference",
          "mindmap": ""
        },
        {
          "title": "Sign-Aware Multistate Jaccard Kernels and Geometry for Real and Complex-Valued Signals",
          "authors": "Vineet Yadav",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19721",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/476a71567ad37a78f3007d1e492eb010eed6a7f8bed8a187d46ae8e80465f4f5_w640_q70.webp",
          "contributions": "",
          "summary": "Sign-Aware Multistate Jaccard Kernels and Geometry for Real and Complex-Valued Signals",
          "mindmap": ""
        },
        {
          "title": "Multiscale Dual-path Feature Aggregation Network for Remaining Useful Life Prediction of Lithium-Ion Batteries",
          "authors": "Zihao Lv, Siqi Ai, Yanbin Zhang",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19719",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6292853f8fb29c3648a6a9e7a018fcb02691dba13e4d6ce37a63f296f046554_w640_q70.webp",
          "contributions": "",
          "summary": "Multiscale Dual-path Feature Aggregation Network for Remaining Useful Life Prediction of Lithium-Ion Batteries",
          "mindmap": ""
        },
        {
          "title": "Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural, and graph-based evaluation of synthetic tabular data",
          "authors": "Vasileios C. Pezoulas, Nikolaos S. Tachos, Eleni Georga, Kostas Marias, Manolis Tsiknakis, Dimitrios I. Fotiadis",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19718",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/690024688881d156d3131447c4b795c922984c2e3732a32a3b139b183a1be23e_w640_q70.webp",
          "contributions": "",
          "summary": "Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural, and graph-based evaluation of synthetic tabular data",
          "mindmap": ""
        },
        {
          "title": "Tiny, On-Device Decision Makers with the MiniConv Library",
          "authors": "Carlos Purves",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19726",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fcebc0e7acd2aa33081184298763fb8df6f0a43f23fb341b0e84da9a69d1bd6_w640_q70.webp",
          "contributions": "",
          "summary": "Tiny, On-Device Decision Makers with the MiniConv Library",
          "mindmap": ""
        },
        {
          "title": "Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking",
          "authors": "Srishti Gupta, Riccardo Balia, Daniele Angioni, Fabio Brau, Maura Pintor, Ambra Demontis, Alessandro Sebastian, Salvatore Mario Carta, Fabio Roli, Battista Biggio",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19725",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a6b157cb48d9bcd740b085c12288bed5c0e95c01a7146a5e7c2e50b7d77787d_w640_q70.webp",
          "contributions": "",
          "summary": "Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking",
          "mindmap": ""
        },
        {
          "title": "Trend Extrapolation for Technology Forecasting: Leveraging LSTM Neural Networks for Trend Analysis of Space Exploration Vessels",
          "authors": "Peng-Hung Tsai, Daniel Berleant",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19727",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2c9404cbdd59c5c56c246473b4a6996ca55f6051700faa6c654323d6990a290_w640_q70.webp",
          "contributions": "",
          "summary": "Trend Extrapolation for Technology Forecasting: Leveraging LSTM Neural Networks for Trend Analysis of Space Exploration Vessels",
          "mindmap": ""
        },
        {
          "title": "End-to-End Data Quality-Driven Framework for Machine Learning in Production Environment",
          "authors": "Firas Bayram, Bestoun S. Ahmed, Erik Hallin",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19723",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e87a73b8e332e90e3cd2839f1faa8882cdeef6cb35e274b072b141a0cabb8572_w640_q70.webp",
          "contributions": "",
          "summary": "End-to-End Data Quality-Driven Framework for Machine Learning in Production Environment",
          "mindmap": ""
        },
        {
          "title": "Hard Negative Sample-Augmented DPO Post-Training for Small Language Models",
          "authors": "Haocheng Lu, Minjun Zhu, Henry Yu",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19728",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c371de45b1b11fd08dee5a184a8d1c0b7de0a0ff5ecd8c96c44e8fddf3eabedc_w640_q70.webp",
          "contributions": "",
          "summary": "Hard Negative Sample-Augmented DPO Post-Training for Small Language Models",
          "mindmap": ""
        },
        {
          "title": "ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures",
          "authors": "Zhonghao Yang, Cheng Luo, Daojing He, Yiming Li, Yu Li",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19730",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66bbe64f6f3cac5df3dfd69a018806ee7d1973ac071d8af57c13752826c622fe_w640_q70.webp",
          "contributions": "",
          "summary": "ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures",
          "mindmap": ""
        },
        {
          "title": "High-Performance Self-Supervised Learning by Joint Training of Flow Matching",
          "authors": "Kosuke Ukita, Tsuyoshi Okita",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19729",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/269f1eff8cf71b204b6147341b992a81faef6468f33e5edd7a5be137a0ac9100_w640_q70.webp",
          "contributions": "",
          "summary": "High-Performance Self-Supervised Learning by Joint Training of Flow Matching",
          "mindmap": ""
        },
        {
          "title": "Leakage-Aware Bandgap Prediction on the JARVIS-DFT Dataset: A Phase-Wise Feature Analysis",
          "authors": "Gaurav Kumar Sharma",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19732",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53da428992d5e5cae28642416139ec0d148864a5c060b683ac9e143fe97079ee_w640_q70.webp",
          "contributions": "",
          "summary": "Leakage-Aware Bandgap Prediction on the JARVIS-DFT Dataset: A Phase-Wise Feature Analysis",
          "mindmap": ""
        },
        {
          "title": "Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems",
          "authors": "Xiangzhong Luo, Weichen Liu",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19731",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c10d82314834148af959284b60a6d6f0fa9e36928106648626f8c43d4f07b79_w640_q70.webp",
          "contributions": "",
          "summary": "Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems",
          "mindmap": ""
        },
        {
          "title": "OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting",
          "authors": "Wilson Fung, Lu Guo, Drake Hilliard, Alessandro Casadei, Raj Ratan, Sreyoshi Bhaduri, Adi Surve, Nikhil Agarwal, Rohit Malshe, Pavan Mullapudi, Hungjen Wang, Saurabh Doodhwala, Ankush Pole, Arkajit Rakshit",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19738",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa995500ed323b0099b96191763cdf14300972d797c5bdf631faa22d483ef8d4_w640_q70.webp",
          "contributions": "",
          "summary": "OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting",
          "mindmap": ""
        },
        {
          "title": "The Deleuzian Representation Hypothesis",
          "authors": "Clément Cornet, Romaric Besançon, Hervé Le Borgne",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19734",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/877f862cfd6b85ad4699167cc9b9bc17de797ef85a7ede6911637da4967d6121_w640_q70.webp",
          "contributions": "",
          "summary": "The Deleuzian Representation Hypothesis",
          "mindmap": ""
        },
        {
          "title": "Simulation-Driven Railway Delay Prediction: An Imitation Learning Approach",
          "authors": "Clément Elliker, Jesse Read, Sonia Vanier, Albert Bifet",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19737",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37e9a570297730f5200e5c0dcac9576f29dc77d8856f607f480d2a083088332_w640_q70.webp",
          "contributions": "",
          "summary": "Simulation-Driven Railway Delay Prediction: An Imitation Learning Approach",
          "mindmap": ""
        },
        {
          "title": "CoPHo: Classifier-guided Conditional Topology Generation with Persistent Homology",
          "authors": "Gongli Xi, Ye Tian, Mengyu Yang, Zhenyu Zhao, Yuchao Zhang, Xiangyang Gong, Xirong Que, Wendong Wang",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19736",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68578352cc68ad1305abf54d27488dc8db7f857ff2484ef8acd9ab80b0db8641_w640_q70.webp",
          "contributions": "",
          "summary": "CoPHo: Classifier-guided Conditional Topology Generation with Persistent Homology",
          "mindmap": ""
        },
        {
          "title": "Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction",
          "authors": "Gangxiong Zhang, Yongchao Long",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19735",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffac075f9a08ac05f63c8e47026ade8aa961ca7bcc7071d314dbbcf27a110f66_w640_q70.webp",
          "contributions": "",
          "summary": "Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction",
          "mindmap": ""
        },
        {
          "title": "EdgeFlex-Transformer: Transformer Inference for Edge Devices",
          "authors": "Shoaib Mohammad, Guanqun Song, Ting Zhu",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19741",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a472c5a3779ea5b970e030fee19030719f5db1d085d239de2dc5df41dffd7439_w640_q70.webp",
          "contributions": "",
          "summary": "EdgeFlex-Transformer: Transformer Inference for Edge Devices",
          "mindmap": ""
        },
        {
          "title": "Asia Cup 2025: A Structured T20 Match-Level Dataset and Exploratory Analysis for Cricket Analytics",
          "authors": "Kousar Raza, Faizan Ali",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19740",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f547c21d7d60b5ce4f92522c83aa7c297fc051a77b6eddf69ef405348fda9d8_w640_q70.webp",
          "contributions": "",
          "summary": "Asia Cup 2025: A Structured T20 Match-Level Dataset and Exploratory Analysis for Cricket Analytics",
          "mindmap": ""
        },
        {
          "title": "On-device Large Multi-modal Agent for Human Activity Recognition",
          "authors": "Md Shakhrul Iman Siam, Ishtiaque Ahmed Showmik, Guanqun Song, Ting Zhu",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19742",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f3cc1dde2877d131c87748e2a0e8975b217b5776f2152e724bf632e2fa6532f_w640_q70.webp",
          "contributions": "",
          "summary": "On-device Large Multi-modal Agent for Human Activity Recognition",
          "mindmap": ""
        },
        {
          "title": "From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning",
          "authors": "Sasan Sharifipour, Constantino Álvarez Casado, Manuel Lage Cañellas, Miguel Bordallo López",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19743",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28b9eb6359f294d9654c6f1fea215bc4726b236c77ba0b6790735d53dbad5ead_w640_q70.webp",
          "contributions": "",
          "summary": "From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning",
          "mindmap": ""
        },
        {
          "title": "OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian Optimization in TinyML Keyword Spotting",
          "authors": "Soumen Garai, Suman Samui",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19739",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9778cadae68709b008dcf5db962164fda68a414cd3d9f0a2847361f564c06bad_w640_q70.webp",
          "contributions": "",
          "summary": "OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian Optimization in TinyML Keyword Spotting",
          "mindmap": ""
        },
        {
          "title": "DeepBridge: A Unified and Production-Ready Framework for Multi-Dimensional Machine Learning Validation",
          "authors": "Gustavo Coelho Haase, Paulo Henrique Dourado da Silva",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19744",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0bccd562ab93d21bfa941fe354c60f5ea1f28b2e74751c8d34532405481aeeca_w640_q70.webp",
          "contributions": "",
          "summary": "DeepBridge: A Unified and Production-Ready Framework for Multi-Dimensional Machine Learning Validation",
          "mindmap": ""
        },
        {
          "title": "How Many Experts Are Enough? Towards Optimal Semantic Specialization for Mixture-of-Experts",
          "authors": "Sumin Park, Noseong Park",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19765",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/515767751abfd73b2b6370592d087c270228e210ccda8fa867a672db0ae07a01_w640_q70.webp",
          "contributions": "",
          "summary": "How Many Experts Are Enough? Towards Optimal Semantic Specialization for Mixture-of-Experts",
          "mindmap": ""
        },
        {
          "title": "Learning to Design City-scale Transit Routes",
          "authors": "Bibek Poudel, Weizi Li",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19767",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48f5a0e4652c24c99f21521fcb359e848d25f4d63cc6c81bee61cbd2088a47c6_w640_q70.webp",
          "contributions": "",
          "summary": "Learning to Design City-scale Transit Routes",
          "mindmap": ""
        },
        {
          "title": "A K-Means, Ward and DBSCAN repeatability study",
          "authors": "Anthony Bertrand, Engelbert Mephu Nguifo, Violaine Antoine, David Hill",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19772",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b606a0690bd58fd61c6e296efa76193ecfe74e0fd82dcf2bd79d638111e3e1d1_w640_q70.webp",
          "contributions": "",
          "summary": "A K-Means, Ward and DBSCAN repeatability study",
          "mindmap": ""
        },
        {
          "title": "Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy",
          "authors": "Deepit Sapru",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19805",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb04e43886d4319737a1d917f74a55c539bf9fd5290a700f1e160de279d18cef_w640_q70.webp",
          "contributions": "",
          "summary": "Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy",
          "mindmap": ""
        },
        {
          "title": "Reduced Order Modeling for Tsunami Forecasting with Bayesian Hierarchical Pooling",
          "authors": "Shane X. Coffing, John Tipton, Arvind T. Mohan, Darren Engwirda",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19804",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e689d3fa5a81d3d8eba58c25d7cb4a0158b1806b3118990d514118edc1fa566_w640_q70.webp",
          "contributions": "",
          "summary": "Reduced Order Modeling for Tsunami Forecasting with Bayesian Hierarchical Pooling",
          "mindmap": ""
        },
        {
          "title": "UCCL-EP: Portable Expert-Parallel Communication",
          "authors": "Ziming Mao, Yihan Zhang, Chihan Cui, Kaichao You, Zhongjie Chen, Zhiying Xu, Scott Shenker, Costin Raiciu, Yang Zhou, Ion Stoica",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19849",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb2d143c0a9d64bd2c5bdf4142e8e3a096290fd8319372321c00c3c17d53b658_w640_q70.webp",
          "contributions": "",
          "summary": "UCCL-EP: Portable Expert-Parallel Communication",
          "mindmap": ""
        },
        {
          "title": "Fine-Tuned In-Context Learners for Efficient Adaptation",
          "authors": "Jorg Bornschein, Clare Lyle, Yazhe Li, Amal Rannen-Triki, Xu Owen He, Razvan Pascanu",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19879",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31980c4d9f6b1c6d6c1f0f41df293fd637d43c4ea2f2de5d26aa825310d8bdbc_w640_q70.webp",
          "contributions": "",
          "summary": "Fine-Tuned In-Context Learners for Efficient Adaptation",
          "mindmap": ""
        },
        {
          "title": "Detecting cyberbullying in Spanish texts through deep learning techniques",
          "authors": "Paúl Cumba-Armijos, Diego Riofrío-Luzcando, Verónica Rodríguez-Arboleda, Joe Carrión-Jumbo",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19899",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d2f7ccac215958604ec2bafb628962c08cfada143b59dda8159af7e4af21661_w640_q70.webp",
          "contributions": "",
          "summary": "Detecting cyberbullying in Spanish texts through deep learning techniques",
          "mindmap": ""
        },
        {
          "title": "Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning",
          "authors": "Jiayun Wu, Jiashuo Liu, Zhiyuan Zeng, Tianyang Zhan, Wenhao Huang",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19920",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33255a480cb8654f8d9838cb7a634102f7086c3eaac9fb63148c10866248563a_w640_q70.webp",
          "contributions": "",
          "summary": "Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning",
          "mindmap": ""
        },
        {
          "title": "Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling",
          "authors": "Indranil Halder, Cengiz Pehlevan",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19905",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1e6db0153f278bc11740f6ab7077ed6a29fff1f323057711a5dc1210d6e99fe_w640_q70.webp",
          "contributions": "",
          "summary": "Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling",
          "mindmap": ""
        },
        {
          "title": "Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra",
          "authors": "Maxime Lacour, Pu Ren, Rie Nakata, Nori Nakata, Michael Mahoney",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19909",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e8e80ac4736f89a1123273e8c6f77a605a941de3087891673a6d7728a3d0998_w640_q70.webp",
          "contributions": "",
          "summary": "Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra",
          "mindmap": ""
        },
        {
          "title": "The Seismic Wavefield Common Task Framework",
          "authors": "Alexey Yermakov, Yue Zhao, Marine Denolle, Yiyu Ni, Philippe M. Wyder, Judah Goldfeder, Stefano Riva, Jan Williams, David Zoro, Amy Sara Rude, Matteo Tomasetto, Joe Germany, Joseph Bakarji, Georg Maierhofer, Miles Cranmer, J. Nathan Kutz",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19927",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b59879c33166ae9f8c44c6fb1768cff1db7963674dc0f8347a443a44df80bf19_w640_q70.webp",
          "contributions": "",
          "summary": "The Seismic Wavefield Common Task Framework",
          "mindmap": ""
        },
        {
          "title": "Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress",
          "authors": "Samruddhi Baviskar",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19935",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f15b3890b1332bf926501d440f418e2e71c3b0c8c5b3dd19380d13e172c25ac_w640_q70.webp",
          "contributions": "",
          "summary": "Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress",
          "mindmap": ""
        },
        {
          "title": "Vehicle-centric Perception via Multimodal Structured Pre-training",
          "authors": "Wentao Wu, Xiao Wang, Chenglong Li, Jin Tang, Bin Luo",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19934",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25393280cf5e09ecc25c375a88de26bef6b6904fd90a6775cf7591ed8a615fe1_w640_q70.webp",
          "contributions": "",
          "summary": "Vehicle-centric Perception via Multimodal Structured Pre-training",
          "mindmap": ""
        },
        {
          "title": "Block-Recurrent Dynamics in Vision Transformers",
          "authors": "Mozes Jacobs, Thomas Fel, Richard Hakim, Alessandra Brondetta, Demba Ba, T. Andy Keller",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19941",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e10f9b4ab6d210902b2c5092ebfe1fcc82dd7a3d2032bfc97fbfadd16867c2a_w640_q70.webp",
          "contributions": "",
          "summary": "Block-Recurrent Dynamics in Vision Transformers",
          "mindmap": ""
        },
        {
          "title": "Spatio-Temporal Graph Neural Networks for Dairy Farm Sustainability Forecasting and Counterfactual Policy Analysis",
          "authors": "Surya Jayakumar, Kieran Sullivan, John McLaughlin, Christine O'Meara, Indrakshi Dey",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19970",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04d93c1a47d24166acae7ed5012d760c9a02de04651f40a319ded305b7aaad13_w640_q70.webp",
          "contributions": "",
          "summary": "Spatio-Temporal Graph Neural Networks for Dairy Farm Sustainability Forecasting and Counterfactual Policy Analysis",
          "mindmap": ""
        },
        {
          "title": "Bloom Filter Encoding for Machine Learning",
          "authors": "John Cartmell, Mihaela Cardei, Ionut Cardei",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19991",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e677d5a3d466425f205097a14f528d2211f9cb2642b715b0647694a1db34a243_w640_q70.webp",
          "contributions": "",
          "summary": "Bloom Filter Encoding for Machine Learning",
          "mindmap": ""
        },
        {
          "title": "A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection",
          "authors": "Tamim Ahasan Rijon, Yeasin Arafath",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19989",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7415dab1086d45b2bb6f7a49ab33be753b3da047de23310de552ead99dc2448_w640_q70.webp",
          "contributions": "",
          "summary": "A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection",
          "mindmap": ""
        },
        {
          "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
          "authors": "Ming Li, Chenrui Fan, Yize Cheng, Soheil Feizi, Tianyi Zhou",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.19995",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf100eeda1c44688829760003993b1a83478a2d2901a0f5a0b9914bc5ef7c3b0_w640_q70.webp",
          "contributions": "",
          "summary": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
          "mindmap": ""
        },
        {
          "title": "Control Variate Score Matching for Diffusion Models",
          "authors": "Khaled Kahouli, Romuald Elie, Klaus-Robert Müller, Quentin Berthet, Oliver T. Unke, Arnaud Doucet",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.20003",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99c1fcd9cfbbb5a241296c106e6f4311b324493691659f27747af21f56a722fe_w640_q70.webp",
          "contributions": "",
          "summary": "Control Variate Score Matching for Diffusion Models",
          "mindmap": ""
        },
        {
          "title": "Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance",
          "authors": "Sukumar Kishanthan, Asela Hevapathige",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.20006",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/767f4e4dcb2738c000b0ea66f1109ff56b696feb519040f06df4fdd1a29c343a_w640_q70.webp",
          "contributions": "",
          "summary": "Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance",
          "mindmap": ""
        },
        {
          "title": "IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense",
          "authors": "Rahul Yumlembam, Biju Issac, Seibu Mary Jacob, Longzhi Yang",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.20004",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea3f077bcaec1c639c8029881602d31bdf125a8cbefc2e15ec9ba3e07c126ee1_w640_q70.webp",
          "contributions": "",
          "summary": "IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense",
          "mindmap": ""
        },
        {
          "title": "LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models",
          "authors": "Jiacheng You, Jingcheng Yang, Yuhang Xie, Zhongxuan Wu, Xiucheng Li, Feng Li, Pengjie Wang, Jian Xu, Bo Zheng, Xinyang Chen",
          "institution": "TBD",
          "link": "https://arxiv.org/pdf/2512.20002",
          "code": null,
          "tags": [],
          "day": "2025-12-24",
          "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3eef74ab05c46cb25ce1372769f7b8dc5bc12b1c381a6076e807a4bdde96f36_w640_q70.webp",
          "contributions": "",
          "summary": "LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models",
          "mindmap": ""
        }
      ]
>>>>>>> d9840a1
    },
    {
      "label": "cs.LO",
      "slug": "cslo",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.MA",
      "slug": "csma",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.MM",
      "slug": "csmm",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.MS",
      "slug": "csms",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.NE",
      "slug": "csne",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.NI",
      "slug": "csni",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.OS",
      "slug": "csos",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.PF",
      "slug": "cspf",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.PL",
      "slug": "cspl",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.RO",
      "slug": "csro",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.SC",
      "slug": "cssc",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.SD",
      "slug": "cssd",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.SE",
      "slug": "csse",
      "week": "20251229-20260104",
      "items": []
    },
    {
      "label": "cs.SI",
      "slug": "cssi",
      "week": "20251229-20260104",
      "items": []
    }
  ]
}