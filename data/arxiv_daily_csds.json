{
  "label": "cs.DS",
  "slug": "csds",
  "week": "20260105-20260111",
  "items": [
    {
      "title": "Bounds on Longest Simple Cycles in Weighted Directed Graphs via Optimum Cycle Means",
      "authors": "Ali Dasdan",
      "institution": "KD Consulting",
      "link": "https://arxiv.org/pdf/2601.00094",
      "code": null,
      "tags": [
        "graph algorithms",
        "longest simple cycle",
        "optimum cycle means",
        "weighted directed graphs",
        "algebraic bounds",
        "heuristic approximations"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f91ce51ca3513f1ba03d0bc651207d28bc3a73c00d26a8987551dc93debe8db2_w640_q70.webp",
      "contributions": "1. Introduces polynomial-time computable strict algebraic bounds for the longest simple cycle using optimum cycle means. 2. Proposes heuristic approximations for the longest cycle's weight and length based on these means, offering high accuracy. 3. Provides a rigorous algebraic analysis linking cycle mean statistics to longest/shortest cycle properties and validates the trade-off between bounds and approximations on benchmark circuits.",
      "summary": "This paper addresses the NP-hard problem of finding the longest simple cycle in a weighted directed graph. It proposes using efficiently computable optimum cycle means to derive both strict algebraic bounds for search space pruning and accurate heuristic approximations for the cycle's value. Experiments on circuit benchmarks show the heuristic approximations achieve low median errors (6-14%), while the strict bounds, though loose, are useful for pruning, and they also observe a correlation between long cycles and large weights.",
      "mindmap": "graph TB\n        A(论文标题: Bounds on Longest Simple Cycles in Weighted Directed Graphs via Optimum Cycle Means) --> B(核心问题: 寻找有向加权图中的最长简单环是NP难问题 / Problem: Finding the longest simple cycle in a weighted directed graph is NP-hard)\n        A --> C(主要方法: 利用最优环均值计算严格代数界和启发式近似 / Method: Using optimum cycle means to derive strict algebraic bounds and heuristic approximations)\n        A --> D(关键结果: 启发式近似误差小(6-14%)，严格界可用于剪枝，长环常具大权重 / Results: Heuristic approximations have low error (6-14%), strict bounds are useful for pruning, long cycles often have large weights)"
    },
    {
      "title": "Efficient Algorithms for Adversarially Robust Approximate Nearest Neighbor Search",
      "authors": "Alexandr Andoni, Themistoklis Haris, Esty Kelman, Krzysztof Onak",
      "institution": "Columbia University, Boston University, Massachusetts Institute of Technology",
      "link": "https://arxiv.org/pdf/2601.00272",
      "code": null,
      "tags": [
        "privacy-preserving algorithms",
        "adaptive adversary",
        "approximate nearest neighbor",
        "locality-sensitive hashing",
        "differential privacy",
        "fairness"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7e3fe5ddcef8f5187b965ed1f0cfa85fc0a705f1f3b4754dfcc600aa13fccab_w640_q70.webp",
      "contributions": "1. Establishes a novel connection between adaptive security and fairness for ANN search, using fair ANN to hide internal randomness. 2. Introduces a novel concentric-annuli LSH construction that synthesizes fairness and differential privacy to break the inherent √n query time barrier. 3. Proposes specialized algorithms with strong \"for-all\" guarantees for the low-dimensional regime, based on novel metric covering constructions for Hamming and ℓ_p spaces.",
      "summary": "This paper tackles the Approximate Nearest Neighbor (ANN) search problem under a powerful adaptive adversary. It proposes a sequence of algorithms, primarily for high dimensions, that combine concepts from fairness and differential privacy with a novel concentric-annuli LSH structure to achieve robust, data-independent performance and break a fundamental query time barrier. For low dimensions, it introduces new metric covering constructions to guarantee correctness on every possible query.",
      "mindmap": "graph TB\n        Root(”Efficient Algorithms for Adversarially Robust Approximate Nearest Neighbor Search<br>对抗鲁棒近似最近邻搜索的高效算法”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n    \n        Problem --> P1(”高维ANN受自适应对手攻击<br>High-dim ANN under Adaptive Adversary”)\n        Problem --> P2(”需要数据无关的性能保证<br>Need Data-Independent Performance”)\n    \n        Method --> M1(”连接自适应安全与公平性<br>Link Adaptive Security & Fairness”)\n        Method --> M2(”使用差分隐私的鲁棒决策<br>Robust Decision via Differential Privacy”)\n        Method --> M3(”新型同心环LSH构造<br>Novel Concentric-Annuli LSH”)\n        Method --> M4(”低维专用算法与度量覆盖<br>Low-dim Algorithms & Metric Coverings”)\n    \n        Results --> R1(”打破√n查询时间壁垒<br>Break √n Query Time Barrier”)\n        Results --> R2(”改进公平ANN结果<br>Improve Fair ANN Results”)\n        Results --> R3(”为所有查询提供强保证<br>Strong 'For-All' Guarantee for Queries”)"
    },
    {
      "title": "Deterministic Coreset for Lp Subspace",
      "authors": "Rachit Chhaya, Anirban Dasgupta, Dan Feldman, Supratim Shit",
      "institution": "Dhirubhai Ambani University, IIT Gandhinagar, University of Haifa, IIIT-Delhi",
      "link": "https://arxiv.org/pdf/2601.00361",
      "code": null,
      "tags": [
        "randomized algorithms",
        "numerical linear algebra",
        "data summarization",
        "coreset",
        "subspace embedding",
        "ℓp regression",
        "deterministic algorithm",
        "iterative algorithm"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b23fc2c49cea022bba3ab19cb79b7de330860628aab1b669177840bc826ecda1_w640_q70.webp",
      "contributions": "1. Introduces the first iterative algorithm for constructing a deterministic ε-coreset for ℓp subspace embedding for any p in [1,∞). 2. Achieves an optimal coreset size of O(d^\\{max(1,p/2)\\}/ε²), removing long-standing logarithmic factors. 3. Provides a deterministic guarantee for the coreset, enabling its use for approximately solving ℓp regression deterministically.",
      "summary": "This paper presents a new iterative algorithm for constructing a deterministic coreset that provides an ℓp subspace embedding for any p≥1. The method ensures bounded loss in each iteration, leading to a coreset whose size is optimal and free of logarithmic factors. The result solves a long-standing open problem and enables deterministic approximate solutions to ℓp regression.",
      "mindmap": "graph TB\n        Root(”Deterministic Coreset for Lp Subspace”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”需要为 ℓp 子空间嵌入构建确定性核心集 / Need deterministic coreset for ℓp subspace embedding”)\n        Problem --> P2(”现有核心集存在对数因子 / Existing coresets have log factors”)\n        Method --> M1(”迭代算法 / Iterative algorithm”)\n        Method --> M2(”确保有界损失 / Ensures bounded loss”)\n        Results --> R1(”核心集大小: O(d^{max(1,p/2)}/ε²) / Coreset size: O(d^{max(1,p/2)}/ε²)”)\n        Results --> R2(”移除对数因子 / Removes log factors”)\n        Results --> R3(”确定性保证 / Deterministic guarantee”)"
    },
    {
      "title": "Mind the Gap. Doubling Constant Parametrization of Weighted Problems: TSP, Max-Cut, and More",
      "authors": "Mihail Stoian",
      "institution": "University of Technology Nuremberg",
      "link": "https://arxiv.org/pdf/2601.00768",
      "code": null,
      "tags": [
        "parameterized complexity",
        "approximation algorithms",
        "doubling constant",
        "Freiman's theorem",
        "polynomial embedding",
        "weighted NP-hard problems",
        "meta-algorithm"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa64a4712c595b8ff5c4fad574e0eace93482c8ffa086d82a78c4f2bb511a9b3_w640_q70.webp",
      "contributions": "1. Introduces a new method to repurpose unweighted problem algorithms for their weighted versions without incurring pseudo-polynomial overhead for instances with small doubling. 2. Applies a constructive version of Freiman's theorem to convert input weights into polynomially bounded integers. 3. Shows that the time complexity for problems like TSP and Max-Cut becomes proportional to their unweighted versions under the small doubling condition.",
      "summary": "This paper addresses the difficulty of speeding up weighted NP-hard problems like TSP and Max-Cut. It proposes a meta-algorithm that uses a constructive Freiman's theorem to convert weights into polynomially bounded integers before applying polynomial embedding, making the runtime proportional to the unweighted version when the set of input weights has a small doubling constant. The main conclusion is that this approach provides a practical speedup for weighted instances with structured weight sets.",
      "mindmap": "graph TB\n        A[Paper Title: Mind the Gap. Doubling Constant Parametrization of Weighted Problems] --> B[核心问题/Problem: Weighted NP-hard problems resist speedups, unlike unweighted versions]\n        A --> C[主要方法/Method: Meta-algorithm using constructive Freiman's theorem to convert weights to polynomially bounded integers]\n        A --> D[关键结果/Results: Time complexity proportional to unweighted version for problems with small doubling constant]"
    },
    {
      "title": "Sparse Random Matrices for Dimensionality Reduction",
      "authors": "Pierre Mackenzie",
      "institution": "University of British Columbia",
      "link": "https://arxiv.org/pdf/2512.23756",
      "code": null,
      "tags": [
        "dimensionality reduction",
        "Johnson-Lindenstrauss lemma",
        "sparse random matrices",
        "matrix-vector multiplication",
        "Achlioptas construction",
        "Kane-Nelson construction"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a670cd5206d0ba658c3f68a4523ce09028f6474d50ca9c6c85877730f6cb232_w640_q70.webp",
      "contributions": "1. Outlines the theoretical constructions and proofs for sparse Johnson-Lindenstrauss transforms from Achlioptas (2003) and Kane & Nelson (2014). 2. Implements the sparse JL constructions for empirical evaluation. 3. Empirically compares the performance of these sparse constructions against the standard dense Gaussian JL matrices.",
      "summary": "This paper explores sparse random matrices for dimensionality reduction to speed up computation. It outlines and implements the sparse Johnson-Lindenstrauss constructions by Achlioptas and Kane & Nelson, and empirically compares them to the standard dense Gaussian approach. The main conclusion is an empirical performance comparison of these faster, sparse methods.",
      "mindmap": "graph TB\n        Root(”Sparse Random Matrices for Dimensionality Reduction<br>稀疏随机矩阵用于降维”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”Dense JL matrices are computationally expensive.<br>稠密JL矩阵计算成本高。”)\n        Method --> M1(”Outline Achlioptas (2003) & Kane-Nelson (2014) sparse JL constructions.<br>概述稀疏JL构造。”)\n        Method --> M2(”Implement sparse JL transforms.<br>实现稀疏JL变换。”)\n        Results --> R1(”Empirical comparison with Gaussian JL.<br>与高斯JL进行实证比较。”)"
    },
    {
      "title": "Kidney Exchange: Faster Parameterized Algorithms and Tighter Lower Bounds",
      "authors": "Aritra Banik, Sujoy Bhore, Palash Dey, Abhishek Sahu",
      "institution": "National Institute of Science Education and Research Bhubaneswar, Indian Institute of Technology Bombay, Indian Institute of Technology Kharagpur",
      "link": "https://arxiv.org/pdf/2512.24037",
      "code": null,
      "tags": [
        "parameterized complexity",
        "kidney exchange",
        "FPT algorithm",
        "W1-hardness",
        "pathwidth",
        "treewidth"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00da3e6d7b1c46c83d5812783ee99ea1cfff1a2c3a708bddcff4c69f9ab7902c_w640_q70.webp",
      "contributions": "1. A new deterministic FPT algorithm for the kidney exchange problem parameterized by the number of patients receiving a kidney, improving the runtime from O*(14^t) to O*((4e)^t) ≈ O*(10.88^t). 2. A proof that the kidney exchange problem is W[1]-hard when parameterized by the pathwidth of the underlying graph, answering a natural question about the parameter's tractability. 3. Additional parameterized intractability results that improve the overall understanding of the problem's complexity landscape.",
      "summary": "This paper studies the computationally hard kidney exchange problem, where patient-donor pairs and altruistic donors exchange kidneys via cycles and paths. The authors present a faster deterministic parameterized algorithm for the standard parameter (number of patients receiving a kidney) and prove that the problem remains intractable (W[1]-hard) even when parameterized by pathwidth, a more restrictive structural parameter than treewidth.",
      "mindmap": "graph TB\n        Root[”Kidney Exchange: Faster Parameterized Algorithms and Tighter Lower Bounds<br>肾脏交换：更快的参数化算法与更紧的下界”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Kidney exchange is NP-complete<br>肾脏交换问题是NP完全问题”] --> P1[”限制/Constraint<br>Exchange via small cycles & paths<br>通过小环和路径交换”]\n        Method[”主要方法/Method<br>Parameterized Complexity<br>参数化复杂度”] --> M1[”参数/Parameter<br>Number of patients (t)<br>患者数量(t)”]\n        Method --> M2[”参数/Parameter<br>Graph pathwidth<br>图路径宽度”]\n        Results[”关键结果/Results”] --> R1[”算法改进/Algorithmic Improvement<br>FPT algorithm: O*((4e)^t)<br>FPT算法: O*((4e)^t)”]\n        Results --> R2[”下界/Lower Bound<br>W[1]-hard for pathwidth<br>对路径宽度是W[1]-难的”]"
    },
    {
      "title": "Faster Algorithms for Global Minimum Vertex-Cut in Directed Graphs",
      "authors": "Julia Chuzhoy, Ron Mosenzon, Ohad Trabelsi",
      "institution": "Toyota Technological Institute at Chicago, University of Haifa",
      "link": "https://arxiv.org/pdf/2512.24355",
      "code": null,
      "tags": [
        "graph algorithms",
        "vertex-cut",
        "directed graphs",
        "randomized algorithms",
        "graph connectivity",
        "minimum cut"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb68cda99b81d93f4242f2707f0a8a7f9ecd4fefbef705767501edf081d6d8b8_w640_q70.webp",
      "contributions": "1. Breaks the long-standing Θ(mn) time barrier for the directed global minimum vertex-cut problem with a new randomized algorithm. 2. Provides a faster algorithm for the unweighted version, nearly matching the best-known time for the edge-based variant. 3. Improves and simplifies the recent algorithm for the undirected vertex-cut problem.",
      "summary": "This paper presents new randomized algorithms that achieve faster running times for the directed global minimum vertex-cut problem. For weighted graphs, it breaks the long-standing Θ(mn) barrier, and for unweighted graphs, it provides an algorithm whose runtime nearly matches the best-known for the edge-based version. These results represent the first significant improvement in nearly three decades for this fundamental graph problem.",
      "mindmap": "graph TB\n        A[Faster Algorithms for Global Minimum Vertex-Cut in Directed Graphs] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[有向图全局最小顶点割/Directed Global Minimum Vertex-Cut]\n        B --> B2[29年未突破的O(mn)算法/29-year-old O(mn) barrier]\n        C --> C1[随机化算法/Randomized Algorithm]\n        C --> C2[处理加权与无权图/Handles Weighted & Unweighted]\n        D --> D1[加权: O(mn^0.976 polylog W)/Weighted: O(mn^0.976 polylog W)]\n        D --> D2[无权: O(min{m^1+o(1) k, n^2+o(1)})/Unweighted: O(min{m^1+o(1) k, n^2+o(1)})]\n        D --> D3[首次突破Θ(mn)障碍/First to break Θ(mn) barrier]"
    },
    {
      "title": "Fair Committee Selection under Ordinal Preferences and Limited Cardinal Information",
      "authors": "Ameet Gadekar, Aristides Gionis, Suhas Thejaswi, Sijing Tu",
      "institution": "CISPA Helmholtz Center for Information Security, KTH Royal Institute of Technology, Aalto University",
      "link": "https://arxiv.org/pdf/2512.24934",
      "code": null,
      "tags": [
        "computational social choice",
        "ordinal preferences",
        "distortion",
        "fair committee selection",
        "k-center",
        "metric space queries"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4abdbd3729a2ab27b55e1cf05a24f80c11aa1d9fbb2ad20cd88656afc99d4b62_w640_q70.webp",
      "contributions": "1. Introduces the ordinal fair k-center problem for fair committee selection with group representation constraints. 2. Proposes a factor-5 distortion algorithm requiring only O(k log² k) queries to the underlying metric space. 3. Presents an improved factor-3 distortion algorithm using O(k²) queries as an intermediate result.",
      "summary": "The paper addresses the problem of selecting a fair committee when agents provide only ordinal preference rankings, which is known to be hard. To overcome this, the authors propose algorithms that make a limited number of queries to the underlying cardinal distance information. Their main result is an algorithm that achieves a constant-factor approximation (distortion of 5) with a near-linear number of queries.",
      "mindmap": "graph TB\n        Root[公平委员会选择与序数偏好及有限基数信息<br/>Fair Committee Selection under Ordinal Preferences and Limited Cardinal Information]\n        Root --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[公平k-委员会选择<br/>Fair k-committee selection]\n        Problem --> P2[仅有序数偏好，无常数失真<br/>Only ordinal preferences, no constant distortion]\n        Method --> M1[查询底层度量空间<br/>Query underlying metric space]\n        Method --> M2[使用有限基数信息<br/>Use limited cardinal information]\n        Results --> R1[5倍失真算法<br/>Factor-5 distortion algorithm]\n        Results --> R2[O(k log² k)次查询<br/>O(k log² k) queries]"
    },
    {
      "title": "Approximations for the Weighted Reversal, Transposition, and Indel Distance Problem with Intergenic Region Information",
      "authors": "Gabriel Siqueira, Alexsandro Oliveira Alexandrino, Zanoni Dias",
      "institution": "Instituto de Computação, Universidade Estadual de Campinas (Unicamp)",
      "link": "https://arxiv.org/pdf/2512.25016",
      "code": null,
      "tags": [
        "computational biology",
        "genome rearrangement",
        "genome rearrangement",
        "weighted distance",
        "intergenic breakpoint graph",
        "approximation algorithm",
        "reversals and transpositions"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/feedbc7a429ecb8df2c88dda2c223f1bf5e052946f3a94334644978ec8ee2855_w640_q70.webp",
      "contributions": "1. Models genomes with gene orientation and intergenic region nucleotide lengths for rearrangement analysis. 2. Introduces the Labeled Intergenic Breakpoint Graph structure to analyze the weighted distance problem. 3. Provides an algorithm with guaranteed approximation ratios for the Weighted Reversal, Transposition, and Indel Distance problem under specific weight sets.",
      "summary": "This paper addresses the Weighted Reversal, Transposition, and Indel Distance problem for genomes with gene orientation and intergenic region information. It proposes an approximation algorithm based on a novel Labeled Intergenic Breakpoint Graph structure. The main result is a solution with guaranteed approximation bounds for specific operation weight sets.",
      "mindmap": "graph TB\n        A[Approximations for the Weighted Reversal, Transposition, and Indel Distance Problem with Intergenic Region Information] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[计算加权基因组重排距离/Computing Weighted Genome Rearrangement Distance]\n        C --> C1[使用带标签的基因间断点图/Using Labeled Intergenic Breakpoint Graph]\n        D --> D1[提出具有保证近似比的算法/Proposing Algorithm with Guaranteed Approximation Ratios]"
    },
    {
      "title": "Approximation Algorithms for Fair Repetitive Scheduling",
      "authors": "Danny Hermelin, Danny Segev, Dvir Shabtay",
      "institution": "Ben-Gurion University of the Negev, Tel Aviv University",
      "link": "https://arxiv.org/pdf/2512.25020",
      "code": null,
      "tags": [
        "approximation algorithms",
        "fair scheduling",
        "total completion time",
        "linear programming",
        "dynamic programming",
        "batching technique"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/790dfca3b0d61a5fed291947f59d03d4109fef73ae535b0d6069acf090e2094b_w640_q70.webp",
      "contributions": "1. A polynomial-time LP-based 2-approximation and a PTAS for the day-dependent processing time setting. 2. A simple polynomial-time (1+√2)/2+ϵ-approximation for the day-invariant processing time setting. 3. A quasi-polynomial-time approximation scheme (QPTAS) for the day-invariant setting with an arbitrary number of days.",
      "summary": "This paper tackles a fair repetitive scheduling problem where jobs from clients must be scheduled daily to minimize the maximum total completion time any client experiences. The authors propose approximation algorithms, including LP-based and dynamic programming approaches using a novel batching technique, achieving constant-factor approximations and approximation schemes for different problem variants. The main conclusion is that provable performance guarantees can be efficiently achieved for this NP-hard problem through these algorithmic techniques.",
      "mindmap": "graph TB\n        A[Approximation Algorithms for Fair Repetitive Scheduling] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[公平重复调度 / Fair Repetitive Scheduling]\n        B --> B2[最小化最大总完成时间 / Minimize Max Total Completion Time]\n        C --> C1[批处理技术 / Batching Technique]\n        C --> C2[线性规划 / Linear Programming]\n        C --> C3[动态规划 / Dynamic Programming]\n        D --> D1[2-近似与PTAS / 2-Approximation & PTAS]\n        D --> D2[(1+√2)/2+ϵ近似 / (1+√2)/2+ϵ Approximation]\n        D --> D3[准多项式时间近似方案 / QPTAS]"
    },
    {
      "title": "EF(X) Orientations: A Parameterized Complexity Perspective",
      "authors": "Sotiris Kanellopoulos, Edouard Nemery, Christos Pergaminelis, Minas Marios Sotiriou, Manolis Vasilakis",
      "institution": "National Technical University of Athens, Archimedes/Athena Research Center, Université Paris-Dauphine, PSL University, Athens University of Economics and Business",
      "link": "https://arxiv.org/pdf/2512.25033",
      "code": null,
      "tags": [
        "parameterized complexity",
        "fair division",
        "graph orientation",
        "envy-free",
        "EFX",
        "parameterized algorithm"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/803ef079ceedaf664f4a8debca603c158162ddd9284611ea7c047ddcf70ddcd5_w640_q70.webp",
      "contributions": "1. Initiates the study of EF (envy-free) orientations in graphs, analyzing their parameterized complexity. 2. Provides results that transfer to and improve upon the understanding of EFX orientations, answering an open question about their complexity on graphs with polynomially-bounded valuations. 3. Introduces and studies the concept of charity in the orientation setting, providing algorithms to find the minimum edges to remove for an EF(X) orientation to exist.",
      "summary": "This paper studies the problem of finding envy-free (EF) orientations in graphs, where edges represent goods contested by neighboring agent vertices. Using a parameterized complexity approach, it presents tractability and hardness results for both EF and EFX orientations, showing that EF can be easier than EFX in some cases like binary valuations. It also provides algorithms for the \"charity\" problem of removing minimal edges to achieve an EF(X) orientation.",
      "mindmap": "graph TB\n        Root[EF(X) Orientations: A Parameterized Complexity Perspective] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[研究EF图定向/Study EF Graph Orientation]\n        P1 --> P2[EFX已研究, EF未探索/EFX studied, EF unexplored]\n        Method[主要方法/Method] --> M1[参数化复杂度分析/Parameterized Complexity Analysis]\n        Results[关键结果/Results] --> R1[EF与EFX的易处理与困难案例/Tractable & Hard Cases for EF & EFX]\n        R1 --> R2[回答EFX开放问题/Answer EFX Open Question]\n        Results --> R3[引入慈善边移除问题/Introduce Charity Edge Removal]"
    },
    {
      "title": "Proper colorings of a graph in linear time using a number of colors linear in the maximum degree of the graph",
      "authors": "Kritika Bhandari, Mark Huber",
      "institution": "None (Inferred from arXiv submission; no explicit affiliation provided on first page)",
      "link": "https://arxiv.org/pdf/2512.24522",
      "code": null,
      "tags": [
        "graph algorithms",
        "proper coloring",
        "graph sampling",
        "linear-time algorithm",
        "maximum degree",
        "Markov chain Monte Carlo"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2df3f6d8f9edf2c385953da77f4effcefb8b3b532fac0ef601829b8505fab4b3_w640_q70.webp",
      "contributions": "1. A new algorithm for exact sampling from the set of proper colorings of a graph., 2. The algorithm achieves an expected running time linear in the graph size for graphs with maximum degree Δ., 3. It is the first algorithm with this guarantee when the number of colors exceeds 3.637Δ + 1.",
      "summary": "This paper presents a new algorithm for exactly sampling proper colorings of a graph. It is the first algorithm whose expected running time is guaranteed to be linear in the graph size when the number of colors is greater than 3.637 times the graph's maximum degree plus one.",
      "mindmap": "graph TB\n        A[Proper colorings of a graph in linear time<br/>图的线性时间正常着色] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Exact sampling from proper colorings<br/>从正常着色中精确采样]\n        C --> C1[New sampling algorithm<br/>新的采样算法]\n        D --> D1[Linear expected runtime<br/>线性期望运行时间]\n        D --> D2[Colors > 3.637Δ + 1<br/>颜色数 > 3.637Δ + 1]"
    },
    {
      "title": "On Circular Threshold Words and Other Stronger Versions of Dejean's conjecture",
      "authors": "Igor N. Tunev",
      "institution": "Ural Federal University",
      "link": "https://arxiv.org/pdf/2512.24581",
      "code": null,
      "tags": [
        "combinatorics on words",
        "Dejean's conjecture",
        "threshold words",
        "circular words",
        "repetition threshold",
        "combinatorial algorithm"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/36e023787ad16a1e81b90fdf42847fd22bb9f7095187b5fbf812ea6df93baee3_w640_q70.webp",
      "contributions": "1. A new method for proving specific cases of Dejean's conjecture for odd n≥5 using polynomial-time computer verification, producing circular threshold words. 2. An improved proof method reducing verification conditions and extending the construction to even cases n≥6. 3. A method to construct stronger threshold words with exponents arbitrarily close to 1 using a threshold word tree with exponential growth.",
      "summary": "This paper presents improved methods for constructing threshold words related to Dejean's conjecture. It introduces algorithms for generating circular threshold words for odd and even alphabet sizes and demonstrates the existence of threshold words where all long factors have exponents arbitrarily close to 1. The work is based on edited versions of the author's earlier student theses.",
      "mindmap": "graph TB\n        Root[”On Circular Threshold Words and Other Stronger Versions of Dejean's conjecture<br>关于圆形阈值词与德让猜想的其他强化版本”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Constructing threshold words (TWs) and circular TWs for Dejean's conjecture<br>为德让猜想构建阈值词与圆形阈值词”] --> P1[”构建无限阈值词<br>Construct infinite TWs”]\n        Problem --> P2[”构建循环移位不变的阈值词<br>Construct circular TWs”]\n        Method[”主要方法/Method<br>Polynomial-time computer verification and TW tree construction<br>多项式时间计算机验证与阈值词树构建”] --> M1[”针对奇数n≥5的验证方法<br>Method for odd n≥5”]\n        Method --> M2[”针对偶数n≥6的改进方法<br>Improved method for even n≥6”]\n        Method --> M3[”基于指数增长树的构建<br>Construction based on exponentially growing tree”]\n        Results[”关键结果/Results<br>Existence proofs and stronger word constructions<br>存在性证明与更强的词构建”] --> R1[”证明了圆形阈值词的存在性<br>Proved existence of circular TWs”]\n        Results --> R2[”构建了指数任意接近1的阈值词<br>Constructed TWs with exponent arbitrarily close to 1”]"
    },
    {
      "title": "Beyond Exact Fairness: Envy-Free Incomplete Connected Fair Division",
      "authors": "Ajaykrishnan E S, Daniel Lokshtanov",
      "institution": "University of California, Santa Barbara",
      "link": "https://arxiv.org/pdf/2512.22475",
      "code": null,
      "tags": [
        "computational complexity",
        "parameterized algorithms",
        "envy-free",
        "fair division",
        "parameterized complexity",
        "approximation scheme",
        "connectivity constraints"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91947bbf6f2aad0aa6ccfabf6ca791e4efb604ee8689ce989678ff084fa60adc_w640_q70.webp",
      "contributions": "Proved the computational hardness of the Envy-Free Incomplete Connected Fair Division problem even on star graphs with unary input, resolving an open problem., Designed an Efficient Parameterized Approximation Scheme for the problem when allowing a small amount of envy, parameterized by p and the number of agent types., Showed the algorithm works on general graphs and remains efficient even with binary input representation.",
      "summary": "This paper studies the Envy-Free Incomplete Connected Fair Division problem, where p graph vertices must be allocated to agents with connected, envy-free shares. It proves the problem is computationally hard for exact fairness but becomes efficiently solvable with a parameterized approximation scheme when slight envy is tolerated.",
      "mindmap": "graph TB\n        A[Beyond Exact Fairness: Envy-Free Incomplete Connected Fair Division] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[分配p个顶点给智能体 / Allocate p vertices to agents]\n        B --> B2[要求份额连通且无嫉妒 / Requires connected, envy-free shares]\n        C --> C1[精确算法 / Exact Algorithm]\n        C --> C2[近似算法 / Approximation Algorithm]\n        C1 --> C1a[证明计算困难性 / Prove computational hardness]\n        C2 --> C2a[设计高效参数化近似方案 / Design Efficient Parameterized Approximation Scheme]\n        D --> D1[精确问题是困难的 / Exact problem is hard]\n        D --> D2[容忍轻微嫉妒则高效 / Efficient if slight envy is tolerated]"
    },
    {
      "title": "Half-Approximating Maximum Dicut in the Streaming Setting",
      "authors": "Amir Azarmehr, Soheil Behnezhad, Shane Ferrante, Mohammad Saneian",
      "institution": "Northeastern University",
      "link": "https://arxiv.org/pdf/2512.22729",
      "code": null,
      "tags": [
        "streaming algorithms",
        "graph algorithms",
        "maximum directed cut",
        "sublinear space",
        "single-pass streaming",
        "approximation algorithm",
        "local algorithm"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2149d7c0d6552c07ed00cab5e3f138c4669e961bcf1b00423d27555b39d1c837_w640_q70.webp",
      "contributions": "1. Proved that a (1/2 - ε)-approximation for the maximum directed cut (dicut) problem is achievable in general graphs using truly sublinear space (n^(1-Ω_ε(1))), matching the known lower bound. 2. Developed a single-pass streaming algorithm that achieves this optimal approximation ratio, improving upon prior work which required two passes or was limited to constant-degree graphs. 3. Introduced a novel analysis technique focusing on how correlation propagates between high-degree and low-degree vertices when simulating a suitable local algorithm.",
      "summary": "This paper studies the maximum directed cut problem in the single-pass streaming model. The authors present an algorithm that, for any ε &gt; 0, achieves a (1/2 - ε)-approximation using truly sublinear space (n^(1-Ω_ε(1))) on general directed graphs. This result shows the known 0.5-approximation lower bound is tight, settling the approximation complexity of the problem.",
      "mindmap": "graph TB\n        A[”Half-Approximating Maximum Dicut in the Streaming Setting<br>流式设置中的最大有向割半近似”] --> B[”核心问题/Problem<br>在单次流式遍历和次线性空间下估计最大有向割的值<br>Estimate max dicut value with single pass & sublinear space”]\n        A --> C[”主要方法/Method<br>分析高/低度顶点间的相关性传播<br>Analyze correlation propagation among high-/low-degree vertices<br>模拟合适的局部算法<br>Simulate a suitable local algorithm”]\n        A --> D[”关键结果/Results<br>在一般图中实现 (1/2-ε) 近似<br>Achieve (1/2-ε)-approx in general graphs<br>使用 n^(1-Ω_ε(1)) 空间<br>Using n^(1-Ω_ε(1)) space<br>证明下界是紧的<br>Prove lower bound is tight”]"
    },
    {
      "title": "Computing parameters that generalize interval graphs using restricted modular partitions",
      "authors": "Flavia Bonomo-Braberman, Eric Brandwein, Ignasi Sau",
      "institution": "Universidad de Buenos Aires, CONICET, LIRMM (Université de Montpellier, CNRS)",
      "link": "https://arxiv.org/pdf/2512.22975",
      "code": null,
      "tags": [
        "parameterized complexity",
        "thinness",
        "simultaneous interval number",
        "modular cardinality",
        "interval graphs",
        "FPT algorithm"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62700ed7803827200c62e014fef205798b789455a6167da23a6753e12fc7b621_w640_q70.webp",
      "contributions": "1. Presents a linear kernel for the Thinness problem parameterized by the interval-modular cardinality. 2. Provides an FPT algorithm for the Simultaneous Interval Number problem parameterized by the cluster-modular cardinality plus the solution size. 3. Establishes negative results, showing that no polynomial kernels exist for Thinness and Simultaneous Interval Number when parameterized by various width parameters (e.g., treewidth, pathwidth) under standard complexity assumptions.",
      "summary": "This paper analyzes the parameterized complexity of computing two graph parameters, thinness and simultaneous interval number, which generalize interval graphs. It proposes new algorithms parameterized by modular cardinality measures, achieving a linear kernel for Thinness and an FPT algorithm for Simultaneous Interval Number. The results also imply new FPT algorithms for these problems when parameterized by well-known measures like neighborhood diversity and vertex cover number, while showing that polynomial kernels are unlikely for many other common parameters.",
      "mindmap": "graph TB\n        Root[Computing parameters that generalize interval graphs using restricted modular partitions] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[计算推广区间图的参数 / Computing parameters generalizing interval graphs]\n        P1 --> P2[分析薄度与同时区间数的复杂度 / Analyzing complexity of thinness and simultaneous interval number]\n        Method[主要方法/Method] --> M1[使用限制模划分 / Using restricted modular partitions]\n        M1 --> M2[参数化: 区间模基数, 簇模基数 / Parameterized by interval-modular, cluster-modular cardinality]\n        Results[关键结果/Results] --> R1[薄度的线性核 / Linear kernel for Thinness]\n        Results --> R2[同时区间数的FPT算法 / FPT algorithm for Simultaneous Interval Number]\n        Results --> R3[对多种宽度参数的无多项式核 / No polynomial kernels for many width parameters]"
    },
    {
      "title": "Lower bounds on pure dynamic programming for connectivity problems on graphs of bounded path-width",
      "authors": "Kacper Kluk, Jesper Nederlof",
      "institution": "University of Warsaw, Utrecht University",
      "link": "https://arxiv.org/pdf/2512.23121",
      "code": null,
      "tags": [
        "parameterized complexity",
        "tropical circuits",
        "pathwidth",
        "communication complexity",
        "Traveling Salesperson Problem",
        "dynamic programming"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86590df9819e19ad7303e5df124f5b2189c5e6ab6d542206119582e1e6c83135_w640_q70.webp",
      "contributions": "1. Proves unconditional lower bounds on the size of tropical circuits (modeling pure dynamic programming) for solving connectivity problems like TSP on graphs of bounded pathwidth. 2. Establishes a connection between tropical circuit complexity and the nondeterministic communication complexity of compatibility matrices. 3. Shows that any tropical circuit for TSP on a certain graph of pathwidth k requires at least 2^Ω(k log log k) gates, which is higher than known algebraic algorithms, suggesting algebra is necessary for competitive worst-case times.",
      "summary": "This paper studies the limitations of pure dynamic programming, modeled by tropical circuits, for solving connectivity problems like the Traveling Salesperson Problem on graphs with small pathwidth. It proves an unconditional lower bound of 2^Ω(k log log k) gates for any tropical circuit solving TSP on a specific graph of pathwidth k. This result, established via a link to communication complexity, suggests that algebraic techniques are unavoidable for achieving the fastest known worst-case running times for these problems.",
      "mindmap": "graph TB\n        A[Lower bounds on pure dynamic programming for connectivity problems on graphs of bounded path-width] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[评估纯动态规划对连通性问题的能力/Assess capability of pure DP for connectivity problems]\n        C --> C1[将热带电路复杂度与通信复杂性关联/Link tropical circuit complexity to communication complexity]\n        D --> D1[证明下界高于已知代数算法/Prove lower bound higher than known algebraic algorithms]"
    },
    {
      "title": "A Deterministic Bicriteria Approximation Algorithm for the Art Gallery Problem",
      "authors": "Khaled Elbassioni",
      "institution": "Khalifa University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.23297",
      "code": null,
      "tags": [
        "computational geometry",
        "art gallery problem",
        "approximation algorithm",
        "set cover",
        "epsilon-net",
        "deterministic algorithm"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26cad9af23205ee64a76b013d0010e95ddf0d344a1483b74292c0fc5d2fce832_w640_q70.webp",
      "contributions": "1. Provides a deterministic bicriteria approximation algorithm for the art gallery problem with holes, 2. Achieves a solution size of O(OPT * log(h+2) * log(OPT * log(h+2))) that sees (1-δ) of the polygon's area, 3. Runs in time polynomial in the number of holes, vertices, vertex bit-length, and log(1/δ).",
      "summary": "This paper presents a deterministic algorithm for the art gallery problem in polygons with holes. The algorithm finds a set of guards whose size is within a polylogarithmic factor of the optimum, and these guards collectively see at least a (1-δ) fraction of the polygon's area. The running time is polynomial in the relevant input parameters.",
      "mindmap": "graph TB\n        A[A Deterministic Bicriteria Approximation Algorithm for the Art Gallery Problem] --> B[核心问题/Problem: 在带洞多边形中找到最小的点集，使得多边形内所有点都可见]\n        A --> C[主要方法/Method: 确定性算法，基于集合覆盖和ε-net理论]\n        A --> D[关键结果/Results: 找到规模为O(OPT·log(h+2)·log(OPT·log(h+2)))的点集，覆盖至少(1-δ)面积，运行时间多项式]"
    },
    {
      "title": "Practical Parallel Block Tree Construction: First Results",
      "authors": "Robert Clausecker, Florian Kurpicz, Etienne Palanga",
      "institution": "Zuse Institute Berlin, University of Münster, TU Dortmund University",
      "link": "https://arxiv.org/pdf/2512.23314",
      "code": "https://github.com/pasta-toolbox/block_tree",
      "tags": [
        "parallel algorithms",
        "block tree",
        "Karp-Rabin fingerprints",
        "shared memory",
        "SIMD",
        "data compression"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fafec35e5cf7fb28c09458b5184ca0f13765b4452149cdff43acdf74dc8c3dd_w640_q70.webp",
      "contributions": "1. Proposed fast and lightweight parallel algorithms for constructing block trees, addressing slow construction and high memory usage. 2. The algorithm matches the speed of the fastest sequential algorithm on one core and achieves up to 4x speedup on 64 cores while using an order of magnitude less memory. 3. Presented a data parallel algorithm for Karp-Rabin fingerprint computation as an independent result.",
      "summary": "The paper addresses the slow and memory-intensive construction of block trees, a compressed text data structure. It proposes new parallel algorithms that achieve comparable single-core speed and up to 4x speedup on 64 cores while significantly reducing memory usage. A key component is a novel data-parallel algorithm for computing Karp-Rabin fingerprints.",
      "mindmap": "graph TB\n        A[Practical Parallel Block Tree Construction] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Block tree construction is slow and memory-heavy]\n        C[主要方法/Method<br>Propose fast, lightweight parallel construction algorithms]\n        D[关键结果/Results<br>4x speedup (64 cores), order of magnitude less memory]"
    },
    {
      "title": "Pseudodeterministic Algorithms for Minimum Cut Problems",
      "authors": "Aryan Agarwala, Nithin Varma",
      "institution": "Max-Planck-Institut für Informatik, Saarland Informatics Campus; University of Cologne",
      "link": "https://arxiv.org/pdf/2512.23468",
      "code": null,
      "tags": [
        "graph algorithms",
        "pseudodeterministic algorithms",
        "global minimum cut",
        "minimum s-t cut",
        "streaming algorithms",
        "cut-query models"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c06a5f7252dc9f1337880cbdb49ac90f3b180fa63fe8a5b0eb2c22a487cedbbd_w640_q70.webp",
      "contributions": "1. Presents efficient pseudodeterministic algorithms for the global minimum cut and minimum s-t cut problems. 2. Achieves an asymptotic running time for global minimum cut that is better than the fastest known sequential deterministic algorithm. 3. Implements the algorithm in multiple computational models (sequential, streaming, PRAM, cut-query) where efficient deterministic algorithms were previously unknown.",
      "summary": "This paper introduces pseudodeterministic algorithms for finding global minimum cuts and minimum s-t cuts in graphs. The proposed method offers replicability by consistently outputting the same answer with high probability, while being faster than the best known deterministic algorithm for global minimum cut. The algorithms are also successfully adapted to work in sequential, streaming, PRAM, and cut-query models.",
      "mindmap": "graph TB\n        A[Pseudodeterministic Algorithms for Minimum Cut Problems] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(确定性算法效率低 / Deterministic algorithms are inefficient)\n        B --> B2(随机算法输出不一致 / Randomized algorithms lack replicability)\n        C --> C1(伪确定性算法 / Pseudodeterministic Algorithms)\n        C --> C2(高概率输出相同解 / Outputs same solution with high probability)\n        D --> D1(渐近更快 / Asymptotically faster)\n        D --> D2(多模型实现 / Implemented in multiple models)"
    },
    {
      "title": "Circle graphs can be recognized in linear time",
      "authors": "Christophe Paul, Ignaz Rutter",
      "institution": "CNRS, Université Montpellier; University of Passau",
      "link": "https://arxiv.org/pdf/2512.23492",
      "code": null,
      "tags": [
        "graph algorithms",
        "circle graphs",
        "PC-tree",
        "split decomposition"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b5cad5c26581c94a94b13e21124e488a0ff9c8db2270ea01c31e6d582e1244c_w640_q70.webp",
      "contributions": "1. Demonstrates that the PC-tree data structure can be used to avoid the union-find data structure for split decomposition in the specific context of circle graphs. 2. Achieves a linear-time computation of the split decomposition for circle graphs, improving upon the previous almost linear-time algorithm. 3. Provides the first linear-time recognition algorithm for the class of circle graphs as a direct consequence of the improved decomposition.",
      "summary": "This paper addresses the problem of recognizing circle graphs efficiently. The authors propose using the PC-tree data structure to compute the split decomposition for circle graphs in linear time, avoiding the slower union-find approach. As a result, they achieve the first linear-time algorithm for circle graph recognition.",
      "mindmap": "graph TB\n        A[Circle graphs can be recognized in linear time] --> B[核心问题/Problem: Best recognition algorithm runs in almost linear time]\n        A --> C[主要方法/Method: Use PC-tree to avoid union-find for split decomposition]\n        A --> D[关键结果/Results: First linear-time recognition algorithm for circle graphs]"
    },
    {
      "title": "Algorithms for Distance Sensitivity Oracles and other Graph Problems on the PRAM",
      "authors": "Vignesh Manoharan, Vijaya Ramachandran",
      "institution": "The University of Texas at Austin",
      "link": "https://arxiv.org/pdf/2512.23604",
      "code": null,
      "tags": [
        "parallel graph algorithms",
        "distance sensitivity oracle",
        "PRAM",
        "work-optimal",
        "replacement paths",
        "minimum weight cycle"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bc864668652bf6d1b4a54204b54580e8385938f0d85ff13636a69d74ba62f1a_w640_q70.webp",
      "contributions": "1. The first PRAM algorithms for constructing Distance Sensitivity Oracles (DSOs) in directed weighted graphs, enabling constant-time queries after preprocessing. 2. The first work-optimal PRAM algorithms for several related graph problems in the sequential O~(mn) complexity class, including Replacement Paths and Minimum Weight Cycle. 3. A range of algorithmic tradeoffs between work and parallel time for the preprocessing step of the DSO construction.",
      "summary": "This paper addresses the problem of computing shortest path distances in a graph after an edge failure, known as the Distance Sensitivity Oracle (DSO) problem. It proposes the first parallel (PRAM) algorithms to construct DSOs for directed weighted graphs, achieving constant query time. The work also provides the first work-optimal parallel solutions for several related graph problems, filling a gap in parallel algorithm research.",
      "mindmap": "graph TB\n        Root[Algorithms for Distance Sensitivity Oracles and other Graph Problems on the PRAM] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>Parallel DSO Construction] --> P1[查询定义/Query: d(x,y,e)]\n        P1 --> P2[背景/Background: Network communication, edge failure]\n        Method[主要方法/Method<br>PRAM Algorithms] --> M1[预处理/Preprocessing]\n        M1 --> M2[工作最优/Work-Optimal]\n        Results[关键结果/Results<br>First in Parallel Setting] --> R1[恒定查询时间/Constant Query Time: O(1)]\n        R1 --> R2[扩展问题/Extended Problems: Replacement Paths, Min Cycle, etc.]"
    },
    {
      "title": "A note on the depth of optimal fanout-bounded prefix circuits",
      "authors": "Igor S. Sergeev",
      "institution": "None explicitly stated; inferred from email: isserg@gmail.com (no clear institutional domain)",
      "link": "https://arxiv.org/pdf/2512.23657",
      "code": null,
      "tags": [
        "circuit complexity",
        "prefix circuits",
        "fanout bounded",
        "circuit depth",
        "zero-deficiency",
        "optimal circuits"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16a80016c1dfb6b2d6696063261404b73445ff9095a6e2412f296eeb7740dc6e_w640_q70.webp",
      "contributions": "1. Derives a precise asymptotic bound for the minimal depth of optimal prefix circuits with bounded fanout, 2. Generalizes the known results for fanout 2 and unbounded fanout to all finite fanout bounds, 3. Characterizes the bound using the root of a specific polynomial equation.",
      "summary": "This paper studies the depth of optimal (zero-deficiency) prefix circuits with a bounded fanout of k. It proves that the minimal depth is asymptotically log_α_k N ± O(1), where α_k is the root of the polynomial 2+x+...+x^\\{k-2\\}-x^k. This generalizes previously known bounds for the special cases of k=2 and k=∞.",
      "mindmap": "graph TB\n        Root[”A note on the depth of optimal fanout-bounded prefix circuits<br>最优扇出有界前缀电路深度研究”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>确定最优前缀电路的最小深度<br>Determine minimal depth of optimal prefix circuits”] --> P1[”约束/Constraint<br>扇出有界 (fanout bounded by k)”]\n        Method[”主要方法/Method<br>理论分析与推导<br>Theoretical analysis and derivation”] --> M1[”关键参数/Key Parameter<br>α_k: 多项式 2+x+...+x^{k-2}-x^k 的正根<br>positive root of polynomial”]\n        Results[”关键结果/Results<br>深度公式: log_α_k N ± O(1)<br>Depth formula”] --> R1[”推广/Generalization<br>统一了 k=2 和 k=∞ 的已知结果<br>Unifies known cases for k=2 and k=∞”]"
    },
    {
      "title": "Coloring Hardness on Low Twin-Width Graphs",
      "authors": "Édouard Bonnet",
      "institution": "Univ Lyon, CNRS, ENS de Lyon, Université Claude Bernard Lyon 1, LIP UMR5668",
      "link": "https://arxiv.org/pdf/2512.23680",
      "code": null,
      "tags": [
        "graph theory",
        "computational complexity",
        "twin-width",
        "graph coloring",
        "NP-hardness",
        "computational complexity",
        "graph classes"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f77a33f460e61bf2610b2bc5a51fb237df8e78440e7279dd902d4d1e4a4dbc60_w640_q70.webp",
      "contributions": "1. Proves that the Min Coloring problem is NP-hard on the class of graphs with twin-width at most 3 (T3). 2. Proves that for every k &gt;= 3, the k-Coloring problem is NP-hard on the class of graphs with twin-width at most 4 (T4). 3. Provides structural observations about the T3 and T4 classes, highlighting their distinct properties and raising open questions about complexity transitions.",
      "summary": "This paper studies the computational complexity of graph coloring problems on graphs with low twin-width. It proves that Min Coloring is NP-hard on graphs of twin-width at most 3, and that k-Coloring is NP-hard on graphs of twin-width at most 4 for all k&gt;=3. These results establish the first hardness for a problem on T3 that is easy on simpler graph classes like trees and cographs.",
      "mindmap": "graph TB\n    A[Coloring Hardness on Low Twin-Width Graphs] --> B[核心问题/Problem: Coloring complexity on bounded twin-width graphs]\n    A --> C[主要方法/Method: NP-hardness proofs via reductions]\n    A --> D[关键结果/Results: Min Coloring hard on T3, k-Coloring hard on T4]"
    },
    {
      "title": "The Minimum Subgraph Complementation Problem",
      "authors": "Juan Gutiérrez, Sagartanu Pal",
      "institution": "Universidad de Ingeniería y Tecnología (UTEC), Bennett University",
      "link": "https://arxiv.org/pdf/2512.23687",
      "code": null,
      "tags": [
        "graph algorithms",
        "subgraph complementation",
        "polynomial-time algorithm",
        "graph transformation",
        "bipartite graphs",
        "chordal graphs"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/94f4a84b23bf20db3f79f2bfc3940394974e0c55bc8afab3dd89a5a7fce1007c_w640_q70.webp",
      "contributions": "1. Presented polynomial-time algorithms for the Minimum Subgraph Complementation problem when transforming graphs between bipartite, co-bipartite, and split graphs. 2. Showed polynomial-time solvability for complementing bipartite regular graphs into chordal graphs and for transforming forests into graphs of fixed degeneracy. 3. Proved that MSC to the class of disconnected graphs and to the class of 2-connected graphs can be solved in polynomial time for arbitrary input graphs.",
      "summary": "This paper studies the Minimum Subgraph Complementation (MSC) problem, which seeks a minimum vertex set whose subgraph complementation transforms a given graph into a target class. The authors propose polynomial-time algorithms for several nontrivial graph classes, including bipartite, co-bipartite, split, and chordal graphs, as well as for connectivity-related classes. They conclude that the optimization variant of this problem is tractable in many settings where the decision version is known to be hard.",
      "mindmap": "graph TB\n        A[The Minimum Subgraph Complementation Problem] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[最小子图补全问题/Minimum Subgraph Complementation]\n        B1 --> B2[寻找最小顶点集S/Finding minimum vertex set S]\n        B2 --> B3[使G⊕S属于目标图类C/So that G⊕S belongs to target class C]\n        C --> C1[多项式时间算法/Polynomial-time algorithms]\n        D --> D1[二分图、共二分图、分裂图/Bipartite, co-bipartite, split graphs]\n        D --> D2[二分正则图到弦图/Bipartite regular to chordal graphs]\n        D --> D3[森林到固定退化图/Forests to fixed degeneracy graphs]\n        D --> D4[连通性类: 不连通图与2-连通图/Connectivity classes: disconnected & 2-connected graphs]"
    },
    {
      "title": "Fast mixing in Ising models with a negative spectral outlier via Gaussian approximation",
      "authors": "Dan Mikulincer, Youngtak Sohn",
      "institution": "University of Washington, Brown University",
      "link": "https://arxiv.org/pdf/2512.22803",
      "code": null,
      "tags": [
        "markov chain mixing",
        "statistical physics",
        "Glauber dynamics",
        "Ising model",
        "spectral outlier",
        "Gaussian approximation",
        "mixing time"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73cfacc7aef08873ccb4938bc528526e0426d28743acf8eb1f6273de4d928ebd_w640_q70.webp",
      "contributions": "1. Developed a new covariance approximation method based on Gaussian approximation and Stein's method for analyzing Ising models with negative spectral outliers. 2. Established a modified logarithmic Sobolev inequality and near-optimal mixing time bounds for Glauber dynamics in regimes where previous spectral width bounds fail. 3. Proved exponential lower bounds on mixing time for low-temperature anti-ferromagnetic Ising models on sparse Erdös-Rényi graphs.",
      "summary": "This paper studies the mixing time of Glauber dynamics for Ising models whose interaction matrix has a single negative spectral outlier. To overcome the limitations of existing methods, the authors develop a new Gaussian approximation-based covariance analysis technique. This leads to near-optimal mixing time bounds in challenging regimes and complementary lower bounds for sparse graphs.",
      "mindmap": "graph TB\n    A[Fast mixing in Ising models with a negative spectral outlier via Gaussian approximation] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[分析具有负谱离群值的伊辛模型的混合时间/Analyze mixing time of Ising models with a negative spectral outlier]\n    C --> C1[基于高斯近化的新协方差近似方法/New covariance approximation method via Gaussian approximation]\n    C --> C2[迭代应用Stein方法于二次倾斜/Iterative application of Stein's method to quadratic tilts]\n    D --> D1[建立MLSI和近乎最优的混合时间边界/Establish MLSI and near-optimal mixing time bounds]\n    D --> D2[证明稀疏Erdös-Rényi图上的指数下界/Prove exponential lower bounds on sparse Erdös-Rényi graphs]"
    },
    {
      "title": "Weighted Fourier Factorizations: Optimal Gaussian Noise for Differentially Private Marginal and Product Queries",
      "authors": "Christian Janos Lebeda, Aleksandar Nikolov, Haohua Tang",
      "institution": "Inria, Université de Montpellier, INSERM, University of Toronto",
      "link": "https://arxiv.org/pdf/2512.21499",
      "code": null,
      "tags": [
        "differential privacy",
        "factorization mechanism",
        "Fourier basis",
        "marginal queries",
        "product queries",
        "Gaussian noise"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/589c857d3c263098ff5b7608b80fed9cb6cf72f1228f01d3ba136496420444dc_w640_q70.webp",
      "contributions": "1. Proposes a simpler, polynomial-time algorithm for releasing weighted marginal queries under differential privacy using Fourier factorization, achieving exact optimality among factorization mechanisms. 2. Extends the algorithm to a more general class of product queries, maintaining exact optimality. 3. Shows the mechanism is almost optimal for extended marginal queries with threshold predicates, achieving optimal noise variance up to lower-order terms.",
      "summary": "This paper proposes a new algorithm for releasing marginal and product queries under differential privacy by adding correlated Gaussian noise. The method works by releasing queries in the Fourier basis with independent, carefully calibrated noise and then reconstructing the answers, which is proven to be exactly optimal among factorization mechanisms and runs in polynomial time. It simplifies and improves upon prior work, extending optimality to more general query classes.",
      "mindmap": "graph TB\n        A[Weighted Fourier Factorizations<br>加权傅里叶分解] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Releasing marginal queries<br>with differential privacy<br>在差分隐私下发布边际查询]\n        C --> C1[Use Fourier basis &<br>independent Gaussian noise<br>使用傅里叶基和独立高斯噪声]\n        C --> C2[Reconstruct via<br>inverse Fourier transform<br>通过逆傅里叶变换重构]\n        D --> D1[Exactly optimal for<br>marginal & product queries<br>对边际和乘积查询精确最优]\n        D --> D2[Polynomial-time algorithm<br>多项式时间算法]\n        D --> D3[Simpler & better than<br>prior work (Xiao et al.)<br>比先前工作更简单更好]"
    },
    {
      "title": "Fully Dynamic Spectral Sparsification for Directed Hypergraphs",
      "authors": "Sebastian Forster, Gramoz Goranci, Ali Momeni",
      "institution": "University of Salzburg, University of Vienna",
      "link": "https://arxiv.org/pdf/2512.21671",
      "code": null,
      "tags": [
        "graph algorithms",
        "spectral sparsification",
        "dynamic algorithm",
        "hypergraph sparsification",
        "spectral sparsifier",
        "batch-dynamic",
        "directed hypergraph"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/937b0b2537d04995b1374f1ced0beb227f0a00fd203eff507e7b317f98955a49_w640_q70.webp",
      "contributions": "1. Presents the first fully dynamic algorithm for maintaining spectral sparsifiers of directed hypergraphs with near-optimal size and efficient update time. 2. Achieves an amortized update time of O(r^2 log^3 m) and a sparsifier size of O(n^2 / ε^2 log^7 m). 3. Extends the algorithm to the parallel batch-dynamic setting, enabling efficient batch processing of hyperedge updates with low depth.",
      "summary": "This paper introduces a new algorithm for dynamically maintaining spectral sparsifiers for directed hypergraphs. The method supports efficient updates (insertions/deletions of hyperedges) and can also process batches of updates in parallel. The main conclusion is that it provides the first spectral-based sparsification algorithm for this dynamic and parallel setting with near-optimal theoretical guarantees.",
      "mindmap": "graph TB\n        A[Fully Dynamic Spectral Sparsification for Directed Hypergraphs] --> B[核心问题/Problem: 有向超图的动态谱稀疏化/Dynamic Spectral Sparsification for Directed Hypergraphs]\n        A --> C[主要方法/Method: 完全动态算法/Fully Dynamic Algorithm]\n        A --> D[关键结果/Results: 近最优大小与更新时间/Near-Optimal Size & Update Time, 扩展到并行批处理/Extension to Parallel Batch-Dynamic]"
    },
    {
      "title": "BLEST: Blazingly Efficient BFS using Tensor Cores",
      "authors": "Deniz Elbek, Kamer Kaya",
      "institution": "Sabanci University",
      "link": "https://arxiv.org/pdf/2512.21967",
      "code": null,
      "tags": [
        "gpu kernels",
        "BFS",
        "Tensor Cores",
        "SpMSpV",
        "Graph Reordering",
        "Kernel Fusion"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da59a50541aea7cf054914628e80911f7ca77a9353af15a6a412588e726ca791_w640_q70.webp",
      "contributions": "1. Introduces Binarised Virtual Slice Sets (BVSS) for warp-level load balancing and eliminating frontier-oblivious work assignment in BFS., 2. Applies two complementary graph reordering strategies (compression-oriented and bandwidth-reducing) to improve memory efficiency and update locality., 3. Develops a batched SpMSpV multiplication pattern using bitwise Tensor Core tiles and combines kernel fusion with a lazy vertex update scheme to reduce synchronization and atomic overheads.",
      "summary": "The paper presents BLEST, a framework that accelerates Breadth-First Search (BFS) on GPUs by efficiently mapping the irregular computation onto dense-math Tensor Cores. The method reformulates the BFS pipeline using a bitmap-oriented structure, specialized load balancing, graph reordering, and kernel fusion. Experiments show that BLEST achieves significant speedups (3.58x to 4.9x) over state-of-the-art GPU-based BFS implementations.",
      "mindmap": "graph TB\n        A[BLEST: Blazingly Efficient BFS using Tensor Cores] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[如何将不规则图BFS映射到密集张量核心/Map irregular BFS to dense Tensor Cores]\n        C --> C1[二值化虚拟切片集/Binarised Virtual Slice Sets (BVSS)]\n        C --> C2[图重排序策略/Graph Reordering Strategies]\n        C --> C3[批处理SpMSpV与核融合/Batched SpMSpV & Kernel Fusion]\n        D --> D1[平均3.58-4.9倍加速/Average 3.58-4.9x Speedup]"
    },
    {
      "title": "A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication",
      "authors": "A. I. Perminov",
      "institution": "Research Center for TAI, Institute for System Programming",
      "link": "https://arxiv.org/pdf/2512.21980",
      "code": null,
      "tags": [
        "matrix multiplication algorithms",
        "additive complexity",
        "rank-23 scheme",
        "ternary coefficients",
        "flip-graph exploration",
        "common subexpression elimination"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7e38e656e47fa623dbd39700639cd1a83f31aa1372de01a6e4de9f8546d9a92_w640_q70.webp",
      "contributions": "1. A new state-of-the-art algorithm for exact 3x3 matrix multiplication that reduces the additive complexity to 58 scalar additions without a change of basis. 2. The use of an automated search methodology combining ternary-restricted flip-graph exploration with greedy intersection reduction for common subexpression elimination. 3. A scheme that uses only coefficients from \\{-1, 0, 1\\}, ensuring portability and efficiency across arbitrary fields.",
      "summary": "This paper presents a new algorithm for multiplying 3x3 matrices over general non-commutative rings. The method uses an automated search combining flip-graph exploration and greedy reduction to discover a rank-23 scheme requiring only 58 scalar additions, improving the previous best of 60 and reducing the total scalar operation count from 83 to 81.",
      "mindmap": "graph TB\n        Root[”A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication<br>论文标题”] --> Problem[”降低3x3矩阵乘法的加法复杂度<br>Reduce Additive Complexity for 3x3 Matrix Multiplication”]\n        Root --> Method[”结合三元限制翻转图探索与贪心交集约简的自动搜索<br>Automated Search with Ternary-Restricted Flip-Graph & Greedy Intersection Reduction”]\n        Root --> Results[”58次加法，秩为23的方案，仅使用{-1,0,1}系数<br>58-Addition, Rank-23 Scheme Using Only {-1,0,1} Coefficients”]"
    },
    {
      "title": "In-Place BWT and Lyndon Array Construction in Constant Space",
      "authors": "Felipe A. Louza, Arnaud Lefebvre",
      "institution": "Universidade Federal de Uberlândia, Universidade Estadual de Campinas, Normandie Univ., UNIROUEN, LITIS",
      "link": "https://arxiv.org/pdf/2512.20869",
      "code": null,
      "tags": [
        "string algorithms",
        "Burrows-Wheeler Transform",
        "Lyndon array",
        "in-place algorithm",
        "constant space",
        "suffix array"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1db3edce1ef31a392927b9b901b098a7db0e0c5e4aabc0e2c3494e811ce57596_w640_q70.webp",
      "contributions": "1. Extends the existing in-place BWT algorithm to also construct the Lyndon array. 2. Achieves the construction using only O(1) extra space (constant space). 3. Provides a conceptually simple method that works for unbounded alphabets, though with quadratic time complexity.",
      "summary": "This paper addresses the problem of constructing the Burrows-Wheeler Transform (BWT) and Lyndon array with minimal memory overhead. It proposes an extension to an existing in-place BWT algorithm that incrementally tracks suffix ranks and then applies a next-smaller-value procedure to derive the Lyndon array. The main conclusion is a novel, conceptually simple algorithm that builds both structures in constant extra space, suitable for unbounded alphabets, albeit with a quadratic running time that limits practical use.",
      "mindmap": "graph LR\n    A[In-Place BWT and Lyndon Array Construction<br>in-Place BWT与Lyndon数组构建] --> B(核心问题/Problem: Construct BWT and Lyndon array with minimal memory.<br>核心问题: 以最小内存构建BWT和Lyndon数组)\n    A --> C(主要方法/Method: Extend in-place BWT algorithm, maintain suffix ranks, use next-smaller-value.<br>主要方法: 扩展原地BWT算法, 维护后缀次序, 使用next-smaller-value过程)\n    A --> D(关键结果/Results: Constant-space O(1) algorithm for unbounded alphabets, quadratic time.<br>关键结果: 适用于无界字母表的常数空间O(1)算法, 二次时间)"
    },
    {
      "title": "Time-Bucketed Balance Records: Bounded-Storage Ephemeral Tokens for Resource-Constrained Systems",
      "authors": "Shaun Scovil, Bhargav Chickmagalur Nanjundappa",
      "institution": "Radius Technology Systems",
      "link": "https://arxiv.org/pdf/2512.20962",
      "code": null,
      "tags": [
        "smart contracts",
        "time-to-live",
        "bounded storage",
        "denial-of-service resistance",
        "data structure",
        "fungible tokens"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b51be242f266cabc91d95594cb0221ca98b4de3e2e562ac90c7dc3881e7c7cd1_w640_q70.webp",
      "contributions": "1. Proposed the time-bucketed balance records data structure that bounds storage to O(k) records per account for fungible tokens with TTL. 2. Proved key properties including bounded storage, guaranteed minimum expiration time, and bounded adversarial cost increase. 3. Provided a practical reference implementation in Solidity with measured gas costs demonstrating efficiency.",
      "summary": "The paper addresses the problem of unbounded storage growth in fungible tokens with time-to-live semantics by proposing a new data structure called time-bucketed balance records. This method discretizes time into buckets to coalesce deposits, guaranteeing tokens never expire early while bounding storage and operation costs. The implementation and analysis show the approach is practical and resistant to denial-of-service attacks.",
      "mindmap": "graph LR\n    A[Time-Bucketed Balance Records] --> B[核心问题/Problem: Unbounded storage & DoS vulnerability for TTL tokens]\n    A --> C[主要方法/Method: Discretize time into k buckets, coalesce deposits]\n    A --> D[关键结果/Results: Bounded O(k) storage, guaranteed TTL, bounded adversarial cost]"
    },
    {
      "title": "Fairness in the k-Server Problem",
      "authors": "Mohammadreza Daneshvaramoli, Helia Karisani, Mohammad Hajiesmaili, Shahin Kamali, Cameron Musco",
      "institution": "University of Massachusetts Amherst, York University",
      "link": "https://arxiv.org/pdf/2512.20960",
      "code": null,
      "tags": [
        "online algorithms",
        "k-server problem",
        "fairness",
        "competitive analysis",
        "Double Coverage Algorithm",
        "oblivious adversary"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d9ce9b45db4e96b8f0700fa006229951ae693c27289e148cbcbc25a0c7ddc81_w640_q70.webp",
      "contributions": "1. Introduced a formal notion of (α, β)-fairness for the k-server problem, aiming to equitably distribute movement costs among servers. 2. Provided deterministic and randomized algorithms that transform optimal/competitive solutions into fair ones with minimal cost increase in offline and online (oblivious adversary) settings. 3. Analyzed the fairness properties of classic algorithms like Double Coverage Algorithm (DCA) and FIFO on specific metrics (line, tree, uniform), showing both positive and negative results.",
      "summary": "This paper formally studies fairness in the k-server problem, introducing a new fairness metric and algorithms to achieve it. The authors show fairness can be added to both offline and online solutions with minimal performance loss against an oblivious adversary, and analyze the fairness of classic deterministic algorithms on specific metric spaces.",
      "mindmap": "graph LR\n    A[Fairness in the k-Server Problem] --> B[核心问题/Problem: 如何在k-server问题中公平分配服务器移动成本?]\n    A --> C[主要方法/Method: 定义(α, β)-公平性; 设计算法将最优/竞争性解转化为公平解]\n    A --> D[关键结果/Results: 离线和在线(非适应性对手)设置下可公平且保持竞争力; DCA在特定度量下公平]"
    },
    {
      "title": "ESCHER: Efficient and Scalable Hypergraph Evolution Representation with Application to Triad Counting",
      "authors": "S. M. Shovan, Arindam Khanda, Sanjukta Bhowmick, Sajal K. Das",
      "institution": "Missouri University of Science and Technology, University of North Texas",
      "link": "https://arxiv.org/pdf/2512.21009",
      "code": null,
      "tags": [
        "parallel computing",
        "hypergraph",
        "GPU",
        "dynamic data structure",
        "triad counting",
        "parallel algorithm"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/335f41f5fe79d6bd117abec93eaf4a43675ba18ef5f201690d075fa6085dfc56_w640_q70.webp",
      "contributions": "1. Proposed ESCHER, a novel GPU-centric parallel data structure for efficient representation and management of large-scale dynamic hypergraphs. 2. Designed a hypergraph triad-count update framework that minimizes redundant computation by leveraging ESCHER's dynamic operation capabilities. 3. Demonstrated significant performance improvements, achieving speedups of up to 104.5x, 473.7x, and 112.5x for different triad counting types on real-world and synthetic datasets.",
      "summary": "The paper addresses the computational challenge of analyzing large, dynamic hypergraphs, which lack efficient specialized data structures. It proposes ESCHER, a GPU-centric data structure for representing hypergraph evolution, and a corresponding triad-counting update framework. The method achieves substantial speedups over state-of-the-art approaches in counting various types of hypergraph triads.",
      "mindmap": "graph LR\n    A[ESCHER: Efficient and Scalable Hypergraph Evolution Representation] --> B[核心问题/Problem: 缺乏分析大规模动态超图的高效数据结构]\n    A --> C[主要方法/Method: 提出GPU并行的ESCHER数据结构与三元组计数更新框架]\n    A --> D[关键结果/Results: 性能显著提升，最高达473.7倍加速]"
    },
    {
      "title": "Approximation Schemes for Planar Graph Connectivity Problems",
      "authors": "Meike Neuwohner, Vera Traub, Rico Zenklusen",
      "institution": "CNRS & DIENS, ENS Paris; ETH Zurich",
      "link": "https://arxiv.org/pdf/2512.21128",
      "code": null,
      "tags": [
        "approximation algorithms",
        "planar graphs",
        "PTAS",
        "connectivity augmentation",
        "k-edge-connected",
        "decomposition technique"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/546114d79c91680ace01463b148cb7f324c4bc0e53497b96d6a60a718d0fbaa2_w640_q70.webp",
      "contributions": "1. A novel decomposition technique for connectivity problems on planar graphs that overcomes the global nature of connectivity constraints. 2. Polynomial-time approximation schemes (PTASs) for finding smallest k-edge-connected and k-vertex-connected spanning subgraphs in planar graphs for any k. 3. A PTAS for planar k-connectivity augmentation for any constant k, complemented by an NP-hardness result showing optimality.",
      "summary": "This paper introduces a new decomposition technique for planar graphs to address classical connectivity problems where standard methods fail due to global connectivity requirements. The technique yields polynomial-time approximation schemes (PTASs) for finding minimum k-edge/vertex-connected spanning subgraphs and for connectivity augmentation in planar graphs. The results are shown to be essentially optimal through a complementary NP-hardness proof.",
      "mindmap": "graph LR\n    A[Approximation Schemes for Planar Graph Connectivity Problems] --> B[核心问题/Problem: APX-hard connectivity problems in general graphs, not well-understood for planar graphs]\n    A --> C[主要方法/Method: Novel decomposition technique for planar graphs to handle global connectivity]\n    A --> D[关键结果/Results: PTAS for k-ECSS/k-VCSS and k-CAP; NP-hardness shows optimality]"
    },
    {
      "title": "An O($nlogn$) approximate knapsack algorithm",
      "authors": "Nick Dawes",
      "institution": "Independent researcher (based on email domain)",
      "link": "https://arxiv.org/pdf/2512.21195",
      "code": null,
      "tags": [
        "algorithms and complexity",
        "knapsack problem",
        "approximation algorithm",
        "dynamic programming",
        "time complexity",
        "error bound"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8d98bd71c58e9c6b0529f95855f5330675f0b0cd5ce83b05eb143a8cc6588bd_w640_q70.webp",
      "contributions": "1. Proposes a modified dynamic programming algorithm for the 0/1 knapsack problem with O(n log n) time and space complexity. 2. Demonstrates that the algorithm has a predictable maximum error, with accuracy improving faster than linearly with the solution size. 3. Shows experimentally that the algorithm is highly efficient, solving large-scale problems (e.g., n=10^6) in seconds on a desktop computer with very low fractional error.",
      "summary": "This paper introduces a modified dynamic programming algorithm for approximately solving large 0/1 knapsack problems. The algorithm achieves O(n log n) time and space complexity while providing a predictable and rapidly decreasing maximum fractional error as the solution size grows. Experimental results show it can solve problems with millions of items in seconds with high accuracy on common hardware.",
      "mindmap": "graph LR\n    A[An O(nlogn) approximate knapsack algorithm] --> B[核心问题/Problem: 大规模0/1背包问题/Large-scale 0/1 Knapsack Problem]\n    A --> C[主要方法/Method: 改进的动态规划算法/Modified Dynamic Programming Algorithm]\n    A --> D[关键结果/Results: O(n log n)复杂度, 可预测误差, 高效实验性能/O(n log n) complexity, predictable error, efficient performance]"
    },
    {
      "title": "An Allele-Centric Pan-Graph-Matrix Representation for Scalable Pangenome Analysis",
      "authors": "Roberto Garrone",
      "institution": "University of Milano-Bicocca",
      "link": "https://arxiv.org/pdf/2512.21320",
      "code": null,
      "tags": [
        "computational genomics",
        "pangenome representation",
        "allele-centric modeling",
        "sparse-dense hybrid encoding",
        "genomic data compression"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47769022333474d616f56daabe2a8f15f3f606c9ff2cc86a0693b945b208b031_w640_q70.webp",
      "contributions": "1. Introduces H1, a novel allele-centric pan-graph-matrix representation that treats alleles as first-class objects and uses adaptive per-allele compression for efficient storage. 2. Introduces H2, a path-centric dual representation derived from the same underlying data, which restores explicit haplotype ordering while maintaining information equivalence. 3. Demonstrates that the representation achieves substantial compression gains, particularly for structural variants, while remaining informationally equivalent to pangenome graphs.",
      "summary": "The paper addresses the need for scalable, population-aware pangenome representations that unify different types of genomic variation. It proposes the H1 pan-graph-matrix, an allele-centric representation with adaptive compression, and its dual H2 representation. The method shows significant compression benefits, especially for structural variants, providing a unified foundation for large-scale pangenome analysis.",
      "mindmap": "graph LR\n    A[An Allele-Centric Pan-Graph-Matrix Representation] --> B(核心问题/Problem: Scalable, unified pangenome representation for population-scale analysis)\n    A --> C(主要方法/Method: H1 allele-centric matrix with adaptive compression & H2 path-centric dual)\n    A --> D(关键结果/Results: Substantial compression gains, equivalent information, unified foundation)"
    },
    {
      "title": "Approximation and parameterized algorithms for covering disjointness-compliable set families",
      "authors": "Zeev Nutov, Anael Vaknin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20180",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61bfd54e6f11feacc870c6f3778976645822489b851ab9a285fb40ba464f71f8_w640_q70.webp",
      "contributions": "",
      "summary": "Approximation and parameterized algorithms for covering disjointness-compliable set families",
      "mindmap": ""
    },
    {
      "title": "On the near-tightness of $χ 2r$: a general $σ$-ary construction and a binary case via LFSRs",
      "authors": "Vinicius T. V. Date, Leandro M. Zatesko",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20598",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cecd66244b3c95d4688ec108350aa614b12ddfe3e5cc1a98c8324648761796db_w640_q70.webp",
      "contributions": "",
      "summary": "On the near-tightness of $χ\\leq 2r$: a general $σ$-ary construction and a binary case via LFSRs",
      "mindmap": ""
    },
    {
      "title": "Certified Lower Bounds and Efficient Estimation of Minimum Accuracy in Quantum Kernel Methods",
      "authors": "Demerson N. Gonçalves, Tharso D. Fernandes, Andrias M. M. Cordeiro, Pedro H. G. Lugao, João T. Dias",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20588",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3376a358771fc30115cfc0c7477ecf025033a36cdd9897d4e03cc089055ce077_w640_q70.webp",
      "contributions": "",
      "summary": "Certified Lower Bounds and Efficient Estimation of Minimum Accuracy in Quantum Kernel Methods",
      "mindmap": ""
    },
    {
      "title": "Fast Rational Search via Stern-Brocot Tree",
      "authors": "Connor Weyers, N. V. Vinodchandran",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18036",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e022cde1df64a2bb5c2d992e85bd17ba696ad0ca4f85f149de33f252c00b206_w640_q70.webp",
      "contributions": "",
      "summary": "Fast Rational Search via Stern-Brocot Tree",
      "mindmap": ""
    },
    {
      "title": "Graph-based Nearest Neighbors with Dynamic Updates via Random Walks",
      "authors": "Nina Mishra, Yonatan Naamad, Tal Wagner, Lichen Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18060",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09ac9385682bd6e4b07b769afe016fb50f71be5c2b6656f027de335845a16aed_w640_q70.webp",
      "contributions": "",
      "summary": "Graph-based Nearest Neighbors with Dynamic Updates via Random Walks",
      "mindmap": ""
    },
    {
      "title": "Constrained Cuts, Flows, and Lattice-Linearity",
      "authors": "Robert Streit, Vijay K. Garg",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18141",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4988963d0df9feb5d6d93950c8695a282967332a6a2d340e258ed92a056f4c86_w640_q70.webp",
      "contributions": "",
      "summary": "Constrained Cuts, Flows, and Lattice-Linearity",
      "mindmap": ""
    },
    {
      "title": "Learning Dependency Models for Subset Repair",
      "authors": "Haoda Li, Jiahui Chen, Yu Sun, Shaoxu Song, Haiwei Zhang, Xiaojie Yuan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18204",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7424f3f1fd625869734b9d930aac4759dd07b6ddc786315e46a14290748c7a25_w640_q70.webp",
      "contributions": "",
      "summary": "Learning Dependency Models for Subset Repair",
      "mindmap": ""
    },
    {
      "title": "Quantization for Vector Search under Streaming Updates",
      "authors": "Ishaq Aden-Ali, Hakan Ferhatosmanoglu, Alexander Greaves-Tunnell, Nina Mishra, Tal Wagner",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18335",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df7364cc42e5bedb16b340dcf01a52bad1a58565d8131a6b77f0557e9720a68f_w640_q70.webp",
      "contributions": "",
      "summary": "Quantization for Vector Search under Streaming Updates",
      "mindmap": ""
    },
    {
      "title": "Constant Approximation of Arboricity in Near-Optimal Sublinear Time",
      "authors": "Jiangqi Dai, Mohsen Ghaffari, Julian Portmann",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18416",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2d1e8ab2bfbbad1a54be27fbc6ec9372aee41b02d5dd2fa72764be35a835910_w640_q70.webp",
      "contributions": "",
      "summary": "Constant Approximation of Arboricity in Near-Optimal Sublinear Time",
      "mindmap": ""
    },
    {
      "title": "Fare Zone Assignment",
      "authors": "Martin Hoefer, Lennart Kauther, Philipp Pabst, Britta Peis, Khai Van Tran",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19493",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/899926283ff7970c7b8c198ac7d46da4bc1977bbe2008fb952cdd9f19e25eee5_w640_q70.webp",
      "contributions": "",
      "summary": "Fare Zone Assignment",
      "mindmap": ""
    },
    {
      "title": "Near-optimal streaming approximation for Max-DICUT in sublinear space using two passes",
      "authors": "Santhoshini Velusamy",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19521",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/419df4dd803b778e372141c734a967ae29cca1a12f34441c79ee9bf22c746dd6_w640_q70.webp",
      "contributions": "",
      "summary": "Near-optimal streaming approximation for Max-DICUT in sublinear space using two passes",
      "mindmap": ""
    },
    {
      "title": "Clustering with Label Consistency",
      "authors": "Diptarka Chakraborty, Hendrik Fichtenberger, Bernhard Haeupler, Silvio Lattanzi, Ashkan Norouzi-Fard, Ola Svensson",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19654",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73fc229d3b4fc51718fe8d59f9abfb97c5b0e2d37b67f22dacfc080be54754bb_w640_q70.webp",
      "contributions": "",
      "summary": "Clustering with Label Consistency",
      "mindmap": ""
    },
    {
      "title": "On Factoring and Power Divisor Problems via Rank-3 Lattices and the Second Vector",
      "authors": "Yiming Gao, Yansong Feng, Honggang Hu, Yanbin Pan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19076",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df17bc838341d48bc4091e7746460983c71413f795cbf8ca3faf3de27c2d6089_w640_q70.webp",
      "contributions": "",
      "summary": "On Factoring and Power Divisor Problems via Rank-3 Lattices and the Second Vector",
      "mindmap": ""
    },
    {
      "title": "Optimizing Text Search: A Novel Pattern Matching Algorithm Based on Ukkonen's Approach",
      "authors": "Xinyu Guan, Shaohua Zhang",
      "institution": "Not specified",
      "link": "https://arxiv.org/pdf/2512.16927",
      "code": null,
      "tags": [
        "pattern matching algorithms",
        "Ukkonen's Algorithm",
        "Suffix Trees",
        "pattern recognition",
        "text-search algorithms"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b065cfa04bf6f3a4b29a1299ffe0b7dd4f84fbabb6368c76abaa339e1a0a77c_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces a novel pattern matching algorithm that combines Ukkonen's Algorithm for constructing Suffix Trees with a new search technique using Python's dynamic link attributes. The optimized algorithm demonstrates linear time and space efficiency, outperforming traditional methods like Naive Search, KMP, and Boyer-Moore, and achieves 100% accuracy in tasks such as genomic sequence pattern recognition.",
      "mindmap": ""
    },
    {
      "title": "Provably Extracting the Features from a General Superposition",
      "authors": "Allen Liu",
      "institution": "UC Berkeley",
      "link": "https://arxiv.org/pdf/2512.15987",
      "code": null,
      "tags": [
        "feature learning",
        "superposition",
        "query algorithm",
        "Fourier space search",
        "overcomplete regime"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper presents an efficient query algorithm that searches in Fourier space to recover hidden feature directions from a general superposition function. It successfully identifies all non-degenerate features and reconstructs the function, working under more general conditions than prior methods.",
      "mindmap": ""
    },
    {
      "title": "Learning Confidence Ellipsoids and Applications to Robust Subspace Recovery",
      "authors": "Chao Gao, Liren Shan, Vaidehi Srinivas, Aravindan Vijayaraghavan",
      "institution": "University of Chicago, Toyota Technological Institute at Chicago, Northwestern University",
      "link": "https://arxiv.org/pdf/2512.16875",
      "code": null,
      "tags": [
        "robust statistics",
        "confidence ellipsoids",
        "minimum volume estimator",
        "primal-dual structure",
        "geometric Brascamp-Lieb inequality",
        "robust subspace recovery"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper presents a polynomial-time algorithm for finding confidence ellipsoids with bounded condition number, achieving a volume approximation factor of O(β^(γd)) while covering at least 1-O(α/γ) probability mass. The method leverages the primal-dual structure of minimum volume enclosing ellipsoids and the geometric Brascamp-Lieb inequality. As a key application, this yields the first polynomial-time algorithm with approximation guarantees for worst-case instances of the robust subspace recovery problem.",
      "mindmap": ""
    },
    {
      "title": "Label-consistent clustering for evolving data",
      "authors": "Ameet Gadekar, Aristides Gionis, Thibault Marette",
      "institution": "CISPA Helmholtz Center for Information Security, KTH Royal Institute of Technology, Digital Futures",
      "link": "https://arxiv.org/pdf/2512.15210",
      "code": null,
      "tags": [
        "clustering algorithms",
        "k-center",
        "approximation algorithms",
        "label-consistent clustering"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes the label-consistent k-center problem, which aims to update an existing clustering solution with new data while limiting changes to a given budget. It introduces two constant-factor approximation algorithms for this problem. The experimental evaluation on real-world datasets demonstrates the effectiveness of the proposed methods.",
      "mindmap": ""
    }
  ]
}